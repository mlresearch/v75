---
title: Online Variance Reduction for Stochastic Optimization
abstract: Modern stochastic optimization methods often rely on uniform sampling which
  is agnostic to the underlying characteristics of the data. This might degrade the
  convergence by  yielding estimates that suffer from a high variance. A possible
  remedy is to employ non-uniform \emphimportance sampling techniques, which take
  the structure of the dataset into account. In this work, we investigate a recently
  proposed setting which poses variance reduction as an online optimization problem
  with bandit feedback. We devise a novel and efficient algorithm for this setting
  that finds a sequence of importance sampling distributions competitive with the
  best fixed distribution in hindsight, the first result of this kind. While we present
  our method for sampling data points, it naturally extends to selecting coordinates
  or even blocks of thereof. Empirical validations underline the benefits of our method
  in several settings.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
id: borsos18a
month: 0
tex_title: Online Variance Reduction for Stochastic Optimization
firstpage: 324
lastpage: 357
page: 324-357
order: 324
cycles: false
bibtex_author: Borsos, Zalan and Krause, Andreas and Levy, Kfir Y.
author:
- given: Zalan
  family: Borsos
- given: Andreas
  family: Krause
- given: Kfir Y.
  family: Levy
date: 2018-07-03
address: 
publisher: PMLR
container-title: Proceedings of the 31st  Conference On Learning Theory
volume: '75'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
pdf: http://proceedings.mlr.press/v75/borsos18a/borsos18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
