---
title: Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural
  Networks with Quadratic Activations
abstract: We show that the gradient descent algorithm provides an implicit regularization
  effect in the learning of over-parameterized matrix factorization models and one-hidden-layer
  neural networks with quadratic activations. Concretely, we show that given $\tilde{O}(dr^{2})$
  random linear measurements of a rank $r$ positive semidefinite matrix $X^{⋆}$, we
  can recover $X^{⋆}$ by parameterizing it by $UU^⊤$ with $U∈\mathbb R^{d\times d}$
  and minimizing the squared loss, even if $r ≪d$. We prove that starting from a small
  initialization, gradient descent recovers $X^{⋆}$ in $\tilde{O}(\sqrt{r})$ iterations
  approximately. The results solve the conjecture of Gunasekar et al.’17 under the
  restricted isometry property.  The technique can be applied to analyzing neural
  networks with one-hidden-layer quadratic activations with some technical modifications.
section: Best Paper Awards
layout: inproceedings
series: Proceedings of Machine Learning Research
id: li18a
month: 0
tex_title: Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural
  Networks with Quadratic Activations
firstpage: 2
lastpage: 47
page: 2-47
order: 2
cycles: false
bibtex_author: Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang
author:
- given: Yuanzhi
  family: Li
- given: Tengyu
  family: Ma
- given: Hongyang
  family: Zhang
date: 2018-07-03
address: 
publisher: PMLR
container-title: Proceedings of the 31st  Conference On Learning Theory
volume: '75'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
pdf: http://proceedings.mlr.press/v75/li18a/li18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
