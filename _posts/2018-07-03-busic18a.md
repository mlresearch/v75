---
title: Action-Constrained {Markov Decision Processes} With {Kullback-Leibler} Cost
abstract: This paper concerns computation of optimal policies in which the one-step
  reward function contains a cost term that models Kullback-Leibler divergence with
  respect to nominal dynamics. This technique was introduced by Todorov in 2007, where
  it was shown under general conditions that the solution to the average-reward optimality
  equations reduce to a simple eigenvector problem. Since then many authors have sought
  to apply this technique to control problems and models of bounded rationality in
  economics.  A crucial assumption is that the input process is essentially unconstrained.
  For example, if the nominal dynamics include randomness from nature (e.g., the impact
  of wind on a moving vehicle), then the optimal control solution does not respect
  the exogenous nature of this disturbance.  This paper introduces a technique to
  solve a more general class of action-constrained MDPs. The main idea is to solve
  an entire parameterized family of MDPs, in which the parameter is a scalar weighting
  the one-step  reward function. The approach is new and practical even in the original
  unconstrained formulation.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
id: busic18a
month: 0
tex_title: Action-Constrained {Markov Decision Processes} With {Kullback-Leibler}
  Cost
firstpage: 1431
lastpage: 1444
page: 1431-1444
order: 1431
cycles: false
bibtex_author: Bu\v{s}i\'{c}, Ana and Meyn, Sean
author:
- given: Ana
  family: Bušić
- given: Sean
  family: Meyn
date: 2018-07-03
address: 
publisher: PMLR
container-title: Proceedings of the 31st  Conference On Learning Theory
volume: '75'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
pdf: http://proceedings.mlr.press/v75/busic18a/busic18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
