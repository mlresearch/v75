---
title: Learning Mixtures of Linear Regressions with Nearly Optimal Complexity
abstract: Mixtures of Linear Regressions (MLR) is an important mixture model with
  many applications. In this model, each observation is generated from one of the
  several unknown linear regression components, where the identity of the generated
  component is also unknown. Previous works either assume strong assumptions on the
  data distribution or have high complexity. This paper proposes a fixed parameter
  tractable algorithm for the problem under general conditions, which achieves global
  convergence and the sample complexity scales nearly linearly in the dimension. In
  particular, different from previous works that require the data to be from the standard
  Gaussian, the algorithm allows the data from Gaussians with different covariances.
  When the conditional number of the covariances and the number of components are
  fixed, the algorithm has nearly optimal sample complexity $N = \tilde{O}(d)$ as
  well as nearly optimal computational complexity $\tilde{O}(Nd)$, where $d$ is the
  dimension of the data space. To the best of our knowledge, this approach provides
  the first such recovery guarantee for this general setting.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
id: li18b
month: 0
tex_title: Learning Mixtures of Linear Regressions with Nearly Optimal Complexity
firstpage: 1125
lastpage: 1144
page: 1125-1144
order: 1125
cycles: false
bibtex_author: Li, Yuanzhi and Liang, Yingyu
author:
- given: Yuanzhi
  family: Li
- given: Yingyu
  family: Liang
date: 2018-06-29
address: 
publisher: PMLR
container-title: Proceedings of the 31st  Conference On Learning Theory
volume: '75'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 6
  - 29
pdf: http://proceedings.mlr.press/v75/li18b/li18b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
