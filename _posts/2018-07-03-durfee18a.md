---
title: "$\\ell_1$ Regression using Lewis Weights Preconditioning and Stochastic Gradient
  Descent"
abstract: We present preconditioned stochastic gradient descent (SGD) algorithms for
  the $\ell_1$ minimization problem $\min_{\boldsymbol{\mathit{x}}}\|\boldsymbol{\mathit{A}}
  \boldsymbol{\mathit{x}} - \boldsymbol{\mathit{b}}\|_1$ in the overdetermined case,
  where there are far more constraints than variables. Specifically, we have $\boldsymbol{\mathit{A}}
  ∈\mathbb{R}^{n \times d}$ for $n ≫d$. Commonly known as the Least Absolute Deviations
  problem, $\ell_1$ regression can be used to solve many important combinatorial problems,
  such as minimum cut and shortest path. SGD-based algorithms are appealing for their
  simplicity and practical efficiency. Our primary insight is that careful preprocessing
  can yield preconditioned matrices $\tilde{\boldsymbol{\mathit{A}}}$ with strong
  properties (besides good condition number and low-dimension) that allow for faster
  convergence of gradient descent. In particular, we precondition using Lewis weights
  to obtain an isotropic matrix with fewer rows and strong upper bounds on all row
  norms. We leverage these conditions to find a good initialization, which we use
  along with recent smoothing reductions and accelerated stochastic gradient descent
  algorithms to achieve $ε$ relative error in $\widetilde{O}(nnz(\boldsymbol{\mathit{A}})
  + d^{2.5} ε^{-2})$ time with high probability, where $nnz(\boldsymbol{\mathit{A}})$
  is the number of non-zeros in $\boldsymbol{\mathit{A}}$. This improves over the
  previous best result using gradient descent for $\ell_1$ regression. We also match
  the best known running times for interior point methods in several settings. Finally,
  we also show that if our original matrix $\boldsymbol{\mathit{A}}$ is approximately
  isotropic and the row norms are approximately equal, we can give an algorithm that
  avoids using fast matrix multiplication and obtains a running time of $\widetilde{O}(nnz(\boldsymbol{\mathit{A}})
  + s d^{1.5}ε^{-2} + d^2ε^{-2})$, where $s$ is the maximum number of non-zeros in
  a row of $\boldsymbol{\mathit{A}}$. In this setting, we beat the best interior point
  methods for certain parameter regimes.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
id: durfee18a
month: 0
tex_title: "$\\ell_1$ Regression using Lewis Weights Preconditioning and Stochastic
  Gradient Descent"
firstpage: 1626
lastpage: 1656
page: 1626-1656
order: 1626
cycles: false
bibtex_author: Durfee, David and Lai, Kevin A. and Sawlani, Saurabh
author:
- given: David
  family: Durfee
- given: Kevin A.
  family: Lai
- given: Saurabh
  family: Sawlani
date: 2018-07-03
address: 
publisher: PMLR
container-title: Proceedings of the 31st  Conference On Learning Theory
volume: '75'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
pdf: http://proceedings.mlr.press/v75/durfee18a/durfee18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
