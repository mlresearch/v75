---
title: Fast and Sample Near-Optimal Algorithms for Learning Multidimensional Histograms
abstract: 'We study the problem of robustly learning multi-dimensional histograms.  A
  $d$-dimensional function $h: D \to \R$ is called a $k$-histogram if there exists
  a partition of the  domain $D ⊆\R^d$ into $k$ axis-aligned rectangles such that
  $h$ is constant within each such rectangle. Let $f: D \to \R$ be a $d$-dimensional
  probability density function  and suppose that $f$ is $\mathrm{OPT}$-close, in $L_1$-distance,  to
  an unknown $k$-histogram (with unknown partition). Our goal is to output a hypothesis
  that is $O(\mathrm{OPT}) + ε$ close to $f$, in $L_1$-distance. We give an algorithm
  for this learning  problem that uses  $n = \tilde{O}_d(k/\eps^2)$ samples and runs
  in time $\tilde{O}_d(n)$. For any fixed dimension, our algorithm has optimal sample
  complexity, up to logarithmic factors, and runs in near-linear time. Prior to our
  work, the time complexity of the $d=1$ case was well-understood,  but significant
  gaps in our understanding remained even for $d=2$.'
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
id: diakonikolas18a
month: 0
tex_title: Fast and Sample Near-Optimal Algorithms for Learning Multidimensional Histograms
firstpage: 819
lastpage: 842
page: 819-842
order: 819
cycles: false
bibtex_author: Diakonikolas, Ilias and Li, Jerry and Schmidt, Ludwig
author:
- given: Ilias
  family: Diakonikolas
- given: Jerry
  family: Li
- given: Ludwig
  family: Schmidt
date: 2018-07-03
address: 
publisher: PMLR
container-title: Proceedings of the 31st  Conference On Learning Theory
volume: '75'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
pdf: http://proceedings.mlr.press/v75/diakonikolas18a/diakonikolas18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
