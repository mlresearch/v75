---
title: An Estimate Sequence for Geodesically Convex Optimization
abstract: We propose a Riemannian version of Nesterovâ€™s Accelerated Gradient algorithm
  (\textsc{Ragd}), and show that for \emph{geodesically} smooth and strongly convex
  problems, within a neighborhood of the minimizer whose radius depends on the condition
  number as well as the sectional curvature of the manifold, \textsc{Ragd} converges
  to the minimizer with acceleration. Unlike the algorithm in (Liu et al., 2017) that
  requires the exact solution to a nonlinear equation which in turn may be intractable,
  our algorithm is constructive and computationally tractable. Our proof exploits
  a new estimate sequence and a novel bound on the nonlinear metric distortion, both
  ideas may be of independent interest.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
id: zhang18a
month: 0
tex_title: An Estimate Sequence for Geodesically Convex Optimization
firstpage: 1703
lastpage: 1723
page: 1703-1723
order: 1703
cycles: false
bibtex_author: Zhang, Hongyi and Sra, Suvrit
author:
- given: Hongyi
  family: Zhang
- given: Suvrit
  family: Sra
date: 2018-07-03
address: 
publisher: PMLR
container-title: Proceedings of the 31st  Conference On Learning Theory
volume: '75'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
pdf: http://proceedings.mlr.press/v75/zhang18a/zhang18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
