---
title: Minimax Bounds on Stochastic Batched Convex Optimization
abstract: " We study the stochastic batched convex optimization problem, in which
  we use many \\emphparallel observations to optimize a convex function given limited
  rounds of interaction.  In each of $M$ rounds, an algorithm may query for information
  at $n$ points, and after issuing all $n$ queries, it receives unbiased noisy function
  and/or (sub)gradient evaluations at the $n$ points.  After $M$ such rounds, the
  algorithm must output an estimator.  We provide lower and upper bounds on the performance
  of such batched convex optimization algorithms in zeroth and first-order settings
  for Lipschitz convex and smooth strongly convex functions.  Our rates of convergence
  (nearly) achieve the fully sequential rate once $M = O(d \\log \\log n)$, where
  $d$ is the problem dimension, but the rates may exponentially degrade as the dimension
  $d$ increases, in distinction from fully sequential settings."
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
id: duchi18a
month: 0
tex_title: Minimax Bounds on Stochastic Batched Convex Optimization
firstpage: 3065
lastpage: 3162
page: 3065-3162
order: 3065
cycles: false
bibtex_author: Duchi, John and Ruan, Feng and Yun, Chulhee
author:
- given: John
  family: Duchi
- given: Feng
  family: Ruan
- given: Chulhee
  family: Yun
date: 2018-07-03
address: 
publisher: PMLR
container-title: Proceedings of the 31st  Conference On Learning Theory
volume: '75'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
pdf: http://proceedings.mlr.press/v75/duchi18a/duchi18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
