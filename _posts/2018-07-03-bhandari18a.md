---
title: A Finite Time Analysis of Temporal Difference Learning With Linear Function
  Approximation
abstract: Temporal difference learning (TD) is a simple iterative algorithm used to
  estimate the value function corresponding to a given policy in a Markov decision
  process. Although TD is one of the most widely used algorithms in reinforcement
  learning, its theoretical analysis has proved challenging and few guarantees on
  its statistical efficiency are available. In this work, we provide a \emphsimple
  and explicit finite time analysis of temporal difference learning with linear function
  approximation. Except for a few key insights, our analysis mirrors standard techniques
  for  analyzing stochastic gradient descent algorithms, and therefore inherits the
  simplicity and elegance of that literature. A final section of the paper shows that
  all of our main results extend to the study of a variant of Q-learning applied to
  optimal stopping problems.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
id: bhandari18a
month: 0
tex_title: A Finite Time Analysis of Temporal Difference Learning With Linear Function
  Approximation
firstpage: 1691
lastpage: 1692
page: 1691-1692
order: 1691
cycles: false
bibtex_author: Bhandari, Jalaj and Russo, Daniel and Singal, Raghav
author:
- given: Jalaj
  family: Bhandari
- given: Daniel
  family: Russo
- given: Raghav
  family: Singal
date: 2018-07-03
address: 
publisher: PMLR
container-title: Proceedings of the 31st  Conference On Learning Theory
volume: '75'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
pdf: http://proceedings.mlr.press/v75/bhandari18a/bhandari18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
