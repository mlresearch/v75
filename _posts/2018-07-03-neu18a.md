---
title: Iterate Averaging as Regularization for Stochastic Gradient Descent
abstract: We propose and analyze a variant of the classic  Polyakâ€“Ruppert averaging
  scheme, broadly used in stochastic gradient methods.  Rather than a uniform average
  of the iterates, we consider a weighted average, with weights decaying in a geometric
  fashion. In the context of linear least-squares regression, we show that this averaging
  scheme has the same regularizing effect, and indeed is asymptotically equivalent,
  to ridge regression. In particular, we derive finite-sample bounds for the proposed
  approach that match the best known results for regularized stochastic gradient methods.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
id: neu18a
month: 0
tex_title: Iterate Averaging as Regularization for Stochastic Gradient Descent
firstpage: 3222
lastpage: 3242
page: 3222-3242
order: 3222
cycles: false
bibtex_author: Neu, Gergely and Rosasco, Lorenzo
author:
- given: Gergely
  family: Neu
- given: Lorenzo
  family: Rosasco
date: 2018-07-03
address: 
publisher: PMLR
container-title: Proceedings of the 31st  Conference On Learning Theory
volume: '75'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
pdf: http://proceedings.mlr.press/v75/neu18a/neu18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
