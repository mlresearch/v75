---
title: Lower Bounds for Higher-Order Convex Optimization
abstract: State-of-the-art methods in mathematical optimization employ higher-order
  derivative information. We explore the limitations of higher-order optimization
  and prove that even for convex optimization, a polynomial dependence on the approximation
  guarantee and higher-order smoothness parameters is necessary. This refutes the
  hope that higher-order smoothness and higher-order derivatives can lead to dimension
  free polynomial time algorithms for convex optimization. As a special case, we show
  Nesterovâ€™s accelerated cubic regularization method and higher-order methods to be
  nearly tight.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
id: agarwal18a
month: 0
tex_title: Lower Bounds for Higher-Order Convex Optimization
firstpage: 774
lastpage: 792
page: 774-792
order: 774
cycles: false
bibtex_author: Agarwal, Naman and Hazan, Elad
author:
- given: Naman
  family: Agarwal
- given: Elad
  family: Hazan
date: 2018-06-29
address: 
publisher: PMLR
container-title: Proceedings of the 31st  Conference On Learning Theory
volume: '75'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 6
  - 29
pdf: http://proceedings.mlr.press/v75/agarwal18a/agarwal18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
