---
title: Averaging {S}tochastic {G}radient {D}escent on {R}iemannian {M}anifolds
abstract: We consider the minimization of a function defined on a Riemannian manifold
  $\mathcal{M}$ accessible only through unbiased estimates of its gradients. We develop
  a geometric framework to transform a sequence of slowly converging iterates generated
  from stochastic gradient descent (SGD) on $\mathcal{M}$ to an averaged iterate sequence
  with a robust and fast $O(1/n)$ convergence rate. We then present an application
  of our framework to geodesically-strongly-convex (and possibly Euclidean non-convex)
  problems.  Finally, we demonstrate how these ideas apply to the case of streaming
  $k$-PCA, where we show how to accelerate the slow rate of the randomized power method  (without
  requiring knowledge of the eigengap) into a robust algorithm achieving the optimal
  rate of convergence.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
id: tripuraneni18a
month: 0
tex_title: Averaging {S}tochastic {G}radient {D}escent on {R}iemannian {M}anifolds
firstpage: 650
lastpage: 687
page: 650-687
order: 650
cycles: false
bibtex_author: Tripuraneni, Nilesh and Flammarion, Nicolas and Bach, Francis and Jordan,
  Michael
author:
- given: Nilesh
  family: Tripuraneni
- given: Nicolas
  family: Flammarion
- given: Francis
  family: Bach
- given: Michael
  family: Jordan
date: 2018-06-29
address: 
publisher: PMLR
container-title: Proceedings of the 31st  Conference On Learning Theory
volume: '75'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 6
  - 29
pdf: http://proceedings.mlr.press/v75/tripuraneni18a/tripuraneni18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
