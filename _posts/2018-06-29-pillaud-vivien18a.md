---
title: Exponential Convergence of Testing Error for Stochastic Gradient Methods
abstract: We consider binary classification problems with positive definite kernels
  and square loss, and study the convergence rates of stochastic gradient methods.
  We show that while the excess testing \emph{loss} (squared loss) converges slowly
  to zero as the number of observations (and thus iterations) goes to infinity, the
  testing \emph{error} (classification error) converges exponentially fast if low-noise
  conditions are assumed. To achieve these rates of convergence we show sharper high-probability
  bounds with respect to the number of observations for stochastic gradient descent.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
id: pillaud-vivien18a
month: 0
tex_title: Exponential Convergence of Testing Error for Stochastic Gradient Methods
firstpage: 250
lastpage: 296
page: 250-296
order: 250
cycles: false
bibtex_author: Pillaud-Vivien, Loucas and Rudi, Alessandro and Bach, Francis
author:
- given: Loucas
  family: Pillaud-Vivien
- given: Alessandro
  family: Rudi
- given: Francis
  family: Bach
date: 2018-06-29
address: 
publisher: PMLR
container-title: Proceedings of the 31st  Conference On Learning Theory
volume: '75'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 6
  - 29
pdf: http://proceedings.mlr.press/v75/pillaud-vivien18a/pillaud-vivien18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
