\section{Proofs from Section 4}\label{sec:sec4proofs}

In this section we provide the omitted proofs from \Cref{sec:sparsitypreserve}. 
\subsection{Proof of Lemma~\ref{lem:uniformSampling}}
\label{subsec:unifSample}

In this section we reduce the number of rows in $\AA$ by uniform sampling while still preserving certain guarantees. Note that we will ultimately sample from $[\AA\;\bb]$, but for simplicity in notation, we will just use $\AA$ here.

%\uniformSampling*

To prove Lemma~\ref{lem:uniformSampling}, we need the following lemma, which states the key fact that the conditions on $\AA$ ensure approximately uniform Lewis weights.

\begin{lemma}[Almost-uniform leverage scores imply almost-uniform Lewis weights]\label{lem:almostUniform}
	Consider a matrix $\AA \in \R^{n \times d}$ such that $\AA^T\AA \approx_{O(1)} \II$ and $\norme{\AA_{i,:}}^2 \approx_{O(1)} d/n$.
	Let $\lw$ denote the $\ell_1$ Lewis weights for $\AA$.
	Then for each row $i$, we have $\lw_i \approx_{O(1)} d/n$.
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem:uniformSampling}]
	Note that by Lemma~\ref{lem:almostUniform} we have 
	\[
	\pp_i = \frac{N}{n} \approx_{O(1)} \frac{d \cdot O(\epsilon^{-2}\log{n})}{n}  \approx_{O(1)} \lw_i \cdot O(\epsilon^{-2}\log{n}).
	\]
	Thus, if we use $\pp_i = N/n$ for each $i$ in \Cref{thm:lewisWeights}, we get the first property while avoiding the cost of computing $\pp_i$'s stated in \Cref{thm:lewisWeights}.
	
The second property follows from Lemma~\ref{lem:l2Sampling} in \Cref{sec:lewis}. Specifically, we have 
\[
\tilde{\AA}^T\tilde{\AA} \approx_{O(1)} \frac{1}{O(\epsilon^{-2}\log{n})}\AA^T\LW^{-1}\AA,
\]
which then implies that
\[
\tilde{\AA}^T\tilde{\AA} \approx_{O(1)} \frac{n}{d \cdot O(\epsilon^{-2}\log{n})}\AA^T\AA \approx_{O(1)} \frac{n}{N} \II.
\]

Let $\tau$ denote the leverage scores for $\AA$.
Now, for the third property, it follows from the definition of leverage scores and the second property that
\[
\tau_i(\tilde{\AA}) = \norme{\left(\tilde{\AA}^T\tilde{\AA}\right)^{-1/2}\tilde{\AA}_{i,:}^T}^2 \approx_{O(1)} \norme{\sqrt{N/n}\tilde{\AA}_{i,:}^T}^2.
\]
Furthermore, Lemma~\ref{lem:lewisGivesEqualLevScores} in \Cref{sec:lewis} shows that $\tau_i(\tilde{\AA}) \approx_{O(1)} d/N$.
Factoring this into the equation gives us
\[ 
\norme{\tilde{\AA}_{i,:}^T}^2 \approx_{O(1)} dn/N^2. 
%\qedhere
\]
\end{proof}

Now, to prove Lemma~\ref{lem:almostUniform}, we need the following definition and lemma.

\begin{definition}[Definition 5.1 of $\alpha$-almost Lewis weights for $\ell_1$ from \cite{cohenpeng}]
	For a matrix $\AA$, an assignment of weights $\ww$ is $\alpha$-\textit{almost Lewis} if
	\begin{align*}
	\AA_{i,:} (\AA^T \WW^{-1}\AA)^{-1} \AA_{:,i}^T \approx_\alpha \ww_i^2,
	\end{align*}
	where $\WW$ is the diagonal matrix form of $\ww$.
\end{definition}

\begin{lemma}[Definition 5.2 and Lemma 5.3 from \cite{cohenpeng}]\label{lem:LewisStability}
	Any set of $\alpha$-almost Lewis weights satisfy
	\begin{align*}
	\lw_i \approx_{\alpha} \ww_i.
	\end{align*}
\end{lemma}


\begin{proof}[Proof of Lemma~\ref{lem:almostUniform}]
We know that $\AA^T \AA \approx_{O(1)} \II$ and for each row $i$, $\norme{\AA_{i,:}}^2 \approx_{O(1)} d/n$. Then,
\begin{align*}
& \tau_i(\AA) = \AA_{i,:} (\AA^T \AA)^{-1} \AA_{i,:}^T \approx_{O(1)} \AA_{i,:} \AA_{i,:}^T\\
\implies & \tau_i(\AA) \approx_{O(1)} d/n.
\end{align*}

That is, all of the leverage scores are approximately equal. Then we can show that $\ww = (d/n)\ones$, where $\ones$ is the all ones vector. Then,
\begin{align*}
\AA_{i,:} (\AA^T \WW^{-1}\AA)^{-1} \AA_{:,i}^T &= (d/n)\AA_{i,:} (\AA^T  \AA)^{-1} \AA_{:,i}^T \approx_{O(1)} d^2/n^2 = \ww_i^2.
\end{align*}
Thus, $\ww$ is $O(1)$-almost Lewis. The result then follows by Lemma~\ref{lem:LewisStability}.
\end{proof}

\input{initialization_proof}

\subsection{Proof of \Cref{thm:noFastMatrixMult}}\label{subsec:achievingSparsity}
%In Sections~\ref{subsec:unifSample} and ~\ref{subsec:l2Minimizer} we reduced the number of rows in $\AA$ to $O(d\epsilon^{-2}\log{n})$ and also found an initialization that was close to the optimum without using fast matrix multiplication and also preserving the row sparsity by uniform sampling and avoiding rotating the matrix. Accordingly, we will now be able to plug in the smoothing and accelerated methods from \Cref{subsec:katyusha} to still achieve a runtime ....

%In this section, we show how to use the matrix achieved in \Cref{subsec:unifSample} and the initialization from \Cref{subsec:l2Minimizer} to achieve fast row-sparsity-preserving $\ell_1$ minimization. We will prove the following main theorem:

\noFastMatrixMult*

\begin{proof}
	We first prove correctness.
	We apply Lemma~\ref{lem:uniformSampling} to $[\AA\;\bb]$ and rescale $[\tilde{\AA}\;\tilde{\bb}]$ by $\sqrt{N/n}$.
	Note that rescaling does not change the relative error of our output $\tilde{x}$.
	From this rescaling, we have $\tilde{[\AA\;\bb]}^T\tilde{[\AA\;\bb]} \approx_{O(1)} \II$ and thus $\tilde{\AA}^T\tilde{\AA} \approx_{O(1)} \II$.
	This implies that $\tau_i([\tilde{\AA}\;\tilde{\bb}]) \approx_{O(1)} \|[\tilde{\AA}\;\tilde{\bb}]_{i,:}\|_2^2$ and $\tau_i(\tilde{\AA}) \approx_{O(1)} \|\tilde{\AA}_{i,:}\|_2^2$.
	By \Cref{fact:boundLev}, we have $\tau_i(\tilde{\AA}) \leq \tau_i([\tilde{\AA}\;\tilde{\bb}])$, so we have $\|\tilde{\AA}_{i,:}\|_2^2 \leq O(d/N)$ for all rows $i$.
	As a result, we can find $\tilde{\xx}_0$ according to Lemma~\ref{lem:initMain}.
	The rest of the correctness follows exactly as in the proof of \Cref{thm:applyKatyusha}.
	
	We now examine the running time and note that uniform sampling will take $O(nnz(\AA) + d^2\epsilon^{-2}\log{n})$ time to produce $[\tilde{\AA}\;\tilde{\bb}]$.
	By Lemma~\ref{lem:initMain}, we can then find $\tilde{\xx}_0$ in time $O(d^2\epsilon^{-2}\log{n})$ because $\tilde{\AA}$ is a $d\epsilon^{-2}\log{n} \times d$ matrix, so $t_{\tilde{\AA}^T\tilde{\AA}} = O(d^2\epsilon^{-2}\log{n})$.
	Finally from the analysis of \Cref{thm:applyKatyusha} we know that accelerated stochastic gradient descent requires $O(d^{2.5}\log^{1/2}{n}\cdot \epsilon^{-2})$ time.
	However, we note that the extra factor of $d$ came from \Cref{thm:katyushaRuntime} where we substituted $d$ for the time per iteration of stochastic gradient descent.
	This value can actually be upper bounded by the maximum number of entries in any row of $\tilde{\AA}$, which because of our uniform sampling is upper bounded by the maximum number of entries in any row of $\AA$.
	Adding a runtime overhead of $\log{n}$ for computing an approximation of the optimal objective, as in \Cref{subsec:binarySearch}, gives the desired runtime.
\end{proof}


