\subsection{Proof of Lemma~\ref{lem:exactInit} and Lemma~\ref{lem:cg}}
\label{subsec:initialization_proofs}

To prove Lemma~\ref{lem:exactInit}, we use the following lemma:
\begin{lemma}\label{lem:subordinatenorm}
	Let $\vv \in \R^n$ be a vector with $\norm{\vv}_1 = 1$.
	Then, for a matrix $\AA \in \R^{n \times d}$,
	\begin{align*}
	\norme{\AA^T \vv} \leq \max_i \norme{\AA_{i,:}}.
	\end{align*}
\end{lemma}



\begin{proof}
	\begin{align*}
	\norme{\AA^T \vv} & = \norme{\sum_i \AA_{i,:} \vv_i} \leq \max_i \norme{\AA_{i,:}}.
	\end{align*}
	where the inequality follows by the convexity of $\norm{\cdot}_2$ and since $\sum_i \abs{\vv_i} = 1$,
\end{proof}




\begin{proof}[Proof of Lemma~\ref{lem:exactInit}]
	By our assumptions on $\AA$, we have $\AA^T\AA + \BB= \II$ for some symmetric $\BB$ where $\norme{\BB}\le O(1)$. Since $\xx_0 = \arg\min_{\xx} \norm{\AA\xx - \bb}_2$, we have $\xx_0 = \AA^\dagger \bb = (\AA^T \AA)^{-1} \AA^T \bb$.
	\begin{align*}
	\norme{\xx_0-\xx^*} &= \norme{(\AA^T \AA)^{-1} \AA^T \bb - \xx^*}\\
	&= \norme{(\AA^T \AA)^{-1} \AA^T \bb - (\AA^T \AA)^{-1} \AA^T \AA \xx^*}\\
	&= \norme{(\AA^T \AA)^{-1} \AA^T ( \bb - \AA\xx^*)}.
	\end{align*}
	Let $\vv = (\AA \xx^* - \bb)/\norm{\AA \xx^* - \bb}_1$.
	\begin{align*}
	\norme{\xx_0 -\xx^*} &= \norme{(\AA^T \AA)^{-1} \AA^T \vv}\norm{\bb - \AA\xx^*}_1\\
	&= \norme{(\AA^T \AA + \BB)(\AA^T \AA)^{-1} \AA^T \vv}\norm{\bb - \AA\xx^*}_1\\
	&= \norme{(\II + \BB (\AA^T \AA)^{-1}) \AA^T \vv}\norm{\bb - \AA\xx^*}_1\\
	&\le \norme{\II + \BB (\AA^T \AA)^{-1}}\norme{\AA^T \vv}\norm{\bb - \AA\xx^*}_1.
	\end{align*}
	Now note:
	\begin{align*}
	\norme{\II + \BB (\AA^T \AA)^{-1}}&\le \norme{\II} + \norme{\BB}\norme{(\AA^T \AA)^{-1}}\\
	&\le O(1).
	\end{align*}
	Also, by Lemma~\ref{lem:subordinatenorm} and the assumptions on $\AA$,
	\begin{align*}
	\norme{\AA^T \vv} \leq O(\sqrt{d/n}).
	\end{align*}
	Thus, we have:
	\begin{align*}
	& \norme{\xx_0 - \xx^*} \le O(\sqrt{d/n}) \norm{\bb - \AA\xx^*}_1. 
%\qedhere
	\end{align*}
\end{proof}

%Although Lemma~\ref{lem:exactInit} shows that $\xx_0 = (\AA^T \AA)^{-1} \AA^T \bb$ would be a good initialization,
%computing $\xx_0$ exactly could take $O(nd^{\omega - 1})$ time, which is prohibitively expensive.
%Instead, we show that approximating $\xx_0$ suffices to get the same performance bounds
%while taking lesser time to compute.
%Lemma~\ref{lem:approxInit} formalizes this statement.

%\begin{theorem}[Theorem 2.17 from \cite{Woodruff14}] \label{thm:woodruff}
%	For any $\AA \in \R^{n\times d}$ and $\bb \in \R^n$,
%	there is a routine $\textsc{L2Refinement}$
%	that finds an $\tilde{\xx}_0$ such that
%	$\norme{\AA \tilde{\xx}_0 - \bb} \le (1+\epsilon) \min_{\xx}\norme{\AA \xx - \bb}$
%	in time $O(nnz(\AA)\log{(n/\epsilon)} + d^{\omega}\log^2{d} + d^2 \log{(1/\epsilon)})$.
%\end{theorem}
%
%\begin{proof}
%	The proof follows exactly from the proof of Theorem 2.17 in \cite{Woodruff14},
%	except that the bound we claim replaces the term $d^3 \log d$
%	in the original theorem	with $d^{\omega} \log^2 d$.
%	The reason for this is that \cite{Woodruff14} attributes $O(d^3)$ time
%	to compute the QR decomposition of a $d \times d$ matrix,
%	whereas using Lemma~\ref{lem:QR}, we can achieve this in $d^{\omega}$ time.	
%\end{proof}

%Given the result from \cite{Woodruff14}, we show the following relative error-type bound.


%\begin{proof}[Proof of Lemma~\ref{lem:approxInit}]
%	Since $\tilde{\xx}_0$ is a $(1+\epsilon)$-minimizer for $\norme{\AA \xx - \bb}$, we have:
%	\begin{align}
%	& \norme{\AA \tilde{\xx}_0 - \bb} \le (1+\epsilon) \norme{\AA \xx_0 - \bb}\notag\\
%	\Rightarrow & \norme{\AA \tilde{\xx}_0 - \bb}^2 \le (1+3\epsilon) \norme{\AA \xx_0 - \bb}^2\notag\\
%	\iff & \norme{\AA \tilde{\xx}_0 - \bb}^2 - \norme{\AA \xx_0 - \bb}^2 \le 3\epsilon \norme{\AA \xx_0 - \bb}^2. \label{eq:prePyth}
%	\end{align}
%	It is easy to prove that $\AA\xx_0 - \bb$ is orthogonal to vectors in the range of $\AA$ \todo{footnote maybe?}. Then by the Pythagorean theorem, we have:
%	\begin{align*}
%	\norme{\AA(\tilde{\xx}_0 - \xx_0)}^2 = \norme{\AA \tilde{\xx}_0 - \bb}^2 - \norme{\AA \xx_0 - \bb}^2.
%	\end{align*}
%Additionally, by the $(C,R)$-goodness of $\AA$,
%$\norme{\AA(\tilde{\xx}_0 - \xx_0)}^2 \ge \frac{1}{1+C}\norme{\tilde{\xx}_0 - \xx_0}^2$.
%Combining this with $\eqref{eq:prePyth}$ gives the desired result.
%\end{proof}


To prove Lemma~\ref{lem:cg}, we use the following theorem from \cite{Sachdeva2014faster}:

\begin{theorem}[Theorem 9.1 from \cite{Sachdeva2014faster}]\label{thm:cg}
	Given an symmetric positive definite matrix $\MM \in \R^{n \times n}$ and a vector $\yy\in \R^{n}$, the Conjugate Gradient method can find a vector $\xx$ such that $\norm{\xx - \MM^{-1} \yy}_{\MM} \le \delta \norm{\MM^{-1} \yy}_{\MM}$ in time $O( (t_{\MM} + n) \cdot \sqrt{\kappa(\MM)} \log (1/\delta))$, where $t_{\MM}$ is the time required to multiply $\MM$ with a given vector and $\kappa(\MM)$ is the condition number of $\MM$.
\end{theorem}


\begin{proof}[Proof of Lemma~\ref{lem:cg}]
Let $\MM = \AA^T \AA$ and $\yy = \AA^T \bb$. Then by \Cref{thm:cg},
the conjugate gradient method finds a vector $\tilde{\xx}_0$ such that
$\norm{\tilde{\xx}_0}_{\AA^T \AA} \le \delta \norm{(\AA^T \AA)^{-1} \AA^T \bb}_{\AA^T \AA}$
in time $O((t_{\AA^T \AA} + d) \log (1/\delta))$.
Noting that $\AA^T \AA \approx_{O(1)} \II$, we get
\begin{align*}
\norme{\tilde{\xx}_0 - \xx_0} \le  O(\delta) \norme{\xx_0}.
\end{align*}

Next, we note:
\begin{align*}
& \norme{\bb} \ge \norme{\AA \xx_0 - \bb} \ge \norme{\AA \xx_0} - \norme{\bb} \\
\implies & \norme{\xx_0} \le O(\norme{\bb}).
\end{align*}

Now, since we assume that $\norme{\bb} \le n^c$ and $\norme{\AA \xx_0 - \bb} \ge 1/n^c$ for some $c$,
we can set $\delta = O(\epsilon/(n^c))$ to get:
\begin{align*}
& \norme{\tilde{\xx}_0 - \xx_0} \le \epsilon/n^c \le \epsilon \norme{\AA\xx_0 - \bb}. 
%\qedhere
\end{align*}
\end{proof}

