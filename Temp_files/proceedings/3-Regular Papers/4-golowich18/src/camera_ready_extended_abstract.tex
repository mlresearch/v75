\documentclass[final,12pt]{colt2018} % Anonymized submission
%\documentclass{colt2018} % Include author names

\usepackage{times,url}

\usepackage{amsfonts,color,float,graphicx,verbatim}
\usepackage{algorithm2e,algorithmic}
\usepackage{enumitem}


%\newtheorem{theorem}{Theorem}
%\newtheorem{proposition}{Proposition}
%\newtheorem{lemma}{Lemma}
%\newtheorem{corollary}{Corollary}
%\newtheorem*{conditions}{Conditions}
%\newtheorem{definition}{Definition}
%\newtheorem{remark}{Remark}

%\renewenvironment{proof}[1][Proof: ]{\noindent \textbf{#1}}{\qed\medskip}
%\renewcommand{\baselinestretch}{2}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\LL}{\mathbb{L}}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin}}}
\newcommand{\argmax}[1]{\underset{#1}{\mathrm{argmax}}}
\newcommand{\summ}{\displaystyle \sum}
\newcommand{\intt}{\displaystyle\int}
\newcommand{\var}{\text{Var}}
\newcommand{\nchoosek}[2]{\left(\begin{array}{*{20}c}#1\\#2\end{array}\right)}


\newcommand{\ba}{\mathbf{a}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bn}{\mathbf{n}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\btau}{\boldsymbol{\tau}}
\newcommand{\bxi}{\boldsymbol{\xi}}
\newcommand{\blambda}{\boldsymbol{\lambda}}
\newcommand{\bepsilon}{\boldsymbol{\epsilon}}
\newcommand{\bsigma}{\boldsymbol{\sigma}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bomega}{\boldsymbol{\omega}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Jcal}{\mathcal{J}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Qcal}{\mathcal{Q}}
\newcommand{\Wcal}{\mathcal{W}}
\newcommand{\KL}{\mbox{KL}}
\newcommand{\Ld}{\tilde{L}}
\newcommand{\uloss}{\ell^\star}
\newcommand{\loss}{\mathcal{L}}
\newcommand{\losst}{\ell_t}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\inner}[1]{\langle#1\rangle}
\renewcommand{\comment}[1]{\textcolor{red}{\textbf{#1}}}
\newcommand{\vol}{\texttt{Vol}}

%\newtheorem{example}{Example}

\newcommand{\secref}[1]{Sec.~\ref{#1}}
\newcommand{\subsecref}[1]{Subsection~\ref{#1}}
\newcommand{\figref}[1]{Fig.~\ref{#1}}
\renewcommand{\eqref}[1]{Eq.~(\ref{#1})}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}
%\newcommand{\corollaryref}[1]{Corollary~\ref{#1}}
\newcommand{\thmref}[1]{Thm.~\ref{#1}}
\newcommand{\propref}[1]{Proposition~\ref{#1}}
\newcommand{\appref}[1]{Appendix~\ref{#1}}
\newcommand{\algref}[1]{Algorithm~\ref{#1}}

\newcommand{\note}[1]{\textcolor{red}{\textbf{#1}}}
\newcommand{\ind}[1]{\ensuremath{{\mathbf I}{\left\{#1\right\}}}}

\newcommand\blfootnote[1]{%
	\begingroup
	\renewcommand\thefootnote{}\footnote{#1}%
	\addtocounter{footnote}{-1}%
	\endgroup
}


\title[Size-Independent Sample Complexity of Neural Networks]{Size-Independent 
Sample Complexity of Neural Networks\\(Extended Abstract)
}
\coltauthor{\Name{Noah Golowich} \Email{ngolowich@college.harvard.edu }\\
	\addr Harvard University
	\AND
	\Name{Alexander Rakhlin} \Email{rakhlin@mit.edu}\\
	\addr MIT
	\AND
	\Name{Ohad Shamir} \Email{ohad.shamir@weizmann.ac.il}\\
	\addr Weizmann Institute of Science\\
	and Microsoft Research	
}
 % Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
% \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \addr Address}


\begin{document}
	
	\maketitle
	
	\begin{abstract}
	We study the sample complexity of learning neural networks, by 
	providing new bounds on their Rademacher complexity assuming norm 
	constraints on the parameter matrix of each layer. Compared to previous 
	work, these complexity bounds have improved dependence on the network 
	depth, and under some additional assumptions, are fully independent of the 
	network size (both depth and width). These results are derived using some 
	novel techniques, which may be of independent interest\footnote{This paper 
	is an extended abstract. The full version appears as arXiv preprint 		
	1712.06541 v3}.
\end{abstract}

\begin{keywords}
	Neural Networks, Deep Learning, Sample Complexity, Rademacher Complexity
\end{keywords}
	
\vskip 1cm

One of the major challenges involving neural networks is explaining their 
ability to generalize well, even if they are very large and have the 
potential to overfit the training data 
\citep{neyshabur2014search,zhang2016understanding}. 
Learning theory teaches us that this must be due to some inductive bias, 
which constrains one to learn networks of specific configurations (either 
explicitly, e.g., via regularization, or implicitly, via the algorithm used to 
train them). However, understanding the nature of 
this inductive bias is still largely an open problem. 

In our work, we consider whether it is possible to prove sample 
complexity bounds for neural networks, which are not strongly dependent on the 
network size, under suitable norm constraints. Such bounds exist for linear 
predictors (or equivalently, one-layer networks), but for networks with more 
layers, existing results strongly depend on the number of layers, sometimes 
exponentially, regardless of the norms of the parameter matrices 
(e.g. 
\citet{anthony2009neural,neyshabur2015norm,bartlett2017spectrally,neyshabur2017pac}).
We make the following contributions:
\begin{itemize}[leftmargin=*]
	\item We show that the exponential depth 
	dependence in Rademacher complexity-based analysis (e.g. 
	\citet{neyshabur2015norm}) can be avoided by applying contraction to a 
	slightly different object than what has become standard since the work 
	of 
	\cite{bartlett2002rademacher}. For 
	example, for networks with $d$ layers, where each layer $j$ has a parameter 
	matrix with Frobenius norm at most 
	$M_F(j)$, and $m$ i.i.d. training examples, one can prove a generalization 
	bound of $\Ocal\left(\sqrt{d}\left(\prod_{j=1}^d 
	M_F(j)\right)/\sqrt{m}\right)$ . 
	The technique can also be applied to other types of norm constraints. 
	For example, if we consider networks where the $1$-norm of each row of the 
	$j$-th parameter matrix is at most $M(j)$, we attain a bound of
	$
	\Ocal\left(\sqrt{d+\log(n)}\left(\prod_{j=1}^{d}M(j)\right)/\sqrt{m}\right)
	$,
	where $n$ is the input dimension. Again, the dependence on $d$ is 
	polynomial and quite mild. 
	\item We develop a generic technique to 
	convert depth-dependent bounds to depth-independent bounds, assuming some 
	control over any Schatten norm of the parameter matrices (which includes, 
	for instance, the Frobenius norm and the trace norm as special cases). The 
	key observation we utilize is that the prediction function computed by such 
	networks can be approximated by the composition of a 
	shallow network and univariate Lipschitz functions. For example, again 
	assuming that the Frobenius norms of the layers are bounded by 
	$M_F(1),\ldots,M_F(d)$, we can 
	further improve the result above to
	\begin{equation}\label{eq:frobb}
	\tilde{\Ocal}\left(\left(\prod_{j=1}^d 
	M_F(j)\right)\cdot\min\left\{\sqrt{\frac{\log\left(\frac{1}{\Gamma}
			\prod_{j=1}^{d}M_F(j)\right)}{\sqrt{m}}}~,~
	\sqrt{\frac{d}{m}}\right\}
	\right)~,
	\end{equation}
	where $\Gamma$ is a lower bound on the product of the \emph{spectral} 
	norms of the parameter matrices (note that $\Gamma\leq \prod_{j}M_F(j)$ 
	always). 
	Assuming that $\prod_{j}M_F(j)\leq 
	R$ for some $R$, this can be upper bounded by 
	$\tilde{\Ocal}(R\sqrt{\log(R/\Gamma)/\sqrt{m}})$, which to the best of 
	our 
	knowledge, is the first explicit bound for standard neural networks 
	which 
	is fully size-independent, assuming only suitable norm constraints. 
	We also apply this technique to get a depth-independent version of the 
	bound in \citep{bartlett2017spectrally}: Specifically, if we assume 
	that the spectral norms satisfy $\norm{W_j}\leq M(j)$ for all $j$, and 
	$\max_j \frac{\norm{W_j^T}_{2,1}}{\norm{W_j}}\leq L$, then the bound in 
	provided by
	\citet{bartlett2017spectrally} becomes
	$
	\tilde{\Ocal}\left(
	BL\prod_{j=1}^{d}M(j)\cdot\sqrt{d^3/m}\right)~.
	$
	In contrast, we show the following bound for any $p\geq 1$ (ignoring 
	some 
	lower-order logarithmic factors):
	\[
	\tilde{\Ocal} \left( BL\prod_{j=1}^d M(j)
	\cdot \min \left\{ \frac{\log \left( \frac{1}{\Gamma} \prod_{j=1}^d 
		M_p(j) 
		\right)^{\frac{1}{\frac{2}{3}+p}} 
	}{m^{\frac{1}{2+3p}}},\sqrt{\frac{d^{3}}{ m}} 
	\right\} \right)~,
	\]
	where $M_p(j)$ is an upper bound on the Schatten $p$-norm of $W_j$, and 
	$\Gamma$ is a lower bound on $\prod_{j=1}^{d} \norm{W_j}$. Again, by 
	upper bounding the $\min$ by 
	its first argument, we get a bound independent of the depth $d$, 
	assuming 
	the norms are suitably constrained.
	\item We provide a lower bound, showing 
	that 
	for any $p$, the class of depth-$d$, width-$h$ neural networks, where 
	each 
	parameter matrix $W_j$ has Schatten $p$-norm at most $M_p(j)$, can have 
	Rademacher complexity of at least
	\[
	\Omega\left(
	\frac{B\prod_{j=1}^{d}M_p(j)\cdot 
		h^{\max\left\{0,\frac{1}{2}-\frac{1}{p}\right\}}}{\sqrt{m}}\right)~.
	\]
	This somewhat improves on \citet[Theorem 
	3.6]{bartlett2017spectrally}, which only showed such a result for 
	$p=\infty$ (i.e. with spectral norm control), and without the $h$ term. 
	For 
	$p=2$, it matches the upper bound in \eqref{eq:frobb} in terms of the 
	norm 
	dependencies and $B$. Moreover, it establishes that controlling the  
	spectral norm alone (and indeed, any Schatten $p$-norm control with 
	$p>2$) 
	cannot lead to bounds independent of the size of the network.
	% Finally, 
	%the 
	%bound shows (similar to \citet{bartlett2017spectrally}) that a 
	%dependence 
	%on products of norms across layers is generally inevitable.
\end{itemize}



%	\bibliographystyle{plainnat}
\bibliography{bib}
\end{document}

