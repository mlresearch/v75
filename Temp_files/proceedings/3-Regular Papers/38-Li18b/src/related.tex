\section{Related Work} \label{sec:related}


Mixtures of Linear Regressions is a popular mixture model (e.g.,~\citep{de1989mixtures,grun2007applications} and \citep{faria2010fitting}), also known as Hierarchical Mixture of Experts in~\citep{jordan1994hierarchical} in the machine learning community. 
It has many applications, such as trajectory clustering~\citep{gaffney1999trajectory} and phase retrieval~\citep{balakrishnan2017statistical}, and has as special cases some popular models, such as piecewise linear regression and locally linear regression.

Learning MLR in general is NP-hard~\citep{yi2014alternating}. Recent interests have been in providing various efficient algorithms for recovering the parameters in MLR under assumptions about the data generation model~\citep{chaganty2013spectral,chen2014convex,yi2014alternating,zhong2016mixed,klusowski2017estimating}. 
%These results assumes the data $x$ in different components are all from the standard Gaussians. 
They are either under restricted assumptions about the data (mixtures of two component or $x$ all from the standard Gaussian)~\citep{chen2014convex,yi2014alternating,balakrishnan2017statistical,klusowski2017estimating}, or have high sample or computational complexity~\citep{chaganty2013spectral,sedghi2016provable}. 

Some works study specific algorithms for the problem, such as  the Expectation Maximization (EM) algorithm~\citep{khalili2007variable,yi2014alternating,balakrishnan2017statistical,klusowski2017estimating}. It is known that without careful initialization EM is only guaranteed to have local convergence~\citep{klusowski2017estimating}. A grid search method for initialization is proposed in~\citep{yi2014alternating} but is only for the two-component case. It is unclear how to generalize these guarantees to our more general setting where the data $x$ from different components are from different Gaussians.
Moreover, EM also often suffers from a high computational cost.
%, for example, exact optimization in each EM step has $O(d^2 N + d^3)$ complexity. 

Another line of works used tensor methods for MLR~\citep{chaganty2013spectral,sedghi2016provable}. The third-order moment is directly estimated in~\citep{chaganty2013spectral} using samples from Gaussian distribution and is estimated from a linear regression problem in~\citep{sedghi2016provable}. A significant drawback of tensor methods is high sample and computational complexity, due to the high cost in estimating and operating over the tensors. 

\citep{chen2014convex} provided a convex relaxation formulation and showed that their algorithm is information-theoretically optimal. However, it is only for the two-component case and suffers from high computational cost in nuclear norm minimization. 

\citep{zhong2016mixed} provided a non-convex objective function that is locally strongly convex in the neighborhood of the ground truth, and proposed to first use a tensor method for initialization and then optimize the provided objective, achieving a global convergence guarantee. The overall algorithm is fixed parameter tractable in the number of components, and achieves nearly optimal sample and time complexity when this parameter is constant. However, it requires all components have the standard Gaussian distribution. It is unclear how to generalize the result to our more general setting where the data $x$ from different components are from different Gaussians. Furthermore, due to the tensor initialization, the algorithm needs complicated assumptions on the moments, while our only essential assumption is that the weight parameters can be separated, which is much simpler and more general (in fact, it is essentially necessary for obtaining any recovery guarantees).

\citep{yi2016solving} gives an improved way of using the tensor method plus alternative minimization so the sample complexity linearly depend on $d$. However, their algorithm  requires that all the data are from the standard Gaussian, and the sample complexity also depends on the minimal singular value of certain moment matrix, which can be $ \Delta^{\Omega(k)}$ small in our setting. 
