\section{Conclusion} \label{sec:conclusion}


In this paper, we present a fixed parameter algorithm that solves mixture of linear regression under Gaussian inputs in time nearly linear in the sample size and the dimension. Moreover, our sample complexity also scales nearly linear with the dimension $d$. In our setting, we allow each mixture to have a different covariance matrix. Thus, unlike the case when the mixtures are spherical, even the best known algorithm for mixture of general Gaussians would require at least $d^2$ sample complexity to recover the covariance. Our algorithm reduces the sample complexity significantly with the additional one dimensional linear information: it can recover the linear classifier (and thus recover the covariance as well) with $\tilde{O}(d)$ samples.  While the dependency on $d$ is nearly optimal, we would also like to point out that when the total number of mixtures are too large, the sample complexity of our algorithm does suffer from an exponential term of $k$. We believe that with our current set of assumptions, the exponential dependency could be necessary: A lower bound of $e^{k}$ has been proved in~\citep{moitra2010settling} in the very similar setting of learning mixture of Gaussians. 


One natural way to get around the exponential dependency is assuming that the covariance $\bSigma_i$ and the hidden vectors $w_i$ satisfies some smoothness assumption (e.g.,~\citep{ge2015learning}). However, the level of smoothness is very subtle in our setting, since the na\"ive application of smoothed analysis often leads to complexity with a large polynomial factor in the dimension. In this paper, near linearity in $d$ is one of our main contributions. We believe that using smoothed analysis while preserving the nearly linear dependency on $d$ is one of the important future directions. 

