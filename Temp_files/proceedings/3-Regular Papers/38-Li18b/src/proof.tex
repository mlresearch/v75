\section{Proof of Warm Start for Learning One of the Weights}
\label{sec:proof_one}


We prove the following lemma related to the output of Algorithm~\ref{alg:one}.

\medskip
\noindent
\textbf{Lemma~\ref{lem:warmstart}}
{\it
With probability at least $1-\delta$, $\min_i \|w_i - a_T\|_2 \le O(\sigma^2\veps)$.
}
\medskip

Before proving this lemma, we first need the following lemma about the clustering, which is crucial for constructing the coefficients. As we shall see, we will use this lemma on $r_i = \| \bSigma_i (w_i - a_t) \|_2^2$. Roughly speaking, $f(\sqrt{r_i})$ is the weight of $\bSigma_i^2$ and $f'(\sqrt{r_i})$ is the weight of $\bSigma_i^2 (w_i - a_t) $. Therefore, we would like $f(\sqrt{r_i})$ to be small compare to  $f'(\sqrt{r_i})$ to identify the subspace spanned by $\bSigma_i^2 (w_i - a_t) $. 

%\begin{lemma}[Coefficients]\label{lem:cluster}
%For every $k \geq 2$, every $\rho > 1$, every $r_1, \cdots, r_k \in [\frac{1}{\rho}, \rho]$, and every $\veps > 0$, one can find in time $O(k\log k)$ an integer $0<s\le k$ and centers $1/\rho \leq z_1 \leq  \cdots \leq z_s \leq \rho$ such that for $f(x) = \prod_{p = 1}^s (x^2 - z_p)$ the following holds.
%\begin{enumerate}
%\item For $r = \min\{r_i\}_{i = 1}^k$ and every $i \in [k]$, $|f(\sqrt{r_i})| \leq \veps |\sqrt{r}f'(\sqrt{r}) |  $. 
%\item $|\sqrt{r}f'(\sqrt{r})| \geq  \left( \frac{ \veps}{\rho} \right)^k$. 
%\item For all $x$ with $x^2\in [1/\rho, \rho]$, $|f'(x)| \leq 2k \rho^k$ and $|f''(x)| \leq 4k^2 \rho^k$.
%\end{enumerate}
%\end{lemma}

%\clusterlemma* % restatable doesn't work

\medskip
\noindent
\textbf{Lemma~\ref{lem:cluster} (Coefficients)}
{\it
For every $k \geq 2$, every $\rho > 1$, every $r_1, \cdots, r_k \in [\frac{1}{\rho}, \rho]$, and every $\veps > 0$, one can find in time $O(k\log k)$ an integer $0<s\le k$ and centers $1/\rho \leq z_1 \leq  \cdots \leq z_s \leq \rho$ such that for $f(x) = \prod_{p = 1}^s (x^2 - z_p)$ the following holds.
\begin{enumerate}
\item For $r = \min\{r_i\}_{i = 1}^k$ and every $i \in [k]$, $|f(\sqrt{r_i})| \leq \veps |\sqrt{r}f'(\sqrt{r}) |  $. 
\item $|\sqrt{r}f'(\sqrt{r})| \geq  \left( \frac{ \veps}{\rho} \right)^k$. 
\item For all $x$ with $x^2\in [1/\rho, \rho]$, $|f'(x)| \leq 2k \rho^k$ and $|f''(x)| \leq 4k^2 \rho^k$.
\end{enumerate}
}
\medskip

\begin{proof}[Proof of Lemma \ref{lem:cluster}]
%Let us without loss of generality assume that $r=r_1 \leq r_2 \leq \cdots \leq r_k$.  Let us define $z_1 = r_1$, and let $j \in [k ]$ be the smallest  index such that $r_j \geq z_1 + \frac{\veps}{{\rho}}$. If no suck index exists, we let $j = k+ 1$.  If $j \leq k$, let us define for $s \leq k$:
%\begin{align}
%z_2 = z_1 + \veps,  \quad z_3 = r_j, z_4 = r_{j + 1}, \cdots, z_s = r_k.
%\end{align}
%
%Otherwise if $j = k + 1$, we just pick $s = 2$ and let $z_1 = r_1$ and $z_2 = z_1 + \veps$. Therefore, we still have that $s \leq k$. 
Let us without loss of generality assume that $r=r_1 \leq r_2 \leq \cdots \leq r_k$.  Let us define $z_1 = r_1$, and let $j \in [k ]$ be the smallest  index such that $r_j \geq z_1 + \frac{\veps}{{\rho}}$. If no such index exists, we let $s = 1$ and the statements in the lemma are true.  If such $j$ exists, let us define:
\begin{align}
z_2 = r_j, z_3 = r_{j + 1}, \cdots, z_s = r_k.
\end{align}

Now, we know that 
\begin{align}
|\sqrt{r}f'(\sqrt{r})| &= 2 r \prod_{p = 2}^{s} |r - z_p| \geq  \left( \frac{ \veps}{{\rho}} \right)^k.
\end{align}

On the other hand, for every $i \geq j$, $f(\sqrt{r_i}) = 0$. For $i < j$ we have: 
\begin{align}
|f(\sqrt{r_i})| &=  |r_i - r|\prod_{p = 2}^{s} |r_i - z_p|
\\
&\leq \frac{\veps}{{\rho}} \prod_{p = 2}^{s} |r_i - z_p| \leq \veps r \prod_{p = 2}^{s} |r - z_p|  \leq \veps  |\sqrt{r} f'(\sqrt{r})|.
\end{align}


We now consider the derivative and second order derivative of $f(x)$ for $x^2 \in [0, \rho]$. By elementary calculation, we know that 
\begin{align}
|f'(x)| &= \left|\sum_{p = 1}^s 2x \prod_{q \not= p} (x^2 - z_q) \right|
\\
& \leq 2\sum_{p = 1}^s |x |  \prod_{q \not= p} \left| x^2 - z_q \right|
\\
& \leq 2 k \rho^k.
\end{align}

Similarly we can get that $|f''(x)| \leq 4k^2 \rho^k$. 
\end{proof}

We also need the following bound for the $k$-SVD of a matrix.

\begin{lemma}\label{lem:k_SVD}
Let $\bX_1, \cdots, \bX_k$ be $k$ rank-one matrices in $\mathbb{R}^{d \times d}$ such that each $\bX_i = x_i x_i^{\top}$, for every $\veps \geq 0$, every PSD matrix $\bM \in \mathbb{R}^{d \times d}$ such that 
\begin{align}
\left\| \bM - \sum_{i = 1}^k \bX_i \right\|_2 \leq \veps \| \bX_1 \|_2
\end{align}
Let $\bU \in \mathbb{R}^{d \times k}$ be the matrix consists of the top-k singular vectors of $\bM$, then we have
\begin{align}
\| x_1^{\top}\bU \|_2 \geq \left(1 - (\veps k)^{1/3}\right)\| x_1 \|_2
\end{align}

\end{lemma}
\begin{proof}[Proof of Lemma \ref{lem:k_SVD}]
Let us denote $\sigma_1 \geq  \cdots \geq \sigma_k \geq \sigma_{k + 1} =  0$ as the $k + 1$ singular values of $\sum_{i = 1}^k \bX_i$ with corresponding singular vectors $v_1, \cdots, v_k$ (and $v_{k + 1}$). For every $v_i$, by definition
\begin{align}
v_i^{\top} \left(\sum_{j = 1}^k \bX_j \right)v_i = \sigma_i
\end{align}
So we have $v_i^{\top} \bX_1 v_i  \leq \sigma_i$. Let $\bV_i  \in \mathbb{R}^{d \times i}$ defined as $\bV_i= (v_1, \cdots, v_i)$. By Gap-free Wedin theorem in~\citep{allen2016lazysvd} (see Lemma~\ref{lem:gapfree_wedin}), we know that 
\begin{align}
\| (\bI - \bU \bU^{\top}) \bV_i \|_2 \leq \frac{\veps \| x_1 \|_2^2}{\sigma_i}.
\end{align}
Thus, $\|x_1^{\top} (\bV_i  \bV_i^{\top})(\bI - \bU \bU^{\top}) \|_2 \leq \frac{\veps \| x_1 \|_2^3}{\sigma_i}$. 

On the other hand, since $x_1 \in \text{span}\{v_1, \cdots, v_k \}$, 
\begin{align}
\|x_1^{\top} (\bI - \bV_i  \bV_i^{\top}) \|_2 &= \|x_1^{\top} (\bV_{k} \bV_{k}^{\top} - \bV_i  \bV_i^{\top}) \|_2 
\\
&\leq  \sum_{j = i + 1}^k |x_i^{\top} v_k| \leq k \sqrt{\sigma_{i + 1}}.
\end{align}
Therefore, we know that 
\begin{align}
\| x_1^{\top}  (\bI - \bU \bU^{\top}) \|_2 \leq \frac{\veps \| x_1 \|_2^3}{\sigma_i} + k \sqrt{\sigma_{i + 1}}.
\end{align}

If $\sigma_1 \geq  \frac{\| x_1\|_2^2 \veps^{2/3}}{k^{2/3}}$, by picking $i$ to the largest index in $[k]$ such that $\sigma_i \geq \frac{\| x_1\|_2^2 \veps^{2/3}}{k^{2/3}}$,  we get that 
\begin{align}
\| x_1^{\top}  (\bI - \bU \bU^{\top}) \|_2 \leq (\veps k)^{1/3} \| x_1 \|_2
\end{align}

If $\sigma_1 \leq  \frac{\| x_1\|_2^2 \veps^{2/3}}{k^{2/3}}$, then we can just use $\|x_1^{\top}  \|_2 \leq k \sqrt{\sigma_1}$ to complete the proof.
%
%Let us denote the singular values of $\bM$ as $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_d \geq 0$ with corresponding singular vectors $x_1, \cdots , x_d$. By Wedin's theorem, we know that for every $i \in [d]$
%\begin{align}
%
%\end{align}
%
%
%
%
%By Wely's theorem, we know that $\sigma_{k + 1}(\bM) \leq  \veps \| \bX_1 \|_2 = \veps \| x_1 \|_2^2$. Thus, multiplying $x_1$ with $\bM$ we have:
%\begin{align}
%x_1^{\top} \bM x_1 = \|x_1^{\top} \bU \|_2^2
%\end{align}
\end{proof}



We are now ready to prove the following important lemma about the correlation between $\bU$ and $\bSigma_i^2(w_i - a_t)$.

\begin{lemma}\label{lem:correlation}
Let $j = \argmin_{1\le i\le k}  \|\bSigma_i (w_i - a_t) \|_2$, we have that in the $t$-th iteration of Algorithm~\ref{alg:one}, the $\bU_t$ satisfies
\begin{align}
\frac{\|\bU_t^{\top} \bSigma_j^2 (w_j - a_t) \|_2}{\| \bSigma_j^2 (w_j - a_t)  \|_2 } \geq \frac{1}{2}.
\end{align}

\end{lemma}
\begin{proof}[Proof of Lemma \ref{lem:correlation}]
Suppose $z \sim \mathcal{N}(0, \bSigma^2)$, we know that $z = \bSigma g$ where $g \sim \mathcal{N}(0, \bI)$. For every vector $a$,
\begin{align}
\E\left[\langle z, a \rangle^{2p} z z^{\top}\right] &= \bSigma \E\left[ \langle g, \bSigma a \rangle^{2p} g g^{\top}\right]  \bSigma
\\
&=  (2p - 1)!! \bSigma \left( 2p  \bSigma a a^{\top} \bSigma \| \bSigma a\|_2^{2p - 2}  + \| \bSigma a \|_2^{2p} \bI \right) \bSigma
\\
& =   (2p - 1)!!  \| \bSigma a\|_2^{2p} \left( 2p \frac{ \bSigma^2 a a^{\top} \bSigma^2 }{ \|\bSigma a \|_2^2 }+  \bSigma^2 \right).
\end{align}
Thus, we have
\begin{align}
\frac{1}{(2p - 1)!!}\E\left[ \alpha_i^{2p} x_i x_i^{\top} \right] &= \sum_{i = 1}^k p_i   \| \bSigma_i (w_i - a_t)\|_2^{2p} \left( 2p \frac{ \bSigma_i^2 (w_i - a_t) (w_i - a_t)^{\top} \bSigma_i^2 }{ \|\bSigma_i (w_i - a_t) \|_2^2 }+  \bSigma_i^2 \right).
\end{align}


Since in the $t$-th iteration, the labels $\alpha_i$ we fit to Algorithm~\ref{alg:pww} comes from $\alpha_{\ell} = \langle x_{\ell}, w^{(\ell)} - a_t \rangle$, we know that 
\begin{align}
\E[\bM] =  \sum_{i = 1}^k p_i    \sum_{p = 0}^k \left( c_p \| \bSigma_i (w_i - a_t)\|_2^{2p} \left( 2p \frac{ \bSigma_i^2 (w_i - a_t) (w_i - a_t)^{\top} \bSigma_i^2 }{ \|\bSigma_i (w_i - a_t) \|_2^2 }+ \bSigma_i^2  \right) \right).
\end{align}
Let us define the signal matrix $\bX_i$ as 
\begin{align}
\bX_i &=   \frac{ \bSigma_i^2 (w_i - a_t) (w_i - a_t)^{\top} \bSigma_i^2  }{ \|\bSigma_i (w_i - a_t) \|_2^2 }  \left( \sum_{p = 0}^k  2p c_p \| \bSigma_i (w_i - a_t)\|_2^{2p} \right) 
\\
&= \frac{ \bSigma_i^2 (w_i - a_t) (w_i - a_t)^{\top} \bSigma_i^2  }{ \|\bSigma_i (w_i - a_t) \|_2^2 }  \left( f'( \| \bSigma_i (w_i - a_t) \|_2) \| \bSigma_i (w_i - a_t) \|_2 \right)
\end{align}
and the noise matrix $\bY_i$ as
\begin{align}
\bY_i &=\bSigma_i^2 \left( \sum_{p = 0}^k  c_p \| \bSigma_i (w_i - a_t)\|_2^{2p} \right) 
\\
&= \bSigma_i^2 f(\| \bSigma_i (w_i - a_t)\|_2)
\end{align}
such that 
\begin{align}
\E[\bM] = \sum_{i=1}^k p_i (\bX_i + \bY_i).
\end{align}


For $j = \argmin\{\| \bSigma_i (w_i - a_t) \|_2) \}_{i = 1}^k$, let us denote 
$$
\beta := f'( \| \bSigma_j (w_j - a_t) \|_2) \| \bSigma_j (w_j - a_t) \|_2.
$$


Let us recall that $\veps^{(g)}$ is the error incurred when estimating $\{\| \bSigma_i (w_i - a_t) \|_2\}_{i = 1}^k$. $\veps^{(p)}$ is the error when constructing the coefficients of the polynomial (for sufficiently large  $\rho$ such that $\rho \geq  \max\{\| \bSigma_i (w_i - a_t) \|_2^2) \}_{i = 1}^k$ as we will show later in this proof). Thus, by Lemma~\ref{lem:cluster}, we know that 
\begin{align}
\| \bY_i \|_2& \leq \| \bSigma_i^2 \|_2 | f(\| \bSigma_i (w_i - a_t)\|_2)|
\\
& \leq \| \bSigma_i^2 \|_2(| f(\sigma_i)| + 2k \rho^k \left|\sigma_i - \| \bSigma_i (w_i - a_t)\|_2 \right| )
\\
&\leq  \| \bSigma_i^2 \|_2 ( \veps^{(p)} \beta + 4k \rho^k \veps^{(g)}).
\end{align} 
Similarly we have
\begin{align}
\|\bX_j\|_2 \geq \sigma_{\min}(\bSigma_j^2) \beta.
\end{align}
And we have $\beta \geq \left(\frac{\veps^{(p)}}{\rho} \right)^k - 8 k^2 \rho^k \veps^{(g)} \sigma^2$.  


Notice that $ \min\{\| \bSigma_i (w_i - a_t) \|_2) \}_{i = 1}^k \leq  \min\{\| \bSigma_i (w_i ) \|_2) \}_{i = 1}^k$, which implies that $\| a_1 \|_2 \leq \sigma^4$. Therefore, we can take $\rho =O\left( \max\left\{2\sigma^{10}, \frac{1}{\veps} \right\}\right)$. Thus, by our choice of parameter, we know that for $\veps^{(e)} \leq \frac{1}{100 k} $, 
\begin{align}
\left\|\E[\bM] - \sum_{i= 1}^k p_i \bX_i \right\|_2 \leq \veps^{(e)}\| \bX_j \|_2/2.
\end{align}
Using the sample complexity bound Lemma~\ref{lem:gsb}, by our choice of $m$ we know that 
\begin{align}
\left\|\bM - \E[\bM] \right\|_2 \leq \veps^{(e)}\| \bX_j \|_2/2.
\end{align}
Thus, apply Lemma \ref{lem:k_SVD} on $\bM$ we know that 
\begin{align}
\frac{\|\bU_t^{\top} \bX_j \bU_t \|_2}{\|\bX_j\|_2} \geq 1 - \left(\veps^{(e)}k\right)^{1/3} \geq \frac{3}{4}.
\end{align}
Indeed, this also implies that 
\begin{align}
\frac{\|\bU_t^{\top} \bSigma_j^2 (w_j - a_t) \|_2}{\| \bSigma_j^2 (w_j - a_t)  \|_2 } \geq \frac{1}{2}
\end{align}
completing the proof.
\end{proof}


Now we can prove the main lemma regarding the per-iteration improvement of Algorithm~\ref{alg:one}.

%\oneiterlemma* % doens't work


\medskip
\noindent
\textbf{Lemma~\ref{lem:one_iter} (Coefficients)}
{\it
For every $t \in \{0, 1, \cdots, T - 1\}$ and $\delta > 0$, as long as $\sigma_t = \Omega ( \sigma \veps)$, then with probability at least $1 - \delta$,
$$
\sigma_{t + 1}^2 \leq \left(1 - \frac{1}{200 k \sigma} \right) \sigma_t^2.
$$
}
\medskip

\begin{proof}[Proof of Lemma~\ref{lem:one_iter}]
At $t$-th iteration let $j = \argmin \{ \|\bSigma_i (w_i - a_t) \|_2\}_{i = 1}^k$, we know that 
\begin{align}
\frac{\|\bU_t^{\top} \bSigma_j^2 (w_j - a_t) \|_2}{\| \bSigma_j^2 (w_j - a_t)  \|_2 } \geq \frac{1}{2}.
\end{align}

By definition, $v = \frac{\bU_t \gamma}{\| \bU_t \gamma \|_2}$ for $\gamma \in \mathcal{N}(0, \bI)$. Thus, using elementary calculation of Gaussian random variables, we have: with probability at least $1/4$, 
\begin{align}
\frac{v^{\top} \bSigma_j^2 (w_j - a_t) }{ \| \bSigma_j^2 (w_j- a_t)  \|_2 } \geq \frac{1}{10 \sqrt{k}}
\end{align}
which implies that
\begin{align}
\left\|\bSigma_j (w_j - a_t  - \eta  v) \right\|_2^2 &= \left\|\bSigma_j (w_j - a_t ) \right\|_2^2 - 2  \eta \langle \bSigma_j (w_j - a_t), \bSigma_j  v\rangle + \eta^2 \| \bSigma_j  v \|_2^2
\\
&=  \left\|\bSigma_j (w_j - a_t ) \right\|_2^2  - 2 \eta \langle \bSigma_j^2 (w_j - a_t),  v\rangle + \eta^2 \| \bSigma_j  v \|_2^2
\\
& \leq  \left\|\bSigma_j (w_j - a_t ) \right\|_2^2  - \frac{\eta}{5 \sqrt{k}} \| \bSigma_j^2 (w_j - a_t) \|_2 + \eta^2  \sigma.
\end{align}


Let $\eta = \frac{\| \bSigma_j^2 (w_j - a_t) \|_2 }{10 \sigma \sqrt{k}}$. Then we know that 
$$
\left\|\bSigma_j (w_j - a_t  - \eta v) \right\|_2^2 \leq \left(1 - \frac{1}{100 k \sigma} \right)  \left\|\bSigma_j (w_j - a_t ) \right\|_2^2.
$$


Thus, since we can estimate $\left\|\bSigma_j (w_j - a_t  - \eta  v) \right\|_2$ up to accuracy $\veps/(k\sigma)$ using the algorithm proposed in~\citep{moitra2010settling}, as long as $\sigma_{t} = \Omega\left( \sigma\veps \right)$, we will have that $\sigma_{t + 1}^2 \leq  \left(1 - \frac{1}{200 k \sigma} \right)\sigma_t^2$. 
\end{proof}

This immediately leads to the main lemma regarding the output of Algorithm~\ref{alg:one}.


%\warmstartlemma*

\begin{proof}[Proof of Lemma~\ref{lem:warmstart}]
By Lemma~\ref{lem:one_iter}, and by the choice of the parameters in the algorithm,
$
  \sigma_T \le O(\sigma \veps).
$
Then for $j = \min_i \{\|\Sigma_i (w_i - a_T)\|_2\}$ we have 
$
  \|\Sigma_j (w_j - a_T)\|_2 \le O(\sigma \veps)
$
and thus
$ 
  \|w_j - a_T\|_2 \le O(\sigma^2 \veps).
$
\end{proof}

\section{Proof for Learning One of the Weights from Warm Start} \label{sec:proof_gd}


Without loss of generality, let us assume that we have an $v$ such that $\| v - w_1 \|_2$ is reasonably small. We will show that the update rule used in the algorithm can recover $w_1$ up to error $\veps$ with this $v$. 
%In particular, we consider the following algorithm: with $v^{(1)} = v_1$, each iteration, update
%$$v^{(t + 1)} = v^{(t)} + \eta_t \E \left[\frac{\sign(\langle w - v^{(t)}, x \rangle)}{|\langle w - v^{(t)}, x \rangle| + \zeta} x \right]$$
%
%Which is equivalent to the gradient descent update on the following concave objective function:
%$$f(v) = -  \E \left [\log (|\langle w - v, x \rangle| + \zeta)  \right]$$
It is equivalent to (the empirical version of) the gradient descent update to minimize the following concave objective function:
$$
  g(v) = \E \left [\log (|\alpha - \langle v, x \rangle| + \zeta)  \right].
$$


%\begin{lemma}[Gradient descent] \label{lem:gradient}
%Suppose $\| w_1 - v \| \le \zeta / \sigma$ and $\Delta > (\zeta / \sigma + 32\zeta/p_{\min})$. Let $\eta_t = c\frac{\zeta p_{\min}}{d} \times \left( 1 - c^3\frac{p_{\min}^2}{d} \right)^t$ for a sufficiently small constant $c>0$, and each gradient step uses $\text{poly}\left(\frac{1}{\zeta}, \frac{1}{p_{\min}}\right)$ examples. Then with high probability, after $T = O\left(\frac{d}{p_{\min}^2}  \log \frac{\zeta}{\veps}\right)$ steps,
%$
  %\| w_1 - v^{(T)} \| \le \veps.
%$
%\end{lemma}

%\gradientlemma*


\medskip
\noindent
\textbf{Lemma~\ref{lem:gradient} (Gradient descent)}
{\it
Suppose there exists $i \in [k]$ such that $\| w_i - v \|_2 \le \zeta / \sigma$. Then with high probability, Algorithm~\ref{alg:gd} outputs a vector $v^{(T)}$ such that
$
  \| w_{i} - v^{(T)} \| \le \veps.
$
}
\medskip

\begin{proof}[Proof of Lemma~\ref{lem:gradient}]
First, suppose we have the gradient on the expectation, i.e., we have $\nabla g(v^{(t)})$. For this gradient descent update rule,
by Lemma~\ref{lem:inverse_gaussian}, we know that 
\begin{align*}
 \left\langle - \nabla g(v^{(t)}), w_1 - v^{(t)} \right\rangle 
  & = \E \left[\frac{\sign(\alpha - \langle v^{(t)}, x \rangle) \langle w_1 - v^{(t)}, x \rangle}{|\alpha - \langle v^{(t)}, x \rangle| + \zeta}  \right] 
	\\
	& = p_1 \E_{y \sim \mathcal{N}(0, 1)} \E \left[\frac{\sign(\langle \bSigma_1 (w_1 - v^{(t)}), y \rangle) \langle \bSigma_1 (w_1 - v^{(t)}), y \rangle}{|\langle \bSigma_1(w_1 - v^{(t)}), y \rangle| + \zeta}  \right]
  \\
  & ~~ + \sum_{j = 2}^k p_j \E_{y \sim \mathcal{N}(0, 1)} \E \left[\frac{\sign(\langle \bSigma_j (w_j - v^{(t)}), y \rangle) \langle \bSigma_j (w_1 - v^{(t)}), y \rangle}{|\langle \bSigma_j(w_j - v^{(t)}), y \rangle| + \zeta}  \right]
  \\
  & \geq \frac{1}{4} p_1 \frac{\| \bSigma_1 (w_1 - v^{(t)})\|_2 }{\|\bSigma_1 (w_1 - v^{(t)}) \|_2+ \zeta} - \sum_{j = 2}^k p_j \frac{\|\bSigma_1 (w_1 - v^{(t)}) \|_2} {\| \bSigma_j (w_j - v^{(t)})\|_2 }.
\end{align*}

Note that our assumption on $\zeta$ satisfies that
\begin{align} \label{eqn:grad_condition}
  \|\bSigma_1 (w_1 - v^{(t)}) \|_2 \leq \zeta, \quad \|\bSigma_j (w_j - v^{(t)}) \|_2 \geq 32 \zeta / p_{\min}, j\neq 1,
\end{align}

Therefore, a direct calculation shows that 
$$
  \left\langle -\nabla g(v^{(t)}), w_1 - v^{(t)} \right\rangle 
	\geq \frac{p_{\min}}{32 } \frac{\|\bSigma_1 (w_1 - v^{(t)}) \|_2}{\zeta} 
	\geq   \frac{p_{\min} \| w_1 - v^{(t)} \|_2}{32 \zeta}.
$$

However, we only have the empirical version of the gradient given as
$$
 -\tilde\nabla g(v^{(t)}) = \E_{(x_\ell,\alpha_\ell)} \nabla g_\ell(v), \mbox{~where~} -\nabla g_\ell(v^{(t)}) = \frac{\sign(\alpha_\ell - \langle v^{(t)}, x_\ell \rangle)}{|\alpha_\ell - \langle v^{(t)}, x_\ell \rangle| + \zeta} x_\ell.
$$

To apply concentration bound on the empirical version, we know that for for every example $(x,\alpha)$, 
$$
  \left\|\frac{\sign(\alpha - \langle  v^{(t)}, x \rangle)}{|\alpha - \langle v^{(t)}, x \rangle| + \zeta} x  \right\|_2 \leq \frac{\|x\|_2}{\zeta}. 
$$

Moreover, we know that the true gradient satisfies
$$
  \left\langle - \nabla g(v^{(t)}) , \frac{ w_1 - v^{(t)}}{\| w_1 - v^{(t)}\|_2}  \right\rangle \geq \frac{p_{\min}}{32 \zeta} 
$$

For every example $(x, \alpha)$, we have
$$
  \left|\left\langle  \frac{\sign(\alpha - \langle v^{(t)}, x \rangle) x }{|\alpha -  \langle v^{(t)}, x \rangle| + \zeta}   , \frac{ w_1 - v^{(t)}}{\| w_1 - v^{(t)}\|_2} \right\rangle \right|
	\leq 
	\frac{\left|\left\langle\frac{ w_1 - v^{(t)}}{\| w_1 - v^{(t)}\|_2}, x  \right\rangle\right|}{\zeta}.
$$

Using an elementary concentration bound of Gaussian random variables, we know that with $\text{poly}\left(\frac{1}{\zeta}, \frac{1}{p_{\min}}, \sigma\right)$ examples, the estimated gradient $\tilde{\nabla} g(v^{(t)})$ satisfies with high probability that
$$
  \| \tilde{\nabla } g(v^{(t)}) \|_2 \leq \frac{4 \sqrt{d}}{\zeta}, 
	\quad 
	\left\langle -\tilde{ \nabla} g(v^{(t)}) , \frac{ w_1 - v^{(t)}}{\| w_1 - v^{(t)}\|_2}  \right\rangle \geq \frac{p_{\min}}{64 \zeta}.
$$ 


Then when $\eta_t = c\frac{\zeta p_{\min} \| w_1 - v^{(t)}\|_2}{ d}$ for a sufficiently small constant $c>0$, and using the assumptions on $v^{(0)}$ and $\Delta$ to satisfy the condition (\ref{eqn:grad_condition}), by induction, we have
$$
 \| w_1 - v^{(t + 1)}\|_2^2 \leq \left( 1 - \Omega\left( \frac{p_{\min}^2}{ d} \right)\right) \| w_1 - v^{(t)} \|_2^2
$$
completing the proof.
\end{proof}

\section{Proof for Learning All the weights} \label{sec:proof_all}

%\maintheorem* % doesn't work
%\begin{restatable}[Main]{theorem}{maintheorem}\label{thm:main} 
\noindent
\textbf{Theorem~\ref{thm:main} (Main)} 
{\it
Assume the model~(\ref{def:mlr}) and assumptions \textbf{(A1)}-\textbf{(A3)}. Then Algorithm~\ref{alg:mlr} takes $N=d \log\left(\frac{d}{\veps}\right)\cdot \left(\frac{\sigma}{\Delta p_{\min}} \right)^{O(k)} +  \left( \frac{\sigma }{\Delta p_{\min} \veps} \right)^{O(k^2)}$ data points and in time $Nd \cdot \textrm{polylog}(k, d, \sigma, \frac{1}{\Delta}, \frac{1}{p_{\min}}, \frac{1}{\veps}) $  outputs a set of vectors $\{v_i\}_{i=1}^k$ that with high probability satisfy
$$
  \|v_i  - w_{\pi(i)} \|_2 \le \veps, \forall i \in [k], ~\mbox{for some permutation $\pi$}.
$$  
}
%\end{restatable}


\begin{proof}[Proof of Theorem~\ref{thm:main}]
The theorem follows from Lemma~\ref{lem:gradient} and Lemma~\ref{lem:one_iter}, the guarantees for the two subroutines used. Note that we recovers each weight up to $\veps_g \le \left(\frac{p_{\min} \Delta}{\sigma d}\right)^{\Omega(k^2)}$. 
Therefore, only a $\left(\frac{p_{\min} \Delta}{\sigma d}\right)^{\Omega(k^2)}$ fraction of data points from this component are not removed, and only a $\left(\frac{p_{\min} \Delta}{\sigma d}\right)^{\Omega(k^2)}$ fraction of data points from other components get removed. These only causes polynomially small errors to the quantities computed in later steps and can be tolerated by our analysis. 
\end{proof}

\section{Tools}


We shall use the following bounds on the Gaussian moments and it's concentration.

\begin{lemma}
Let $g \sim \mathcal{N}(0, \bI)$, then for every unit vector $w$, we have that for every non-negative integer $p$, 
$$
 \E\left[\langle w, g \rangle^{2p} g g^{\top}\right] =  (2p + 1)!! w w^{\top} + (2p - 1)!! (\bI - w w^{\top} ).
$$
\end{lemma}


Using a standard Matrix Bernstein bound, we can get:
\begin{lemma}[Gaussian sample bound]\label{lem:gsb}
Let $g \sim \mathcal{N}(0, \bSigma^2)$, let $g_1, \cdots, g_m$ be $m$ independent samples of $g$.  Then for every vector $w$ and every non-negative integer $p$ and every $\delta > 0$, we have that 
\begin{align}
\Pr\left[\left\|\frac{1}{m}\sum_{i = 1}^m \langle w, g_i \rangle^{2p} g_i g_i^{\top} - \E\left[\langle w, g \rangle^{2p} g g^{\top}\right] \right\|_2 = \Omega\left( \sqrt{  \frac{\| \bSigma w \|_2^{4p} \left\| \bSigma \right\|^4_2 d \log \frac{1}{\delta}}{ m} } \right)\right] \leq \delta
\end{align}
\end{lemma}




The following lemma gives an estimation of a (modified) inverse Gaussian, which is used for analyzing the gradient descent step of our algorithm.

\begin{lemma}\label{lem:inverse_gaussian}
Suppose $y \sim \mathcal{N}(0, \bI)$. For every $\zeta > 0$, for every vectors $a, b \in \mathbb{R}^d$, with $\rho = \frac{\langle a, b \rangle}{\|a\|_2 \|b\|_2}$,
$$ 
  \frac{1}{4}\frac{\rho \|a\|_2}{\zeta + \|b\|_2}  \leq \E\left[\frac{\sign(\langle b, y \rangle)\langle a , y \rangle}{| \langle b, y \rangle |+ \zeta} \right]  \leq \frac{\rho \|a\|_2}{\|b\|_2} \leq \frac{ \|a\|_2}{\|b\|_2}.
$$
\end{lemma}


\begin{proof}[Proof of Lemma \ref{lem:inverse_gaussian}]
Without loss of generality assume $b = \|b\|_2 e_1$ and $a =  \|a\|_2 (\rho e_1 + \sqrt{1 - \rho^2} e_2 )$. Then 
\begin{align*}
\E\left[\frac{\sign(\langle b, y \rangle)\langle a , y \rangle}{| \langle b, y \rangle |+ \zeta} \right]&= \E\left[\frac{ \|a\|_2 (\rho y_1+ \sqrt{1 - \rho^2} y_2 ) \sign(y_1)}{ \| b\|_2 |y_1| + \zeta} \right] 
\\
& = \rho \|a\|_2 \E\left[ \frac{|y _1|}{\|b\|_2 |y_1 |+ \zeta} \right]
\end{align*}
We know that 
$$  
  \frac{|y _1|}{\|b\|_2 |y_1 |+ \zeta} \leq \frac{1}{\|b\|_2},
$$
and when $|y_1| \ge 1$
$$
  \frac{|y _1|}{\|b\|_2 |y_1 |+ \zeta} \ge \frac{1}{\zeta + \|b\|_2}.
$$
%On the other hand,
%$$ \frac{|y _1|}{\|b\|_2 |y_1 |+ \zeta} \geq  \left\{ \begin{array}{ll}
        %\frac{1}{\zeta + \|b\|_2} & \mbox{if $|y_1| \geq 1$};\\
          %\frac{|y_1 |}{\zeta + \|b\|_2} & \mbox{if $|y_1| < 1$}.\end{array} \right.  $$
Therefore, we have
$$ \frac{1}{4}\frac{\rho \|a\|_2}{\zeta + \|b\|_2}  \leq \E\left[\frac{\sign(\langle b, y \rangle)\langle a , y \rangle}{| \langle b, y \rangle |+ \zeta} \right]  \leq \frac{\rho \|a\|_2}{\|b\|_2}.
$$
where the first inequality follows from $\E[1_{|y_1| \ge 1}] \ge 1/4$.
\end{proof}

We will also need the Gap-Free Wedin Theorem from~\citep{allen2016lazysvd}. 

\begin{lemma}[Gap-Free Wedin Theorem, Lemma B.3 in~\citep{allen2016lazysvd}] \label{lem:gapfree_wedin}
For $\veps \ge 0$, let $A, B$ be two PSD matrices such that $\|A - B\|_2 \le \veps$. For every $\mu \ge 0, \tau > 0$, let $U$ be the column orthonormal matrix consisting of eigenvectors of $A$ with eigenvalue $\le \mu$, let $V$ be column orthonormal matrix consisting of eigenvectors of $B$ with eigenvalue $\ge \mu + \tau$, then we have:
$$
  \| U\top V\| \le \frac{\epsilon}{\tau}.
$$
\end{lemma}