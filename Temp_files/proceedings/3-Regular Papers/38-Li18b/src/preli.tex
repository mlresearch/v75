\section{Problem Definition and Our Result} \label{sec:preli}

In the Mixtures of Linear Regressions (MLR) model, the data $(x, \alpha) \in \mathbb{R}^{d+1}$ is generated by
\begin{align} \label{def:mlr}
  z \sim \mbox{multinomial}(p), ~x \sim \mathcal{D}_z,~ \alpha = \langle w_z, x \rangle
\end{align}
where $p \in \mathbb{R}^k$ is the proportion of different components satisfying $\sum_{i=1}^k p_i=1$, $\mathcal{D}_i$ is the distribution of the $i$-th component, and $\{w_i \in \mathbb{R}^d\}_{i=1}^k $ are the ground truth parameters. The goal is then to recover $\{w_i\}_i$ given a dataset $\{(x_\ell, \alpha_\ell)\}_{\ell=1}^{N}$, where each $(x_\ell, \alpha_\ell)$ is i.i.d. generated by (\ref{def:mlr}).


\paragraph{Notations.} $[k]$ is used to denote the set $\{1, 2, \ldots, k\}$. With high probability or w.h.p.\ means with probability $1 - d^{-C}$ for some sufficiently large constant $C>1$. $1_\set{E}$ is the indicator function of the event $\set{E}$.


\paragraph{Assumptions.} We make the following assumptions about the distributions $\mathcal{D}_i$'s and $w_i$'s.
\begin{enumerate}
\item[\textbf{(A1)}] Each $\mathcal{D}_i = \mathcal{N}(0, \bSigma_i^2)$, where $\bI \preceq  \bSigma_i \preceq \sigma \bI$ for some $\sigma \ge 1$.

\item[\textbf{(A2)}] For every $i \in [k]$, $p_i \ge p_{\min}$ for some $p_{\min} > 0$.

\item[\textbf{(A3)}] Each $\| w_i\|_2 \leq1$, and for some $\Delta \in (0,1)$, 
$
  \| w_i - w_j \|_2 \geq \Delta
$
for any $i \neq j \in [k]$.
\end{enumerate}

Assumption \textbf{(A1)} allows the data $x$ in different components to come from Gaussian distributions with different unknown covariances.\footnote{In the standard linear regression model, the covariance of $x$ can be assumed to be the identity by doing a linear transformation. However, in the mixture of linear regression models, different components have different covariances and thus can not be simultaneously transformed to the identity since which data point comes from which component is unknown.}
This is more general than all the previous works that assume they all come from the standard Gaussian distribution. This also causes difficulties in applying known techniques for MLR, and thus requires new algorithmic approaches.  Moreover, our result can also be easily generalized to the case that the mixtures come from \emph{different} subspaces. That is, there can be zero singular values for $\Sigma_i$'s and the \emph{non-zero} singular values of each component is in $[1, \sigma]$. 

Assumption \textbf{(A2)} controls the imbalance of the components. We should require that there are enough data from each component so that it is possible to recover the corresponding parameter. On the other hand, our technique can also be generalized to the case when there is enough difference between the probabilities. In this case, we could also treat some components as noise and only recover the leading ones. 

Assumption \textbf{(A3)} assumes that the ground truth parameters are separated vectors, which is indeed required for exact recovery. Previous works also in general have some form of separation assumptions, many of which are much more sophisticated than ours (e.g.,~\citep{zhong2016mixed,yi2016solving}). 

\paragraph{Our result.} 
We are now ready to present our result formally.

%
%\begin{restatable}[Main]{theorem}{maintheorem}
%\label{thm:main} 
%Assume the model and the assumptions. Then there is an algorithm that takes $N=d \log\left(\frac{d}{\veps}\right)\cdot \left(\frac{\sigma}{\Delta p_{\min}} \right)^{O(k)} +  \left( \frac{\sigma }{\Delta p_{\min} \veps} \right)^{O(k^2)}$ data points and in time $Nd \cdot \textrm{polylog}(k, d, \sigma, \frac{1}{\Delta}, \frac{1}{p_{\min}}, \frac{1}{\veps}) $  outputs a set of vectors $\{v_i\}_{i=1}^k$ that with high probability satisfy
%$$
  %\|v_i  - w_{\pi(i)} \|_2 \le \veps, \forall i \in [k], ~\mbox{for some permutation $\pi$}.
%$$ 
%\end{restatable}
%
%
%\begin{corollary}
%\label{cor:main} 
%In the simpler setting where $k,\sigma, p_{\min}, \Delta$ are constants, there is an algorithm that takes $N=O\left(d \log\left(\frac{d}{\veps}\right)\right) +  \textrm{poly}(1/\veps)$ data points and in time $Nd \cdot \textrm{polylog}(d, \frac{1}{\veps}) $  outputs a set of vectors $\{v_i\}_{i=1}^k$ that with high probability satisfy
%$$
  %\|v_i  - w_{\pi(i)} \|_2 \le \veps, \forall i \in [k], ~\mbox{for some permutation $\pi$}.
%$$ 
%\end{corollary}
%

\begin{restatable}[Main]{theorem}{maintheorem}
\label{thm:main} 
Assume the model~(\ref{def:mlr}) and assumptions \textbf{(A1)}-\textbf{(A3)}. Then Algorithm~\ref{alg:mlr} takes 
%$N=d \log\left(\frac{d}{\veps}\right)\cdot \left(\frac{\sigma}{\Delta p_{\min}} \right)^{O(k)} +  \left( \frac{\sigma }{\Delta p_{\min} \veps} \right)^{O(k^2)}$ 
$N=d \log\left(\frac{d}{\veps}\right)\cdot \textrm{poly}\left(\frac{k\sigma}{\Delta p_{\min}} \right) +  \left( \frac{\sigma }{\Delta p_{\min} } \right)^{O(k^2)}$ 
data points and in time $Nd \cdot \textrm{polylog}(k, d, \sigma, \frac{1}{\Delta}, \frac{1}{p_{\min}}, \frac{1}{\veps}) $  outputs a set of vectors $\{v_i\}_{i=1}^k$ that with high probability satisfy
$$
  \|v_i  - w_{\pi(i)} \|_2 \le \veps, \forall i \in [k], ~\mbox{for some permutation $\pi$}.
$$ 
\end{restatable}

The theorem shows that the proposed algorithm achieves global convergence. The run time is polylog in $1/\veps$ for recovery error $\veps$, i.e., the algorithm can achieve exact recovery efficiently. Furthermore, in the case where $k, \sigma,$ $p_{\min}$, and $\Delta$ are fixed constants, the sample complexity is nearly linear in the dimension $d$ of the data space, which is nearly optimal in the key parameter $d$. 
%This case already subsumes the settings studied in previous works. 
The algorithm still works for wider range of $k, \sigma$, $p_{\min}$, and $\Delta$, but with an exponential dependence on $k$. 
%The dependence on $d$ and $k$ still matches the best known guarantees~\citep{zhong2016mixed}, while our results hold for the more general setting with different variances.
%
%We also note that if we use a different existing algorithm as a subroutine in Algorithm~\ref{alg:1_d} then $N = d 
%\log\left(\frac{d}{\veps}\right)\cdot \left(\frac{\sigma}{\Delta p_{\min}} \right)^{O(k)} + 
%\log\left(\frac{d}{\veps}\right)\cdot \left(\frac{\sigma}{\Delta p_{\min}} \right)^{O(k^2)}$. In any case we achieve $N = d \log\left(\frac{d}{\veps}\right)\cdot \left(\frac{\sigma}{\Delta p_{\min}} \right)^{O(k)} + n$ where 
%%$n = \min\left\{\log\left(\frac{d}{\veps}\right)\cdot \left(\frac{\sigma}{\Delta p_{\min}} \right)^{O(k^2)}, \textrm{poly}(\frac{\sigma k}{\Delta p_{\min} \veps}) + \left(k \log \frac{\sigma k}{\Delta p_{\min} \veps} \right)^{O(k^4)}\right\}$ 
%$n$ is a minor term. 


Table~\ref{tab:previous} shows the comparison with some recent works.
Since for $k=2$ our settings and results subsumes the existing ones, we mainly compare to previous works handling multiple components $k \ge 2$. Algorithms using the tensor method have $\text{poly}(1/\veps)$ dependence~\citep{chaganty2013spectral,yi2014alternating,sedghi2016provable}.
This can be improved by using tensor method only for initialization. 
\citep{zhong2016mixed} provided such an algorithm fixed parameter tractable in the number of components, achieving $N = \tilde{O}(k^k d)$ sample complexity and $\tilde{O}(Nd)$ computational complexity. However, the result is only for the case where the components have data $x$ from the same distribution $\set{D}_i = \mathcal{N}(0, \bI)$. \citep{yi2016solving} provided an algorithm with sample complexity nearly linear in $d$ and polynomial in $k$ but again it is only for the case with $\set{D}_i = \mathcal{N}(0, \bI)$, and furthermore, the sample complexity depends on the minimal singular value of certain moment matrix, which can also be  $\left( \frac{1}{\Delta} \right)^{k}$ small in our setting.  
\citep{sedghi2016provable} provided algorithms for the case where there are $k\geq 2$ components and $\mathcal{D}_i$ are the same (but can be distributions other than Gaussians). It is based on tensor methods and when applied to Gaussian inputs has high sample and computational complexity. 

%Comparison with some recent works are presented in Table~\ref{tab:previous}, and some discussions involving technical details are deferred to Section~\ref{sec:discussion}. 

We also note that it is interesting to compare to results for learning mixture of Gaussians. When the covariance matrix is not axis-aligned, to the best of our knowledge, there is no algorithm for learning  mixture of Gaussians with sample complexity linear in the dimension. Thus, solving the mixture of Gaussian first and then rescale the covariances to identity would clearly fail in our setting. Our result shows how to make use of this small amount of side information (the label $\alpha$) to lower the sample and computational complexity significantly. We refer to for example~\citep{ashtiani2017sample} for some discussions. 


%1. \citep{yi2016solving}
%2. \citep{zhong2016mixed}
%3. tensor: \citep{chaganty2013spectral,sedghi2016provable} 
%4. convex: \citep{chen2014convex}
%5. EM \citep{klusowski2017estimating}

\begin{table}
	\centering
\scriptsize
		\begin{tabular}{c| c| c | c}
		\hline
			    & main model assumptions  &  sample complexity $N$   & computational complexity \\
		 \hline
\multirow{2}{*}{\citep{yi2016solving}}  &  $\set{D}_i = \set{N}(0, \bI), k \ge 2$,  separation $\Delta > 0$,  & \multirow{2}{*}{ $\text{poly}(k) \frac{d}{\sigma_k^5 \Delta^2} $ }          &   \multirow{2}{*}{$\text{poly}(k) d^3$ } \\
    & singular value of some moment matrix $\sigma_k$ & & 
\\ \hline
\citep{zhong2016mixed}  &  $\set{D}_i = \set{N}(0, \bI), k \ge 2$, separation $\Delta > 0$          &  $ O(d (k \log(d))^k)$ & $O(Nd \log(d/\veps))$ 
\\ \hline
%3. tensor: \citep{chaganty2013spectral} &   & 
%\\ \hline
\multirow{2}{*}{\citep{sedghi2016provable}}  &  $\set{D}_i$ are the same, $k \ge 2$,  &  \multirow{2}{*}{$O\left(\frac{k^4 d^3}{\veps^2 s^2}\right) $ for Gaussian input} & \multirow{2}{*}{much higher than $\tilde{O}(d^2)$ }
\\
& singular values of weight matrix $\ge s>0$ & &
\\ \hline
%4. convex: \citep{chen2014convex}  & & 
%\\ \hline
\multirow{2}{*}{\citep{klusowski2017estimating}} & $\set{D}_i = \set{N}(0, \bI)$, $k = 2$, & \multirow{2}{*}{$\tilde{O}(d)$}  & \multirow{2}{*}{$\tilde{O}(Nd)$ }
\\
& local convergence of EM algorithm  & &
\\ \hline \hline
		 \multirow{2}{*}{Ours}   &  $\set{D}_i = \set{N}(0, \bSigma_i^2), \bI \preceq \bSigma_i \preceq \sigma\bI, k \ge 2$, &  \multirow{2}{*}{$ d \log\left(\frac{d}{\veps}\right) \textrm{poly}\left(\frac{k\sigma}{\Delta} \right)$ + minor term}   & \multirow{2}{*}{$\tilde{O}(Nd)$  }
		\\
		& separation $\|w_i - w_j\| \ge \Delta > 0 (\forall i\neq j)$&  
		\\
		\hline
		\end{tabular}
	\caption{Comparison with some recent related works. Please refer to the papers for details about the model assumptions and dependence on some other less important parameters, which are omitted here for clarity. In particular, the separation parameters in the related work have different meaning from ours and more complicated. \yingyu{k2}}
	\label{tab:previous}
\end{table}
\normalsize

