\section{Introduction}\label{sec:intro}
This paper studies the problem of learning Mixtures of Linear Regressions (MLR). In this model, one is given i.i.d.\ observations from a mixture of $k$ unknown linear regression components, and the goal is to recover the hidden parameters in the $k$ linear regressions.
In particular, each component $i$ has a sampling probability $p_i$, a data distribution $\set{D}_i$, a hidden parameter $w_i$, and each observation $(x,\alpha)$ is generated by first sampling a component $i$ according to $p_i$'s, then sampling $x$ from $\set{D}_i$ and setting $\alpha = \langle x, w_i\rangle$. 


% existing results and drawbacks 
The MLR model is a popular mixture model and has many applications due to its effectiveness in capturing non-linearity and its model simplicity~\citep{de1989mixtures,jordan1994hierarchical,faria2010fitting,zhong2016mixed}. It has also been a recent theoretical topic for analyzing benchmark algorithms for nonconvex optimization (e.g., \citep{chaganty2013spectral,klusowski2017estimating}) or designing new algorithms~(e.g., \citep{chen2014convex}).
However, most of the existing works either restrict to very special settings (e.g., $x$ of different components all from the standard Gaussian, or only $k=2$ components)~\citep{chen2014convex,yi2014alternating,zhong2016mixed,balakrishnan2017statistical,klusowski2017estimating}, or have high sample or computational complexity far from optimal~\citep{chaganty2013spectral,sedghi2016provable}. 

Moreover, to the best of our knowledge, all the existing works require the $\mathcal{D}_i$ being identical. Most works requiring them to be the standard Gaussian, with the exception of those using tensor methods.
However, since the ultimate goal of MLR is to use different linear classifiers to capture different types of data points, it is important to allow different types to have different covariances, and was mentioned as an important open problem in \citep{sedghi2016provable}. 
%In this paper, we make a first step towards addressing this problem. 

% our contributions
We propose a novel fixed parameter tractable algorithm for learning Mixtures of Linear Regressions in a setting significantly more general than those in previous works. 
In particular, our setting allows $k \geq 2$ components of data from different distributions $\mathcal{D}_i = \mathcal{N}(0, \bSigma_i^2)$ with $\bI \preceq \bSigma_i \preceq \sigma \bI$, and only requires a necessary separation between the ground truth parameters that any two weight parameters should be at least $\Delta$ apart for some separation parameter $\Delta$.
The algorithm can recover the ground truth to any additive error $\veps$ using 
%$N = d \log\left( \frac{d}{\veps}\right) \left(\frac{\sigma}{p_{\min} \Delta}\right)^{O(k)}  + n$  
$N = d \log\left( \frac{d}{\veps}\right) \textrm{poly}\left(\frac{k\sigma}{p_{\min} \Delta}\right)  + n$  
examples and $Nd \cdot \textrm{polylog}(k,d,\sigma,\frac{1}{\veps}, \frac{1}{\Delta}, \frac{1}{p_{\min}})$ computational time, where $p_{\min} = \min_i p_i$ and $n$ is a minor term for fixed $k$. It is tractable in the number of components $k$, the bound on the differences between the different variances $\sigma$, the separation parameter $\Delta$, and the minimum proportion $p_{\min}$ of the components. When these parameters are fixed, it can recover the ground truth to any additive error $\veps$, with nearly optimal sample complexity which is nearly linear in $d$, % and $\log(1/\veps)$ ignoring the minor term, 
and with nearly optimal computational complexity which is nearly linear in $Nd$. % and $\textrm{polylog}(1/\veps)$.
%See Section~\ref{sec:preli} for more details of the result and Section~\ref{sec:algo} for the description of the algorithm. 

Novel algorithmic techniques are proposed since existing ones are not known to generalize to this setting. 
One main technical contribution of our work is a new ``method of moments descent'' technique, that allows us to break ties between different mixture components \emph{gradually}: Unlike most of the previous algorithms which use method of moments to obtain a warm start in one shot, we use it to find a direction to perform one ``gradient descent'' step and gradually refine our solution.  We believe our  techniques are potentially useful in even more general cases.
%See Section~\ref{sec:proofsketch} and~\ref{sec:algo} for more intuitions and see the appendix for the complete proof. 

%We point out that unlike in standard linear regression model, where the covariance of $x$ can be assumed to be identity by doing a linear transformation, in mixture of linear models, different components have different covariance, and thus can not be simultaneously transformed to identity since we do not know which data point comes from which component. Indeed, since the ultimate goal of MLR is to use different linear classifiers to capture different types of data points, it is important to allow different components to have different covariances. In this paper, we make a first step towards addressing this problem. 


%(see especially the related work in Mixed Linear Regressions with Multiple Components; Estimating the coefficients of a mixture of two linear regressions by expectation maximization)
%
%0. all are for standard gaussians (need to check: Spectral Experts for Estimating Mixtures of Linear Regressions; Provable Tensor Methods for Learning Mixtures of Generalized Linear Models)
%
%1. some are for restricted conditions: 2 components 
%
%2. most have high complexity (need to note that Mixed Linear Regression with Multiple Components has near-optimal but uses tensor method and thus cannot handle more general settings, also their assumptions are quite complicated)
%
%3. some focuses on specific algorithms like EM
%
%
%our methods/results 
%
%1. designed a simple algorithm
%
%2. general and intuitive assumptions
%
%3. shown its global convergence and near-optimal complexity


\paragraph{Organization.} 
Section~\ref{sec:related} reviews the related work, and 
Section~\ref{sec:preli} formalizes the problem and presents our result. An overview of the intuition for designing and analyzing the algorithm is provided in Section~\ref{sec:proofsketch} while the algorithm and the key lemmas are presented in Section~\ref{sec:algo}. The formal proofs are provided in the appendix.
