\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{raskutti2010restricted,rudelson2013reconstruction,sivakumar2015beyond,oliveira2016lower,lecue2017sparse}
\citation{tibshirani1996regression}
\citation{candes2007dantzig}
\citation{candes2005decoding}
\citation{eldar2012compressed,hastie2015statistical}
\jmlr@workshop{31st Annual Conference on Learning Theory}
\jmlr@title{Restricted Eigenvalue from Stable Rank \\with Applications to Sparse Linear Regression}{Restricted Eigenvalue from Stable Rank \\with Applications to Sparse Linear Regression}
\jmlr@author{\Name {Shiva Prasad Kasiviswanathan}\Email {kasivisw@gmail.com}\\ \addr {Amazon AWS AI, Palo Alto, CA, USA. } \AND \Name {Mark Rudelson}\thanks {Partially supported by NSF grant, DMS-1464514.} \Email {rudelson@umich.edu }\\\addr {University of Michigan, Ann Arbor, MI, USA}}{\Name {Shiva Prasad Kasiviswanathan}\Email {kasivisw@gmail.com}\\ \addr {Amazon AWS AI, Palo Alto, CA, USA. } \AND \Name {Mark Rudelson}\thanks {Partially supported by NSF grant, DMS-1464514.} \Email {rudelson@umich.edu }\\\addr {University of Michigan, Ann Arbor, MI, USA}}
\newlabel{jmlrstart}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.0.1}}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.0.1}{}}
\newlabel{eqn:linear}{{1}{1}{Introduction}{equation.0.1.1}{}}
\citation{bickel2009simultaneous}
\citation{bickel2009simultaneous}
\citation{mendelson2008uniform,adamczak2011restricted,rudelson2008sparse,bourgain2011explicit,cheraghchi2011coding}
\citation{raskutti2010restricted,rudelson2013reconstruction,sivakumar2015beyond,oliveira2016lower,lecue2017sparse}
\citation{dobriban2016regularity}
\citation{de2014optimal}
\citation{raskutti2010restricted,rudelson2013reconstruction}
\citation{bickel2009simultaneous,wainwright2009sharp}
\newlabel{eqn:REthm}{{2}{3}{Introduction}{equation.0.1.2}{}}
\citation{bickel2009simultaneous}
\citation{raskutti2011minimax}
\citation{bourgain2011explicit,bandeira2017conditional,bandeira2016derandomizing}
\citation{raskutti2010restricted}
\citation{rudelson2013reconstruction}
\citation{sivakumar2015beyond,oliveira2016lower,lecue2017sparse}
\citation{rudelson2013reconstruction}
\citation{rudelson2013reconstruction}
\citation{hastie2015statistical}
\citation{zhou2009compressed}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A distributed data setting, where $n$ devices generating $(\mathbf  x_1,y_1),\dots  ,(\mathbf  x_n,y_n)$ are sending a compressed representation $(\Phi \mathbf  x_1,y_1), \dots  ,(\Phi \mathbf  x_n,y_n)$ to the cloud server, which then computes the regression parameter.}}{5}{figure.1}}
\newlabel{fig:skconv}{{1}{5}{A distributed data setting, where $n$ devices generating $(\x _1,y_1),\dots ,(\x _n,y_n)$ are sending a compressed representation $(\Phi \x _1,y_1), \dots ,(\Phi \x _n,y_n)$ to the cloud server, which then computes the regression parameter}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Related Work}{5}{subsection.0.1.1}}
\newlabel{sec:related}{{1.1}{5}{Related Work}{subsection.0.1.1}{}}
\citation{lee2015communication}
\citation{TCS-060}
\citation{maillard2009compressed,fard2012compressed,kaban2014new}
\citation{bickel2009simultaneous}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Preliminaries}{6}{subsection.0.1.2}}
\citation{negahban2012unified}
\citation{bickel2009simultaneous}
\citation{wainwright2009sharp}
\citation{tibshiranisparsity}
\newlabel{def:RE}{{1}{7}{Preliminaries}{theorem.1}{}}
\newlabel{thm:lassoanalysis}{{2}{7}{Preliminaries}{theorem.2}{}}
\newlabel{rem:assumptions}{{3}{7}{Preliminaries}{theorem.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Restricted Eigenvalue from Stable Rank}{7}{section.0.2}}
\newlabel{sec:REfromSR}{{2}{7}{Restricted Eigenvalue from Stable Rank}{section.0.2}{}}
\newlabel{def:REmod}{{4}{8}{Restricted Eigenvalue from Stable Rank}{theorem.4}{}}
\newlabel{thm:RE}{{5}{8}{Restricted Eigenvalue from Stable Rank}{theorem.5}{}}
\newlabel{cor:RE}{{7}{8}{Restricted Eigenvalue from Stable Rank}{theorem.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Proof of Theorem\nobreakspace  {}\ref  {thm:RE}: Technical Ingredients}{9}{subsection.0.2.1}}
\newlabel{sec:RE}{{2.1}{9}{Proof of Theorem~\ref {thm:RE}: Technical Ingredients}{subsection.0.2.1}{}}
\newlabel{lem: small ball product}{{8}{9}{Proof of Theorem~\ref {thm:RE}: Technical Ingredients}{theorem.8}{}}
\newlabel{lem: product large deviation}{{9}{10}{Proof of Theorem~\ref {thm:RE}: Technical Ingredients}{theorem.9}{}}
\newlabel{prop: single direction}{{10}{10}{Proof of Theorem~\ref {thm:RE}: Technical Ingredients}{theorem.10}{}}
\newlabel{thm: global}{{11}{10}{Proof of Theorem~\ref {thm:RE}: Technical Ingredients}{theorem.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Applications to Sparse Linear Regression}{11}{section.0.3}}
\newlabel{sec:applications}{{3}{11}{Applications to Sparse Linear Regression}{section.0.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Application 1: Bounding the $\ell _2$-error with Random Design $Z=X \Phi ^\top \Phi $}{11}{subsection.0.3.1}}
\newlabel{sec:app1}{{3.1}{11}{Application 1: Bounding the $\ell _2$-error with Random Design $Z=X \Phi ^\top \Phi $}{subsection.0.3.1}{}}
\newlabel{eqn:lassomod}{{3}{11}{Application 1: Bounding the $\ell _2$-error with Random Design $Z=X \Phi ^\top \Phi $}{equation.0.3.3}{}}
\newlabel{prop:app1}{{12}{11}{Application 1: Bounding the $\ell _2$-error with Random Design $Z=X \Phi ^\top \Phi $}{theorem.12}{}}
\citation{raskutti2011minimax}
\citation{bickel2009simultaneous}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Application 2: Sparse Linear Regression with Compressed Features}{12}{subsection.0.3.2}}
\newlabel{sec:compsparse}{{3.2}{12}{Application 2: Sparse Linear Regression with Compressed Features}{subsection.0.3.2}{}}
\newlabel{prop:main}{{13}{12}{Application 2: Sparse Linear Regression with Compressed Features}{theorem.13}{}}
\bibdata{pacparity}
\bibcite{adamczak2011restricted}{{1}{2011}{{Adamczak et~al.}}{{Adamczak, Litvak, Pajor, and Tomczak-Jaegermann}}}
\bibcite{bandeira2016derandomizing}{{2}{2016}{{Bandeira et~al.}}{{Bandeira, Fickus, Mixon, and Moreira}}}
\bibcite{bandeira2017conditional}{{3}{2017}{{Bandeira et~al.}}{{Bandeira, Mixon, and Moreira}}}
\bibcite{bickel2009simultaneous}{{4}{2009}{{Bickel et~al.}}{{Bickel, Ritov, and Tsybakov}}}
\bibcite{bourgain2011explicit}{{5}{2011}{{Bourgain et~al.}}{{Bourgain, Dilworth, Ford, Konyagin, Kutzarova, et~al.}}}
\bibcite{candes2007dantzig}{{6}{2007}{{Candes et~al.}}{{Candes, Tao, et~al.}}}
\bibcite{candes2005decoding}{{7}{2005}{{Candes and Tao}}{{}}}
\bibcite{cheraghchi2011coding}{{8}{2011}{{Cheraghchi}}{{}}}
\bibcite{de2014optimal}{{9}{2014}{{De~Castro}}{{}}}
\bibcite{dobriban2016regularity}{{10}{2016}{{Dobriban and Fan}}{{}}}
\bibcite{eldar2012compressed}{{11}{2012}{{Eldar and Kutyniok}}{{}}}
\bibcite{fard2012compressed}{{12}{2012}{{Fard et~al.}}{{Fard, Grinberg, Pineau, and Precup}}}
\bibcite{hanson1971bound}{{13}{1971}{{Hanson and Wright}}{{}}}
\bibcite{hastie2015statistical}{{14}{2015}{{Hastie et~al.}}{{Hastie, Tibshirani, and Wainwright}}}
\bibcite{kaban2014new}{{15}{2014}{{Kab{\'a}n}}{{}}}
\bibcite{lecue2017sparse}{{16}{2017}{{Lecu{\'e} and Mendelson}}{{}}}
\bibcite{lee2015communication}{{17}{2015}{{Lee et~al.}}{{Lee, Sun, Liu, and Taylor}}}
\bibcite{maillard2009compressed}{{18}{2009}{{Maillard and Munos}}{{}}}
\bibcite{mendelson2008uniform}{{19}{2008}{{Mendelson et~al.}}{{Mendelson, Pajor, and Tomczak-Jaegermann}}}
\bibcite{negahban2012unified}{{20}{2012}{{Negahban et~al.}}{{Negahban, Ravikumar, Wainwright, and Yu}}}
\bibcite{oliveira2016lower}{{21}{2016}{{Oliveira}}{{}}}
\bibcite{raskutti2010restricted}{{22}{2010}{{Raskutti et~al.}}{{Raskutti, Wainwright, and Yu}}}
\bibcite{raskutti2011minimax}{{23}{2011}{{Raskutti et~al.}}{{Raskutti, Wainwright, and Yu}}}
\bibcite{rudelson2008sparse}{{24}{2008}{{Rudelson and Vershynin}}{{}}}
\bibcite{RVHanson-Wright}{{25}{2013}{{Rudelson and Vershynin}}{{}}}
\bibcite{rudelson2013reconstruction}{{26}{2013}{{Rudelson and Zhou}}{{}}}
\bibcite{sivakumar2015beyond}{{27}{2015}{{Sivakumar et~al.}}{{Sivakumar, Banerjee, and Ravikumar}}}
\bibcite{tibshirani1996regression}{{28}{1996}{{Tibshirani}}{{}}}
\bibcite{tibshiranisparsity}{{29}{2015}{{Tibshirani and Wasserman}}{{}}}
\bibcite{wainwright2009sharp}{{30}{2009}{{Wainwright}}{{}}}
\bibcite{TCS-060}{{31}{2014}{{Woodruff}}{{}}}
\bibcite{zhou2009compressed}{{32}{2009}{{Zhou et~al.}}{{Zhou, Lafferty, and Wasserman}}}
\citation{hastie2015statistical}
\@writefile{toc}{\contentsline {section}{\numberline {A}Additional Preliminaries}{15}{section.0.A}}
\newlabel{app:addl}{{A}{15}{Additional Preliminaries}{section.0.A}{}}
\newlabel{eqn:sparsereg}{{4}{15}{Additional Preliminaries}{equation.0.A.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Lasso Regression.}{15}{equation.0.A.4}}
\newlabel{prop:epsnet}{{15}{15}{Lasso Regression}{theorem.15}{}}
\citation{hanson1971bound}
\citation{RVHanson-Wright}
\citation{RVHanson-Wright}
\newlabel{def:subgauss}{{16}{16}{Lasso Regression}{theorem.16}{}}
\newlabel{def:subgaussnorm}{{17}{16}{Lasso Regression}{theorem.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Complete Proof of Theorem\nobreakspace  {}\ref  {thm:RE}}{16}{section.0.B}}
\newlabel{app:compproof}{{B}{16}{Complete Proof of Theorem~\ref {thm:RE}}{section.0.B}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Hanson-Wright Preliminaries}{16}{subsection.0.B.1}}
\newlabel{app: preliminaries}{{B.1}{16}{Hanson-Wright Preliminaries}{subsection.0.B.1}{}}
\newlabel{thm: HW}{{18}{16}{Hanson-Wright Preliminaries}{theorem.18}{}}
\newlabel{cor: product norm}{{19}{16}{Hanson-Wright Preliminaries}{theorem.19}{}}
\newlabel{eq: product norm concentration}{{5}{17}{Hanson-Wright Preliminaries}{equation.0.B.5}{}}
\newlabel{cor: small ball}{{20}{17}{Hanson-Wright Preliminaries}{theorem.20}{}}
\newlabel{cor: product HS norm}{{21}{17}{Hanson-Wright Preliminaries}{theorem.21}{}}
\newlabel{cor: thiple product HS norm}{{22}{17}{Hanson-Wright Preliminaries}{theorem.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Bounds for a Fixed Vector}{18}{subsection.0.B.2}}
\newlabel{app: fixed vector}{{B.2}{18}{Bounds for a Fixed Vector}{subsection.0.B.2}{}}
\newlabel{applem: small ball product}{{23}{18}{Bounds for a Fixed Vector}{theorem.23}{}}
\newlabel{applem: product large deviation}{{24}{19}{Bounds for a Fixed Vector}{theorem.24}{}}
\newlabel{appprop: single direction}{{25}{20}{Bounds for a Fixed Vector}{theorem.25}{}}
\newlabel{eq: sd4}{{6}{20}{Bounds for a Fixed Vector}{equation.0.B.6}{}}
\newlabel{eq: sd3}{{7}{20}{Bounds for a Fixed Vector}{equation.0.B.7}{}}
\newlabel{eq: sd1}{{8}{21}{Bounds for a Fixed Vector}{equation.0.B.8}{}}
\newlabel{eq: sd2}{{9}{21}{Bounds for a Fixed Vector}{equation.0.B.9}{}}
\newlabel{eq: sd5}{{10}{22}{Bounds for a Fixed Vector}{equation.0.B.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Finishing the Proof of Theorem\nobreakspace  {}\ref  {thm:RE}: Net Argument}{22}{subsection.0.B.3}}
\newlabel{app: proof}{{B.3}{22}{Finishing the Proof of Theorem~\ref {thm:RE}: Net Argument}{subsection.0.B.3}{}}
\newlabel{appthm: global}{{26}{22}{Finishing the Proof of Theorem~\ref {thm:RE}: Net Argument}{theorem.26}{}}
\newlabel{eq: global1}{{11}{23}{Finishing the Proof of Theorem~\ref {thm:RE}: Net Argument}{equation.0.B.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Missing Details from Section\nobreakspace  {}\ref  {sec:applications}}{24}{section.0.C}}
\newlabel{app:applications}{{C}{24}{Missing Details from Section~\ref {sec:applications}}{section.0.C}{}}
\newlabel{appprop:app1}{{27}{24}{Missing Details from Section~\ref {sec:applications}}{theorem.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Missing Details from Section\nobreakspace  {}\ref  {sec:compsparse}}{25}{subsection.0.C.1}}
\newlabel{eqn:holder}{{12}{25}{Missing Details from Section~\ref {sec:compsparse}}{equation.0.C.12}{}}
\newlabel{applem: noise bound}{{28}{26}{Missing Details from Section~\ref {sec:compsparse}}{theorem.28}{}}
\newlabel{eq: expected}{{13}{26}{Missing Details from Section~\ref {sec:compsparse}}{equation.0.C.13}{}}
\newlabel{eq: B_2 1}{{14}{27}{Missing Details from Section~\ref {sec:compsparse}}{equation.0.C.14}{}}
\newlabel{eq: B_2 2}{{15}{28}{Missing Details from Section~\ref {sec:compsparse}}{equation.0.C.15}{}}
\newlabel{eq: B_2 3}{{16}{28}{Missing Details from Section~\ref {sec:compsparse}}{equation.0.C.16}{}}
\newlabel{rem:follow}{{29}{29}{Missing Details from Section~\ref {sec:compsparse}}{theorem.29}{}}
\newlabel{eqn:condition}{{17}{29}{Missing Details from Section~\ref {sec:compsparse}}{equation.0.C.17}{}}
\newlabel{eqn:thetahat}{{18}{30}{Missing Details from Section~\ref {sec:compsparse}}{equation.0.C.18}{}}
\citation{rudelson2013reconstruction}
\@writefile{toc}{\contentsline {section}{\numberline {D}Stable Rank vs. Restricted Eigenvalue Condition}{31}{section.0.D}}
\newlabel{app:comp1}{{D}{31}{Stable Rank vs. Restricted Eigenvalue Condition}{section.0.D}{}}
\newlabel{jmlrend}{{D}{31}{end of Restricted Eigenvalue from Stable Rank \protect \\with Applications to Sparse Linear Regression}{section*.1}{}}
