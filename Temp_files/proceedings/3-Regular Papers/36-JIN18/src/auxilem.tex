%!TEX root = main.tex

\section{Auxiliary Lemma}
In this section, we present some auxiliary lemmas which are used 
in proving Lemma \ref{lem:largegrad_nonconvex}, Lemma \ref{lem:largegrad_convex} and Lemma \ref{lem:2nd_seq}.  These deal with the large-gradient scenario (nonconvex component), the large-gradient scenario (strongly convex component), and the negative curvature scenario, respectively.

The first two lemmas establish some facts about powers of the structured 
matrices arising in~\nag.
\begin{lemma}\label{lem:aux_matrix_form}
Let the $2\times 2$ matrix $\A$ have following form, for arbitrary $a, b\in \R$:
\begin{equation*}
\A = \pmat{ a &  b \\1 & 0}.
\end{equation*}
Letting $\mu_1, \mu_2$ denote the two eigenvalues of $\A$ (can be repeated or complex eigenvalues), then, for any $t\in \N$:
\begin{align*}
\pmat{1 & 0 } \A^t =&
\left(\sum_{i=0}^t \mu_1^i \mu_2^{t-i}, \quad - \mu_1\mu_2\sum_{i=0}^{t-1} \mu_1^{i} \mu_2^{t-1-i}\right)\\
\pmat{0 & 1 } \A^t =& \pmat{1 & 0 } \A^{t-1}.
\end{align*}
% \begin{equation*}
% \pmat{1 & 0 }\A^t\pmat{1 \\ 0 }
% = \sum_{i=0}^t \mu_1^i \mu_2^{t-i}
% \quad\text{~and~}\quad
% \pmat{1 & 0 }\A^t\pmat{0 \\ -1 }
% = \mu_1\mu_2\sum_{i=0}^{t-1} \mu_1^{i} \mu_2^{t-1-i}
% \end{equation*}
\end{lemma}
\begin{proof}
When the eigenvalues $\mu_1$ and $\mu_2$ are distinct, the matrix $\A$ 
can be rewritten as $\pmat{\mu_1+\mu_2 & -\mu_1\mu_2 \\ 1 & 0 }$, and it 
is easy to check  that the two eigenvectors have the form 
$\pmat{\mu_1 \\ 1}$ and $\pmat{\mu_2 \\ 1}$. Therefore, we can 
write the eigen-decomposition as:
\begin{equation*}
\A = \frac{1}{\mu_1 - \mu_2} \pmat{\mu_1 & \mu_2 \\1 & 1}
\pmat{\mu_1 & 0 \\ 0 & \mu_2}
\pmat{1 & -\mu_2 \\ -1 & \mu_1 },
\end{equation*}
and the $t$th power has the general form:
\begin{equation*}
\A^t = \frac{1}{\mu_1 - \mu_2} \pmat{\mu_1 & \mu_2 \\1 & 1}
\pmat{\mu_1^t & 0 \\ 0 & \mu_2^t}
\pmat{1 & -\mu_2 \\ -1 & \mu_1 }
\end{equation*}

When there are two repeated eigenvalue $\mu_1$, the matrix 
$\pmat{a & b \\ 1 & 0}$ can be rewritten as $\pmat{
2\mu_1 & -\mu_1^2 \\ 1 & 0 
}$. It is easy to check that $\A$ has the following Jordan normal form:
\begin{equation*}
\A = - \pmat{\mu_1 & \mu_1+1 \\1 & 1}\pmat{\mu_1 & 1 \\0 & \mu_1}
\pmat{1 & -(\mu_1 + 1) \\-1 & \mu_1},
\end{equation*}
which yields:
\begin{equation*}
\A^t = - \pmat{\mu_1 & \mu_1+1 \\1 & 1}
\pmat{\mu^t_1 & t\mu_1^{t-1} \\0 & \mu^t_1}
\pmat{1 & -(\mu_1 + 1) \\-1 & \mu_1}.
\end{equation*}

The remainder of the proof follows from simple linear algebra 
calculations for both cases.
\end{proof}

\begin{lemma}\label{lem:aux_matrix_equality}
Under the same setting as Lemma \ref{lem:aux_matrix_form}, for any $t\in \N$: 
\begin{equation*}
(\mu_1 - 1)(\mu_2 - 1)
\pmat{1 & 0} \sum_{\tau = 0}^{t-1} \A^\tau \pmat{1 \\ 0}
 = 1 - \pmat{1 & 0}  \A^t \pmat{1 \\ 1}.
\end{equation*}
\end{lemma}

\begin{proof}
When $\mu_1$ and $\mu_2$ are distinct, we have:
\begin{equation*}
\pmat{1 & 0 } \A^t =
\left(\frac{\mu_1^{t+1} - \mu_2^{t+1}}{\mu_1 - \mu_2}, \quad - \frac{\mu_1\mu_2(\mu_1^t - \mu_2^t)}{\mu_1 - \mu_2}\right).
\end{equation*}
When $\mu_1, \mu_2$ are repeated, we have:
\begin{equation*}
\pmat{1 & 0 } \A^t =
\left((t+1)\mu_1^t, \quad -t \mu_1^{t+1}\right).
\end{equation*}
The remainder of the proof follows from Lemma \ref{lem:aux_geometric_power} 
and linear algebra.
\end{proof}

\noindent
The next lemma tells us when the eigenvalues of the~\nag~matrix are real and when they are complex.
\begin{lemma}\label{lem:aux_eigenvalues}
Let $\theta \in (0, \frac{1}{4}]$, $\x \in [-\frac{1}{4}, \frac{1}{4}]$ and
define the $2\times 2$ matrix $\A$ as follows:
\begin{equation*}
\A = \pmat{(2-\theta) (1 - x)&  -(1-\theta) (1 - x) \\ 1 & 0}
\end{equation*}
Then the two eigenvalues $\mu_1$ and $\mu_2$ of $\A$ are solutions of the
following equation:
\begin{equation*}
\mu^2 - (2-\theta)(1-x)\mu + (1-\theta)(1-x) = 0.
\end{equation*}
Moreover, when $x \in [-\frac{1}{4}, \frac{\theta^2}{(2-\theta)^2}]$, 
$\mu_1$ and $\mu_2$ are real numbers, and when
$x \in (\frac{\theta^2}{(2-\theta)^2}, \frac{1}{4}]$, 
$\mu_1$ and $\mu_2$ are conjugate complex numbers.
\end{lemma}

\begin{proof}
An eigenvalue $\mu$ of the matrix $\A$ must satisfy the following equation:
\begin{align*}
\det (\A - \mu \I) = \mu^2 - (2-\theta)(1-x)\mu + (1-\theta)(1-x) = 0.
\end{align*}
The discriminant is equal to
\begin{align*}
\Delta = &(2-\theta)^2(1-x)^2 - 4(1-\theta)(1-x) \\
= &(1-x)(\theta^2 - (2-\theta^2)x).
\end{align*}
Then $\mu_1$ and $\mu_2$ are real if and only if $\Delta \ge 0$, 
which finishes the proof.
\end{proof}

Finally, we need a simple lemma for geometric sums.
\begin{lemma}\label{lem:aux_geometric_power}
For any $\lambda >0$ and fixed $t$, we have:
\begin{equation*}
 \sum_{\tau = 0}^{t-1} (\tau +1) \lambda^\tau =
 \frac{1-\lambda^t}{(1-\lambda)^2} - \frac{t\lambda^t}{1-\lambda}.
 \end{equation*} 
\end{lemma}
\begin{proof}
Consider the truncated geometric series:
\begin{equation*}
 \sum_{\tau = 0}^{t-1} \lambda^\tau =
 \frac{1-\lambda^t}{1-\lambda}.
\end{equation*}
Taking derivatives, we have:
\begin{equation*}
 \sum_{\tau = 0}^{t-1} (\tau +1) \lambda^\tau = \frac{\mathrm{d}}{\mathrm{d}\lambda}\sum_{\tau = 0}^{t-1} \lambda^{\tau+1} =
 \frac{\mathrm{d}}{\mathrm{d}\lambda}\left[\lambda \cdot\frac{1-\lambda^t}{1-\lambda}\right]
 =\frac{1-\lambda^t}{(1-\lambda)^2} - \frac{t\lambda^t}{1-\lambda}.
\end{equation*}
\end{proof}

\subsection{Large-gradient scenario (nonconvex component)}
All the lemmas in this section are concerned with the behavior 
of the~\nag~matrix for eigen-directions of the Hessian with 
eigenvalues being negative or small and positive, as used 
in proving Lemma \ref{lem:largegrad_nonconvex}.
The following lemma bounds the smallest eigenvalue of 
the~\nag~matrix for those directions.
\begin{lemma}\label{lem:aux_nonconvex_mu2}
Under the same setting as Lemma \ref{lem:aux_eigenvalues}, 
and for $x \in [-\frac{1}{4}, \frac{\theta^2}{(2-\theta)^2}]$, 
where $\mu_1 \ge \mu_2$, we have:
\begin{equation*}
\mu_2 \le 1 - \frac{1}{2}\max\{\theta, \sqrt{|x|}\}.
\end{equation*}
\end{lemma}

\begin{proof}
The eigenvalues satisfy:
\begin{align*}
\det (\A - \mu \I) = \mu^2 - (2-\theta)(1-x)\mu + (1-\theta)(1-x) = 0.
\end{align*}
Let $\mu = 1+u$.  We have 
\begin{align*}
&& (1+u)^2 - (2-\theta)(1-x)(1+u) + (1-\theta)(1-x) &= 0 \\
\Rightarrow  && u^2 +  ((1-x)\theta + 2x) u + x&= 0.
\end{align*}
Let $f(u) = u^2 + \theta u + 2xu - x\theta u + x$.  
To prove $\mu_2(\A) \le 1 - \frac{\sqrt{|x|}}{2}$ 
when $x\in [-\frac{1}{4}, -\theta^2]$, we only need to verify $f(-\frac{\sqrt{|x|}}{2}) \le 0$:
\begin{align*}
f(-\frac{\sqrt{|x|}}{2}) = &\frac{|x|}{4} - \frac{\theta\sqrt{|x|}}{2}
+ |x|\sqrt{|x|} - \frac{|x|\sqrt{|x|}\theta}{2} - |x| \\
\le & |x|\sqrt{|x|}(1-\frac{\theta}{2}) - \frac{3|x|}{4} \le 0
\end{align*}
The last inequality follows because $|x| \le \frac{1}{4}$ by assumption.

For $x\in [-\theta^2, 0]$, we have:
\begin{align*}
f(-\frac{\theta}{2}) = \frac{\theta^2}{4} -\frac{\theta^2}{2} - x\theta
+ \frac{x\theta^2}{2} + x
= -\frac{\theta^2}{4} + x(1-\theta) + \frac{x\theta^2}{2} \le 0.
\end{align*}
On the other hand, when $x \in [0, \theta^2/(2-\theta)^2]$, both 
eigenvalues are still real, and the midpoint of the two roots is:
\begin{align*}
\frac{u_1 + u_2}{2} = -\frac{(1-x)\theta + 2x}{2}
=-\frac{\theta + (2-\theta)x}{2}
\le -\frac{\theta}{2}.
\end{align*}
Combining the two cases, we have shown that when 
$x \in [-\theta^2, \theta^2/(2-\theta)^2]$ we have 
$\mu_2 (\A) \le 1-\frac{\theta}{2}$.

In summary, we have proved that
\begin{equation*}
\mu_2(\A) \le 
\begin{cases}
1 - \frac{\sqrt{|x|}}{2}, & x \in  [-\frac{1}{4}, -\theta^2]\\
1-\frac{\theta}{2}.& x \in  [-\theta^2, \theta^2/(2-\theta)^2],
\end{cases}
\end{equation*}
which finishes the proof.
\end{proof}
\noindent
In the same setting as above, the following lemma bounds the largest eigenvalue.
\begin{lemma}\label{lem:aux_nonconvex_mu1}
Under the same setting as Lemma \ref{lem:aux_eigenvalues}, and with
$x \in [-\frac{1}{4}, \frac{\theta^2}{(2-\theta)^2}]$, and letting
$\mu_1 \ge \mu_2$, we have:
\begin{equation*}
\mu_1 \le 1 + 2 \min\{\frac{|x|}{\theta}, \sqrt{|x|}\}.
\end{equation*}
\end{lemma}
\begin{proof}
By Lemma \ref{lem:aux_eigenvalues} and Vieta's formulaï¼Œ we have:
$$(\mu_1 - 1)(\mu_2-1) = \mu_1\mu_2 - (\mu_1 + \mu_2) + 1 = x.$$
An application of Lemma \ref{lem:aux_nonconvex_mu2} finishes the proof.
\end{proof}
The following lemma establishes some properties of the powers of the~\nag~matrix.
\begin{lemma}\label{lem:aux_nonconvex_inequal}
Consider the same setting as Lemma \ref{lem:aux_eigenvalues}, and let
$x \in [-\frac{1}{4}, \frac{\theta^2}{(2-\theta)^2}]$.
Denote:
\begin{equation*}
(a_t, ~-b_t) = \pmat{1 & 0 } \A^t.
\end{equation*}
Then, for any $t \ge \frac{2}{\theta} + 1$, we have:
\begin{align*}
\sum_{\tau = 0}^{t-1} a_\tau \ge& \Omega (\frac{1}{\theta^2}) \\
\frac{1}{b_t}\left(\sum_{\tau = 0}^{t-1} a_\tau \right) \ge& \Omega(1)\min\left\{\frac{1}{\theta}, \frac{1}{\sqrt{|x|}}\right\}.
\end{align*}
\end{lemma}

\begin{proof}
We prove the two inequalities seperately.

\noindent \textbf{First Inequality:}
By Lemma \ref{lem:aux_matrix_form}:
\begin{align*}
\sum_{\tau = 0}^t \pmat{ 1 & 0 } \A^\tau \pmat{ 1 \\ 0 }
% = &\sum_{\tau = 0}^t\frac{\mu_1^{\tau+1} - \mu_2^{\tau+1}}{\mu_1 - \mu_2}
= &\sum_{\tau=0}^t \sum_{i=0}^\tau \mu_1^{\tau-i}\mu_2^{i} 
= \sum_{\tau=0}^t (\mu_1\mu_2)^{\frac{\tau}{2}}\sum_{i=0}^\tau (\frac{\mu_1}{\mu_2})^{\frac{\tau}{2} - i} \\
\ge& \sum_{\tau=0}^t [(1-\theta)(1-x)]^{\frac{\tau}{2}} \cdot \frac{\tau}{2}
\end{align*}
The last inequality holds because in $\sum_{i=0}^\tau (\frac{\mu_1}{\mu_2})^{\frac{\tau}{2}-i}$ at least $\frac{\tau}{2}$ terms are greater than one. 
Finally, since $x \le \theta^2/(2-\theta)^2 \le \theta^2\le \theta$, we have $1-x \ge 1-\theta$, thus:
\begin{align*}
\sum_{\tau=0}^t [(1-\theta)(1-x)]^{\frac{\tau}{2}} \cdot \frac{\tau}{2}
\ge & \sum_{\tau=0}^t (1-\theta)^{\tau} \cdot \frac{\tau}{2}
\ge \sum_{\tau=0}^{1/\theta} (1-\theta)^{\tau} \cdot \frac{\tau}{2}\\
\ge & (1-\theta)^{\frac{1}{\theta}}\sum_{\tau=0}^{1/\theta}  \frac{\tau}{2}
\ge \Omega(\frac{1}{\theta^2}),
\end{align*}
which finishes the proof.

\noindent \textbf{Second Inequality:}
Without loss of generality, assume $\mu_1 \ge \mu_2$. 
Again by Lemma \ref{lem:aux_matrix_form}:
\begin{align*}
\frac{\sum_{\tau = 0}^{t-1} a_\tau}{b_t}
=& \frac{\sum_{\tau = 0}^{t-1} \sum_{i=0}^{\tau} \mu_1^i \mu_2^{\tau-i}}
{\mu_1\mu_2\sum_{i=0}^{t-1} \mu_1^{i} \mu_2^{t-1-i}} 
= \frac{1}{\mu_1\mu_2}
\sum_{\tau = 0}^{t-1} \frac{\sum_{i=0}^{\tau} \mu_1^i \mu_2^{\tau-i}}
{\sum_{i=0}^{t-1} \mu_1^{i} \mu_2^{t-1-i}} \\
\ge & \frac{1}{\mu_1\mu_2}
\sum_{\tau = (t-1)/2}^{t-1} \frac{\sum_{i=0}^{\tau} \mu_1^i \mu_2^{\tau-i}}
{\sum_{i=0}^{t-1} \mu_1^{i} \mu_2^{t-1-i}} 
\ge  \frac{1}{\mu_1\mu_2} \sum_{\tau = (t-1)/2}^{t-1} \frac{1}{2 \mu_1^{t-1-\tau}} \\
= & \frac{1}{2\mu_1\mu_2} \left[1 + \frac{1}{\mu_1} + \cdots + \frac{1}{\mu_1^{(t-1)/2}}\right] 
\ge  \frac{1}{2\mu_1\mu_2} \left[1 + \frac{1}{\mu_1} + \cdots + \frac{1}{\mu_1^{1/\theta}}\right].
\end{align*}
The second-to-last inequality holds because it is easy to check
\begin{equation*}
2 \mu_1^{t-1-\tau}  \sum_{i=0}^{\tau} \mu_1^i \mu_2^{\tau-i} \ge \sum_{i=0}^{t-1} \mu_1^{i} \mu_2^{t-1-i},
\end{equation*}
for any $\tau \ge (t-1)/2$. Finally, by Lemma \ref{lem:aux_nonconvex_mu1}, we have
\begin{equation*}
\mu_1 \le 1 + 2\min \{\frac{|x|}{\theta}, \sqrt{|x|} \}.
\end{equation*}
Since $\mu_1 = \Theta(1)$, $\mu_2 = \Theta(1)$, we have that when $|x| \le \theta^2$, 
\begin{equation*}
\frac{\sum_{\tau = 0}^{t-1} a_\tau}{b_t} 
\ge \Omega(1) \left[1 + \frac{1}{\mu_1} + \cdots + \frac{1}{\mu_1^{1/\theta}}\right]
\ge \Omega(1) \cdot \frac{1}{\theta} \cdot \frac{1}{(1+\theta)^{\frac{1}{\theta}}}
\ge \Omega(\frac{1}{\theta}).
\end{equation*}
When $|x| > \theta^2$, we have:
\begin{equation*}
\frac{\sum_{\tau = 0}^{t-1} a_\tau}{b_t} 
\ge \Omega(1) \left[1 + \frac{1}{\mu_1} + \cdots + \frac{1}{\mu_1^{1/\theta}}\right]
= \Omega(1)
\frac{1 - \frac{1}{\mu_1^{1/\theta + 1}}}{1- \frac{1}{\mu_1}}
=\Omega(\frac{1}{\mu_1 - 1}) = \Omega(\frac{1}{\sqrt{|x|}}).
\end{equation*}
Combining the two cases finishes the proof.
\end{proof}

\subsection{Large-gradient scenario (strongly convex component)}
All the lemmas in this section are concerned with the behavior 
of the~\nag~matrix for eigen-directions of the Hessian with eigenvalues 
being large and positive, as used in proving Lemma \ref{lem:largegrad_convex}.
The following lemma gives eigenvalues of the~\nag~matrix for those directions.

\begin{lemma}\label{lem:aux_convex_rphi}
Under the same setting as Lemma \ref{lem:aux_eigenvalues}, and 
with $x \in (\frac{\theta^2}{(2-\theta)^2}, \frac{1}{4}]$, we have
$\mu_1 = r e^{i\phi}$ and $\mu_2 = r e^{-i\phi}$, where:
\begin{equation*}
r = \sqrt{(1-\theta)(1-x)}, \quad\quad \sin{\phi} = \sqrt{((2-\theta)^2x - \theta^2)(1-x)}/2r.
\end{equation*}
\end{lemma}
\begin{proof}
By Lemma \ref{lem:aux_eigenvalues}, we know that $\mu_1$ and $\mu_2$ 
are two solutions of 
\begin{equation*}
\mu^2 - (2-\theta)(1-x)\mu + (1-\theta)(1-x) = 0.
\end{equation*}
This gives $r^2 = \mu_1\mu_2 = (1-\theta)(1-x)$. 
On the other hand, discriminant is equal to
\begin{align*}
\Delta =& (2-\theta)^2(1-x)^2 - 4(1-\theta)(1-x) \\
=& (1-x)(\theta^2 - (2-\theta^2)x).
\end{align*}
Since $\Im(\mu_1) = r \sin \phi = \frac{\sqrt{-\Delta}}{2}$, 
the proof is finished.
\end{proof}

Under the same setting as above, the following lemma delineates some 
properties of powers of the~\nag~matrix.
\begin{lemma}\label{lem:aux_convex_entry}
Under the same setting as in Lemma \ref{lem:aux_eigenvalues}, 
and with $x \in (\frac{\theta^2}{(2-\theta)^2}, \frac{1}{4}]$, denote:
\begin{equation*}
(a_t, ~-b_t) = \pmat{1 & 0 } \A^t.
\end{equation*}
Then, for any $t\ge 0$, we have:
\begin{equation*}
\max\{|a_t|, ~|b_t|\} \le (t+1) (1-\theta)^{\frac{t}{2}}.
\end{equation*}
\end{lemma}

\begin{proof}
By Lemma \ref{lem:aux_matrix_form} and Lemma \ref{lem:aux_convex_rphi}, 
using $|\cdot|$ to denote the magnitude of a complex number, we have:
\begin{align*}
|a_t| =& \left|\sum_{i=0}^t \mu_1^i \mu_2^{t-i}\right|
\le \sum_{i=0}^t |\mu_1^i \mu_2^{t-i}| = (t+1)r^{t} \le (t+1)(1-\theta)^{\frac{t}{2}} \\
|b_t| =& \left|\mu_1\mu_2\sum_{i=0}^{t-1} \mu_1^{i} \mu_2^{t-1-i}\right|
\le \sum_{i=0}^{t-1} |\mu_1^{i+1}\mu_2^{t-i}|
\le t r^{t+1} \le t (1-\theta)^{\frac{t+1}{2}}.
\end{align*}
Reorganizing these two equations finishes the proof.
\end{proof}

The following is a technical lemma which is useful in bounding the change in 
the Hessian by the amount of oscillation in the iterates.
\begin{lemma}\label{lem:aux_convex_trigonometry}
Under the same setting as Lemma \ref{lem:aux_convex_rphi}, for any $T\ge 0$, any sequence $\{\epsilon_t\}$, and any $\varphi_0 \in [0, 2\pi]$:
\begin{equation*}
\sum_{t=0}^{T} r^t \sin(\phi t + \varphi_0) \epsilon_t
% \le &\sum_{t=0}^{T} r^t \sin(\phi t) \epsilon_0
% + \frac{1}{\sqrt{x}} \sum_{t=1}^T |\epsilon_t - \epsilon_{t-1}| \\
\le  O(\frac{1}{\sin\phi}) \left(|\epsilon_0|+  \sum_{t=1}^T |\epsilon_t - \epsilon_{t-1}|\right).
\end{equation*}
% we also have \cnote{say something more about second statement in proof}:
% \begin{align*}
% \sum_{t=0}^{T} r^t \cos(\phi t) \epsilon_t
% \le  & O(\frac{1}{\sqrt{x}}) \left(|\epsilon_0|+  \sum_{t=1}^T |\epsilon_t - \epsilon_{t-1}|\right)
% \end{align*}
\end{lemma}

\begin{proof}
Let $\tau = \lfloor 2\pi/\phi\rfloor$ be the approximate period, and $J = \lfloor T/\tau \rfloor$ be the number of periods that exist within time $T$. Then, we can group the summation by each period:
% \begin{align*}
% \sum_{t=0}^{T} r^t \sin(\phi t) \epsilon_t =&\sum_{j=0}^{J} \left[\sum_{t = j\tau}^{\min\{(j+1)\tau-1, T\}} r^t \sin(\phi t)\right] \epsilon_{j\tau}\\
% =&\sum_{j=0}^{J - 1} \left[\sum_{t = j\tau}^{(j+1)\tau-1} r^t \sin(\phi t)\right] \epsilon_{j\tau}
% + \sum_{t = J \tau}^{T}
% \\
% = &\sum_{j=0}^{\lfloor T/\tau \rfloor} \left[\sum_{t = j\tau}^{\min\{(j+1)\tau-1, T\}} r^t \sin(\phi t)\right] (\epsilon_0 + \epsilon_{j\tau} - \epsilon_0)\\
% \le& 
% \sum_{j=0}^{\lfloor T/\tau \rfloor - 1} r^{j\tau} \frac{\theta + x}{\phi^2}(|\epsilon_0| + |\epsilon_{j\tau} - \epsilon_0|)
% + \sum_{t = \lfloor T/\tau \rfloor \tau}^{T} (|\epsilon_0| + |\epsilon_{\lfloor T/\tau \rfloor \tau} - \epsilon_0|)\\
% \le &\left[\frac{1}{1-r^\tau}\frac{\theta + x}{\phi^2} + \tau\right] \cdot
% \left[|\epsilon_0|+\sum_{t=1}^T |\epsilon_t - \epsilon_{t-1}|\right]
% \le \left[\frac{1}{\phi} + \tau\right] \cdot
% \left[|\epsilon_0|+\sum_{t=1}^T |\epsilon_t - \epsilon_{t-1}|\right]
% \end{align*}

\begin{align*}
\sum_{t=0}^{T} r^t \sin(\phi t) \epsilon_t
=& \sum_{j=0}^{J} \left[\sum_{t = j\tau}^{\min\{(j+1)\tau-1, T\}} r^t \sin(\phi t+ \varphi_0) \epsilon_t \right]\\
= &
\sum_{j=0}^{J} \left[\sum_{t = j\tau}^{\min\{(j+1)\tau-1, T\}} r^t \sin(\phi t+ \varphi_0) [\epsilon_{j\tau}
+ (\epsilon_t - \epsilon_{j\tau})] \right]\\
\le & 
\underbrace{\sum_{j=0}^{J} \left[\sum_{t = j\tau}^{\min\{(j+1)\tau-1, T\}} r^t \sin(\phi t+ \varphi_0)\right] \epsilon_{j\tau}}_{\text{Term 1}} + \underbrace{\sum_{j=0}^{J} \left[\sum_{t = j\tau}^{\min\{(j+1)\tau-1, T\}}r^t|\epsilon_t - \epsilon_{j\tau}| \right]}_{\text{Term 2}}.
\end{align*}
We prove the lemma by bounding the first term and the second term 
on the right-hand-side of this equation separately.

\noindent \textbf{Term 2:} Since $r\le 1$, it is not hard to see:
\begin{align*}
\text{Term 2} =& \sum_{j=0}^{J} \left[\sum_{t = j\tau}^{\min\{(j+1)\tau-1, T\}}r^t|\epsilon_t - \epsilon_{j\tau}|\right] \\ 
\le& \sum_{j=0}^{J} \left[\sum_{t = j\tau}^{\min\{(j+1)\tau-1, T\}}r^t\right]
\left[\sum_{t = j\tau+1}^{\min\{(j+1)\tau-1, T\}}|\epsilon_t - \epsilon_{t-1}|\right] \\
\le& 
\tau \sum_{j=0}^{J} \left[\sum_{t = j\tau+1}^{\min\{(j+1)\tau-1, T\}}|\epsilon_t - \epsilon_{t-1}|\right]
\le \tau \sum_{t = 1}^T |\epsilon_t - \epsilon_{t-1}|.
\end{align*}


\noindent \textbf{Term 1:} We first study the inner-loop factor, $\sum_{t = j\tau}^{(j+1)\tau-1} r^t \sin(\phi t)$. Letting $\psi = 2\pi - \tau \phi$ be the offset 
for each approximate period, we have that for any $j < J$:
\begin{align*}
\left|\sum_{t = j\tau}^{(j+1)\tau-1} r^t \sin(\phi t+ \varphi_0)\right|
=& \left|\Im\left[\sum_{t = 0}^{\tau-1} r^{j\tau + t} e^{i\cdot [\phi(j\tau + t)+ \varphi_0]}\right]\right|\\
\le& r^{j\tau}\norm{\sum_{t = 0}^{\tau-1} r^{t} e^{i\cdot \phi t}}
\le r^{j\tau}\norm{\frac{1- r^{\tau} e^{i\cdot (2\pi - \psi)}}{1-r e^{i\cdot \phi}}} \\
=& r^{j\tau} \sqrt{\frac{(1 - r^{\tau}\cos \psi)^2 + (r^{\tau}\sin\psi)^2}
{(1 - r\cos \phi)^2 + (r\sin\phi)^2}}.
% \le r^{j\tau} O(\frac{\theta + x}{\phi^2})
\end{align*}
Combined with the fact that for all $y\in[0, 1]$ we have $e^{-3y} \le 1-y \le e^{-y} $, we obtain the following:
\begin{equation}\label{eq:aux_r_tau}
1 - r^{\tau} = 1 - [(1-\theta)(1-x)]^{\frac{\tau}{2}}
= 1 - e^{-\Theta((\theta+x)\tau)} = \Theta ((\theta + x)\tau)
= \Theta \left(\frac{(\theta + x)}{\phi}\right)
\end{equation}
Also, for any $a, b\in [0, 1]$, we have $
(1-ab)^2 \le (1-\min\{a, b\})^2 \le (1-a^2)^2 + (1-b^2)^2$, and by definition of $\tau$, we immediately have $\psi \le \phi$. This yields:
\begin{align*}
\frac{(1 - r^{\tau}\cos \psi)^2 + (r^{\tau}\sin\psi)^2}
{(1 - r\cos \phi)^2 + (r\sin\phi)^2}
\le& \frac{2(1 - r^{2\tau})^2+  2(1-\cos^2 \psi)^2 + (r^{\tau}\sin\psi)^2}
{(r\sin\phi)^2} \\
\le&O\left(\frac{1}{\sin^2 \phi}\right) \left[\frac{(\theta + x)^2}{\phi^2}  +  \sin^4\phi + \sin^2 \phi\right]
\le O\left(\frac{(\theta + x)^2}{\sin^4 \phi}\right)
% \approx O(1)\frac{(\theta + x)^2}{\phi^4}
\end{align*}
The second last inequality used the fact that $r = \Theta(1)$ (although note $r^{\tau}$ is not $\Theta(1)$).
The last inequality is true since by Lemma \ref{lem:aux_convex_rphi}, we know
$(\theta+x)/ \sin^2 \phi \ge \Omega(1)$. This gives:
$$\left|\sum_{t = j\tau}^{(j+1)\tau-1} r^t \sin(\phi t + \varphi_0)\right|
\le r^{j\tau}  \cdot \frac{\theta + x}{\sin^2\phi},$$
and therefore, we can now bound the first term:
\begin{align*}
\text{Term 1} =& \sum_{j=0}^{J} \sum_{t = j\tau}^{\min\{(j+1)\tau-1, T\}} r^t \sin(\phi t+ \varphi_0) \epsilon_{j\tau} 
= \sum_{j=0}^{J} \left[\sum_{t = j\tau}^{\min\{(j+1)\tau-1, T\}} r^t \sin(\phi t+ \varphi_0)\right] (\epsilon_0 + \epsilon_{j\tau} - \epsilon_0)\\
\le&  O(1)\sum_{j=0}^{J - 1} \left[r^{j\tau} \frac{\theta + x}{\sin^2\phi}\right](|\epsilon_0| + |\epsilon_{j\tau} - \epsilon_0|)
+ \sum_{t = J\tau}^{T} (|\epsilon_0| + |\epsilon_{J \tau} - \epsilon_0|)\\
\le& O(1) \left[\frac{1}{1-r^\tau}\frac{\theta + x}{\sin^2\phi} + \tau\right] \cdot
\left[|\epsilon_0|+\sum_{t=1}^T |\epsilon_t - \epsilon_{t-1}|\right]
\le \left[O(\frac{1}{\sin\phi}) + \tau\right] \cdot
\left[|\epsilon_0|+\sum_{t=1}^T |\epsilon_t - \epsilon_{t-1}|\right].
\end{align*}
The second-to-last inequality used Eq.\eqref{eq:aux_r_tau}. In conclusion, since $\tau \le \frac{2\pi}{\phi} \le \frac{2\pi}{\sin\phi}$, we have:
\begin{align*}
\sum_{t=0}^{T} r^t \sin(\phi t+ \varphi_0) \epsilon_t
\le& \text{Term 1} + \text{Term 2}
\le \left[O(\frac{1}{\sin\phi}) + 2\tau\right] \cdot 
\left[|\epsilon_0|+\sum_{t=1}^T |\epsilon_t - \epsilon_{t-1}|\right] \\
\le& O\left(\frac{1}{\sin\phi}\right)\left[|\epsilon_0|+\sum_{t=1}^T |\epsilon_t - \epsilon_{t-1}|\right].
\end{align*}
\end{proof}

\noindent
The following lemma combines the previous two lemmas to bound the
approximation error in the quadratic.
\begin{lemma}\label{lem:aux_convex_inequal}
Under the same setting as Lemma \ref{lem:aux_eigenvalues}, and 
with $x \in (\frac{\theta^2}{(2-\theta)^2}, \frac{1}{4}]$, denote:
\begin{equation*}
(a_t, ~-b_t) = \pmat{1 & 0 } \A^t.
\end{equation*}
Then, for any sequence $\{\epsilon_\tau\}$, any $t \ge \Omega(\frac{1}{\theta})$, we have:
\begin{align*}
\sum_{\tau = 0}^{t-1} a_\tau  \epsilon_\tau \le& O(\frac{1}{x})\left(|\epsilon_0| + \sum_{\tau = 1}^{t-1}|\epsilon_\tau - \epsilon_{\tau -1}|\right)\\
\sum_{\tau = 0}^{t-1} (a_\tau - a_{\tau-1})  \epsilon_\tau \le& O(\frac{1}{\sqrt{x}})\left(|\epsilon_0| + \sum_{\tau = 1}^{t-1}|\epsilon_\tau - \epsilon_{\tau -1}|\right).
\end{align*}
\end{lemma}

\begin{proof}
We prove the two inequalities separately.

\noindent \textbf{First Inequality:} Since $x \in (\frac{\theta^2}{(2-\theta)^2}, 
\frac{1}{4}]$, we further split the analysis into two cases:

\noindent \textbf{Case $x \in (\frac{\theta^2}{(2-\theta)^2}, \frac{2\theta^2}{(2-\theta)^2}]$:}
By Lemma \ref{lem:aux_matrix_form}, we can expand dthe left-hand-side as:
\begin{equation*}
\sum_{\tau = 0}^{t-1} a_\tau  \epsilon_\tau
\le \sum_{\tau = 0}^{t-1} |a_\tau|  (|\epsilon_0| + |\epsilon_\tau - \epsilon_0|)
\le \left[\sum_{\tau = 0}^{t-1} |a_\tau|\right]  \left(|\epsilon_0| + \sum_{\tau = 1}^{t-1}|\epsilon_\tau - \epsilon_{\tau -1}|\right).
\end{equation*}
Noting that in this case $x = \Theta(\theta^2)$, by Lemma \ref{lem:aux_convex_entry} and Lemma \ref{lem:aux_geometric_power}, we have for $t \ge O(1/\theta)$:
\begin{equation*}
\sum_{\tau = 0}^{t-1} |a_\tau|
 \le \sum_{\tau = 0}^{t-1} (\tau+1)(1-\theta)^{\frac{\tau}{2}}
 \le O(\frac{1}{\theta^2}) = O(\frac{1}{x}).
\end{equation*}

\noindent \textbf{Case $x \in (\frac{2\theta^2}{(2-\theta)^2}, \frac{1}{4}]$:}
Again, we expand the left-hand-side as:
\begin{equation*}
\sum_{\tau = 0}^{t-1} a_\tau  \epsilon_\tau
= \sum_{\tau =0}^{t-1} \frac{\mu_1^{\tau+1} -  \mu_2^{\tau+1}}{\mu_1 -\mu_2}\epsilon_\tau
= \sum_{\tau =0}^{t-1}\frac{r^{\tau+1}\sin[(\tau+1)\phi]}{r\sin[\phi]} \epsilon_\tau.
\end{equation*}
Noting in this case that $x = \Theta(\sin^2\phi)$ by Lemma \ref{lem:aux_convex_rphi}, then by Lemma \ref{lem:aux_convex_trigonometry} we have:
\begin{equation*}
\sum_{\tau = 0}^{t-1} a_\tau  \epsilon_\tau 
\le O(\frac{1}{\sin^2\phi})\left(|\epsilon_0| + \sum_{\tau = 1}^{t-1}|\epsilon_\tau - \epsilon_{\tau -1}|\right)
\le O(\frac{1}{x})\left(|\epsilon_0| + \sum_{\tau = 1}^{t-1}|\epsilon_\tau - \epsilon_{\tau -1}|\right).
\end{equation*}

\noindent\textbf{Second Inequality:} 
Using Lemma \ref{lem:aux_matrix_form}, we know:
\begin{align*}
a_\tau - a_{\tau-1} 
=&  \frac{(\mu_1^{\tau+1} -  \mu_2^{\tau+1}) - (\mu_1^{\tau} -  \mu_2^{\tau}) }{\mu_1 -\mu_2} \\
=& \frac{r^{\tau+1}\sin[(\tau+1)\phi]
- r^{\tau}\sin[\tau\phi]}{r\sin[\phi]} \\
=& \frac{r^{\tau}\sin[\tau\phi](r\cos\phi - 1)
+ r^{\tau+1}\cos[\tau\phi]\sin\phi}{r\sin[\phi]} \\
=& \frac{r\cos\phi - 1}{r\sin\phi} \cdot r^{\tau}\sin[\tau\phi]  + r^{\tau}\cos[\tau\phi],
\end{align*}
where we note $r = \Theta(1)$ and the coefficient of the first 
term is upper bounded by the following:
\begin{equation*}
\left|\frac{r\cos\phi - 1}{r\sin\phi}\right|
\le \frac{(1-\cos^2\phi) + (1-r^2)}{r\sin\phi}
\le O\left(\frac{\theta + x}{\sin\phi}\right).
\end{equation*}
As in the proof of the first inequality, we split the analysis 
into two cases:

\noindent \textbf{Case $x \in (\frac{\theta^2}{(2-\theta)^2}, \frac{2\theta^2}{(2-\theta)^2}]$:} Again, we use
\begin{equation*}
\sum_{\tau = 0}^{t-1} (a_\tau - a_{\tau-1} )   \epsilon_\tau
\le \sum_{\tau = 0}^{t-1} |a_\tau- a_{\tau-1} |  (|\epsilon_0| + |\epsilon_\tau - \epsilon_0|)
\le \left[\sum_{\tau = 0}^{t-1} |a_\tau- a_{\tau-1} |\right]  \left(|\epsilon_0| + \sum_{\tau = 1}^{t-1}|\epsilon_\tau - \epsilon_{\tau -1}|\right).
\end{equation*}
Noting $x = \Theta(\theta^2)$, again by Lemma \ref{lem:aux_geometric_power} and $|\frac{\sin \tau\phi}{\sin \phi}| \le \tau$, we have:
\begin{equation*}
\left[\sum_{\tau = 0}^{t-1} |a_\tau- a_{\tau-1} |\right]
\le O(\theta + x)\sum_{\tau = 0}^{t-1}\tau (1-\theta)^{\frac{\tau}{2}} + \sum_{\tau = 0}^{t-1} (1-\theta)^{\frac{\tau}{2}}
\le O(\frac{1}{\theta}) = O(\frac{1}{\sqrt{x}}).
\end{equation*}

\noindent \textbf{Case $x \in (\frac{2\theta^2}{(2-\theta)^2}, \frac{1}{4}]$:}
From the above derivation, we have:
\begin{align*}
\sum_{\tau = 0}^{t-1} (a_\tau - a_{\tau-1})  \epsilon_\tau
% =& \sum_{\tau =0}^{t-1} \frac{(\mu_1^{\tau+1} -  \mu_2^{\tau+1}) - (\mu_1^{\tau} -  \mu_2^{\tau}) }{\mu_1 -\mu_2}\epsilon_\tau \\
% =& \sum_{\tau =0}^{t-1}\frac{r^{\tau+1}\sin[(\tau+1)\phi]
% - r^{\tau}\sin[\tau\phi]}{r\sin[\phi]} \epsilon_\tau \\
% =& \sum_{\tau =0}^{t-1}\frac{r^{\tau}\sin[\tau\phi](r\cos\phi - 1)
% + r^{\tau+1}\cos[\tau\phi]\sin\phi}{r\sin[\phi]} \epsilon_\tau\\
=& \frac{r\cos\phi - 1}{r\sin\phi} \sum_{\tau =0}^{t-1}r^{\tau}\sin[\tau\phi]\epsilon_\tau  + \sum_{\tau =0}^{t-1}r^{\tau}\cos[\tau\phi]\epsilon_\tau.
\end{align*}
According to Lemma \ref{lem:aux_convex_rphi}, in this case $ x = \Theta(\sin^2\phi)$, $r = \Theta(1)$ and since $\Omega(\theta^2)\le x \le O(1)$, we have:
\begin{equation*}
\left|\frac{r\cos\phi - 1}{r\sin\phi}\right|
\le O\left(\frac{\theta + x}{\sin\phi}\right)
\le O\left(\frac{\theta + x}{\sqrt{x}}\right) \le O(1).
\end{equation*}
Combined with Lemma \ref{lem:aux_convex_trigonometry}, this gives:
\begin{equation*}
\sum_{\tau = 0}^{t-1} (a_\tau - a_{\tau-1})  \epsilon_\tau
\le O(\frac{1}{\sin\phi})\left(|\epsilon_0| + \sum_{\tau = 1}^{t-1}|\epsilon_\tau - \epsilon_{\tau -1}|\right)
\le O(\frac{1}{\sqrt{x}})\left(|\epsilon_0| 
+ \sum_{\tau = 1}^{t-1}|\epsilon_\tau - \epsilon_{\tau -1}|\right).
\end{equation*}

% \begin{align*}
% \sum_{\tau = 0}^{t-1} (a_\tau - a_{\tau-1})  \epsilon_\tau
% =& \sum_{\tau =0}^{t-1} \frac{(\mu_1^{\tau+1} -  \mu_2^{\tau+1}) - (\mu_1^{\tau} -  \mu_2^{\tau}) }{\mu_1 -\mu_2}\epsilon_\tau \\
% =& \sum_{\tau =0}^{t-1}\frac{r^{\tau+1}\sin[(\tau+1)\phi]
% - r^{\tau}\sin[\tau\phi]}{r\sin[\phi]} \epsilon_\tau \\
% =& \sum_{\tau =0}^{t-1}\frac{r^{\tau}\sin[\tau\phi](r\cos\phi - 1)
% + r^{\tau+1}\cos[\tau\phi]\sin\phi}{r\sin[\phi]} \epsilon_\tau\\
% =& \frac{r\cos\phi - 1}{r\sin\phi} \sum_{\tau =0}^{t-1}r^{\tau}\sin[\tau\phi]\epsilon_\tau  + \sum_{\tau =0}^{t-1}r^{\tau}\cos[\tau\phi]\epsilon_\tau
% \end{align*}
% According to Lemma \ref{lem:aux_convex_rphi}, in this case $ x = \Theta(\sin^2\phi)$, $r = \Theta(1)$ and $ \Omega(\theta^2)\le x \le O(1)$, we have:
% \begin{equation*}
% \left|\frac{r\cos\phi - 1}{r\sin\phi}\right|
% \le \frac{(1-\cos^2\phi) + (1-r^2)}{r\sin\phi}
% \le O\left(\frac{\theta + x}{\sqrt{x}}\right) \le O(1)
% \end{equation*}
% Combined with Lemma \ref{lem:aux_convex_trigonometry}, this gives:
% \begin{equation*}
% \sum_{\tau = 0}^{t-1} (a_\tau - a_{\tau-1})  \epsilon_\tau
% \le O(\frac{1}{\sin\phi})\left(|\epsilon_0| + \sum_{\tau = 1}^{t-1}|\epsilon_\tau - \epsilon_{\tau -1}|\right)
% \le O(\frac{1}{\sqrt{x}})\left(|\epsilon_0| 
% + \sum_{\tau = 1}^{t-1}|\epsilon_\tau - \epsilon_{\tau -1}|\right)
% \end{equation*}

\noindent
Putting all the pieces together finishes the proof.
\end{proof}




\subsection{Negative-curvature scenario}
In this section, we will prove the auxiliary lemmas required for 
proving Lemma \ref{lem:2nd_seq}. 

The first lemma lower bounds the largest eigenvalue of 
the~\nag~matrix for eigen-directions whose eigenvalues are negative.
\begin{lemma}\label{lem:aux_negcurve_mu1}
Under the same setting as Lemma \ref{lem:aux_eigenvalues}, and 
with $x \in [-\frac{1}{4}, 0]$, and $\mu_1 \ge \mu_2$, we have:
\begin{equation*}
\mu_1 \ge 1 + \frac{1}{2}\min\{\frac{|x|}{\theta}, \sqrt{|x|}\}.
\end{equation*}
\end{lemma}

\begin{proof}
The eigenvalues satisfy:
\begin{align*}
\det (\A - \mu \I) = \mu^2 - (2-\theta)(1-x)\mu + (1-\theta)(1-x) = 0.
\end{align*}
Let $\mu = 1+u$. We have 
\begin{align*}
&& (1+u)^2 - (2-\theta)(1-x)(1+u) + (1-\theta)(1-x) &= 0 \\
\Rightarrow  && u^2 +  ((1-x)\theta + 2x) u + x&= 0.
\end{align*}
Let $f(u) = u^2 + \theta u + 2xu - x\theta u + x$.  To prove $\mu_1(\A) \ge 1 + \frac{\sqrt{|x|}}{2}$ when $x\in [-\frac{1}{4}, -\theta^2]$, we only need to verify $f(\frac{\sqrt{|x|}}{2}) \le 0$:
\begin{align*}
f(\frac{\sqrt{|x|}}{2}) = &\frac{|x|}{4} + \frac{\theta\sqrt{|x|}}{2}
- |x|\sqrt{|x|} + \frac{|x|\sqrt{|x|}\theta}{2} - |x| \\
\le &  \frac{\theta\sqrt{|x|}}{2}- \frac{3|x|}{4} -|x|\sqrt{|x|}(1-\frac{\theta}{2})\le 0
\end{align*}
The last inequality holds because $\theta \le \sqrt{|x|}$ in this case.

For $x\in [-\theta^2, 0]$, we have:
\begin{align*}
f(\frac{|x|}{2\theta}) = \frac{|x|^2}{4\theta^2} + \frac{|x|}{2} - \frac{|x|^2}{\theta}
+ \frac{|x|^2}{2} - |x|
= \frac{|x|^2}{4\theta^2} - \frac{|x|}{2} -|x|^2(\frac{1}{\theta} - \frac{1}{2})\le 0,
\end{align*}
where the last inequality is due to $\theta^2 \ge |x|$.

In summary, we have proved
\begin{equation*}
\mu_1(\A) \ge 
\begin{cases}
1 + \frac{\sqrt{|x|}}{2}, & x \in  [-\frac{1}{4}, -\theta^2]\\
1 + \frac{|x|}{2\theta}.& x \in  [-\theta^2, 0],
\end{cases}
\end{equation*}
which finishes the proof.
\end{proof}



The next lemma is a technical lemma on large powers.
\begin{lemma} \label{lem:aux_eigen_combo_inequal} 
Under the same setting as Lemma \ref{lem:aux_eigenvalues}, 
and with $x \in [-\frac{1}{4}, 0]$, denote
\begin{equation*}
(a_t, ~-b_t) = \pmat{1 & 0 } \A^t.
\end{equation*}
Then, for any $0\le\tau\le t$, we have 
\begin{equation*}
|a^{(1)}_{t-\tau}||a^{(1)}_{\tau} - b^{(1)}_{\tau}| \\
\le  [\frac{2}{\theta} + (t+1)] |a^{(1)}_{t+1} - b^{(1)}_{t+1}|.
\end{equation*}
\end{lemma}
\begin{proof}
Let $\mu_1$ and $\mu_2$ be the two eigenvalues of the matrix $\A$,
where $\mu_1 \ge \mu_2$.  Since $x\in [-\frac{1}{4}, 0]$, 
according to Lemma \ref{lem:aux_eigenvalues} and 
Lemma \ref{lem:aux_nonconvex_mu2}, we have 
$0\le \mu_2\le 1 - \frac{\theta}{2} \le 1 \le \mu_1$, 
and thus expanding both sides using Lemma \ref{lem:aux_matrix_form} yields:
% \begin{align*}
% \frac{\mu_1^{t+1-\tau} - \mu^{t+1 - \tau}_2}{\mu_1 -\mu_2}\cdot
% \frac{\mu_1^{\tau+1} - \mu_2^{\tau+1} - \mu_1\mu_2 (\mu_1^\tau -\mu_2^\tau)}{\mu_1 - \mu_2}
% \le& [\frac{2}{\theta}+ (t+1)]
% \frac{\mu_1^{t+2} - \mu_2^{t+2} - \mu_1\mu_2 (\mu_1^{t+1} -\mu_2^{t+1})}{\mu_1 - \mu_2}
% \end{align*}
\begin{align*}
\text{LHS} =& 
\left[\sum_{i=0}^{t-\tau}\mu_1^{t-\tau-i}\mu_2^i\right]
\left[(1-\mu_2)\left(\sum_{i=0}^{\tau-1}\mu_1^{\tau-i}\mu_2^i\right) + \mu_2^\tau \right] \\
=& \left[\sum_{i=0}^{t-\tau}\mu_1^{t-\tau-i}\mu_2^i\right](1-\mu_2)\left(\sum_{i=0}^{\tau-1}\mu_1^{\tau-i}\mu_2^i\right)
+ \left[\sum_{i=0}^{t-\tau}\mu_1^{t-\tau-i}\mu_2^i\right] \mu_2^\tau \\
\le& (t-\tau+1)\mu_1^{t-\tau} (1-\mu_2)\left(\sum_{i=0}^{\tau-1}\mu_1^{\tau-i}\mu_2^i\right)
 + \left[\sum_{i=0}^{t-\tau}\mu_1^{t-\tau-i}\mu_2^i\right] \\
\le& (t+1)(1-\mu_2)\left(\sum_{i=0}^{\tau-1}\mu_1^{t+1-i}\mu_2^i\right)
+\frac{2}{\theta}(1-\mu_2)\left[\sum_{i=0}^{t-\tau}\mu_1^{t+1-i}\mu_2^i\right] \\
\le& [\frac{2}{\theta}+ (t+1)]\left[(1-\mu_2)\sum_{i=0}^{t}\mu_1^{t+1-i}\mu_2^i
+ \mu_2^{t+1}\right]
 =  \text{RHS},
\end{align*}
which finishes the proof.
\end{proof}

\noindent
The following lemma gives properties of the $(1,1)$ element of large 
powers of the~\nag~matrix.
\begin{lemma}\label{lem:aux_increase_x}
Let the $2\times 2$ matrix $\A(x)$ be defined as follows and let
$x\in[-\frac{1}{4}, 0]$ and $\theta \in (0, \frac{1}{4}]$.
\begin{equation*}
\A(x) = \pmat{(2-\theta) (1 - x)&  -(1-\theta) (1 - x) \\1 & 0}.
\end{equation*}
For any fixed $t>0$, letting $g(x) = \abs{\pmat{1 & 0 }[\A(x)]^{t}\pmat{1 \\ 0 }}$, 
then we have:
\begin{enumerate}
\item $g(x)$ is a monotonically decreasing function for $x \in [-1, \theta^2/(2-\theta)^2]$.
\item For any $x \in [\theta^2/(2-\theta)^2, 1]$, we have $g(x) \le g(\theta^2/(2-\theta)^2)$.
\end{enumerate}

\end{lemma}
\begin{proof}
For $x \in [-1, \theta^2/(2-\theta)^2]$, we know that $\A(x)$ has two 
real eigenvalues $\mu_1(x)$ and $\mu_2(x)$, Without loss of generality, 
we can assume $\mu_1(x) \ge \mu_2(x)$.
By Lemma \ref{lem:aux_matrix_form}, we know:
\begin{align*}
g(x) = \abs{\pmat{1 & 0 }[\A(x)]^{t}\pmat{1 \\ 0 }}
=\sum_{i=0}^t [\mu_1(x)]^i [\mu_2(x)]^{t-i}
= [\mu_1(x)\mu_2(x)]^{\frac{t}{2}} \sum_{i=0}^t \left[\frac{\mu_1(x)}{\mu_2(x)}\right]^{\frac{t}{2} - i}.
\end{align*}
By Lemma \ref{lem:aux_eigenvalues} and Vieta's formulas, 
we know that $[\mu_1(x)\mu_2(x)]^{\frac{t}{2}} 
= [(1-\theta) (1 - x)]^{\frac{t}{2}}$ is monotonically decreasing in $x$.
On the other hand, we have that:
\begin{align*}
\frac{\mu_1(x)}{\mu_2(x)} + \frac{\mu_2(x)}{\mu_1(x)}
+ 2 = \frac{[\mu_1(x) + \mu_2(x)]^2}{\mu_1(x)\mu_2(x)}
= \frac{(2-\theta)^2(1-x)}{1-\theta}
\end{align*}
is monotonically decreasing in $x$, implying that 
$\sum_{i=0}^t \left[\frac{\mu_1(x)}{\mu_2(x)}\right]^{\frac{t}{2} - i}$ 
is monotonically decreasing in $x$. Since both terms are positive, 
this implies the product is also monotonically decreasing in $x$, 
which finishes the proof of the first part.

For $x \in [\theta^2/(2-\theta)^2, 1]$, the two eigenvalues 
$\mu_1(x)$ and $\mu_2(x)$ are conjugate, and we have:
\begin{equation*}
[\mu_1(x)\mu_2(x)]^{\frac{t}{2}} = [(1-\theta) (1 - x)]^{\frac{t}{2}} \le [\mu_1(\theta^2/(2-\theta)^2)\mu_2(\theta^2/(2-\theta)^2)]^{\frac{t}{2}}
\end{equation*}
which yields:
\begin{equation*}
\sum_{i=0}^t \left[\frac{\mu_1(x)}{\mu_2(x)}\right]^{\frac{t}{2} - i}
\le \norm{\sum_{i=0}^t \left[\frac{\mu_1(x)}{\mu_2(x)}\right]^{\frac{t}{2} - i}}
\le \sum_{i=0}^t \norm{\frac{\mu_1(x)}{\mu_2(x)}}^{\frac{t}{2} - i}
=t+1
= \sum_{i=0}^t \left[\frac{\mu_1(\theta^2/(2-\theta)^2)}{\mu_2(\theta^2/(2-\theta)^2)}\right]^{\frac{t}{2} - i},
\end{equation*}
and this finishes the proof of the second part.
%\cnote{fill in more detail.}
\end{proof}

\noindent
The following lemma gives properties of the sum of the first 
row of large powers of the~\nag~matrix.
\begin{lemma}\label{lem:aux_increase_t}
Under the same setting as Lemma \ref{lem:aux_eigenvalues}, 
and with $x \in [-\frac{1}{4}, 0]$, denote
\begin{equation*}
(a_t, ~-b_t) = \pmat{1 & 0 } \A^t.
\end{equation*}
Then we have
$$|a_{t+1} - b_{t+1}| \ge |a_t - b_t|$$
and
$$|a_t - b_t| \ge \frac{\theta}{2}\left(1 + \frac{1}{2}\min\{\frac{|x|}{\theta}, \sqrt{|x|}\}\right)^t.$$
\end{lemma}
\begin{proof}
Since $x<0$, we know that $\A$ has two distinct real eigenvalues. 
Let $\mu_1$ and $\mu_2$ be the two eigenvalues of $\A$.
For the first inequality, by Lemma \ref{lem:aux_matrix_form}, we only need to prove:
\begin{align*}
\mu_1^{t+1} - \mu_2^{t+1} - \mu_1\mu_2(\mu_1^{t} - \mu_2^{t})
\ge \mu_1^{t} - \mu_2^{t} - \mu_1\mu_2(\mu_1^{t-1} - \mu_2^{t-1}).
\end{align*}
Taking the difference of the LHS and RHS, we have:
\begin{align*}
& \mu_1^{t+1} - \mu_2^{t+1} - \mu_1\mu_2(\mu_1^{t} - \mu_2^{t}) - 
(\mu_1^{t} - \mu_2^{t}) + \mu_1\mu_2(\mu_1^{t-1} - \mu_2^{t-1}) \\
=& \mu_1^{t}(\mu_1 - \mu_1\mu_2 - 1  + \mu_2) - \mu_2^{t}(\mu_2 - \mu_1\mu_2 - 1 +\mu_1)\\
=& (\mu_1^t - \mu_2^t)(\mu_1 - 1)(1-\mu_2).
\end{align*}
According to Lemma \ref{lem:aux_eigenvalues} and Lemma \ref{lem:aux_nonconvex_mu2},
$\mu_1 \ge 1 \ge \mu_2 \ge 0$, which finishes the proof of the first claim.

For the second inequality, again by Lemma \ref{lem:aux_matrix_form}, 
since both $\mu_1$ and $\mu_2$ are positive, we have:
\begin{align*}
a_t - b_t = \sum_{i=0}^t \mu_1^i \mu_2^{t-i} - \mu_1\mu_2\sum_{i=0}^{t-1} \mu_1^{i} \mu_2^{t-1-i}
\ge (1-\mu_2)\sum_{i=0}^t \mu_1^i \mu_2^{t-i} \ge (1-\mu_2)\mu_1^t.
\end{align*}
By Lemma \ref{lem:aux_nonconvex_mu2} we have $1-\mu_2 \ge \frac{\theta}{2}$, 
By Lemma \ref{lem:aux_negcurve_mu1} we know $\mu_1 \ge 1 + \frac{1}{2}\min\{\frac{|x|}{\theta}, \sqrt{|x|}\}$.
Combining these facts finishes the proof.
\end{proof}


% \begin{lemma}\label{lem:trigonometry}
% For $r = \sqrt{(1-\theta)(1-x)}$,  $\sin{\phi} = \sqrt{(2-\theta)^2x - \theta^2}/2r$, any $T \ge 0$, we have:
% \begin{align*}
% \sum_{t=0}^{T} r^t \sin(\phi t) \epsilon_t
% % \le &\sum_{t=0}^{T} r^t \sin(\phi t) \epsilon_0
% % + \frac{1}{\sqrt{x}} \sum_{t=1}^T |\epsilon_t - \epsilon_{t-1}| \\
% \le  & O(\frac{1}{\sqrt{x}}) \left(|\epsilon_0|+  \sum_{t=1}^T |\epsilon_t - \epsilon_{t-1}|\right)
% \end{align*}
% we also have \cnote{say something more about second statement in proof}:
% \begin{align*}
% \sum_{t=0}^{T} r^t \cos(\phi t) \epsilon_t
% \le  & O(\frac{1}{\sqrt{x}}) \left(|\epsilon_0|+  \sum_{t=1}^T |\epsilon_t - \epsilon_{t-1}|\right)
% \end{align*}
% \end{lemma}

% \begin{proof}
% Let $\tau = \lfloor 2\pi/\phi\rfloor$ be the period, and $\psi = 2\pi - \tau \phi$ be the offset.
% Then we have:
% \begin{align*}
% \sum_{t=0}^{T} r^t \sin(\phi t) \epsilon_t
% =& \sum_{j=0}^{\lfloor T/\tau \rfloor} \sum_{t = j\tau}^{\min\{(j+1)\tau-1, T\}} r^t \sin(\phi t) \epsilon_t \\
% = &
% \sum_{j=0}^{\lfloor T/\tau \rfloor} \sum_{t = j\tau}^{\min\{(j+1)\tau-1, T\}} r^t \sin(\phi t) [\epsilon_{j\tau}
% + (\epsilon_t - \epsilon_{j\tau})] \\
% \le & \sum_{j=0}^{\lfloor T/\tau \rfloor} \sum_{t = j\tau}^{\min\{(j+1)\tau-1, T\}} r^t \sin(\phi t) \epsilon_{j\tau} + \sum_{j=0}^{\lfloor T/\tau \rfloor} \sum_{t = j\tau}^{\min\{(j+1)\tau-1, T\}}r^t|\epsilon_t - \epsilon_{j\tau}| 
% \end{align*}
% For the second term, we have:
% \begin{align*}
% \sum_{j=0}^{\lfloor T/\tau \rfloor} \sum_{t = j\tau}^{\min\{(j+1)\tau-1, T\}}r^t|\epsilon_t - \epsilon_{j\tau}| \le& \sum_{j=0}^{\lfloor T/\tau \rfloor} \left[\sum_{t = j\tau}^{\min\{(j+1)\tau-1, T\}}r^t\right]
% \left[\sum_{t = j\tau+1}^{\min\{(j+1)\tau-1, T\}}|\epsilon_t - \epsilon_{t-1}|\right] \\
% \le& \tau \sum_{t = 1}^T |\epsilon_t - \epsilon_{t-1}|
% \end{align*}

% For the $j < \lfloor T/\tau \rfloor$, we have:
% \begin{align*}
% \left|\sum_{t = j\tau}^{(j+1)\tau-1} r^t \sin(\phi t)\right|
% =& \left|\Im\left[\sum_{t = 0}^{\tau-1} r^{j\tau + t} e^{i\cdot \phi(j\tau + t)}\right]\right|\\
% \le& r^{j\tau}\norm{\sum_{t = 0}^{\tau-1} r^{t} e^{i\cdot \phi t}}
% \le r^{j\tau}\norm{\frac{1- r^{\tau} e^{i\cdot (2\pi - \psi)}}{1-r e^{i\cdot \phi}}} \\
% =& r^{j\tau} \sqrt{\frac{(1 - r^{\tau}\cos \psi)^2 + (r^{\tau}\sin\psi)^2}
% {(1 - r\cos \phi)^2 + (r\sin\phi)^2}}
% \le r^{j\tau} O(\frac{\theta + x}{\phi^2})
% \end{align*}
% In last equality, we used the fact:
% \begin{equation*}
% r^{\tau} = [(1-\theta)(1-x)]^{\frac{\tau}{2}}
% \ge e^{-\frac{3(\theta + x)\tau}{2}}
% \ge 1- \frac{3}{2}(\theta + x)\tau
% \ge 1- \frac{3\pi(\theta + x)}{\phi}
% \end{equation*}
% Also, for any $a, b\in [0, 1]$, we have $(1-(1-a)(1-b))^2 = (a + b -ab)^2 \le 2a^2 + 2b^2$, this gives:
% \begin{align*}
% \frac{(1 - r^{\tau}\cos \psi)^2 + (r^{\tau}\sin\psi)^2}
% {(1 - r\cos \phi)^2 + (r\sin\phi)^2}
% \le& \frac{2(1 - r^{\tau})^2+  2(1-\cos \psi)^2 + (r^{\tau}\sin\psi)^2}
% {(r\sin\phi)^2} \\
% \le&18\frac{\pi^2(\theta + x)^2}{(r\phi\sin\phi)^2}  +  6\frac{\sin^4\frac{\phi}{2}}{(r\sin\phi)^2} + 1
% \approx O(1)\frac{(\theta + x)^2}{\phi^4}
% \end{align*}
% \cnote{Need to be more rigorous here}.
% This gives the first term:
% \begin{align*}
% &\sum_{j=0}^{\lfloor T/\tau \rfloor} \sum_{t = j\tau}^{\min\{(j+1)\tau-1, T\}} r^t \sin(\phi t) \epsilon_{j\tau}= \sum_{j=0}^{\lfloor T/\tau \rfloor} \left[\sum_{t = j\tau}^{\min\{(j+1)\tau-1, T\}} r^t \sin(\phi t)\right] (\epsilon_0 + \epsilon_{j\tau} - \epsilon_0)\\
% \le& 
% \sum_{j=0}^{\lfloor T/\tau \rfloor - 1} r^{j\tau} \frac{\theta + x}{\phi^2}(|\epsilon_0| + |\epsilon_{j\tau} - \epsilon_0|)
% + \sum_{t = \lfloor T/\tau \rfloor \tau}^{T} (|\epsilon_0| + |\epsilon_{\lfloor T/\tau \rfloor \tau} - \epsilon_0|)\\
% \le &\left[\frac{1}{1-r^\tau}\frac{\theta + x}{\phi^2} + \tau\right] \cdot
% \left[|\epsilon_0|+\sum_{t=1}^T |\epsilon_t - \epsilon_{t-1}|\right]
% \le \left[\frac{1}{\phi} + \tau\right] \cdot
% \left[|\epsilon_0|+\sum_{t=1}^T |\epsilon_t - \epsilon_{t-1}|\right]
% \end{align*}
% \end{proof}

% \begin{lemma}
% For all $x\in[0, 1]$ we have $1-x \le e^{-x} \le 1-x/3$.
% \end{lemma}
