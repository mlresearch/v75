%!TEX root = main.tex

\subsection{Related Work}


In this section, we review related work from the perspective of both nonconvex optimization and momentum/acceleration. For clarity of presentation, when discussing rates, we focus on the dependence on the accuracy $\epsilon$ and the dimension $d$ while assuming all other problem parameters are constant. Table~\ref{tab:main} presents a comparison of the current work with previous work.

\textbf{Convergence to first-order stationary points:} Traditional analyses in this case assume only Lipschitz gradients (see Definition \ref{def:smooth}).~\cite{nesterov1998introductory} shows that~\gd~finds an~\EFSP~in $O(1/\epsilon^2)$ steps.~\cite{ghadimi2016accelerated}~guarantee that~\nag~also converges in $\otilde{1/\epsilon^2}$ steps. \red{Under the additional assumption of Lipschitz Hessians (see Definition \ref{def:HessianLip}), \citet{carmon2017convex} propose NCE steps, and develop a new algorithm that converges in $\otilde{1/\epsilon^{7/4}}$ iterations. 
% Their algorithm utilize ``negative curvature exploitation," steps, which inspired a similar procedure in our algorithm. 
%We emphasize that while the analysis of~\nce~step is rather straightforward, the main challenge is to show why the algorithm is efficient in remaining cases, where \citet{carmon2017convex} studied a nested-loop algorithm and repeatedly adding proximal terms to inner loop objectives, so that when AGD is applied within their inner loops, analysis similar to standard convex ones can be directly used. In contrast, this work studies~\nag~in its original single-loop form in general nonconvex setting which is less understood, and develops new analyses through a novel Hamiltonian framework.
We emphasize that while the analysis of the~\nce~step is rather straightforward, the main challenge is to deal with the case when~\nce~is not triggered.~\citet{carmon2017convex} study a nested-loop algorithm, repeatedly adding proximal terms to the objective in the inner loop and solving the modified functions using~\nag. When~\nag~is applied to these modified functions, they can be analyzed similarly to the standard analysis in the convex case. In contrast, the
current work studies~\nag~in its original, single-loop form, in a general nonconvex setting,
and develops a new analysis based on a Hamiltonian perspective.}
% Their algorithm is a nested-loop algorithm, which relies on 
% A key novelty in their algorithm is the idea of ``negative curvature exploitation," which inspired a similar step in our algorithm.
% In addition to the qualitative and quantitative differences between~\cite{carmon2017convex} and the current work, as summarized in Table~\ref{tab:main}, we note that while~\cite{carmon2017convex} analyze~\nag~ in inner loop similar to convex analysis due to additional proximal term, we analyze~\nag~applied directly to nonconvex functions 
% through a novel Hamiltonian framework.

\textbf{Convergence to second-order stationary points:} All results in this setting assume Lipschitz
continuity of both gradient and Hessian.
%Since second-order stationary point requires Hessian to be almost positive definite. A natural way to guarantee Hessian property is by directly using Hessian information per step.
Classical approaches, such as cubic regularization~\citep{nesterov2006cubic} and trust region algorithms~\citep{curtis2014trust}, require access to Hessians, and are known to find $\epsilon$-second-order stationary points in $O(1/\epsilon^{1.5})$ steps. However, the requirement of these algorithms to form the Hessian makes them infeasible for 
high-dimensional problems. A second set of algorithms utilize only Hessian-vector products instead of the full Hessian. Such products can be approximated by differentiating the gradients at two very close points, and in many applications they can also be directly computed efficiently.
Rates of $\widetilde{O}(1/\epsilon^{7/4})$ have been established for such algorithms with nested loops~\citep{carmon2016accelerated,agarwal2017finding,royer2017complexity}.
Finally, in the realm of purely gradient-based algorithms,
%(which is typically simple, easy to implement and efficient in high dimensional setting),
%(simple to implement and efficient in high-dimensional settings)
\citet{ge2015escaping} present the first polynomial guarantees for a perturbed version of~\gd, and~\citet{jin2017escape} sharpen it to $\widetilde{O}(1/\epsilon^2)$. For the special case of quadratic functions,~\cite{o2017behavior} analyze the behavior of~\nag~around critical points and show that it escapes saddle points faster than~\gd,~but it does not address the setting of general nonconvex functions.
%We note that this work is the first single-loop algorithm achieving a rate of $\widetilde{O}(1/\epsilon^{7/4})$ for general nonconvex functions. 
Parallel to this work, \citet{allen2017neon2, xu2017neon+} also propose gradient-based algorithms achieving $\widetilde{O}(1/\epsilon^{7/4})$ rate,
but their algorithms are also based on the framework of nested loops and repeatedly adding proximal terms, similar to~\citet{carmon2016accelerated}.
%\cnote{add Wright paper, quadratic case.}

\textbf{Acceleration:} There is also a rich literature that aims to understand momentum methods; 
e.g., \citet{allen2014linear} view~\nag~as a linear coupling of~\gd~and mirror descent, \citet{su2016differential} and \citet{wibisono2016} view~\nag~as the discretization of a second-order differential equation, and \citet{bubeck2015geometric} view~\nag~from a geometric perspective. Most of this work is tailored to the convex setting, and it is unclear and nontrivial to generalize the results to a nonconvex setting. There are also several papers that study~\nag~with relaxed versions of convexity---see~\citet{necoara2015linear,li2017provable} and references therein for 
overviews of these results.


% {\small
% \begin{table}[t]
%   \begin{center}
%     {\renewcommand{\arraystretch}{1.3}
%     \begin{tabular}  {l | l | l | l | l}
%        \toprule
% \textbf{Guarantees}  & \textbf{Oracle}  & \textbf{Algorithm} & \textbf{Iterations} & \textbf{Simplicity}\\ 
% \midrule 
% \multirow{3}{*}{\makecell[l]{First-order\\Stationary\\Point}} & \multirow{3}{*}{Gradient}  & GD \citep{nesterov1998introductory} & $O(1/\epsilon^2)$  & Single-loop\\ 
% & & AGD \citep{ghadimi2016accelerated} & $O(1/\epsilon^2)$ &  Single-loop\\ 
%  &  & \cite{carmon2017convex} & $\otilde{1/\epsilon^{7/4}}$ &  Nested-loop\\
%       \midrule
% % \multirow{6}{*}{\makecell[l]{Second-order\\Stationary\\Point}} & \multirow{3}{*}{\makecell{Hessian\\-vector}} & \citet{carmon2016gradient} & $\widetilde{O}(1/\epsilon^2)$ &   Nested-loop\\ 
% %  & & \citet{agarwal2016finding}
% %  & $\widetilde{O}(1/\epsilon^{7/4})$ & Nested-loop\\
% \multirow{5}{*}{\makecell[l]{Second-order\\Stationary\\Point}} & \multirow{2}{*}{\makecell{Hessian\\-vector}}  & \citet{carmon2016accelerated}
%  & $\widetilde{O}(1/\epsilon^{7/4})$ & Nested-loop\\
%   & & \citet{agarwal2017finding}
%  & $\widetilde{O}(1/\epsilon^{7/4})$ & Nested-loop\\ 
%       \cmidrule {2-5}
% & \multirow{3}{*}{Gradient} &  Noisy GD \citep{ge2015escaping} & $O(\poly(d/\epsilon))$ &  Single-loop\\ 
%  % & $O(\poly(d/\epsilon))$ & Gradient &  SOSP &Single-loop\\
%  &  &Perturbed GD \citep{jin2017escape} & $\widetilde{O}(1/\epsilon^{2})$ & Single-loop \\ 
%  &  &\textbf{Perturbed AGD [This Work]} & $\widetilde{O}(1/\epsilon^{7/4})$ &  Single-loop\\ 

%     \bottomrule
%     \end{tabular}
%       \caption{Complexity of finding stationary points. $\widetilde{O}(\cdot)$ ignores polylog factors in $d$ and $\epsilon$.}
%       \label{tab:main}
%     }
%   \end{center}
%   % \praneeth{I think it will be cleaner to make the dependence on smoothness paramters explicit here.}
%   \vspace{-2ex}
% \end{table}
% }


{\small
\begin{table}[t]
  \begin{center}
    {\renewcommand{\arraystretch}{1.3}
    \begin{tabular}  {l | l | l | l }
       \toprule
\textbf{Guarantees}  & \textbf{Simplicity}  & \textbf{Algorithm} & \textbf{Iterations}  \\ 
\midrule 
\multirow{3}{*}{\makecell[l]{First-order\\Stationary Point}} & Nested-loop & \cite{carmon2017convex} & $\otilde{1/\epsilon^{7/4}}$ \\
\cmidrule {2-4}
& \multirow{2}{*}{Single-loop}  & GD \citep{nesterov1998introductory} & $O(1/\epsilon^2)$  \\ 
& & AGD \citep{ghadimi2016accelerated} & $O(1/\epsilon^2)$ \\ 
      \midrule
\multirow{5}{*}{\makecell[l]{Second-order\\Stationary Point}} & \multirow{2}{*}{\makecell{Nested-loop}}  & \citet{carmon2016accelerated}
 & $\widetilde{O}(1/\epsilon^{7/4})$ \\
  & & \citet{agarwal2017finding}
 & $\widetilde{O}(1/\epsilon^{7/4})$ \\ 
      \cmidrule {2-4}
& \multirow{3}{*}{Single-loop} &  Noisy GD \citep{ge2015escaping} & $O(\poly(d/\epsilon))$ \\ 
 &  &Perturbed GD \citep{jin2017escape} & $\widetilde{O}(1/\epsilon^{2})$  \\ 
 &  &\textbf{Perturbed AGD [This Work]} & $\widetilde{O}(1/\epsilon^{7/4})$ \\ 

    \bottomrule
    \end{tabular}
      \caption{Complexity of finding stationary points. $\widetilde{O}(\cdot)$ ignores polylog factors in $d$ and $\epsilon$.}
      \label{tab:main}
    }
  \end{center}
  % \praneeth{I think it will be cleaner to make the dependence on smoothness paramters explicit here.}
  \vspace{-2ex}
\end{table}
}





% \pn{May be do the above in contributions section?}

% \noindent
% \textbf{Saddle points and local minima in high dimensional problems}: There is rich literature on understanding the prevalence and suboptimality of saddle points as well as local minima in high dimensional problems, especially those arising in statistical physics~\cite{bray2007statistics,fyodorov2007replica} and neural networks~\cite{baldi1989neural}. See~\cite[Section~2]{dauphin2014identifying} for an overview of these results and more references. A study of this phenomenon for a broader class of problems picked up steam only in the last few years.

% %Cite Bray and Dean 2007, Fyodor and Williams 2007, Baldi and Hornik 1989 (all from~\cite{dauphin2014identifying})

% \noindent
% \textbf{Acceleration for nonconvex optimization}:
% %In the last several years, there has been a fair amount of work investigating acceleration techniques for nonconvex optimization.~\cite{nesterov2008accelerating} develops an accelerated version of cubic regularized Newton method and obtains improved convergence rates for smooth nonconvex functions with Lipschitz Hessians (i.e., both gradients and Hessians are Lipschitz). This method is however a second order method i.e., requires access to both gradients and Hessians of the function to be optimized. There has been some work on implementing this algorithm 
% ~\cite{ghadimi2016accelerated} seems to be the first to explore the possibility of acceleration for nonconvex problems by investigating the performance of~\nag~for smooth nonconvex functions i.e., having Lipschitz gradients. It shows that the convergence rate of~\nag~to first order stationary points is the same as that of~\gd~i.e., it does not show any improvements over~\gd. With the additional assumption of Lipschitz Hessians,~\cite{nesterov2006cubic} obtains better convergence rates than~\gd, and to second order stationary points. While~\cite{nesterov2006cubic} works in the second order oracle model (i.e., access to gradients and Hessians), their results suggest that in principle, acceleration might be possible in the first order oracle model as well.~\cite{carmon2016accelerated} show that accelerated convergence to second order stationary points is possible with access to gradients and Hessian-vector products.~\cite{carmon2017convex} works fully in the first order oracle model but shows accelerated convergence only to first order stationary points.
% %Understanding whether acceleration is possible for smooth nonconvex functions (i.e., Lipschitz gradients but not Lipschitz Hessians) is an open problem.
% %Check Guanghui Lan's work and see if they have worked out AGD for first order stationary point with only gradient Lipschitz? Accelerated cubic Newton.
% Trust region methods?

% \noindent
% \textbf{Stochastic algorithms for nonconvex optimization}: There has also been a fair amount of recent work on designing stochastic algorithms for nonconvex optimization starting with~\cite{ghadimi2016accelerated}. For the special case of finite sums, there has been a long series of papers designing efficient algorithms. See~\cite{agarwal2017finding} and references therein for an overview of these results.
% %Lan's paper, Finite sum accelerated SVRG.

% \pn{Anything else?}


% \cnote{Talk about previous understanding of AGD}

% \cnote{Talk about AGD for relaxed version of convex function}
