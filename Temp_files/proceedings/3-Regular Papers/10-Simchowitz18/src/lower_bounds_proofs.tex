%!TEX root = LWM_colt.tex
\section{Lower Bounds}

%\subsection{Remarks on the Looseness of Theorem~\ref{thm:info_lb_d}\label{sec:Lower_Bound_Loose}}
\subsection{Proof of Information Theoretic Lower Bounds, Theorems~\ref{thm:info_lb_1d} and~\ref{thm:info_lb_d}\label{sec:lb_proofs}}
 In this section we prove Theorem~\ref{thm:info_lb_1d} and~\ref{thm:info_lb_d}. We shall the $\Pr_{A}^{(T)}$ denote the law of the iterates $X_{t+1} = AX_{t} + \noise_t$, where $\noise_t \sim \calN(0,I)$, for $t = 1,2,\dots,T$. We shall prove Theorems~\ref{thm:info_lb_1d} and~\ref{thm:info_lb_d} using Birge's Inequality, a bound which is qualitatively similar to Fano's inequality, but yields sharp high-probability lower bounds in low-dimensional settings.
\begin{lem}[Variant of Birge's Inequality]\label{lem:Birge} Let $\calE_0,\calE_1,\dots,\calE_N$ be disjoint events, $\Pr_0,\Pr_1,\dots,\Pr_N$ be probability laws, and let $\min_{i}\Pr(\calE_i^c) \le 1/2$. Then, for any $\delta \in (0,1/2)$,
\begin{eqnarray}
\sum_{i=1}^N \KL(\Pr_i,\Pr_0) \ge (1-2\delta) \log(N/2\delta)~.
\end{eqnarray}
In particular, fix an $\epsilon > 0$ and $\delta \in (0,1/2)$, and suppose that for a finite set $\calN \subset \R^{n \times n}$, all $A_1 \ne A_2 \in \calN$ satisfy $\|\calA_1 - \calA_2\|_{\op} \ge 2\epsilon$. Then if $\inf_{\ALS}\sup_{A \in \calN} \Pr_{A}[\|A - \ALS(T)\|_{\op} \ge \epsilon] \le \delta$, then $T$, $\delta$ and $|\calN|$ satisfy the following inequality for any $A_0 \in \calN$:
\begin{eqnarray}
\sup_{A \in \calN - \{A_0\}}\KL(\Pr_{A}^{(T)},\Pr_{A_0}^{(T)}) \ge (1-2\delta) \log\left(\frac{|\calN| - 1|}{2\delta}\right)~.
\end{eqnarray}
%  (1-\delta)\log (N\frac{1-\delta}{\delta}) + \delta \log( \frac{\delta}{1 - (\delta)/N})
\end{lem}
We prove Lemma~\ref{lem:Birge} from a standard statment of Birge's inequality from \citep[Theorem 4.20]{boucheron13}, in Section~\ref{BirgeSec}. Lemma~\ref{lem:Birge} relates the probability of error to the $\KL$-divergences between laws $\Pr_{A}^{(T)}$ in $2\epsilon$-separated set $\calN$. Thus, our first step will be to compute the term $\KL(\Pr_{A}^{(T)},\Pr_{A_0}^{(T)})$. This amounts to a straightforward computation, carried out in Section~\ref{sec:kl_comp}.
\begin{lem}\label{kl_comp} Let $O,O' \in \Od$. Then, $\KL(\Pr_{\rho}^{(T)},\Pr_{A}^{(T)}) =  \fronorm{\eig O - A}^2 \cdot \sum_{t=1}^T \gamma_{t}(\eig)$, where we recall $\gamma_{t}(\eig) = \sum_{s=0}^{t-1} |\eig|^{2s}$.
\end{lem}
We are now in a position to prove the lower bound in one-dimension:
\subsection{1-D Lower Bound: Proof of Theorem~\ref{thm:info_lb_1d}}
\begin{proof} Fix an $\eig \in \R$, and let $\eig' \in \{\eig - 2\epsilon, \eig + 2\epsilon\}$. Viewing $\eig,\eig'$ as matrices in $\R^{1 \times 1}$, we have Lemma~\ref{kl_comp}, implies $\KL(\Pr_{\eig}^{(T)},\Pr_{\eig'}^{(T)}) = 4\epsilon^2 \cdot \sum_{t=1}^T \gamma_{t}(\eig)$. Then, applying Lemma~\ref{lem:Birge} with $A_0 = \eig$ and $\calN = \{\eig,\eig'\}$, we have for if $\sup_{a \in \eig,\eig'}\Pr_a[|\widehat{a}(T) - a| < \epsilon ] \le \delta$, then, $\epsilon^2 \cdot \sum_{t=1}^T \gamma_{t}(\eig) \ge (1-2\delta) \log\left(\frac{1}{2\delta}\right)$. Hence, we need $T\gamma_T(\eig) \ge\sum_{t = 1}^T\gamma_t(\eig) \ge \frac{1}{4\epsilon}(1-2\delta) \log\left(\frac{1}{2\delta}\right)$.
\end{proof}
\subsection{$d$-Dimensional Lower Bound}
If we chose $\calN$ to be a $2\epsilon$-packing of the set $\eig\Od$, then Lemma~\ref{lem:Birge} and Lemma~\ref{kl_comp} imply that, for any estimate $\widehat{A}$ such that
\begin{eqnarray*}
&\sup_{ O \in \Od}\Pr_{\eig O}[\|\widehat{A}(T) - \eig O\|_{\op} \ge \epsilon] \ge \delta \text{ for any } \\
&T \text{ such that } (1-2\delta)\log \frac{|\calN|}{2\delta} \ge \left(\sum_{t=1}^T \gamma_{t}(\eig)\right) \cdot \max_{\eig O,\eig O' \in \calN}\fronorm{\eig O - \eig O'}^2~.
\end{eqnarray*}
In light of the above, our goal will be to construct a $2\epsilon$-packing $\calN$ such that $\inf_{\eig O,\eig O' \in \calN}\fronorm{\eig O - \eig O'}^2$ is as small as possible. This is achieved by the following proposition, which lifts a $1/2$-packing of the unit ball in $d-1$-dimensions to a packing $\calN_0$ of $\Od$, proved in Section~\ref{prop:packing_prop}:
\begin{prop}\label{prop:packing_prop} Fix an $\epsilon_0 \le 1/256$, and let $\calT$ be an $1/2$-packing of $B_{d-1}(1)$. Then, there exists a set $\overline{\calN} \subset \Od$ with $|\calN_0| = |\calT|$ and, for all $A_1 \ne A_2 \in \overline{\calN}$,
\begin{eqnarray}
\opnorm{A_1-A_2} \ge \epsilon_0 /4 \quad \text{ and } \quad \fronorm{A_1-A_2} \le 4\epsilon_0 \:.
\end{eqnarray}
\end{prop}
We now reparameterize the above proposition with $\epsilon_0 = \frac{8\epsilon}{\eig}$. Let $\calT$ be a maximal $1/2$-packing of $\calB_{d-1}(1)$; a standard fact shows that $|\calT| \ge 2^{d-1}$. Them, as long as $\epsilon \le \frac{\eig}{2048}$, $\calN = \eig \overline{\calN}$ is a $2\epsilon$-packing of the set $\eig \Od$, and for all $\eig O, \eig O' \in \calN$, $\fronorm{A_1-A_2} \le 32\epsilon$,
\begin{eqnarray*}
&\sup_{O \in \Od}\Pr_{\eig O}[\|\widehat{A}(T) - \eig O\|_{\op} \ge \epsilon] \ge \delta \text{ for any } \\
&T \text{ such that } (1-2\delta)\log \frac{2^d}{4\delta} \ge \left(\sum_{t=1}^T \gamma_t \right) \cdot (32\epsilon)^{2} \:.
\end{eqnarray*}
In particular, for $\delta \le 1/4$, we see that there exists a universal constant $c_0$ such that $(1-2\delta)\log \frac{2^d}{4\delta} \ge c_0(d + \log (1/\delta))$, and hence for $c = c_0/32^2$, we see that
\begin{eqnarray*}
\sup_{\eig O \in \Od}\Pr[\|\widehat{A}(T) - \eig O\|_{\op} \ge \epsilon] \ge \delta \text{ for any } T: \frac{c_0 \left( d + \log(1/\delta)\right)}{\epsilon^2} \ge \sum_{t=1}^T \gamma_{t}(\eig) \:.
\end{eqnarray*}
Bounding $\sum_{t=1}^T \gamma_{t}(\eig) \le T \gamma_{T}(\eig)$ concludes the proof.
\subsection{Proof of Proposition~\ref{prop:packing_prop}\label{sec:packing_sec}}
We now construction of the packing $\overline{\calN}$. If we define the set $\Skew(d) := \{X \in \R^{d\times d}: X^\top = - X\}$, and recall the matrix exponential $\exp(X) = \sum_{j=0}^\infty X^j/j!$, a well-known theorem in Lie Theory ensures that $\exp(\Skew(d)) \subset \Od$ (see, e.g.~\cite{knapp2016representation}). Moreover, $\exp$ is an approximate isometry (in both $\|\cdot\|_{\op}$ and $\fronorm{\cdot}$) from a small neighborhood of $0 \in \Skew(d)$ to a small neighborhood of the identity $I \in \Od$. Hence, our strategy will be to construct a packing in $\Skew(d)$, and then push it to $\Od$ under the $\exp$ mapping.

Formally, given $\epsilon \le 1/256$, and a $1/2$ pcking of  $B_{d-1}(1)$ $\calT$, define for $w \in \calT$ the matrix
\begin{eqnarray*}
M(w) := \epsilon \left(e_1(0,w)^\top + (0,w)e_1^\top\right) \in \Skew(d)~,
\end{eqnarray*}
where $e_1$ denotes the first canonical basis vector in $R^d$. Observe that $\|M(w)\|_F = \sqrt{2\|w\|^2} = \sqrt{2}\|w\|$ and, since the singular value of $M(w)$ are paired, we have $\|M(w)\|_{\op} = \|w\|$. Hence, for every $w_1 \ne w_2 \in \calB_{d-1}(1)$, we have
\begin{eqnarray*}
\|M(w_1-w_2)\|_{\op} &=& \epsilon \|w_1 - w_2\|_{2} \ge \epsilon /2 ~\text{and}~ \\
\fronorm{M(w_1-w_2)} &=&\sqrt{2}\epsilon \|w_1 - w_2\|_{2}  \le 2\sqrt{2\epsilon} \:.
\end{eqnarray*}
Now, we define our packing $\overline{\calN}$ formally as
\begin{eqnarray*}
\overline{\calN} := \{\exp(M(w)): w \in \calT\} \:.
\end{eqnarray*}
We now introduce the following lemma, proved in Section~\ref{sec:exp_map_proof}, which precisely describes the extent to which $\exp()$ is an isometry:
\begin{lem}\label{lem:exp_map} Let $\|\|$ be a sub-multiplicative norm (e.g., $\opnorm{\cdot}$ or $\fronorm{\cdot}$), and $X,Y \in \R^{d \times d}$. Then,
\begin{eqnarray}
\|\exp(X+Y) -\exp(X) - Y\| \le e^{2K} - 1 - 2K, \text{ where } K = \max\{\|X\|,\|Y\|\}~.
\end{eqnarray}
\end{lem}
We apply the above  with $Y = M(w_1) - M(w_2) = M(w_1 - w_2)$, and $X = M(w_2)$. Then, $X + Y = M(w_1)$, $\max\{\|X\|_{\op},\|Y\|_{\op}\} \le 2\epsilon$, and $\max\{\|X\|_{F},\|Y\|_{F}\} \le 2\sqrt{2}\epsilon$. Hence, Lemma~\ref{lem:exp_map} implies that
	\begin{equation}\label{eq:exp_diff}
	\begin{aligned}
	\|\exp(M(w_1))-\exp(M(w_2)) - M(w_1 - w_2)\|_\op &\le& e^{8\epsilon} - 1 - 8\epsilon \text{ and } \\
	\|\exp(M(w_1))-\exp(M(w_2)) - M(w_1 - w_2)\|_F &\le& e^{8\sqrt{2}\epsilon} - 1 - 8\sqrt{2}\epsilon ~.
	\end{aligned}
	\end{equation}
	We can upper bound both displays in ~\eqref{eq:exp_diff} using the following short technical lemma:
	\begin{lem}\label{claim:exp_claim} Let $t \in [0,\log 2]$. Then $e^t - 1 - t \le t^2$.
	\end{lem}
	\begin{proof}
	Let $f(t) = e^t -1 - t$, and $g(t) = t^2$. Then, $f(0) = f'(0) = g(0) = g'(0) = 0$. Moreover, $f''(t) = e^{t}$, and $g''(t) = 2$. Hence, as long as $0 \le t \le \log 2$, $f(t) = \int_{0}^t \int_{0}^u f''(s)dsdu \le \int_{0}^t \int_{0}^u g''(s)dsdu = g(t)$.
	\end{proof}
	Hence for $\epsilon \le \log 2/4\sqrt{2} \le 1/256$, \eqref{eq:exp_diff} and Lemma~\ref{claim:exp_claim} combine to imply
	\begin{eqnarray*}
	\opnorm{\exp(M(w_1))-\exp(M(w_2)) - M(w_1 - w_2)} &\le& 64\epsilon^2 \text{ and } \\
	\fronorm{\exp(M(w_1))-\exp(M(w_2)) - M(w_1 - w_2)} &\le& 128\epsilon^2~.
	\end{eqnarray*}
	Hence, by the triangle inequality, for $\epsilon \le 1/256$
	\begin{eqnarray*}
	\|\exp(M(w_1))-\exp(M(w_2))\|_{\op} &\ge& \|M(w_1 - w_2)\|_{\op} - 64\epsilon^2 \\
	&=& \|w_1 - w_2\| - 64\epsilon^2  \ge \epsilon/2 - 64\epsilon^2 \ge \epsilon/4~,
	\end{eqnarray*}
	and, again, for $\epsilon \le 1/256$
	\begin{eqnarray*}
	\fronorm{\exp(M(w_1))-\exp(M(w_2))} &\le& \|M(w_1 - w_2)\|_{F} + 128\epsilon^2 \\
	&=& \sqrt{2}\|w_1 - w_2\| + 128\epsilon^2 \le 2\sqrt{2}\epsilon + \epsilon/2 \le 4\epsilon~.
	\end{eqnarray*}



\subsubsection{Proof of Lemma~\ref{lem:exp_map}\label{sec:exp_map_proof}}

	Let $M_{i,j}(X,Y)$ denote the homogenous monomial of degree $j$ such consisting of the $\binom{j}{i}$-products of $X$ $i$-times, and $Y$ $j-i$-times. Note then that $M_{j,j}(X,Y) = X^j$, so that $(X+Y)^{j}-X^j = -X^j + \sum_{i=0}^j M_{i,j}(X,Y) = \sum_{i=0}^{j-1} M_{i,j}(X,Y)$. Moreover, by the sub-multiplicativity of $\|\cdot\|$, we have $\|M(X,Y)\| \le \binom{j}{i}\|X\|_2^{i}\|Y\|_2^{j-i} = \binom{j}{i}K^j$.
	\begin{eqnarray*}
	\|\exp(X + Y) - \exp(X) - Y\| &=&  \|(j!)^{-1}\sum_{j=2}^{\infty} (X+Y)^{j}-X^j\|_\op\\
	&=& \|\sum_{j=2}^{\infty}(j!)^{-1}\sum_{i=0}^{j} M_{i,j}(X,Y) \| \\
	&\le& \sum_{j=2}^{\infty} (j!)^{-1}\sum_{i=0}^{j-1} \|M_{i,j}(X,Y) \| \\
	&\le& \sum_{j=2}^{\infty} (j!)^{-1}\sum_{i=0}^{j-1}  \binom{j}{i}K^j\\
	&\le& \sum_{j=2}^{\infty} (j!)^{-1}(2K)^j = e^{2K} - 1 - 2K~.
	\end{eqnarray*}

\subsubsection{Proof of Lemma~\ref{kl_comp}\label{sec:kl_comp}}
For a matrix $M$, let $M_i$ denote the $i$-th row of $M$, and let $M^{\otimes 2} := M^\top M$
\begin{eqnarray*}
\KL(\Pr_{\eig O}^{(T)},\Pr_{A}^{(T)})
&=& \Exp_{\eig O}\left[\sum_{t=1}^T\sum_{i=1}^n\left\langle (\eig O-A)_i, X_t \right\rangle^2 \right]\\
&=& \Exp_{\epsilon_1,\dots,\epsilon^T}\left[\sum_{t=1}^T\sum_{i=1}^n\left\langle (\eig O-A)_i , \sum_{s=1}^t \eig^{t - s}O^{t-s}\epsilon_s \right\rangle^2 \right]\\
&=& \sum_{t=1}^T\sum_{i=1}^n\left\langle (\eig O - A)_i, \Exp_{\epsilon_1,\dots,\epsilon_T}\left[ \left(\sum_{s=1}^t \eig^{t - s}O^{t-s}\epsilon_s\right)^{\otimes 2}\right]\left(\eig O-A\right)_i \right\rangle~.\\
\end{eqnarray*}
We may now compute that, for any $t \in [T]$,
\begin{eqnarray*}
&&\Exp_{\epsilon_1,\dots,\epsilon_T}\left[ \left(\sum_{s=1}^t \eig^{t - s}O^{t-s}\epsilon_s\right)^{\otimes 2}\right] \\
&=& \sum_{s = 1}^t \Exp[ \eig^{2(t-s)} (O^{2(t-s)})^{\top} O^{2(t-s)} \epsilon_s^2] + \sum_{1 = s \ne s' \le t} \Exp[ \eig^{2t-s - s'} (O^{2(t-s)})^{\top} O^{2(t-s')} \epsilon_s \epsilon_{s'}]\\
 &=& \sum_{s = 1}^t \Exp[ \eig^{2(t-s)} (O^{2(t-s)})^{\top} O^{2(t-s)}] = \sum_{s = 1}^t \eig^{2(t-s)}  I~.
\end{eqnarray*}
Hence, we have,
\begin{eqnarray*}
\KL(\Pr_{\eig O}^{(T)},\Pr_{A}^{(T)})  &=& \sum_{t=1}^T\sum_{i=1}^n\left\langle (\eig O-A)_i, \left(\sum_{s=1}^t \eig^{2(t - s)}\right) I  \cdot (\eig O-A )_i \right\rangle\\
&=& \sum_{t=1}^T\left(\sum_{s=1}^t \eig^{2(t - s)}\right) \sum_{i=1}^n \|(\eig O-A)_i\|^2_2  \\
&=& \|\eig O-A\|_F^2 \sum_{t=1}^T (\sum_{s=1}^t \eig^{2(t - s)} = \|\eig O-A\|_F^2\left( \sum_{t=1}^T \sum_{s=0}^{t-1} \eig^{2s}\right) ~.
\end{eqnarray*}


\subsection{Proof of Lemma~\ref{lem:Birge}\label{BirgeSec}}Birge's inequality states that $\sum_{i=1}^N \KL(\Pr_i,\Pr_0) \ge (1-\delta)\log (N\frac{1-\delta}{\delta}) + \delta \log( \frac{\delta}{1 - \delta/N})$ \citep{boucheron13}. Observe that $\delta \log(\frac{\delta}{1 - \delta/N}) \ge \delta \log \frac{\delta}{N(1-\delta)}  = -\delta \log\frac{N(1-\delta)}{\delta}$. Hence $\sum_{i=1}^N \KL(\Pr_i,\Pr_0) \ge (1- 2\delta) \log( \frac{(1-\delta)N}{\delta}) \ge (1- 2\delta) \log \frac{N}{2\delta}$ for $\delta < 1/2$. For the second statement, choose $\calE_{A} := \{\opnorm{A - \ALS(T)} < \epsilon\}$ for $A \in \calN$. Since $\calN$ is $2\epsilon$-separated in $\opnorm{\cdot}$, all $\calE_i$ are disjoint. Hence, for any $A_0 \in \calN$
\begin{eqnarray}
(1-2\delta) \log(|\calN|/2\delta) \le \frac{1}{|\calN|-1}\sum_{A \in \calN - \{A_0\}}^N \KL(\Pr_A^{(T)},\Pr_{A_0}^{(T)}) \le \sup_{A \in \calN - \{A_0\}}^N \KL(\Pr_A^{(T)},\Pr_{A_0}^{(T)}) \:.
\end{eqnarray}
Since $A_0$ was arbitrary, we may pass to an $\inf$ over all $A_0 \in \calN$.







\begin{comment}

For $\eig = 1$, then we have $\sum_{t=1}^T (\sum_{s=1}^t \eig^{2s)} = \sum_{t=1}^T t = T^2/2$. For $\eig \ne 1$,
\begin{eqnarray*}
\sum_{t=1}^T \sum_{s=1}^t\eig^{2t} &=& \sum_{t=1}^T \eig^2\frac{\eig^{2t} - 1}{\eig^2 - 1} \\
&=&\frac{\eig^2}{\eig^2 - 1}\left( -T + \sum_{t=1}^T\eig^{2t} \right) = \frac{\eig^2}{\eig^2 - 1}\left(\eig^2\frac{\eig^{2T} - 1}{\eig^2 - 1} - T  \right)\\
\end{eqnarray*}
\begin{eqnarray*}
\|\exp(X + Y) - \exp(X) - Y\|_\op &=&  \|(j!)^{-1}\sum_{j=2}^{\infty} (X+Y)^{j}-X^j\|_\op\\
&=& \|(j!)^{-1}\sum_{j=2}^{\infty}\sum_{i=0}^{j-1} M_{i,j}(X,Y) \|_\op \\
&\le& \sum_{j=2}^{\infty} (j!)^{-1}\sum_{i=0}^{j-1} \|M_{i,j}(X,Y) \| \\
&\le& \sum_{j=2}^{\infty} (j!)^{-1}\sum_{i=0}^{j-1}  \binom{j}{i}\|X\|_2^{i}\|Y\|_2^{j-i} \\
&=& \|Y\|_2\sum_{j=2}^{\infty}(j!)^{-1}\sum_{i=0}^{j-1} \binom{j}{i}\|X\|_2^{i}\|Y\|_2^{j-i-1} \\
&\le& \|Y\|_2\sum_{j=2}^{\infty}(j!)^{-1}\sum_{i=0}^{j-1} \binom{j}{i} K^{j-1}\\
&\le& \|Y\|_2\sum_{j=2}^{\infty}(j!)^{-1}\frac{1}{K}\sum_{i=0}^{j} \binom{j}{i} K^{j}\\
&=& \|Y\|_2\sum_{j=2}^{\infty}(j!)^{-1}\frac{1}{K}(2K)^j \\
&=& 2\|Y\|_2\sum_{j=2}^{\infty}(j!)^{-1}(2K)^{j-1} = 2\|Y\|_2(\exp(2K)-1)
\end{eqnarray*}
Similarly, one can also just bound $\sum_{i=0}^{j-1} \|M_{i,j}(X,Y) \| \le $
\end{comment}





