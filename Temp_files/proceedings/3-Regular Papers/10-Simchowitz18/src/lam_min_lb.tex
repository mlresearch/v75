%!TEX root = LWM_colt.tex
\section{Analysis of Standard LS\label{app:op_norm_ls}}
Here, we show that given a regression of the form $Y_t = \Ast X_t + \noise_t$ for $Y_t,X_t \in \R^d$, $\Ast \in \R^{d \times d}$, where $\noise_t$ are an i.i.d. Gaussian noise sequence, then $\|\ALS - \Ast\|_{\op} \gtrsim \sqrt{d/ \lambda_{\min}(\sum_{t=1}^T X_tX_t^\top)}$ is in fact necessary. This is a consequence of the fact that the operator-norm error involves a supremum over all directions in $\calS^{d-1}$. Formally,
\begin{thm}\label{thm:alg_spec_lb} Let $X_1,\dots,X_T \in \R^d$ be an arbitrary dynamical process, let $\noise_1,\dots,\noise_t \overset{i.i.d.}{\sim} \calN(0,I_d)$, and independence of $X_1,\dots,X_T$. Then, given observations $Y_t = \Ast X_t + \noise_t$ for $\Ast \in \R^{d\times d}$ and $t \in \{1,\dots,T\}$, the least-squares estimator $\ALS$ satisfies the lower bound
\begin{eqnarray*}
\Exp\left[\|\ALS - A_*\|_{\op}^2 \big{|} X_1,\dots,X_T\right] \ge  \frac{d}{\lambda_{\min}\left(\sum_{t=1}^T X_tX_t\right)}
\end{eqnarray*}
\end{thm}
\begin{proof} Conditioning on $X_1,\dots,X_T$, we may assume without loss of generality that $\matX$ is deterministic. We let
	$\matX \in \R^{T \times d}$ denote the matrix whose rows are $X_t$, and $\mateps \in \R^{T \times d}$ denote the matrix
	whose rows are $\epsilon_t$. Then, $\widehat{A}(T) - A_* = \matX^{\dagger} \mateps
	$. Moreover, if $v_* \in \arg\max_{v: \|v\|_2 = 1}\|v^{\top}\matX^{\dagger}\|_2$, then $\|v^{\top}\matX^{\dagger}\|_2 = \lambda_{\min}(\sum_{t=1}^T X_tX_t)^{-1/2}$. Moreover, $v_*$ depends only on $\matX$ which by construction is independent of $\mateps$. Hence, $v^{\top}\matX^{\dagger}\mateps \sim \mathcal{N}(0,\lambda_{\min}(\sum_{t=1}^T X_tX_t)^{-1} \cdot I_d) $. And hence,
	\begin{eqnarray*}
	\Exp\left[\|\widehat{A}(T) - A_*\|_{\op}^2\right] &=& \Exp\left[\left\|\matX^{\dagger} \mateps\right\|_{\op}^2\right] = \Exp\left[\sup_{v \in \calS^{d-1}} \left\|v^{\top}\matX^{\dagger} \mateps\right\|_2^2\right] \\
	&\ge& \Exp\left[ \left\|v_*^{\top}\matX^{\dagger} \mateps\right\|_2^2\right] \\
	&=& \Exp\left[\left\|w\right\|^2 \big{|} w \sim \mathcal{N}(0,\lambda_{\min}(\sum_{t=1}^T X_tX_t)^{-1} \cdot I_d)\right] = d\lambda_{\min}\left(\sum_{t=1}^T X_tX_t\right)^{-1}~.
	\end{eqnarray*}
\end{proof}