%!TEX root = LWM.tex
\section{Existence of $k$ for $\rho(\Ast) \leq 1$\label{app:consistent}}
\label{sec:appendix:kst}

\begin{prop}\label{prop:det_bound} Let $\Ast = SJS^{-1}$ and $T \ge d$, where $J$ has block sizes $b_{1},\dots,b_L$. Then,
\begin{eqnarray} 
 \log \det(\Gamma_T \Gamma_{k}^{-1}) \le 2d\log \cond(S) + d \log \frac{T}{k} +  4\log T \sum_{\ell: b_{\ell} \ge 2} b_{\ell}^2 
\end{eqnarray}
where $\cond(S)$ denotes the complex condition number of $S$, namely $\sqrt{\lambda_{\max}(S^*S)/\lambda_{\min}(S^*S)}$.
\end{prop}




The above proposition directly implies consistency for whenever $\rho(A_*) \le 1$:
\begin{cor}\label{cor:consistent}[Consistency of Least Squares] There exists a universal constants $C,c > 0$ such that for any time horizon $T$, $\delta \in (0,1/2)$, and any $\Ast$ with $\rho(A_*) \le 1$ and Jordan decomposition $\Ast = SJS^{-1}$, where $J$ has block sizes $b_{1},\dots,b_L$, $k \in \mathbb{N}$ satisfies the conditions of Theorem~\ref{stable_thm} provided that
\begin{eqnarray}\label{eq:T_over_K_jordan}
\frac{T}{k} \ge c\left( d \log \left(\frac{d\cond(S)T}{k \delta }\right) + \log T \sum_{\ell: b_{\ell} \ge } b_{\ell}^2  \right)~,
\end{eqnarray}
Taking $k =1$, implies a minimax-rate of estimation of 
\begin{eqnarray}\label{eq:minmax_rate}
\|A - A^*\|_{\op} \le C\sqrt{ \frac{  \log (dT\cond(S)/\delta) + \log T \sum_{\ell: b_{\ell} \ge 2} b_{\ell}^2 }{T}}
\end{eqnarray}
which holds as long as $T \ge c(d \log (\cond(S)\frac{d}{k \delta } + \sum_{\ell: b_{\ell} \ge 1} b_{\ell}^2 \log (\sum_{\ell: b_{\ell} \ge 1} b_{\ell}^2))$
\end{cor}
\begin{rem}\label{rem:consistent} Observe that if the Jordan decomposition is such that $\sum_{\ell: b_{\ell} \ge 1} b_{\ell}^2 \lesssim d$, then our minimax rate coincides with the minimax rate of linear regression with isotropic covariates up to logarithmic factors. Moreover, we above that 
\end{rem}
For diagonalizable matrices, the rates can be made more explicit:
\begin{cor}\label{cor:diag}
There exists a universal constants $C,c > 0$ such that the following holds. Fix any time horizon $T$, $\delta \in (0,1/2)$, let $\Ast =  SDS^{-1}$ be diagonalizable with $\rho(A_*) \le 1$ and minimum eigenvalue-magnitude $\underline{\rho}$. Then,  $k \in \mathbb{N}$ satisfies the conditions of Theorem~\ref{stable_thm} provided that
\begin{eqnarray}\label{eq:T_over_K_diag}
\frac{T}{k} \ge c d \log (\cond(S)/\delta) 
\end{eqnarray}
This implies that for $T \ge c d \log (\cond(S)/\delta)$
\begin{eqnarray}
\|A - A^*\|_{\op} &\le& C\sqrt{ \frac{ d \log (\cond(S)/\delta)  }{T \Gamma_{\floor{T/c d \log (\cond(S)/\delta)}}}} \label{eq:gamma_diag_bound}\\
&\le& \sqrt{ \frac{d \log (\cond(S)/\delta)  }{T (1 + \cond(S)^{-2}\sum_{s=1}^{\floor{T/c d \log (\cond(S)/\delta)}-1 } \underline{\rho}^{2s}  }} \label{eq:no_gamma_diag_bound}
\end{eqnarray}
\end{cor}


\subsection{Proof of Corrollary~\ref{cor:consistent}}

By Theorem~\ref{stable_thm}, $T$ and $k$ must satisfy the inequality
\begin{eqnarray}\label{eq:cond_burnin}
T/k \ge c(d \log \frac{d}{\delta} + \log \det \Gamma_{T}\Gamma_{k}^{-1}) 
\end{eqnarray}
Equation~\eqref{eq:T_over_K_jordan} follows directly from Proposition~\ref{prop:det_bound}. Specializing to $k = 1$, Proposition~\ref{prop:det_bound} and Theorem~\ref{stable_thm} immediately imply \eqref{eq:minmax_rate}. To upper bound the burn-in time for $T$, we note that by Proposition~\ref{prop:det_bound}, the condition~\eqref{eq:cond_burnin} holds as soon as
\begin{eqnarray}\label{eq:diag_suff}
T \ge c'(d \log \frac{d\cond(S)}{\delta}) \text{ and } T \ge c' \log T(d+ \sum_{\ell:b_{\ell} \ge 2}b_{\ell}^2) 
\end{eqnarray}
for a universal constant $c'$. We can bound $d+ \sum_{\ell:b_{\ell} \ge 2}b_{\ell}^2 \le \sum_{\ell} b_{\ell}^2$, where the latter sum is over all Jordan blocks. We now invoke the following lemma, which we prove shortly:
\begin{lem}\label{alpha:lem}  Let $\alpha \ge 1$. Then for any $T \in \mathbb{N}$, $T \ge \alpha \log T$ as soon as $T \ge 2\alpha \log 4\alpha$
\end{lem}
The lemma implies that it is enough to ensure $T \ge c'(d \log \frac{d\cond(S)}{\delta})$ and that $T \ge 2c' \log T( 4\sum_{\ell} b_{\ell}^2) $, both of which can be ensured by choosing the constant $c$ in Corollary~\ref{cor:consistent} to be sufficiently large.

\begin{proof}[Proof of Lemma~\ref{alpha:lem}]
Taking derivatives, $T \mapsto T - \alpha \log T$ is increasing in $T$ for all $T \ge \alpha$. Hence, it suffices to show that for $T = 2\alpha \log 4\alpha$, $T \ge \alpha \log T$. Observe that for this choice of $\alpha$,
\begin{eqnarray*}
\alpha \log T = \alpha \log (2\alpha \log (4 \alpha) )\\
&\le& \alpha \log ((2\log 4)\cdot \alpha   + 2\alpha^2 )\\
&\le& \alpha \log ( (2 \log 4 + 2) )  \alpha^2) \quad \text{since } \alpha \ge 1\\
&=& 2\alpha \log \sqrt{ 2 \log 4 + 2} \le 2 \alpha \log 4~.
\end{eqnarray*}
\end{proof}

\subsection{Proof of Corrollary~\ref{cor:diag}}
By Theorem~\ref{stable_thm}, $T$ and $k$ must satisfy the inequality
\begin{eqnarray*}
T/k \ge c(d \log \frac{d}{\delta} + \log \det \Gamma_{T}\Gamma_{k}^{-1}) 
\end{eqnarray*}
Using the upper bound on $\log \det \Gamma_T \Gamma_k^{-1}$ from Proposition~\ref{prop:det_bound}, it is enough to ensure that 
\begin{eqnarray}\label{eq:diag_stuff}
\frac{T}{k} \ge c'(d \log \frac{d\cond(S)}{\delta}) \text{ and } \frac{T}{k} \ge c' d \log \frac{T}{k}
\end{eqnarray}
for some universal constant $c'$ (note that the term $\sum_{\ell: b_{\ell} \ge 2} b_{\ell}^2$ vanishes for diagonalizable $A_*$). By inflating $c'$, we may assume $c' \ge 1$. Appling Lemma~\ref{alpha:lem} with change of variables $T \leftarrow T/k$,~\eqref{eq:diag_stuff} holds as long as as long as $T/k \ge 2c' d \log(4 c' d)$, and $T/k \ge c'(d \log \frac{d\cond(S)}{\delta})$, which holds as long as
\begin{eqnarray*}
T/k \ge c'''(d \log \frac{d\cond(S)}{\delta})) 
\end{eqnarray*}
for some constant $c'''$. This proves~\eqref{eq:T_over_K_diag}. We then see that that~\eqref{eq:gamma_diag_bound} in Corollary~\ref{cor:diag} is an immediate consequence of~\ref{cor:diag}, and~\eqref{eq:gamma_diag_bound} follows from the Lowner Lower bound, with $\Ast = SDS^{-1}$:
\begin{eqnarray*}
\Gamma_T &=& I + \sum_{t=1}^{T-1} (\Ast^t)(\Ast^t)^{*}\\
&=& I + \sum_{t=1}^{T-1} (SD^tS^{-1})(SD^tS^{-1})^{*}\\
&=& I + S\sum_{t=1}^{T-1} SD^tS^{-1}S^{-*}D^{t*}S^{*}\\
&\succeq& I + \lambda_{\min}(S^{-1}S^{-*})\sum_{t=1}^{T-1} SD^tD^{t*}S^{*}\\
&\succeq& I + \lambda_{\min}(S^{-1}S^{-*})SS^{*}(\sum_{t=1}^{T-1} \underline{\rho}^{2t})\\
&\succeq& I + \lambda_{\min}(S^{-1}S^{-*})\lambda_{\max}(SS^{*})(\sum_{t=1}^{T-1} \underline{\rho}^{2t})I\\
&=& I(1+\cond(S)^{-2}(\sum_{t=1}^{T-1} \underline{\rho}^{2t}))
\end{eqnarray*}





\subsection{Proof of Proposition~\ref{prop:det_bound}}
Let $\Ast = SJS^{-1}$, where $J$ is a Jordan-Block matrices with blocks $J_1,\dots,J_L$ of sizes $b_1,\dots,b_L$. Note that even those $\Ast$ is real, $S$ and $J$ may be complex valued, so we shall use adjoints instead of transposes. We can compute 
\begin{eqnarray*}
\Gamma_{t} =  S\sum_{s=0}^{t-1} (J^s)S^{-1}S^{-\adj}J^{s\adj}S^{\top}
\end{eqnarray*}
Hence, 
\begin{eqnarray*}
&&\log \det (\Gamma_T \Gamma_k^{-1}) \\
&=&  \log \det \{S\sum_{s=0}^{T-1} J^sS^{-1}S^{-\adj}J^{s\adj}S^\top(S\sum_{s=0}^{k-1} J^sS^{-1}S^{-\adj}J^{s\adj}S^\adj)^{-1} \}\\
&=&  \log \det \{S\sum_{s=0}^{T-1} J^sS^{-1}S^{-\adj}J^{s\adj}(\sum_{s=0}^{k-1} (J^s)S^{-1}S^{-\adj}J^{s\adj})^{-1}S^{-1} \}\\
&=&  \log \det \{\sum_{s=0}^{T-1} J^sS^{-1}S^{-\adj}J^{s\adj}(\sum_{s=0}^{k-1} (J^s)S^{-1}S^{-\adj}J^{s\adj})^{-1} \}\\
\end{eqnarray*}
Lower bounding $S^{-1}S^{-\adj} \lesssim \sigma_{\min}(S)^{-1}$ and $S^{-1}S^{-\adj} \gtrsim 1/\sigma_{\max}(S)^2$, we can upper bound the above by
\begin{eqnarray*}
\log \det (\Gamma_T \Gamma_k^{-1}) &\le& \log \det \{\cond(S)^2\sum_{s=0}^{T-1} J^sJ^{s\adj}(\sum_{s=0}^{k-1} (J^s)J^{s\adj})^{-1} \} \\\\
&=&2 d \log \cond(S) + \log \det \{\sum_{s=0}^{T-1} (J^s)J^{s\adj}(\sum_{s=0}^{k-1} J^sJ^{s\adj})^{-1} \} \\
\end{eqnarray*}
To continue the bound, write the Jordan matrices $J = \mathrm{block}(J_1,\dots,J_L)$ as block diagonal matrices. Then 
\begin{eqnarray*}
\log \det (\Gamma_T \Gamma_k^{-1}) &\le& \log \det \{\cond(S)^2\sum_{s=0}^{T-1} (J^s)J^{s\adj}(\sum_{s=0}^{k-1} J^sJ^{s\adj})^{-1} \} \\\\
&=&2 d \log \cond(S) \sum_{b=1}^B \log \det \{\sum_{s=0}^{T-1} J_{\ell}^sJ_{\ell}^{s\adj}(\sum_{s=0}^{k-1} J_{\ell}^sJ_{\ell}^{s\adj})^{-1} \} \\
\end{eqnarray*}
If $J_{\ell} = a_{\ell}$ is a Jordan matrix with block size equal to $1$, then 
\begin{eqnarray*}
\log \det \{\sum_{s=0}^{T-1} (J_{\ell}^s)J_{\ell}^{s\adj}(\sum_{s=0}^{k-1} J_{\ell}^sJ_{\ell}^{s\adj})^{-1} \} &=& \log \frac{\sum_{s=0}^{T-1} |a_{\ell}|^{2s}}{\sum_{s=0}^{k-1}|a_{\ell}|^{2s}}\\
&\le& \log \frac{\ceil{T/k}\sum_{s=0}^{k-1} |a_{\ell}|^{2s}}{\sum_{s=0}^{k-1}a_{\ell}^{2s}} = \log(\ceil{T/k})\\
\end{eqnarray*}
where the inequality uses the fact that $a_{\ell}^{2s}$ is decreasing. If $\dim (J_{\ell}) > 1$, we shall use the following lemma:
\begin{lem}\label{dom_lem} Let $A \succeq 0$ be a $d \times d$ Complex Hermitian matrix. Then $A \preceq d \mathrm{Diag}(A)$, where $\mathrm{Diag}(A)$ is the diagonal matrices whose diagonal entries are those of $A$. 
\end{lem}
\begin{proof}
We can write
\begin{eqnarray*}
d\mathrm{Diag}(A) - A &=&  (d-1)\mathrm{Diag}(A) +  \sum_{1 \le i \ne j \le d} A_{ij}e_ie_j^{\top} + \overline{A_{ij}}e_je_i^{\top}\\
&=&   \sum_{1 \le i \ne j \le d} A_{ii}e_{i}e_i^\adj + A_{ij}(e_ie_j^{\top} + e_je_i^\top) + A_{jj}e_{j}e_j^\top \succeq 0
\end{eqnarray*}
\end{proof}
We can then bound 
\begin{eqnarray*}
\log \det \{\sum_{s=0}^{T-1} J_{\ell}^sJ_{\ell}^{s\adj}(\sum_{s=0}^{T-1} (J_{\ell}^s)J_{\ell}^{s\adj})^{-1} \} &\overset{(i)}{\le}& \log \det \{\sum_{s=0}^{T-1} J_{\ell}^sJ_{\ell}^{s\adj} \} \\
&\overset{(ii)}{\le}& \log \det \{\dim(J_{\ell})\mathrm{Diag}(\sum_{s=0}^{T-1} (J_{\ell}^s)J_{\ell}^{s\adj}) \} \\\\
&=& \dim(J_{\ell})\log \dim(J_{\ell}) + \sum_{i=1}^{\dim(J_{\ell})} \log (\sum_{s=0}^{T-1}(J_{\ell}^sJ_{\ell}^{s\adj})_{ii} ) \\
&=&\dim(J_{\ell})\log \dim(J_{\ell}) + \sum_{i=1}^{\dim(J_{\ell})} \log (\sum_{s=0}^{T-1}\sum_{j}(J_{\ell}^s)_{ij}^2) 
\end{eqnarray*}
where $(i)$ uses that $\sum_{s=0}^{T-1} J_{\ell}^sJ_{\ell}^{s\adj} \succeq I$, and $(ii)$ uses Lemma~\ref{dom_lem}. We can then compute that if $J_{\ell}$ has diagonals $a_{\ell}$,
\begin{eqnarray*}
(J_{\ell}^s)_{i,j} = \begin{cases} \binom{s}{j - i}a_{\ell}^{s - (j-i)\vee 0} & i \le j \\
0 & \text{ otherwise} \end{cases}
\end{eqnarray*}
So that 
\begin{eqnarray*}
\sum_{j}(J_{\ell}^s)_{ij}^2 &=& \sum_{j \ge 1}(\binom{s}{j - i})^2|a_{\ell}|^{2(T - (j-i)\vee 0)}\\
&=&  \dim(J_\ell)^2 s^{2(\dim(J_\ell) - i)}
\end{eqnarray*}
Hence, 
\begin{eqnarray*}
\sum_{i=1}^{\dim(J_{\ell})} \log (\sum_{s=0}^{T-1}\sum_{j}(J_{\ell}^s)_{ij}^2) &\le& \sum_{i=1}^{\dim(J_{\ell})} \log( \dim(J_{\ell})^2 \sum_{s=0}^{T-1} s^{2(\dim(J_{\ell}) - i)} )\\
&\le& \sum_{i=1}^{\dim(J_{\ell})} \log(\dim(J_{\ell})^2  T^{2(\dim(J_{\ell}) - i) + 1}) \\
&=& 2\dim(J_{\ell})\log (\dim(J_{\ell})) + \sum_{i=1}^{\dim(J_{\ell})} 2(\dim(J_b) - i)+1\log(T)  \\
&=& 2\dim(J_{\ell})\log (\dim(J_{\ell})) + \log(T) \cdot\sum_{i=1}^{\dim(J_{\ell})} i   \\
&\le& 2\dim(J_{\ell})\log (\dim(J_{\ell})) + \dim(J_{\ell})^2 \log T  \\
&\le& 4 \dim(J_{\ell})^2 \log T  \\
\end{eqnarray*}
where the last line uses that $T \ge d \ge \dim(J_{\ell})$, and that $\dim(J_{\ell}) \ge 2$. 









\begin{comment}

\iffalse



To show the existence of $k$ satisfying Theorem~\ref{stable_thm}, we consider the Jordan decomposition $\Ast = SJS^{-1}$.
%
Let $b_{\max}$ denote the maximum block size of $J$.
%
We note that $S$, $J$, and $S^{-1}$ may be complex valued, so we will let $|z| := z\cdot \overline{z}$ for $z \in \C$, 
%
Lemma~\ref{lem:A_upper bound} uses this decomposition to obtain an upper bound on $\tr(\Gamma_T)$ which holds whenever $\rho(\Ast) \le 1$. This implies the following theorem:
\begin{thm}\label{thm:mxistence_kst} Suppose $\rho(\Ast) \le 1$ and fix a $\delta > 0$. Then there exist a universal constant $c'$ such that
\begin{eqnarray}\label{eq:KT_eq}
k(T,\delta) := \floor{\frac{c'T}{d} \cdot \left(\log(d\cond(S)/\delta) + b_{\max} \log T\right)^{-1}}
\end{eqnarray} satisfies the conditions of Theorem~\ref{thm:mxistence_kst} (as long as $k(T,\delta) \ge 1$).
\end{thm}
Note that the scaling of $k(T,\delta)$ in $T$ is $\BigOmega{T/\log T}$, and is thus guaranteed to be strictly positive for $T$ sufficiently large. Moreover, once $k(T,\delta) \ge 1$, we can state the following minimax rate:
\begin{thm}\label{thm:minimax_rate_app} Suppose that $T$ is sufficiently large that $k(T,\delta)$ in~\eqref{eq:KT_eq} is $\ge 1$. Then, there exists a universal constant $C$ such that, with probability at least $1-\delta$,
\begin{eqnarray*}
\opnorm{\ALS - \Ast} \le \sqrt{\frac{C \cdot d \cdot b_{\max}\log(d\cond(S)/\delta)}{\lambda_{\min}(\Gamma_{k(T,\delta)})T}} \le \sqrt{\frac{C \cdot d \cdot b_{\max}\log(d\cond(S)/\delta)}{T}} \:.
\end{eqnarray*}
\end{thm}
\begin{remark}\label{rem:app_minimax_remark}
The rate in Theorem~\ref{thm:minimax_rate_app} coincides with the standard minimax rate for linear regression, $\sqrt{\frac{d+\log(1/\delta)}{T}}$, up to a factor of $\sqrt{b_{\max}\log(d\cond(S)/\delta)}$. Note that for diagonalizable systems, $b_{\max} = 1$, and in view of Remark~\ref{rem:improved_mixing}, one can sharpen $\log$ factors using more involved concentration arguments.
\end{remark}
Lastly, we see from~\eqref{eq:KT_eq} that for any $\delta > 0$, $\lim_{T \to \infty} k(T,\delta) = \infty$. Hence, if $\lim_{k \to \infty} \lambda_{\min}(\Gamma_{k}) < \infty$, we obtain the following corollary:
\begin{cor}\label{cor:app_marg_stable_cor} Suppose that $\lim_{k \to \infty} \lambda_{\min}(\Gamma_{k}) < \infty$; abusing notation slightly denote this quantity $\lambda_{\min}(\Gamma_{\infty})$. Then, there exists a universal constant $C$ and a $T_0$ depending on $\Ast$ and $\delta$ such that, for all $T \ge T_0$,
\begin{eqnarray*}
\opnorm{\ALS - \Ast} \le \sqrt{\frac{C \cdot d \cdot b_{\max}\log(d\cond(S)/\delta)}{\lambda_{\min}(\Gamma_{\infty})T}}
\end{eqnarray*}
\end{cor}
Note that if $\lim_{k \to \infty} \lambda_{\min}(\Gamma_{k}) = \infty$, one can also show that $\opnorm{\ALS - \Ast} \le \BigOh{\log(T)/T}$, hiding constants dependence on $d$,$\delta$, and $\Ast$. To prove Theorem~\ref{thm:mxistence_kst}, we shall prove the following lemma, which bounds the operator norm of $\|A^T\|$ in terms of $b_{\max}$ and $\cond(S)$:
\begin{lem}\label{lem:A_upper bound}
$\|A^T\|_{\op} \le b_{\max} T^{b_{\max}-1}\cond(S)$ as long as $\rho(\Ast) \le 1$. Hence,
\begin{eqnarray}\label{eq:bound_tr_gam}
 \tr(\Gamma_{T}) \le d b_{\max}^2 T^{2b_{\max}-1}\cond(S)^2 \:.
 \end{eqnarray}
\end{lem}
There are many matrices for which $\rho(\Ast) \le 1$, but $\opnorm{A} > 1$, for example the matrix $\Ast = \begin{bmatrix} \rho & 1.2 \\ 0 & \rho \end{bmatrix}$ for any $\rho \le 1$. For such matrices, the crude bound $\opnorm{A^T} \le \opnorm{A}^T$ will degrade exponential in $T$ whereas the bound Lemma~\ref{lem:A_upper bound} degrades only polynomially in $T$. Before proving the lemma, we show how it implies Theorem~\ref{thm:mxistence_kst}:
\begin{proof}[Theorem~\ref{thm:mxistence_kst}]
Let $c$ is the constant in Theorem~\ref{stable_thm}, and recall that $k$  satisfies the conditions of Theorem~\ref{stable_thm} provided $\frac{T}{k} \ge cd\log\left(\frac{\tr(\Gamma_T)}{\delta\lambda_{\min}(\Gramm_k)}\right)$. Since $\Gamma_k \succeq I$, ~\eqref{eq:bound_tr_gam} implies that we may upper bound the RHS of this previous expression by
\begin{eqnarray*}
cd \log(b_{\max} d T^{2b_{\max}-1}\cond(S)^2/\delta) \le c'd(\log( \cond(S) d /\delta) + b_{\max}\log T)~,
\end{eqnarray*}
which concludes the proof.
\end{proof}
Theorem~\ref{thm:minimax_rate_app} is proved by plugging in the bound on $\tr(\Gamma_T)$ from Lemma~\ref{lem:A_upper bound} into Theorem~\ref{thm:mxistence_kst}.
\subsubsection{Proof of Lemma~\ref{lem:A_upper bound}}

We have $\|A^T\| = \|SJ^TS^{-1}\|_{\op} \le \opnorm{S}\opnorm{J^T}\opnorm{S^{-1}} \le \cond(S)\opnorm{J^T}$. Note that $J^T$ is block diagonal, and its operator norm is therefore equal to the largest operator norm of any of its Jordan blocks. Hence, for any $J_0 \in \R^{b,b}$ be an Jordan sub-block of $J$ of size $b \le b_{\max}$ with diagonal element $\lambda$, it suffices to bound $\|J_0^T\|_{\op} \le  b T^{b - 1}$. We compute that
\begin{eqnarray*}
(J_0^T)_{i,j} = \begin{cases} \binom{T}{j - i}\lambda^{T - (j-i)\vee 0} & i \le j \\
0 & \text{ otherwise} \end{cases}
\end{eqnarray*}
Since $1 \le i,j \le b$, we always have $\binom{T}{j - i} \le T^{b - 1}$. Morover, $|\lambda^{T - (j-i) \vee 0}| \le \rho(\Ast)^{T - (j-i) \vee 0} \le 1$. Hence, $|(J_0^T)_{i,j}| \le T^{b - 1}$, and thus,
\begin{eqnarray*}
\|J_0^T\|_{\op} \le \|J_0^T\|_F &=& \sqrt{ \sum_{1 \le i,j \le b} \left(\binom{T}{b - 1}|\lambda|^{T - (b - 1)}\right)^2  }\\
&\le& \sqrt{ (b)^2  \cdot (T^{b_0 - 1})^2 }\\
&\le& b T^{b - 1}, \text{ as needed}.
\end{eqnarray*}
Lastly, we compute
\begin{eqnarray*}
 \tr(\Gamma_{T}) \le Td \|(A^T)(A^T)^{\top}\|_{\op} \le TD\|A^T\|_{\op}^2 \le d b_{\max}^2 T^{2b_{\max}-1}\cond(S)^2~.
 \end{eqnarray*}

\end{comment}