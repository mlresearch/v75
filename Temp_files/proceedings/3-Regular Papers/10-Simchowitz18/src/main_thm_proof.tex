
%!TEX root = LWM.tex

\section{Proof of Theorem~\ref{main_thm}\label{sec:mn_thm_proof}}

Again, we let
$\matX \in \R^{T \times d}$ denote the matrix whose rows are $X_t$, and $\Noise \in \R^{T \times n}$ denote the matrix
whose rows are $\noise_t$, and consider the compact SVD of $\matX$ and
$\matX = \matU \matSig \matV^\top$, where $\matSig,\matV \in \R^{d \times d}$
and $\matU \in \R^{T \times d}$. Recalling~\eqref{eq:error_decomp}, we have $\opnorm{\ALS(T) - A_\star} \le \sigma_{d}(\matX)^{-1}\opnorm{\matU^{\top}\Noise}.$
Let $K$ denote a threshold parameter to be chosen later.
Then~$\opnorm{\ALS(T) - A_\star} \le \sigma_{d}(\matX)^{-1}\opnorm{\matU^{\top}\Noise}$ implies the following set-theoretic inclusions,
\begin{align*}
  &\left\{ \opnorm{\matX^{\dagger} \Noise} \geq \frac{4K}{p\sqrt{k\floor{T/k}\lambda_{\min}(\Gamsb) }} \right\} \cap \left\{ \matX\matX^{\top} \succeq \frac{k \floor{T/k} p^2 \Gamsb}{16} \right\} \\
  &\qquad \subseteq \left\{ \opnorm{\matU^{\top} \Noise} \geq \frac{4K}{p\sqrt{k\floor{T/k}\lambda_{\min}(\Gamsb) }} \sigma_{\min}(\matX)  \right\}\cap \left\{ \matX\matX^{\top} \succeq \frac{k \floor{T/k} p^2 \Gamsb}{16} \right\} \\
  &\qquad \subseteq \left\{ \opnorm{\matU^{\top} \Noise} \geq K \right\} \cap \left\{ \matX\matX^{\top} \succeq \frac{k \floor{T/k} p^2 \Gamsb}{16} \right\} \:.
\end{align*}
Now define the following events
\begin{align*}
  \calE_1 := \left\{ \opnorm{\matU^{\top} \Noise} \geq K \right\} \:, \quad \calE_2 := \left\{\matX^\top \matX \succeq \frac{k\lfloor T/k\rfloor p^2 \Gamsb }{16}\right\} \:, \quad \calE_3 := \left\{ \matX^\top \matX  \npreceq \Gambar \right\} \:.
\end{align*}


Then we have
\begin{align*}
  &\Pr\left[ \opnorm{\ALS(T) - A_\star} \geq \frac{4K}{p\sqrt{k\floor{T/k} \lambda_{\min}(\Gamsb) }} \right] \quad\leq \quad\Pr\left[ \left\{\opnorm{\matX^\dagger \Noise} \geq \frac{ 4K}{p\sqrt{k\floor{T/k}\lambda_{\min}(\Gamsb) }} \right\} \right] \\
  &\qquad\leq \Pr\left[ \left\{\opnorm{\matX^\dagger \Noise} \geq \frac{ 4K}{p\sqrt{k\floor{T/k}\lambda_{\min}(\Gamsb) } } \right\} \cap \calE_3^c \right] + \Pr[ \calE_3 ] \\
  &\qquad\leq \Pr\left[ \left\{\opnorm{\matX^\dagger \Noise} \geq \frac{ 4K}{p\sqrt{k\floor{T/k}\lambda_{\min}(\Gamsb) }} \right\} \cap \calE_2 \cap \calE_3^c \right] + \Pr[ \calE_2^c \cap \calE_3^c] + \Pr[ \calE_3 ] \\
  &\qquad\leq \Pr\left[ \calE_1 \cap \calE_2 \cap \calE_3^c \right] + \Pr[ \calE_2^c \cap \calE_3^c] + \Pr[ \calE_3 ] \:.
  %&\leq& \underbrace{\Pr\left[ \left\{\opnorm{\matU^\top \mateps} \geq K \right\} \cap \calE_1 \right]}_{T_1} + \underbrace{\Pr\left[ \calE_1^c \cap \left\{\sigma_{\max}(\matX)^2 \leq \itlamax\right\}\right]}_{T_2} + \underbrace{\Pr\left[\sigma_{\max}(\matX)^2 \geq \itlamax\right]}_{T_3}~.
\end{align*}
By assumption $\Pr[\calE_3] \leq \delta$. Our task is to show that both $\Pr\left[ \calE_1 \cap \calE_2 \cap \calE_3^c \right]$ and $\Pr[ \calE_2^c \cap \calE_3^c]$ are upper bounded by $\delta$ for the choice of $K =  20\sigma \sqrt{n + d\log(10/p) + \log \det(\Gambar\Gamsb^{-1}) + \log(1/\delta) }$. Both bounds are proven in detail in Appendix~\ref{app:main_thm}, but here we state the main technical arguments required for their proof. All supplementary technical results (Lemma~\ref{lem:eig_Packing_Lem}, Lemma~\ref{lem:martingale_lem} and Proposition~\ref{prop:Small_Ball}) are proven in Appendix~\ref{sec:proof_technical}.

Our bound on $\Pr[ \calE_2^c \cap \calE_3^c]$ comes directly from the BMSB assumption. Applying Proposition~\ref{prop:Small_Ball}, we have that for any fixed $\direc \in \calS^{d-1}$, $ \Pr\left[\sum_{t=1}^T \langle \direc, X_t \rangle^2 \le \frac{\direc^\top\Gamsb \direc p^2}{8} k\floor{T/k}\right] \leq   e^{-\frac{\floor{T/k}p^2}{8}}$. To obtain a Lowner lower-bound $\matX^\top\matX$, we need to strengthen the above pointwise bound into a lower bound on $\inf_{\direc \in \calS^{d-1} }\sum_{t=1}^T \langle \direc, X_t \rangle^2 $. This is achieved through the following covering lemma, proved in Appendix~\ref{sec:proof_lem_eig_pack}:
  \begin{lem}\label{lem:eig_Packing_Lem}
  Let $Q \in \R^{T \times d}$ and consider matrices $0 \prec \Gamin \preceq \Gamax \in R^{d \times d}$. Let $\calT$ be a $1/4$-net of $\sphereGamin$ in the metric $\normGamax$. Then, if  $\inf_{\direc \in \calT} w^{\top}Q^\top Qw \ge 1$ and $Q^\top Q \preceq \Gamax$, we have 
  \begin{eqnarray}
  Q^\top Q \succeq \Gamin/2~.
  \end{eqnarray}
  \end{lem}
  Choosing $Q = \matX\matX^{\top}$, this lemma gives us a bound on the granularity at which we need to cover $\calS^{d-1}$ in terms of a uniform Lowner upper bound $\Gamax = T\Gambar$, and pointwise Lowner lower bound with $\Gamin = \frac{\Gamsb p^2}{8} k\floor{T/k}$. The details are worked through in Appendix~\ref{app:main_sigm_min}. The larger $\Gamax$ is compared to $\Gamin$ in a Lowner sense, the larger the cardinality of the net $\calT$ must be. Crucially, this term enters logarithmically into our final bound via the relative volume $\log \det (\Gamax \Gamin^{-1})$ of the ellipsoids induced by $\Gamax$ and $\Gamin$. 

  Lastly, we bound the probability of $\calE_1 \cap \calE_2 \cap \calE_3^c$, which is the event that $\left\{ \opnorm{\matU^{\top} \Noise} \geq K \right\} $ when the spectrum of $\matX^\top \matX$ is bounded in some desired range. $\matU$ is difficult to control directly. Instead, we work with quantities in terms of $\matX$, namely $\opnorm{\matU^{\top} \Noise} = \sup_{v \in \calS^{n-1}, \direc \in \R^{d}} \frac{\direc^{\top}\matX \Noise v}{\|\matX\direc\|}$.  The key idea here now is to use a martingale-Chernoff bound to show that, for any fix $\direc \in \calS^{d-1}$ and $v \in \calS^{n-1}$, either $\direc^\top \matX^\top \Noise v$ concentrates like a $\sigma^2 \|\matX \direc\|_2^2$-sub-Gaussian random variable, or $\direc^\top \matX^\top \Noise v $ is much smaller than the lower bound on $\sigma_{\min}(\matX)$ under $\calE_2 \cap \calE_3^c$.

  We emphasize that our bound controls $\direc^\top \matX^\top \Noise v$ in terms of $\sigma^2 \|\matX \direc\|_2^2$, which is a proxy for variance. This is subtle yet powerful because it yields an immediate cancellation between the numerator and denominator of $ \frac{\direc^{\top}\matX \Noise v}{\|\matX\direc\|}$. In particular, this implies that $\Pr( \{\frac{\direc^{\top}\matX \Noise v}{\|\matX\direc\|} \gtrsim \log(1/\delta)\} \cap \calE_2 \cap \calE_3^c ) \lesssim \delta$. This lets us reduce our problem to finding an appropriate discretization (see Lemma~\ref{lem:Lin_packing}). We stress that an approach which bounds $\direc^{\top}\matX \Noise v$ and $\|\matX\direc\|$ separately would be considerably less sharp, and would degrade for slower-mixing systems. Our data-dependent concentration bound is a consequence of the following technical lemma, which we apply with $Z_t := \direc^\top X_t$ and $\noiseb_t = \noise_{t}^\top v$.
\begin{lem}\label{lem:martingale_lem} Let $\{\calF_{t}\}_{t \ge 0}$ be a filtration, and $\{Z_t\}_{t\ge 1}$ and $\{\noiseb_t\}_{t\ge 1}$ be real-valued processes adapted to $\calF_{t}$ and $\calF_{t + 1}$ respectively. Moreover, assume $\noiseb_t | \calF_{t}$ is mean zero and $\sigma^2$-sub-Gaussian.  Then, for any positive real numbers $\alpha$, $\beta$, $\beta_-$, $\beta_+$ we have
\begin{eqnarray}\label{eq:normalized}
&\mbox{(a) }& \Pr\left[ \left\{\sum_{t = 1}^T Z_t \noiseb_t  \ge \alpha\right\} \cap \left\{\sum_{t = 1}^T Z_t^2  \le \beta \right\}\right] \le \exp\left( - \frac{\alpha^2}{2 \sigma^2 \beta}\right).\\
&\mbox{(b) }& \Pr\left[\left\{\frac{\sum_{t=1}^T Z_t \noiseb_t}{\sqrt{\sum_{t=1}^T Z_t^2}} > \alpha\right\} \cap \left\{ \sum_{t=1}^TZ_t^2 \in [\beta_-,\beta_+]\right\}\right] \le \log{\left\lceil\frac{\beta_+}{\beta_-}\right\rceil}\exp\left(\frac{-\alpha^2}{6 \sigma^2}\right).
\end{eqnarray}
  \end{lem}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "LWM"
%%% End:
