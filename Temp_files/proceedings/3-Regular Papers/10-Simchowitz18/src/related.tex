%!TEX root = LWM.tex
\subsection{Related Work}

Most directly related to our work is a recent series of papers by 
\cite{faradonbeh17a,faradonbeh17b}, who study the
linear system identification problem by proving a non-asymptotic rate
on the convergence of the OLS estimator to the true system matrices.
%Faradonbeh et al.\ distinguish between two cases: (a) when the $\Ast$ matrix
%is stable (i.e. spectral radius $\rho(\Ast)$ is bounded by one), and
%(b) when it is not.
In the regime where $\Ast$ is stable, Faradonbeh et al.\ recover 
a similar rate as our result. The major
difference is that the dependence of their analysis on the spectral properties of
$\Ast$ are qualitatively suboptimal, and difficult to interpret precisely.
%
Their analysis is based on separately establishing concentration of the
sample covariance matrix $\sum_{t=1}^{T} X_t X_t^\top$ to the stationary
covariance matrix and bounding the martingale difference term $\sum_{t=1}^{T} X_t \noise_t$.
This decoupled analysis inevitably picks up a dependence on the condition
number of the stationary covariance matrix, which means that
as the system becomes more unstable, their bound deteriorates. 
Indeed, such a strategy is unable to provide any insight into the behavior of OLS when,
for example, $\Ast$ is a scaled orthogonal matrix.
%\maxs{What does their bound say about marginally stable? Does it even hold} 
On the other hand, our analysis does not decouple the two terms, and as
a result our bounds only degrade in the \emph{logarithm} of the condition
number of the finite-time controllability Gramian $\Gamma_T$.
\cite{faradonbeh17a} also provide a bound in the \emph{unstable regime}, which
we believe can be sharpened using our analysis techniques which couple the
covariate- and noise-processes. We leave this to future work. Moreover,
our analysis of one-dimensional, unstable systems corroborates the linear
convergence behavior that \cite{faradonbeh17a} obtain for ``explosive'' systems,
which are systems where
\emph{all} eigenvalues of $\Ast$ lie outside the complex unit disk.


Another closely related work is the scalar analysis by \cite{rantzer18}.
In fact, our proof technique for scalar systems can be seen as an extension of
his technique. The main difference is that by more carefully tracking the terms
that appear in the moment generating function of the noise and covariate processes, we are able to discriminate behaviors that
arise when $\Ast$ is stable versus unstable, and uncover a linear rate of convergence
in the unstable regime.

Our result qualitatively matches the behavior of the
rate given in \cite{dean17},
in that the key spectral quantity governing the rate of convergence is the
minimum eigenvalue of the finite-time controllability Gramian.
The major difference is that the analysis
in Dean et al.\ uses multiple independent trajectories, and discards all but the last
state-transition in each trajectory. This decouples the covariates, and reduces
the analysis to that of random design linear regression with independent covariates.
We note, however, that the analysis in Dean et al.\ applies even when $\Ast$ is
unstable.

More broadly, there has been recent interest in non-asymptotic analysis of linear system
identification problems. Some of the earlier non-asymptotic literature in system identification
include \cite{campi2002finite} and \cite{vidyasagar2008learning}.
The results provided in this line of work are often quite conservative,
featuring quantities which are exponential in the degree of the system.
Furthermore, the rates given are often difficult to interpret.
More recently, \cite{shah12} pose the problem of recovering
a single-input, single-output (SISO) LTI system from linear measurements in the frequency domain as a sparse recovery
problem, proving polynomial sample complexity for recovery in the $\calH_2$-norm.
\cite{hardt16} show that under fairly restrictive assumptions on the
$\Ast$ matrix, projected gradient descent recovers the state-space representation 
of an LTI system with only a polynomial number of samples. 
The analysis from both Shah et al. and Hardt et al. both degrade polynomially
in $\frac{1}{1-\rho(\Ast)}$, where $\rho(\Ast)$ is the spectral radius of underlying $\Ast$.
%
On the other hand, \cite{hazan17} propose a new spectral filtering algorithm 
for online prediction of linear systems where the rates do not degenerate as $\rho(\Ast) \to 1$,
with the caveat that the analysis only applies to symmetric $\Ast$ matrices. \cite{hazan18} extends the analysis to diagonalizable matrices, but the obtained error rates are polynomial in problem parameters. Both works also consider the more general setting where $X_t$ is observed indirectly via $Y_t = CX_t$ for an unknown observation matrix $C$.
%
Moreover, the main metric of interest in both \cite{hardt16} and \cite{hazan17,hazan18}
is the prediction error.  It is not clear how
prediction error guarantees can be used in downstream robust control synthesis, 
whereas the operator norm bounds we provide can be used as direct
inputs into robust synthesis for optimal control problems~\citep{dean17}.


The most well-established technique in the statistics literature for dealing
with non-independent, time-series data is the use of mixing-time arguments~\citep{yu94}.
In the machine learning literature, mixing arguments have been used to develop
generalization bounds~\citep{mohri07,mohri08,kuznetsov17,mcdonald17b} which
are analogous to the classical generalization bounds for i.i.d.\ data.
As mentioned previously, a fundamental limitation of mixing-time arguments is that
the bounds all degrade as the mixing-time increases. This has two implications for
linear system identification: (a) none of these existing results can correctly capture
the qualitative behavior as the $\Ast$ matrix reaches instability, and (b)
these techniques cannot be applied to the regime where $\Ast$ is unstable, for which
estimation is not only well-posed, but should be quite easy.
It is for these reasons we do not pursue such arguments in this work.



%\maxs{what about Cyril's paper?}
%\stephen{TODO: maybe make a short blurb about Mendelson's small ball method.}

%\stephen{---------------------------------------------------------}
%
%\paragraph{Estimation papers.}
%
%\stephen{michigan paper~\cite{faradonbeh17a,faradonbeh17b}}
%\\
%\stephen{dean et al~\cite{dean17}}
%\\
%\stephen{cyril zhang paper~\cite{hazan17}}
%\\
%\stephen{tengyu paper~\cite{hardt16}}
%\\
%\stephen{rantzer paper}
%\\
%\stephen{campi papers}
%
%\paragraph{Mixing papers.}
%
%\stephen{yu94~\cite{yu94}, mohri~\cite{mohri07,mohri08}, kuznetsov, mcdonald and shalizi~\cite{mcdonald17b}, my recent LQR+LSTD paper}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
