%!TEX root = LWM.tex
\section{Introduction}


% As robotics become commonplace in everyday life, it is crucial that such hardware satisfy robust performance and safety guarantees. Such guarantees are underwritten by confident \emph{system identification} of the underlying robot's dynamics. 
%
%%
%%
%In this paper, we take an important step towards a non-asymptotic theory for system identification.
%
%It is not-yet understood how many number of measurements are required to identify an unknown system, even for systems whose dynamics take simple parametric forms. 

System identification---the problem of estimating the parameters of a dynamical system given a time series of its trajectories--- is a fundamental problem in time-series analysis, control theory, robotics, and reinforcement learning.
%
Despite its importance, sharp, non-asymptotic analyses for the sample complexity of system identification are rare. In particular, it is not known how many trajectories are required to identify the parameters of an unknown \emph{linear} system.
%
Properly characterizing this sample complexity would have profound implications, since accurate error bounds are indispensable for designing robust and high-performing control systems. It is important that the bounds be sharp, in the sense that they do not drastically \emph{overestimate} the number of required measurements from system trajectories, which are often time-consuming and prohibitively expensive to collect. 
More broadly, a deeper understanding of system identification would inform other statistical problems where one wishes to learn from non-i.i.d.\ or time-correlated data.

We focus on the problem of identifying a discrete-time \emph{linear dynamical system}
from an observed trajectory. Such systems are described by two parameter matrices 
$\Ast$ and $B_*$, and the dynamics evolve according to the law $X_{t+1} = \Ast X_t + B_* u_t + \noise_t$, where $X_t\in \R^d$ is the state of the system, $u_t$ is the input of the system, and $\noise_t \in \R^d$ denotes unobserved process noise. Linear systems are fundamental in control theory, since they are able to capture the behavior of many natural systems and also able to accurately describe the evolution of an even broader class of systems near their equilibria.
%
% with many applications relying on optimal LQR or iLQR control. 
Despite the importance of understanding the statistical properties of system identification, the relationship between the matrix $\Ast$ and the statistical rate for estimating this matrix remains poorly understood. 
We note that the larger the state vectors $X_t$ are in comparison to the process noise, the larger the \emph{signal-to-noise ratio} 
for estimating $\Ast$ is. As a result, larger matrices $\Ast$ (larger in an appropriate sense, discussed later) lead to states $X_t$ of larger norm, which in turn should make the estimation of $\Ast$ easier. However, it is difficult to theoretically formalize this intuition because the sequence of measurements $X_0, X_1, \ldots, X_{T - 1}$ used for estimation is not i.i.d. and it is dependent on the noise $\noise_0, \noise_1, \ldots, \noise_{T - 2}$. Even the computationally 
straightforward ordinary least-squares ($\OLS$) estimator is difficult to analyze. Standard analyses for $\OLS$ on random design linear regression~\citep{hsu14} cannot be used due to the dependency between the covariates $X_t$ and the process noise $\noise_t$.


In the statistics and machine learning literature, correlated data is usually
dealt with using mixing-time arguments~\citep{yu94}, which relies
on fast convergence to a stationary distribution that
allows correlated
samples to be treated roughly as if they were independent.  While this approach has
been successfully used to develop generalization bounds for time-series
data~\citep{mohri07}, a fundamental limitation of mixing-time arguments is that
the bounds deteriorate when the underlying process is slower to mix. 
%
In the case of linear systems, this behavior is qualitatively incorrect.
%
For linear systems, the rate of mixing is intimately tied to 
the eigenvalues of the matrix $\Ast$, specifically the \emph{spectral radius} $\rho(\Ast)$.
When $\rho(\Ast) < 1$ (i.e. when the system is \emph{stable}), 
the process mixes to a stationary distribution at a rate that 
deteriorates as $\rho(\Ast)$ approaches the boundary of one.
%On the other hand, when $\rho(\Ast) \geq 1$, the system does not mix at all.
%
However, as discussed above, as $\rho(\Ast)$ increases
we expect estimation to become easier 
due to better signal-to-noise ratio, and not harder as mixing-time arguments suggest. 
%
%In the case of linear systems, this behavior is qualitatively incorrect.
%The eigenvalues of the matrix $\Ast$ determine the mixing-time of the 
%system's trajectories; larger eigenvalues yield slower mixing rates
%to the stationary distribution, but states of larger norm.  
%As discussed above, in this situation estimation becomes easier due 
%to better signal-to-noise ratio, and not harder as mixing-time arguments suggest. 
%
We note that recent work by \cite{faradonbeh17a} studying the estimation
problem for linear systems relies in the stable case
on concentration of measure arguments which also degrade as the mixing-time of the
system grows.
%and more recently
%to derive bounds for OLS in our setting~\citep{faradonbeh17a} \horia{(does this paper use mixing arguments?)} \stephen{(not directly, but they do use the spirit of these kind of arguments by arguing about convergence to the stationary covariance matrix)}, 
 
We address these difficulties and offer a new statistical analysis of the ordinary least-squares ($\OLS$) estimator  
of the dynamics $X_{t + 1} = \Ast X_{t} + \noise_t$ with no inputs, when the spectral radius of $\Ast$ is at most one ($\rho(\Ast) \leq 1$, a regime known 
as \emph{marginal stability}). Our results, detailed in Section~\ref{sec:results}, show 
that the statistical performance of $\OLS$ is determined by the minimum eigenvalue of the (finite-time) controllability Gramian $\Gramm_T = \sum_{s = 0}^{T - 1}\Ast^s(\Ast^\top)^s$. The controllability Gramian is a fundamental quantity in the theory of linear systems; the eigenvalues of the Gramian quantify how much white process noise $\noise_t \overset{i.i.d}{\sim} \mathcal{N}(0,\sigma^2 I)$ can excite the system. We show that a larger $\lambda_{\min}(\Gramm_T)$ leads to faster estimation of $\Ast$ in operator norm, and we also prove that up to log factors the $\OLS$ estimator is minimax optimal. Furthermore, in Section~\ref{sec:meta_thm} we offer similar statistical guarantees for a more general class of linear response time-series. 


    
      
