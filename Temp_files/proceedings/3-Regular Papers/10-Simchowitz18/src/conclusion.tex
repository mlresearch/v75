%!TEX root = LWM_arxiv.tex
\section{Discussion and future work}
In this paper, we analyzed the the performance of the $\OLS$ estimator for the estimation of linear dynamics $X_{t + 1} = \Ast X_t + \noise_t$ from a single trajectory $X_0, X_1, \ldots, X_T$, as a special case of linear estimation in time series. We show that, up to logarithmic factors, the $\OLS$ estimator attains an information-theoretic lower bound for $\rho(\Ast) < 1$, provided that $T \gtrsim \frac{d}{1-\rho(\Ast)}$. Moreover, we present an analysis that eschews both mixing and concentration arguments for estimation in time series. We believe that there are several promising directions for future work:
\begin{itemize}
\item Our lower and upper bounds do not perfectly match, even when $\rho(\Ast) < 1$. We believe resolving these discrepancies may shed greater insight into learning in dynamical systems.

\item While our guarantees are stated in the operator norm, control applications may require more granular notions of error which vary for different modes of $\Ast$. Developing error bounds which capture the error rate at each mode may result in more applicable bounds for control applications downstream.

\item While our analysis can accomodate an unknown $\Bst$ as a consequence of Theorem~\ref{main_thm}, 
the resulting rates do not distinguish between the error in the estimation of $\Ast$ and that of $\Bst$. In future, we hope to develop sharp error rates for $\Ast$ and $\Bst$ individually, similar to \cite{dean17} in the independent covariates setting.


\item Our convergences rates degrade for systems with $\rho(\Ast) > 1$, whereas we know from~\cite{faradonbeh17a} that a large class of these systems are still identifiable with $\OLS$. Is there a unified analysis for systems with stable and unstable modes?

\item In many systems, we do not observe $X_t$ directly, but only view $CX_t$ for a matrix $C \in \R^{n_o \times n}$, where $n_0 \le n$. \cite{hazan17,hazan18} provide filtering techniques to minimize regret for diagonalizable matrices; it would be interesting to understand the sample complexity for estimating arbitrary matrices with these limited observations.
\item Ultimately, we would like to understand what sequences of control inputs $u_t$ yield the most accurate estimation of the system $(\Ast,\Bst)$. This would inform adaptive algorithms which adjust the sequence $u_t$ in a sequential fashion, and online algorithms which ensure low regret relative to a given cost functional over time.
\end{itemize}

% From simple linear algebra we know that the error of the $\OLS$ estimator is equal to $\ALS - \Ast = (\matX^\top \matX)^{-1}\matX^\top \Noise$, where $\Noise$ is the $T \times d$ matrix whose rows are $\noise_t^\top$. There are two main difficulties in upper-bounding the size of the estimation error $\opnorm{\ALS - \Ast}$. On one hand it is difficult to quantify the size of the minimum singular value of $\matX$
% because of the dependency structure between the covariates $X_t$. Furthermore, the noise matrix $\Noise$ is not independent of the pseudo-inverse
% of the covariates $\matX^\dagger = (\matX^\top \matX)^{-1} \matX^\top \Noise$. As a result we needed to account for how the noise matrix $\Noise$
% aligns with the right singular vectors of $\matX^\top$. This difficulty lead as to upper bounding the operator norm of the error by $\opnorm{\ALS - \Ast} \leq \sigma_d(\matX)^{-1}\opnorm{\matU^\top \Noise}$ and separately controlling the terms $\sigma_d(\matX)^{-1}$ and $\opnorm{\matU^\top \Noise}$.
% We note that the independent data case the random variables $\opnorm{\matU^\top \Noise}$ and $\opnorm{\Noise}$ have the same distribution, and as a result the analysis is much simpler in that case.

% Our upper bound the terms $\sigma_d(\matX)^{-1}$ and $\opnorm{\matU^\top \Noise}$ through a mix of martingale concentration arguments
% and small ball probabilities, avoiding the need for making mixing time arguments. As a result, we were able to show that when a linear dynamical
% system is less stable it becomes easier to estimate from one trajectory. In particular, we show a fast rate of estimation $\BigOm(\tfrac{d}{T}\log(\tfrac{dT}{\delta}))$ when all the eigenvalues of $\Ast$ lie on the unit disk. In this case, the process $X_t$ does not mix, making mixing
% time arguments void.

% There are several exciting directions for future work

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "LWM"
%%% End:
