% !TEX root = main.tex

\begin{abstract}
Matrix completion is a well-studied problem with many machine learning applications. In practice, the problem is often solved by non-convex optimization algorithms. However, the current theoretical analysis for non-convex algorithms relies crucially on the assumption that each entry of the matrix is observed with exactly the same probability $p$, which is not realistic in practice.

In this paper, we investigate a more realistic semi-random model, where the probability of observing each entry is {\em at least} $p$. %semi-random model: %the effectiveness of these non-convex algorithms when this average-case assumption does not hold. We consider a semi-random model for matrix completion: 
%after each entry is revealed with equal probability, an adversary is allowed to examine the matrix and reveal additional entries. 
Even with this mild semi-random perturbation, %existing non-convex algorithms can no longer work %are not robust in this setting, by 
we can construct counter-examples where existing non-convex algorithms get stuck in bad local optima.

In light of the negative results, we propose a pre-processing step that tries to re-weight the semi-random input, so that it becomes ``similar'' to a random input. We give a nearly-linear time algorithm for this problem, and show that after our pre-processing, all the local minima of the non-convex objective can be used to approximately recover the underlying ground-truth matrix.

%If we view the set of revealed entries as edges of a bipartite graph between the rows and columns of the matrix, the algorithmic question of the pre-processing step is a generalization of the graph sparsification problem in a semi-random model. We exploit this connection and adapt techniques from linear-sized graph sparsification algorithms to solve our pre-processing problem. We believe our ideas can be useful in other semi-random models and is of independent interest.
\end{abstract}