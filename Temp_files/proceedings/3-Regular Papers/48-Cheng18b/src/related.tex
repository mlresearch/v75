% !TEX root = main.tex

%\subsection*{Related Work: Linear-Sized Spectral Sparsification}
%We pose and study the problem of recovering an identity matrix (Problem~\ref{prob:main}). %, which might be of independent interest.
%\todo{Say it has immediate implication to the matrix completion problem, and potentially more applications. We use it to recover a subgraph whose spectrum is close to the original graph (before the adversary added extra edges).}
%
%The main difference between our problem and the linear-sized graph sparsification problem \cite{BatsonSS12, AllenLO15, LeeS15, LeeS17} is the following: we do not know which subset of the vectors sum up to $I$ (while all the rank-one matrices sum up to $I$ for graph sparsification).
%This rules out some of the well-known techniques in graph sparsification (e.g., sampling by effective resistance \cite{SpielmanS11, LeeS15}).
%Moreover, we want a robust algorithm that works well even the assumption holds only approximately: the hidden subset of matrices only sum up \emph{approximately} to $I$.
%In other words, our goal is to find a set of weights $\{w_i\}_i$ so that $\sum_i w_i b_i b_i^\top$ is as close to $I$ as possible.
%
%One of our contributions is to show that the framework of \cite{BatsonSS12} is not only limited to linear-sized graph sparsification;
%The fact that the algorithm in \cite{BatsonSS12} picks one edge at a time \emph{deterministically} makes it much more powerful.
%% The algorithm in \cite{BatsonSS12} works as long as there \emph{exists} a subset of vectors whose outer product sum up to $I$.
%\todo{Related work for BSS. The gradual improvement of the running time, with \cite{LeeS17} speeding it up to nearly-linear time.}
%We adapt the approach in \cite{LeeS17} and present a nearly-linear time algorithm that solves Problem~\ref{prob:main}.
%Our algorithm and analysis are both simpler than those of \cite{LeeS17}, since we do not require the solution to be sparse.\footnote{While Problem~\ref{prob:main} does not ask for a sparse solution, we could obtain $\tilde O(n/\eps^2)$ sparsity by running a linear-sized sparsification algorithm (e.g., \cite{LeeS17}) on our output if desired.}
%We remark that variants of the earlier algorithms for linear-sized sparsification \cite{AllenLO15, LeeS17} also work for our recovery problem.

\subsection{Related Work}
\label{sec:related}
\paragraph{Matrix Completion.} The first theoretical guarantees on matrix completion come from convex relaxation~\citep{srebro2005rank,recht2011simpler,candes2010power,candes2009exact, negahban2012restricted}.
In particular, \cite{candes2010power} and \cite{recht2011simpler} showed that if $\Omega((n_1+n_2)r  \mu^2  \log^2 (n_1+n_2))$ entries are observed randomly (where $\mu$ is the incoherence parameter, see Section~\ref{sec:matprelim}), the nuclear norm convex relaxation recovers the exact underlying low-rank matrix. There have been many works trying to improve the running time (e.g., \citep{srebro2004maximum,mazumder2010spectral,hastie2014matrix} and the references therein).

For the non-convex approaches, the first set of results require a good initialization. \cite{keshavan2010matrix,keshavan2010matrixnoisy} showed that well-initialized gradient descent recovers $\Ms$. Later, it was shown that several other non-convex algorithms, including alternating minimization
\citep{jain2013low,hardt2014fast,hardt2014understanding} and gradient descent \citep{chen2015fast}, converge to the correct solution with a careful initialization. 

Recently, the work of \cite{sun2015guaranteed} (and subsequent works \citep{zhao2015nonconvex,zheng2016convergence,tu2015low}) established a common framework for matrix completion algorithms with a good initialization. In particular, they proved an analog of strong convexity in the neighborhood of the optimal solution. As a result, many different algorithms can converge to $\Ms$. 

For guarantees without careful initialization, \cite{DBLP:conf/icml/SaRO15} analyzed stochastic gradient descent from random initialization. More recently, \cite{ge2016matrix,park2016non,GeJZ17,chen2017memory} showed that the non-convex objective (with careful regularization) does not have any bad local minima.

All of the works above require uniformly random observations. There have also been works that try to solve matrix completion problem under deterministic assumptions \citep{BhojanapalliJ14, LiLR16}. \citep{BhojanapalliJ14} uses convex relaxations, and the conditions in \citep{LiLR16} does not apply to the semi-random model.

\paragraph{Graph Sparsification.}
The goal of graph sparsification is to use a few weighted edges to approximate a given graph. We focus on the notion of spectral similarity in this paper (see Definition~\ref{def:spectral}).~\footnote{For an undirected graph $G$, we use $m$ and $n$ to denote the number of edges and vertices respectively.} The seminal work of~\cite{SpielmanT11} showed that, for any undirected graph $G$, an $\eps$-spectral sparsifier of $G$ with $\tilde O(n / \eps^2)$ edges can be constructed in nearly-linear time. In a beautiful piece of work, \cite{BatsonSS12} showed that an $\eps$-spectral sparsifier with a linear number of $O(n/\eps^2)$ edges exist and can be computed in polynomial time.
Since then, there have been many efforts \citep{AllenLO15, LeeS15, LeeS17} on speeding up the construction of linear-sized sparsifiers. Recently, \cite{LeeS17} gave an algorithm for this problem that runs in near-linear time $\tilde{O}(m/\epsilon^{O(1)})$.



%\paragraph{Non-Convex Optimization.} Recently, a line of work analyzes non-convex optimization by separating the problem into two aspects: the geometric aspect which shows the function has no spurious local minimum and the algorithmic aspect which designs efficient algorithms can converge to local minimum that satisfy first and (relaxed versions) of second order necessary conditions.
%
%Our result is the first that explains the geometry of the matrix completion objective. Similar geometric results are only known for a few problems: SVD/PCA phase retrieval/synchronization, orthogonal tensor decomposition, dictionary learning \cite{baldi1989neural,srebro2003weighted, ge2015escaping,sun2015nonconvex, bandeira2016low}. The matrix completion objective requires different tools due to the sampling of the observed entries, as well as carefully managing the regularizer to restrict the geometry. Parallel to our work Bhojanapalli et al.\cite{bhojanapalli2016personal} showed similar results for matrix sensing, which is closely related to matrix completion. Loh and Wainwright~\cite{DBLP:journals/jmlr/LohW15} showed that for many statistical settings that involve missing/noisy data and non-convex regularizers, any stationary point of the non-convex objective is close to global optima; furthermore, there is a unique stationary point that is the global minimum under stronger assumptions \cite{loh2014support}.
%
%On the algorithmic side, it is known that second order algorithms like cubic regularization \cite{nesterov2006cubic} and trust-region \cite{sun2015nonconvex} algorithms converge to local minima that approximately satisfy first and second order conditions. Gradient descent is also known to converge to local minima \cite{ lee2016gradient} from a random starting point. Stochastic gradient descent can converge to a local minimum in polynomial time from any starting point \cite{pemantle1990nonconvergence,ge2015escaping}. All of these results can be applied to our setting, implying various heuristics used in practice are guaranteed to solve matrix completion. 

\paragraph{Semi-Random Model.} The semi-random model was first proposed by \cite{blum1995coloring} as an intermediate model between average-case and worst-case. Algorithms for semi-random models were developed for many graph problems, including planted clique~\citep{Jerrum92, feige2000finding}, community detection/stochastic block model~\citep{feige2001heuristics,perry2017semidefinite,moitra2016robust}, graph partitioning~\citep{Kucera95,makarychev2012approximation} and correlation clustering~\citep{mathieu2010correlation,makarychev2015correlation}. Most of these works use convex relaxations, and non-convex approaches (including spectral algorithms) were known to fail~\citep{feige2001heuristics,moitra2016robust} for some of these problems. To the best of our knowledge, our work is the first one that tries to fix the non-convex approach using a light-weight convex pre-processing step.

\paragraph{Non-Convex Optimization.} Although non-convex optimization is NP-hard in general, under reasonable assumptions it is possible to find a local minimum efficiently (e.g., \citep{ge2015escaping,agarwal2016finding,jin2017escape}). It follows from Theorem~\ref{thm:main} that, by running these algorithms after our pre-processing step, we get non-convex algorithms that can recover the ground-truth matrix $\Ms$ approximately even in the semi-random model.
% Using these algorithms for finding the local minimum, combined with Theorem~\ref{thm:main} we get a non-convex algorithm that can recover the ground truth matrix $\Ms$ approximately even in the semi-random model.