%!TEX root = main.tex


\subsection{Related Work}
This paper continues the line of work focused on analyzing gradient descent and related heuristics for non-convex optimization problems, examples of which we have discussed before. Theoretically analyzing t-SNE, in particular, was recently considered in a work of \citet{linderman2017clustering} who showed that running t-SNE with early exaggeration causes points from the same cluster to move towards each other (i.e., embedding of any cluster shrinks). As discussed before, however, this does not imply that t-SNE ends up with a visualization as all the clusters could potentially collapse into each other.
Another work by \cite{shaham2017stochastic} derived a theoretical property of SNE, but their result is only nontrivial when the number of clusters is significantly larger than the number of points per cluster, which is an unrealistic assumption.


Mixture models are natural average-case generative models for clusterable data which have been studied as benchmarks for analyzing various clustering algorithms and have a long history of theoretical work. By now, a sequence of results~\citep{DBLP:conf/soda/DasguptaHKM07,DBLP:conf/esa/DasguptaHKM06,arora2005learning,vempala2004spectral,DBLP:conf/colt/AchlioptasM05,DBLP:conf/colt/KannanSV05,DBLP:conf/colt/Vempala07,MR3385380-Hsu13,DBLP:conf/stoc/GeHK15,DBLP:journals/cacm/KalaiMV12,DBLP:conf/focs/BelkinS10,DBLP:conf/stoc/KalaiMV10,DBLP:journals/corr/abs-1711-07465,DBLP:journals/corr/abs-1711-07454,DBLP:journals/corr/abs-1711-07211} have identified efficient algorithms for clustering data from such models under various natural assumptions. %Mixtures of log-concave distributions have also been studied in a similar context~\citep{}.