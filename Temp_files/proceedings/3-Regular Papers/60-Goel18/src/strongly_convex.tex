In this section we discuss performance guarantee for strongly convex cost functions with respect to the norm of switching cost, i.e., for all $t$, there exist $m>0$, such that $f_t(x) - f_t(y) \le \nabla f_t(y)^T (x-y) + \frac{m}{2}\norm{x-y}^2$. 
 \todo{can we do something about this case?}

% For this class of function, we slightly modify Algorithm \ref{alg: online-projection}.
%
% The overall idea is the following:
% \begin{enumerate}
% 	\item We can show that if all $f_t$ are $m$ strongly convex, then $\norm{x_t^* - v_t} \le h(m)$, for some $h$ to be determined later.
% 	\item Hence, instead of projecting to a sublevel set such that the movement cost and hit cost is balanced, we can make potentially bolder move that ensures $\norm{x_t - v_t} \le h(m)$, while still respecting the geometry (that $x_{t-1} - x_t \in \partial f_t(x_t)$). (this step need to be more rigorous, but let's suppose this can be done...)
% 	\item If $H_t \le H_t^*$, then $x_t$ is already in the $h_m$ neighborhood. We can follow the same step.
% 	\item If $H_t > H_t^*$, then we need the analogous result for Lemma \ref{lem: potential-change}. However, we cannot get the result without making further assumption about the cost sequence, we need something like $\norm{v_t - v_{t-1}} \le K$ only happening $O(1)$ times, and appeal to the fact that $ \norm{x_t - v_t} + \norm{v_t - v_{t-1}} + \norm{v_{t-1}-x_{t-1}} \le \norm{x_t - x_{t-1}}$.
% \end{enumerate}
%
% \begin{algorithm}
% 	\begin{algorithmic}[1]
% 			\FOR{$t=1, \ldots, T$}
% 			\STATE If $\norm{x_{t-1}-v_t} \le \beta f_t(v_t)$, then set $x_t = v_t$.
% 			\STATE Otherwise, choose the $m$-sublevel set $K_l$ of $f_t$, i.e., $K_l = \{ x | f_t(x) \le l\}$, such that,
% 			 $d(x_{t-1}, K_l) = \beta l$
% 			\label{projection-step}
% 			 \STATE $x_t = \Pi_{K_l}(x_{t-1})$.
% 			\ENDFOR
% 	\end{algorithmic}
% \caption{Online projection algorithm for $m$-strongly convex functions}
% \label{alg: online-projection-sconvex}
% \end{algorithm}
%