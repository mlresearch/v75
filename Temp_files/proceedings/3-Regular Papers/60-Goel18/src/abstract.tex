We study \emph{smoothed online convex optimization}, a version of online convex optimization where the learner incurs a penalty for changing her actions between rounds. Given a $\Omega(\sqrt{d})$ lower bound on the competitive ratio of any online algorithm, where $d$ is the dimension of the action space, we ask under what conditions this bound can be beaten. We introduce a novel algorithmic framework for this problem, \ouralg\ (\ourack), which works by iteratively projecting the previous point onto a carefully chosen level set of the current cost function so as to balance the switching costs and hitting costs. We demonstrate the generality of the \ourack\ framework by showing how, with different choices of ``balance,'' \ourack\ can improve upon state-of-the-art performance guarantees for both competitive ratio and regret; in particular,  \ourack\ is the first algorithm to achieve a dimension-free competitive ratio, $3 + O(1/\alpha)$,  for locally polyhedral costs, where $\alpha$ measures the ``steepness'' of the costs.  We also prove bounds on the dynamic regret of OBD when the balance is performed in the dual space that are dimension-free and imply that \ourack\ has sublinear static regret.












%We study \emph{smoothed online convex optimization}, a version of online convex optimization where the learner incurs a penalty for changing her actions between rounds.  A fundamental question in this area is whether it is possible for an online algorithm to be constant competitive.  In general, the answer is ``no'' -- there is a fundamental lower bound of $\Omega(\sqrt{d})$, where $d$ is the dimension of the action space. However, in this work, we break through this $\sqrt{d}$ barrier and show that it is possible to have a constant competitive ratio for the class of locally polyhedral cost functions. In particular, we introduce a novel algorithmic framework for this problem, \ouralg, which works by iteratively projecting the previous point onto a carefully chosen level set of the current cost function so as to balance the switching cost incurred with the improvement in the hitting cost. We demonstrate the generality of the \ourack\ framework by showing how, with different choices of ``balance,'' \ourack\ can improve upon state-of-the-art performance guarantees for both competitive ratio and regret.  \ourack\ achieves a dimension-free competitive ratio, $3 + O(1/\alpha)$,  for locally polyhedral costs, where $\alpha$ measures the ``steepness''.  Additionally, we prove bounds on the dynamic regret of OBD when the balance is performed in the dual space that are dimension-free and imply that \ourack\ has sublinear static regret.













% Over the past thirty years, much attention in the the theoretical computer science community has focused around online learning and online algorithms. The online learning community measures 
% algorithm performance via regret, and often studies problems where the online player encounters a sequence of convex costs (online convex optimization).
% The online algorithms community generally measures  performance via the competitive ratio, and often studies problems where switching costs play a central role ($k$-server, metrical task systems). Smoothed Online Convex Optimization (SOCO) is a challenging problem which combines the features of both sets of problems. We introduce a new algorithmic framework, \ouralg, which combines ideas from both communities and generalizes both online gradient descent and mirror descent. Our approach gives new, state-of-the-art algorithms in many challenging settings; in particular, we give the first competitive algorithm for SOCO in the case where the cost functions are``norm-like".


% Over the past thirty years, much attention in the the theoretical computer science community has focused around online learning and online algorithms. The online algorithms community generally measures algorithm performance via the competitive ratio, whereas regret is the standard performance metric in the online learning community. Previous work has shown that achieving both low regret and constant competitive ratio is impossible for the challenging problem of Smoothed Online Convex Optimization (SOCO). In the present article, we show that although achieving both simultaneously might be unachievable, 
% there exists a general framework which subsumes previous results from both communities and gives new, state of the art algorithms in many challenging settings. In particular, we give the first constant competitive algorithm for SOCO in high dimensions.
