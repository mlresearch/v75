\begin{theorem} \label{t:kstepconvergence}
Let $p^{(n)}$ be the distribution of the iterate of Algorithm \ref{ulmcmc} after $n$ steps starting with the initial distribution $p^{(0)}(x,v) =1_{x=x^{(0)}} \cdot 1_{v=0}$. Let the initial distance to optimum satisfy $  \lv x^{(0)} - x^* \rv_2^2 \le \mathcal{D}^2$. If we set the step size to be  
$$\d = \min\left\{\frac{\varepsilon }{104\kappa} \sqrt{\frac{1}{d/m + \mathcal{D}^2}},1\right\} $$
and run Algorithm \ref{ulmcmc} for $n$ iterations with 
$$n \ge  \max\left\{\frac{208\kappa^2}{\varepsilon}\cdot\sqrt{\frac{d}{m}+\mathcal{D}^2},2\kappa\right\}\cdot \log\left( \frac{24\left(\frac{d}{m}+ \mathcal{D}^2 \right)}{\varepsilon}\right),$$
then we have the guarantee that 
\begin{align*}
W_2(p^{(n)} , p^*)\leq \varepsilon.
\end{align*}
\end{theorem}
\begin{remark}\label{r:comparetolangevin}
The dependence of the runtime on $d,\varepsilon$ is thus $\tilde{\mathcal{O}}\left(\frac{\sqrt{d}}{\varepsilon}\right)$, which is a significant improvement over the corresponding $\mathcal{O}\left(\frac{d}{\varepsilon^2}\right)$ runtime of (overdamped) Langevin diffusion by \cite{durmus}. Also note that in almost all regimes of interest $\delta \ll 1$. 
\end{remark}
We note that the $\log(24(d/m + \mathcal{D}^2)/\varepsilon)$ factor can be shaved off by using a time-varying step size. We present this result as Theorem \ref{t:loggone} in Appendix \ref{s:stepsizechange}. In neither theorem have we attempted to optimize the constants. 
\subsubsection{Result with Stochastic Gradients}\label{ss:stochasticgradient}
Now we state convergence guarantees when we have access to noisy gradients, $\nh f(x) = \nabla f(x) + \xi$, where $\xi$ is a independent random variable that satisfies
%stochastic gradients at each step. We assume that the following about the  stochasticity
\begin{enumerate}
\item The noise is unbiased -- $\E{\xi}=0$.

\item The noise has bounded variance -- $\mathbb{E}[\|\xi\|_2^2]\leq d\sigma^2.$
\end{enumerate} 
Each step of the dynamics is now driven by the SDE,
\begin{align}
\label{e:discretelangevindiffusionwithnoise}
d\vh_t &= -\gamma \vh_t dt -u \nh f(\xh_0) dt + (\sqrt{2\gamma u}) dB_t \\
d \xh_t &= \vh_s dt, \nonumber
\end{align}
with an initial condition $(\xh_0,\vh_0)\sim \ph_0$.
Let $\ph_t$ and $\Phih_t$ be defined analogously to $p_t$ and $\Phi_t$ for $(x_t,v_t)$ in Section \ref{ss:underdampedlangevindiffusionnotation}.
\begin{theorem}[Proved in Appendix \ref{app:stochastic}] \label{t:stochasticconvergence}
 Let $p^{(n)}$ be the distribution of the iterate of Algorithm \ref{alg:ulmcmcnoise} (presented in Appendix \ref{app:stochastic}) after $n$ steps starting with the initial distribution $p^{(0)}(x,v) =1_{x=x^{(0)}} \cdot 1_{v=0}$. Let the initial distance to optimum satisfy $  \lv x^{(0)} - x^* \rv_2^2 \le \mathcal{D}^2$. If we set the step size to be  
$$\d = \min\lrbb{\frac{\varepsilon }{310\kappa} \sqrt{\frac{1}{d/m + \mathcal{D}^2}}, \frac{\varepsilon^2L^2}{1440 \sigma^2 d\kappa},1}, $$
and run Algorithm \ref{ulmcmc} for $n$ iterations with 
$$n \ge \max\left\{\frac{2880\kappa^2\sigma^2 d}{\varepsilon^2 L^2},\frac{620\kappa^2}{\varepsilon}\cdot\sqrt{\frac{d}{m}+\mathcal{D}^2},2\kappa \right\} \cdot \log\left( \frac{36\left(\frac{d}{m}+\mathcal{D}^2 \right)}{\varepsilon}\right),$$
then we have the guarantee that 
\begin{align*}
W_2(p^{(n)} , p^*)\leq \varepsilon.
\end{align*}
\end{theorem}
\begin{remark} Note that when the variance in the gradients -- $\sigma^2d$ is large we recover back the rate of overdamped Langevin diffusion and we need $\tilde{\mathcal{O}}(\sigma^2 \kappa^2 d /\epsilon^2)$ steps to achieve accuracy of $\varepsilon$ in $W_2$.
\end{remark}