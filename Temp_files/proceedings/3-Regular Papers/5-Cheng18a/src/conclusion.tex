We present an MCMC algorithm based on the underdamped Langevin diffusion and provide guarantees for its convergence to the invariant distribution in 2-Wasserstein distance. Our result is a quadratic improvement in both dimension ($\sqrt{d}$ instead of $d$) as well as error ($1/\varepsilon$ instead of $1/\varepsilon^2$) for sampling from strongly log-concave distributions compared to the best known results for overdamped Langevin MCMC. In its use of underdamped, second-order dynamics, our work also has connections to Nesterov acceleration \citep{nesterov} and to Polyak's heavy ball method \citep{polyak}, and adds to the growing body of work that aims to understand acceleration of first-order methods as a discretization of continuous-time processes. 

An interesting open question is whether we can improve the dependence on the condition number from $\kappa^2$ to $\kappa$. Another interesting direction would to explore if our approach can be used to sample efficiently from \emph{non-log-concave} distributions. Also, lower bounds in the MCMC field are largely unknown and it would extremely useful to understand the gap between existing algorithms and optimal achievable rates. Another question could be to explore the wider class of second-order Langevin equations and study if their discretizations provide better rates for sampling from particular distributions.