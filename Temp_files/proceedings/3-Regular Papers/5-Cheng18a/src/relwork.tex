The first explicit proof of non-asymptotic convergence of overdamped Langevin MCMC for  log-smooth and strongly log-concave distributions was given by \citet{dalalyan}, where it was shown that discrete, overdamped Langevin diffusion achieves $\varepsilon$ error, in total variation distance, in $\mathcal{O}\left(\frac{d}{\varepsilon^2}\right)$ steps. Following this, \citet{durmus} proved that the same algorithm achieves $\varepsilon$ error, in 2-Wasserstein distance, in $\mathcal{O}\left(\frac{d}{\varepsilon^2}\right)$ steps. \citet{cheng2017convergence} obtained results similar to those by \citet{dalalyan} when the error is measured by KL-divergence. Recently \citet{raginsky2017non} and \citet{dalalyan2017user} also analyzed convergence of overdamped Langevin MCMC with stochastic gradient updates. Asymptotic guarantees for overdamped Langevin MCMC was established much earlier by \citet{gelfand1991recursive,roberts1996exponential}.

Hamiltonian Monte Carlo (HMC) is a broad class of algorithms which involve Hamiltonian dynamics in some form. We refer to \citet{ma} for a survey of the results in this area. Among these, the variant studied in this paper (Algorithm \ref{ulmcmc}), based on  the discretization of \eqref{e:exactlangevindiffusion}, has a natural physical interpretation as the evolution of a particle's dynamics under a force field and drag. This equation was first proposed by \citet{kramers1940brownian} in the context of chemical reactions. The continuous-time process has been studied extensively~\citep{,brockett1997oscillatory,herau2002isotropic,dric2009hypocoercivity,bolley2010trend,calogero2012exponential,mischler2014exponential,dolbeault2015hypocoercivity,gorham2016measuring,baudoin2016wasserstein,  eberle2017couplings}. 


However, to the best of our knowledge, prior to this work, there was no polynomial-in-dimension convergence result for any version of HMC under a log-smooth or strongly log-concave assumption for the target distribution. Most closely related to our work is the recent paper \citet{eberle2017couplings} who demonstrated a contraction property of the continuous-time process defined \eqref{e:exactlangevindiffusion}. That result deals, however, with a much larger class of functions, and because of this the distance to the invariant distribution scales exponentially with dimension $d$. Subsequent to the appearance of the arXiv version of this work, two recent papers also analyzed and provided non-asymptotic guarantees for different versions of HMC. \citet{lee2017convergence} analyzed Riemannian HMC for sampling from polytopes using a logarithmic barrier function. \citet{mangoubi2017rapid} studied a different variant of HMC under similar assumptions to this paper to get a mixing time bound of $\mathcal{O}(\frac{\sqrt{d}\kappa^{6.5}}{\varepsilon})$ in $1$-Wasserstein distance (same as our result in $d$ and $\varepsilon$ but worse in the condition number $\kappa$). They also establish mixing time bounds for higher order integrators (both with and without a Metropolis correction) which have improved dependence in both $d$ and $\varepsilon$ but under a much stronger separability assumption\footnote{They assume that the potential function $f$ is a sum of $d/c$ functions $\{f_i\}_{i=1}^{\lceil\frac{d}{c}\rceil}$, where each $f_i$ only depends on a distinct set of $c$ coordinates, for some constant $c \in \mathbb{N}$.}.

Also related is the recent work on understanding acceleration of first-order optimization methods as discretizations of second-order differential equations \citep{su2014differential,krichene2015accelerated, wibisono2016variational}.