\label{s:stepsizechange}
Here we provide a sharper analysis of underdamped Langevin MCMC by using a varying step size. By choosing an adaptive step size we are able to shave off the log factor appearing in Theorem \ref{t:kstepconvergence}.

\begin{theorem}\label{t:loggone}
Let the initial distribution $p^{(0)}(x,v) =1_{x=x^{(0)}} \cdot 1_{v=0}$ and let the initial distance to optimum satisfy $ \lv x^{(0)} - x^* \rv_2^2 \le \mathcal{D}^2$. Also let $W_2(p^{(0)},p^*) \le 3\left(\frac{d}{m}+\mathcal{D}^2\right) < \epsilon_0$.  We set the initial step size to be  
$$\d_1 = \frac{\epsilon_0 }{2\cdot 104 \kappa} \sqrt{\frac{1}{d/m + \mathcal{D}^2}}, $$
and initial number of iterations,
$$n_1 =  \frac{208\kappa^2}{ \epsilon_0}\cdot\left(\sqrt{\frac{d}{m}+ \mathcal{D}^2}\right)\cdot \log(16).$$
We define a sequence of $\ell$ epochs with step sizes $(\delta_1,\ldots,\delta_{\ell})$ and number of iterations $(n_1,\ldots,n_{\ell})$ where $\delta_1$ and $n_1$ are defined as above. Choose $\ell = \lceil \log(\epsilon^0/\varepsilon)/\log(2)\rceil$ and, for $i\ge 1$ set $\delta_{i+1} = \delta_{i}/2$ and $n_{i+1} = 2 n_{i}$. 

We run $\ell$ epochs of underdamped Langevin MCMC (Algorithm \ref{ulmcmc}) with step size sequence $(\delta_1,\delta_2,\ldots,\delta_{\ell})$ with number of iterations $(n_1,n_2,\ldots,n_{\ell})$ corresponding to each step size. Then we have the guarantee 
\begin{align*}
W_2(p^{(n)} , p^*)\leq \varepsilon,
\end{align*}
with total number of steps $n=n_1+n_2+\ldots+n_{\ell}$ being 
\begin{align*}
n = \frac{416 \log(16)\kappa^2}{ \varepsilon}\cdot \left( \sqrt{\frac{d}{m}+ \mathcal{D}^2}\right).
\end{align*}
\end{theorem}
\begin{Proof} Let the initial error in the probability distribution be $W_2(p^{(0)},p^*) = \epsilon_0$. Then by the results of Theorem \ref{t:kstepconvergence} if we choose the step size to be
\begin{align*}
\delta_1 = \frac{\epsilon_0 }{2\cdot 104 \kappa} \sqrt{\frac{1}{d/m + \mathcal{D}^2}},
\end{align*}
then we have the guarantee that in 
\begin{align*}
n_1 = \frac{208 \kappa^2}{ \epsilon_0}\cdot\left(\sqrt{\frac{d}{m}+ \mathcal{D}^2}\right)\cdot \log(16)
\end{align*}
steps the error will be less than $\epsilon_1 = \epsilon_0/2$. At this point we half the step size $\delta_2 = \delta_1/2$ and run for $n_2 = 2 n_1$ steps. After that we set $\delta_3 = \delta_2/2$ and run for double the steps $n_3= 2 n_2$ and so on. We repeat this for $\ell$ steps. Then at the end if the probability distribution is $p^{(n)}$ by Theorem \ref{t:kstepconvergence} we have the guarantee that $W_2(p^{(n)},p^*) \le \epsilon_0/2^{\ell}<\varepsilon$. The total number of steps taken is 
\begin{align*}
n_1+ n_2 \ldots + n_{\ell}& = \sum_{i=1}^{\ell} n_{i}\\
& = \frac{208\kappa^2}{ \epsilon_0}\cdot \left( \sqrt{\frac{d}{m}+ \mathcal{D}^2}\right)\cdot \log(16) \left\{ \sum_{i=0}^{\ell-1} 2^i\right\}\\
& = 104 \log(16)\kappa^2 \cdot\frac{2^{\ell}}{\epsilon_0}\cdot \left( \sqrt{\frac{d}{m}+ \mathcal{D}^2}\right)\left\{ \sum_{i=0}^{\ell-1} 2^{-i}\right\}\\
& \le 104 \log(16)\kappa^2\cdot\frac{2}{\varepsilon}\cdot \left( \sqrt{\frac{d}{m}+ \mathcal{D}^2}\right)\left\{ 2\right\}\\
& = \frac{416 \log(16)\kappa^2}{\varepsilon}\cdot \left( \sqrt{\frac{d}{m}+ \mathcal{D}^2}\right),
\end{align*}
where the inequality follows by the choice of $\ell$ and an upper bound on the sum of the geometric series.
\end{Proof} 