Our main contribution in this paper is to prove that Algorithm \ref{ulmcmc}, a variant of HMC algorithm, converges to $\varepsilon$ error in 2-Wasserstein distance after $\mathcal{O}\left(\frac{\sqrt{d}\kappa^2}{\varepsilon}\right)$ iterations, under the assumption that the target distribution is of the form $p^*\propto \exp(-(f(x))$, where $f$ is $L$ smooth and $m$ strongly convex (see section \ref{ss:assumptions}), with $\kappa= L/m$ denoting the condition number. Compared to the results of \cite{durmus} on the convergence of Langevin MCMC in $W_2$ in $\mathcal{O}\left(\frac{d\kappa^2}{\varepsilon^2}\right)$ iterations, this is an improvement in both $d$ and $\epsilon$. We also analyze the convergence of chain when we have noisy gradients with bounded variance and establish non-asymptotic convergence guarantees in this setting.