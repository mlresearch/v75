\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand{\transparent@use}[1]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{liu2017accelerated}
\citation{absil2009optimization}
\citation{khachiyan1980polynomial}
\citation{karmarkar1984new}
\citation{nesterov1983method}
\citation{johnson2013accelerating,schmidt2013minimizing,defazio2014saga}
\citation{boumal2016non,ge2017no,sun2017complete,kawaguchi2016deep}
\citation{ghadimi2013stochastic,Reddi16,Agarwal16,Yair16}
\citation{Polyak1963,zhang2016riemannian,attouch2013convergence,shamir2015stochastic}
\citation{boumal2016global,zhang2016first,zhang2016riemannian,mishra2016}
\citation{ambrosio2014metric,bacak2014convex}
\citation{nesterov1983method}
\citation{nesterov1983method}
\citation{nemirovsky1983problem,nesterov2004introductory}
\jmlr@workshop{31st Annual Conference on Learning Theory}
\jmlr@title{An Estimate Sequence for Geodesically Convex Optimization}{An Estimate Sequence for Geodesically Convex Optimization}
\jmlr@author{\Name {Hongyi Zhang} \Email {hongyiz@mit.edu}\\ \addr BCS and LIDS, Massachusetts Institute of Technology \AND \Name {Suvrit Sra} \Email {suvrit@mit.edu}\\ \addr EECS and LIDS, Massachusetts Institute of Technology }{\Name {Hongyi Zhang} \Email {hongyiz@mit.edu}\\ \addr BCS and LIDS, Massachusetts Institute of Technology \AND \Name {Suvrit Sra} \Email {suvrit@mit.edu}\\ \addr EECS and LIDS, Massachusetts Institute of Technology }
\newlabel{jmlrstart}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.0.1}}
\citation{zhang2016first}
\citation{liu2017accelerated}
\citation{liu2017accelerated}
\citation{nesterov1983method}
\citation{nesterov2004introductory}
\citation{su2014differential,flammarion2015averaging,wibisono2016variational}
\citation{allen2014linear}
\citation{arjevani2015lower}
\citation{bubeck2015geometric}
\citation{lessard2016analysis}
\citation{absil2009optimization}
\citation{udriste1994convex}
\citation{zhang2016first}
\citation{zhang2016first}
\citation{zhang2016riemannian,kasai2016riemannian}
\citation{ferreira2002proximal}
\citation{boumal2016global}
\citation{absil2009optimization}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Related work}{2}{subsection.0.1.1}}
\citation{jost2011riemannian}
\citation{burago2001course}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Summary of results}{3}{subsection.0.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{3}{section.0.2}}
\citation{nesterov2004introductory}
\newlabel{assumption:1}{{1}{4}{Background}{assumption.1}{}}
\newlabel{assumption:2}{{2}{4}{Background}{assumption.2}{}}
\newlabel{assumption:3}{{3}{4}{Background}{assumption.3}{}}
\newlabel{assumption:4}{{4}{4}{Background}{assumption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Proposed algorithm: \textsc  {Ragd}}{4}{section.0.3}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Riemannian-Nesterov($x_0, \gamma _0, \{h_k\}_{k=0}^{T-1}, \{\beta _k\}_{k=0}^{T-1}$)}}{5}{algocf.1}}
\newlabel{alg:riemannian-ag}{{1}{5}{Riemannian-Nesterov($x_0, \gamma _0, \{h_k\}_{k=0}^{T-1}, \{\beta _k\}_{k=0}^{T-1}$)}{algocf.1}{}}
\newlabel{ln:y_k}{{1}{5}{Riemannian-Nesterov($x_0, \gamma _0, \{h_k\}_{k=0}^{T-1}, \{\beta _k\}_{k=0}^{T-1}$)}{AlgoLine.0.1}{}}
\newlabel{ln:x_k+1}{{2}{5}{Riemannian-Nesterov($x_0, \gamma _0, \{h_k\}_{k=0}^{T-1}, \{\beta _k\}_{k=0}^{T-1}$)}{AlgoLine.0.2}{}}
\newlabel{eq:x-k+1}{{2}{5}{Riemannian-Nesterov($x_0, \gamma _0, \{h_k\}_{k=0}^{T-1}, \{\beta _k\}_{k=0}^{T-1}$)}{AlgoLine.0.2}{}}
\newlabel{ln:v_k+1}{{3}{5}{Riemannian-Nesterov($x_0, \gamma _0, \{h_k\}_{k=0}^{T-1}, \{\beta _k\}_{k=0}^{T-1}$)}{AlgoLine.0.3}{}}
\newlabel{eq:v-k+1}{{3}{5}{Riemannian-Nesterov($x_0, \gamma _0, \{h_k\}_{k=0}^{T-1}, \{\beta _k\}_{k=0}^{T-1}$)}{AlgoLine.0.3}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Constant Step Riemannian-Nesterov($x_0, h, \beta $)}}{5}{algocf.2}}
\newlabel{alg:constant-step}{{2}{5}{Constant Step Riemannian-Nesterov($x_0, h, \beta $)}{algocf.2}{}}
\citation{nesterov1983method}
\citation{nesterov2004introductory}
\citation{carmon2017convex}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of the geometric quantities in Algorithm \ref  {alg:riemannian-ag}. \textbf  {Left:} iterates and minimizer $x^*$ with $y_{k}$'s tangent space shown schematically. \textbf  {Right:} the inverse exponential maps of relevant iterates in $y_{k}$'s tangent space. Note that $y_k$ is on the geodesic from $x_k$ to $v_k$ (Algorithm \ref  {alg:riemannian-ag}, Line \ref  {ln:y_k}); $\mathrm  {Exp}_{y_k}^{-1}(x_{k+1})$ is in the opposite direction of $\mathrm  {grad} f(y_k)$ (Algorithm \ref  {alg:riemannian-ag}, Line \ref  {ln:x_k+1}); also note how $\mathrm  {Exp}_{y_k}^{-1}(v_{k+1})$ is constructed (Algorithm \ref  {alg:riemannian-ag}, Line \ref  {ln:v_k+1}).}}{6}{figure.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Analysis of a new estimate sequence}{6}{section.0.4}}
\newlabel{sec:general-analysis}{{4}{6}{Analysis of a new estimate sequence}{section.0.4}{}}
\newlabel{def:weak-estimate-sequence}{{1}{6}{Analysis of a new estimate sequence}{theorem.1}{}}
\newlabel{eq:weak-estimate-sequence-definition}{{1}{6}{Analysis of a new estimate sequence}{equation.0.4.1}{}}
\citation{nesterov2004introductory}
\newlabel{thm:estimate-sequence-construction}{{2}{7}{Analysis of a new estimate sequence}{theorem.2}{}}
\newlabel{eq:alpha-k-not-summable}{{4}{7}{Analysis of a new estimate sequence}{Item.7}{}}
\newlabel{eq:phi-recursion}{{3}{7}{Analysis of a new estimate sequence}{equation.0.4.3}{}}
\newlabel{eq:phi-less-overline-phi}{{4}{7}{Analysis of a new estimate sequence}{equation.0.4.4}{}}
\newlabel{thm:estimate-sequence-implication}{{3}{7}{Analysis of a new estimate sequence}{theorem.3}{}}
\citation{nesterov2004introductory}
\newlabel{thm:estimate-sequence-lemma}{{4}{8}{Analysis of a new estimate sequence}{theorem.4}{}}
\newlabel{eq:overline-gamma-k+1}{{5}{8}{Analysis of a new estimate sequence}{equation.0.4.5}{}}
\newlabel{eq:phi-k+1-star}{{7}{8}{Analysis of a new estimate sequence}{equation.0.4.7}{}}
\newlabel{eq:base-change-assumption}{{8}{8}{Analysis of a new estimate sequence}{equation.0.4.8}{}}
\newlabel{thm:complete-square}{{5}{8}{Analysis of a new estimate sequence}{theorem.5}{}}
\newlabel{thm:x-k-bound}{{6}{8}{Analysis of a new estimate sequence}{theorem.6}{}}
\newlabel{thm:main-theorem-general-scheme}{{7}{8}{Analysis of a new estimate sequence}{theorem.7}{}}
\newlabel{eq:convergence-algorithm-1}{{12}{8}{Analysis of a new estimate sequence}{equation.0.4.12}{}}
\transparent@use{.50197}
\transparent@use{.50197}
\@writefile{toc}{\contentsline {section}{\numberline {5}Local fast rate with a constant step scheme}{9}{section.0.5}}
\newlabel{sec:constant-step-analysis}{{5}{9}{Local fast rate with a constant step scheme}{section.0.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A schematic illustration of the geometric quantities in Theorem \ref  {thm:squared-distance-ratio-bound}. Tangent spaces of $y_{k}$ and $y_{k+1}$ are shown in separate figures to reduce cluttering. Note that even on a sphere (which has constant positive sectional curvature), $d(x^*, v_{k+1}), \delimiter "026B30D \mathrm  {Exp}_{y_{k}}^{-1}(x^*)-\mathrm  {Exp}_{y_{k}}^{-1}(v_{k+1})\delimiter "026B30D $ and $ \delimiter "026B30D \mathrm  {Exp}_{y_{k+1}}^{-1}(x^*)-\mathrm  {Exp}_{y_{k+1}}^{-1}(v_{k+1})\delimiter "026B30D $ generally do not equal.}}{9}{figure.2}}
\newlabel{fig:change-base}{{2}{9}{A schematic illustration of the geometric quantities in Theorem \ref {thm:squared-distance-ratio-bound}. Tangent spaces of $y_{k}$ and $y_{k+1}$ are shown in separate figures to reduce cluttering. Note that even on a sphere (which has constant positive sectional curvature), $d(x^*, v_{k+1}), \|\Exp _{y_{k}}^{-1}(x^*)-\Exp _{y_{k}}^{-1}(v_{k+1})\|$ and $ \|\Exp _{y_{k+1}}^{-1}(x^*)-\Exp _{y_{k+1}}^{-1}(v_{k+1})\|$ generally do not equal}{figure.2}{}}
\newlabel{thm:hyperbolic-squared-distance-distortion}{{8}{9}{Local fast rate with a constant step scheme}{theorem.8}{}}
\citation{burago2001course}
\citation{burago2001course}
\citation{meyer1989toponogov}
\newlabel{thm:hypersphere-squared-distance-distortion}{{9}{10}{Local fast rate with a constant step scheme}{theorem.9}{}}
\newlabel{thm:squared-distance-ratio-bound}{{10}{10}{Local fast rate with a constant step scheme}{theorem.10}{}}
\newlabel{eq:y-k-squared-distance-ratio}{{16}{10}{Local fast rate with a constant step scheme}{equation.0.5.16}{}}
\citation{zhang2016first}
\newlabel{eq:y-k+1-squared-distance-ratio}{{17}{11}{Local fast rate with a constant step scheme}{equation.0.5.17}{}}
\newlabel{thm:convergence-induction}{{11}{11}{Local fast rate with a constant step scheme}{theorem.11}{}}
\newlabel{eq:convergence-rate}{{18}{11}{Local fast rate with a constant step scheme}{equation.0.5.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{11}{section.0.6}}
\bibstyle{abbrvnat}
\bibdata{main_agd17}
\bibcite{absil2009optimization}{{1}{2009}{{Absil et~al.}}{{Absil, Mahony, and Sepulchre}}}
\bibcite{Agarwal16}{{2}{2016}{{Agarwal et~al.}}{{Agarwal, {Allen Zhu}, Bullins, Hazan, and Ma}}}
\bibcite{allen2014linear}{{3}{2014}{{Allen-Zhu and Orecchia}}{{}}}
\bibcite{ambrosio2014metric}{{4}{2014}{{Ambrosio et~al.}}{{Ambrosio, Gigli, Savar{\'e}, et~al.}}}
\bibcite{arjevani2015lower}{{5}{2015}{{Arjevani et~al.}}{{Arjevani, Shalev-Shwartz, and Shamir}}}
\bibcite{attouch2013convergence}{{6}{2013}{{Attouch et~al.}}{{Attouch, Bolte, and Svaiter}}}
\bibcite{bacak2014convex}{{7}{2014}{{Bac{\'a}k}}{{}}}
\bibcite{boumal2016global}{{8}{2016{a}}{{Boumal et~al.}}{{Boumal, Absil, and Cartis}}}
\bibcite{boumal2016non}{{9}{2016{b}}{{Boumal et~al.}}{{Boumal, Voroninski, and Bandeira}}}
\bibcite{bubeck2015geometric}{{10}{2015}{{Bubeck et~al.}}{{Bubeck, Lee, and Singh}}}
\bibcite{burago2001course}{{11}{2001}{{Burago et~al.}}{{Burago, Burago, and Ivanov}}}
\bibcite{Yair16}{{12}{2016}{{Carmon et~al.}}{{Carmon, Duchi, Hinder, and Sidford}}}
\bibcite{carmon2017convex}{{13}{2017}{{Carmon et~al.}}{{Carmon, Hinder, Duchi, and Sidford}}}
\bibcite{defazio2014saga}{{14}{2014}{{Defazio et~al.}}{{Defazio, Bach, and Lacoste-Julien}}}
\bibcite{ferreira2002proximal}{{15}{2002}{{Ferreira and Oliveira}}{{}}}
\bibcite{flammarion2015averaging}{{16}{2015}{{Flammarion and Bach}}{{}}}
\bibcite{ge2017no}{{17}{2017}{{Ge et~al.}}{{Ge, Jin, and Zheng}}}
\bibcite{ghadimi2013stochastic}{{18}{2013}{{Ghadimi and Lan}}{{}}}
\bibcite{johnson2013accelerating}{{19}{2013}{{Johnson and Zhang}}{{}}}
\bibcite{jost2011riemannian}{{20}{2011}{{Jost}}{{}}}
\bibcite{karmarkar1984new}{{21}{1984}{{Karmarkar}}{{}}}
\bibcite{kasai2016riemannian}{{22}{2016}{{Kasai et~al.}}{{Kasai, Sato, and Mishra}}}
\bibcite{kawaguchi2016deep}{{23}{2016}{{Kawaguchi}}{{}}}
\bibcite{khachiyan1980polynomial}{{24}{1980}{{Khachiyan}}{{}}}
\bibcite{lessard2016analysis}{{25}{2016}{{Lessard et~al.}}{{Lessard, Recht, and Packard}}}
\bibcite{liu2017accelerated}{{26}{2017}{{Liu et~al.}}{{Liu, Shang, Cheng, Cheng, and Jiao}}}
\bibcite{meyer1989toponogov}{{27}{1989}{{Meyer}}{{}}}
\bibcite{mishra2016}{{28}{2016}{{Mishra and Sepulchre}}{{}}}
\bibcite{nemirovsky1983problem}{{29}{1983}{{Nemirovsky and Yudin}}{{}}}
\bibcite{nesterov1983method}{{30}{1983}{{Nesterov}}{{}}}
\bibcite{nesterov2004introductory}{{31}{2004}{{Nesterov}}{{}}}
\bibcite{Polyak1963}{{32}{1963}{{Polyak}}{{}}}
\bibcite{Reddi16}{{33}{2016}{{Reddi et~al.}}{{Reddi, Hefny, Sra, P{\'{o}}czos, and Smola}}}
\bibcite{schmidt2013minimizing}{{34}{2013}{{Schmidt et~al.}}{{Schmidt, Roux, and Bach}}}
\bibcite{shamir2015stochastic}{{35}{2015}{{Shamir}}{{}}}
\bibcite{su2014differential}{{36}{2014}{{Su et~al.}}{{Su, Boyd, and Candes}}}
\bibcite{sun2017complete}{{37}{2017}{{Sun et~al.}}{{Sun, Qu, and Wright}}}
\bibcite{udriste1994convex}{{38}{1994}{{Udriste}}{{}}}
\bibcite{wibisono2016variational}{{39}{2016}{{Wibisono et~al.}}{{Wibisono, Wilson, and Jordan}}}
\bibcite{zhang2016first}{{40}{2016}{{Zhang and Sra}}{{}}}
\bibcite{zhang2016riemannian}{{41}{2016}{{Zhang et~al.}}{{Zhang, J.~Reddi, and Sra}}}
\citation{nesterov2004introductory}
\@writefile{toc}{\contentsline {section}{\numberline {A}Constant step scheme}{15}{section.0.A}}
\newlabel{thm:constant-step-scheme}{{12}{15}{Constant step scheme}{theorem.12}{}}
\newlabel{eq:mu-and-gamma-0}{{20}{15}{Constant step scheme}{equation.0.A.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Proof of Lemma \ref  {thm:estimate-sequence-construction}}{15}{section.0.B}}
\newlabel{prf:estimate-sequence-construction}{{B}{15}{Proof of Lemma \ref {thm:estimate-sequence-construction}}{section.0.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Proof of Lemma \ref  {thm:complete-square}}{16}{section.0.C}}
\newlabel{prf:complete-square}{{C}{16}{Proof of Lemma \ref {thm:complete-square}}{section.0.C}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Proof of Lemma \ref  {thm:x-k-bound}}{16}{section.0.D}}
\newlabel{prf:x-k-bound}{{D}{16}{Proof of Lemma \ref {thm:x-k-bound}}{section.0.D}{}}
\citation{zhang2016first}
\@writefile{toc}{\contentsline {section}{\numberline {E}Proof of Lemma \ref  {thm:hyperbolic-squared-distance-distortion}}{17}{section.0.E}}
\newlabel{thm:large-c-hyperbolic}{{13}{17}{Proof of Lemma \ref {thm:hyperbolic-squared-distance-distortion}}{theorem.13}{}}
\newlabel{thm:small-c-hyperbolic}{{14}{17}{Proof of Lemma \ref {thm:hyperbolic-squared-distance-distortion}}{theorem.14}{}}
\newlabel{eq:euclidean_law_of_cosines}{{23}{18}{Proof of Lemma \ref {thm:hyperbolic-squared-distance-distortion}}{equation.0.E.23}{}}
\newlabel{eq:hyperbolic_law_of_cosines}{{24}{18}{Proof of Lemma \ref {thm:hyperbolic-squared-distance-distortion}}{equation.0.E.24}{}}
\newlabel{eq:taylor_hyperbolic}{{25}{18}{Proof of Lemma \ref {thm:hyperbolic-squared-distance-distortion}}{equation.0.E.25}{}}
\newlabel{eq:cosh_a_bar}{{26}{18}{Proof of Lemma \ref {thm:hyperbolic-squared-distance-distortion}}{equation.0.E.26}{}}
\newlabel{eq:alpha-absolute-difference}{{{29}}{19}{Proof of Lemma \ref {thm:hyperbolic-squared-distance-distortion}}{AMS.4}{}}
\newlabel{thm:multinomial-identities}{{15}{20}{Proof of Lemma \ref {thm:hyperbolic-squared-distance-distortion}}{theorem.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {F}Proof of Theorem \ref  {thm:convergence-induction}}{20}{section.0.F}}
\newlabel{prf:convergence-induction}{{F}{20}{Proof of Theorem \ref {thm:convergence-induction}}{section.0.F}{}}
\newlabel{eq:xstar-y0-}{{33}{20}{Proof of Theorem \ref {thm:convergence-induction}}{equation.0.F.33}{}}
\newlabel{eq:base-xstar-y-}{{34}{21}{Proof of Theorem \ref {thm:convergence-induction}}{equation.0.F.34}{}}
\newlabel{eq:base-beta-}{{35}{21}{Proof of Theorem \ref {thm:convergence-induction}}{equation.0.F.35}{}}
\newlabel{jmlrend}{{F}{22}{end of An Estimate Sequence for Geodesically Convex Optimization}{section*.5}{}}
