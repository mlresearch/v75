\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Absil et~al.(2009)Absil, Mahony, and Sepulchre]{absil2009optimization}
P-A Absil, Robert Mahony, and Rodolphe Sepulchre.
\newblock \emph{Optimization algorithms on matrix manifolds}.
\newblock Princeton University Press, 2009.

\bibitem[Agarwal et~al.(2016)Agarwal, {Allen Zhu}, Bullins, Hazan, and
  Ma]{Agarwal16}
Naman Agarwal, Zeyuan {Allen Zhu}, Brian Bullins, Elad Hazan, and Tengyu Ma.
\newblock Finding approximate local minima for nonconvex optimization in linear
  time.
\newblock \emph{CoRR}, abs/1611.01146, 2016.

\bibitem[Allen-Zhu and Orecchia(2014)]{allen2014linear}
Zeyuan Allen-Zhu and Lorenzo Orecchia.
\newblock Linear coupling: An ultimate unification of gradient and mirror
  descent.
\newblock \emph{arXiv:1407.1537}, 2014.

\bibitem[Ambrosio et~al.(2014)Ambrosio, Gigli, Savar{\'e},
  et~al.]{ambrosio2014metric}
Luigi Ambrosio, Nicola Gigli, Giuseppe Savar{\'e}, et~al.
\newblock Metric measure spaces with {R}iemannian {R}icci curvature bounded
  from below.
\newblock \emph{Duke Mathematical Journal}, 163\penalty0 (7):\penalty0
  1405--1490, 2014.

\bibitem[Arjevani et~al.(2015)Arjevani, Shalev-Shwartz, and
  Shamir]{arjevani2015lower}
Yossi Arjevani, Shai Shalev-Shwartz, and Ohad Shamir.
\newblock On lower and upper bounds for smooth and strongly convex optimization
  problems.
\newblock \emph{arXiv:1503.06833}, 2015.

\bibitem[Attouch et~al.(2013)Attouch, Bolte, and
  Svaiter]{attouch2013convergence}
Hedy Attouch, J{\'e}r{\^o}me Bolte, and Benar~Fux Svaiter.
\newblock Convergence of descent methods for semi-algebraic and tame problems:
  proximal algorithms, forward--backward splitting, and regularized
  {G}auss--{S}eidel methods.
\newblock \emph{Mathematical Programming}, 137\penalty0 (1-2):\penalty0
  91--129, 2013.

\bibitem[Bac{\'a}k(2014)]{bacak2014convex}
Miroslav Bac{\'a}k.
\newblock \emph{Convex analysis and optimization in Hadamard spaces},
  volume~22.
\newblock Walter de Gruyter GmbH \& Co KG, 2014.

\bibitem[Boumal et~al.(2016{\natexlab{a}})Boumal, Absil, and
  Cartis]{boumal2016global}
Nicolas Boumal, P-A Absil, and Coralia Cartis.
\newblock Global rates of convergence for nonconvex optimization on manifolds.
\newblock \emph{arXiv:1605.08101}, 2016{\natexlab{a}}.

\bibitem[Boumal et~al.(2016{\natexlab{b}})Boumal, Voroninski, and
  Bandeira]{boumal2016non}
Nicolas Boumal, Vlad Voroninski, and Afonso Bandeira.
\newblock The non-convex {B}urer-{M}onteiro approach works on smooth
  semidefinite programs.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2757--2765, 2016{\natexlab{b}}.

\bibitem[Bubeck et~al.(2015)Bubeck, Lee, and Singh]{bubeck2015geometric}
S{\'e}bastien Bubeck, Yin~Tat Lee, and Mohit Singh.
\newblock A geometric alternative to {N}esterov's accelerated gradient descent.
\newblock \emph{arXiv:1506.08187}, 2015.

\bibitem[Burago et~al.(2001)Burago, Burago, and Ivanov]{burago2001course}
Dmitri Burago, Yuri Burago, and Sergei Ivanov.
\newblock \emph{A course in metric geometry}, volume~33.
\newblock American Mathematical Society Providence, 2001.

\bibitem[Carmon et~al.(2016)Carmon, Duchi, Hinder, and Sidford]{Yair16}
Yair Carmon, John~C. Duchi, Oliver Hinder, and Aaron Sidford.
\newblock Accelerated methods for non-convex optimization.
\newblock \emph{CoRR}, abs/1611.00756, 2016.

\bibitem[Carmon et~al.(2017)Carmon, Hinder, Duchi, and
  Sidford]{carmon2017convex}
Yair Carmon, Oliver Hinder, John~C Duchi, and Aaron Sidford.
\newblock " convex until proven guilty": Dimension-free acceleration of
  gradient descent on non-convex functions.
\newblock \emph{arXiv preprint arXiv:1705.02766}, 2017.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock {SAGA}: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1646--1654, 2014.

\bibitem[Ferreira and Oliveira(2002)]{ferreira2002proximal}
OP~Ferreira and PR~Oliveira.
\newblock Proximal point algorithm on {R}iemannian manifolds.
\newblock \emph{Optimization}, 51\penalty0 (2):\penalty0 257--270, 2002.

\bibitem[Flammarion and Bach(2015)]{flammarion2015averaging}
Nicolas Flammarion and Francis Bach.
\newblock From averaging to acceleration, there is only a step-size.
\newblock In \emph{Conference on Learning Theory}, pages 658--695, 2015.

\bibitem[Ge et~al.(2017)Ge, Jin, and Zheng]{ge2017no}
Rong Ge, Chi Jin, and Yi~Zheng.
\newblock No spurious local minima in nonconvex low rank problems: A unified
  geometric analysis.
\newblock \emph{arXiv:1704.00708}, 2017.

\bibitem[Ghadimi and Lan(2013)]{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Johnson and Zhang(2013)]{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  315--323, 2013.

\bibitem[Jost(2011)]{jost2011riemannian}
J{\"u}rgen Jost.
\newblock \emph{Riemannian Geometry and Geometric Analysis}.
\newblock Springer Science \& Business Media, 2011.

\bibitem[Karmarkar(1984)]{karmarkar1984new}
Narendra Karmarkar.
\newblock A new polynomial-time algorithm for linear programming.
\newblock In \emph{Proceedings of the sixteenth annual ACM symposium on Theory
  of computing}, pages 302--311. ACM, 1984.

\bibitem[Kasai et~al.(2016)Kasai, Sato, and Mishra]{kasai2016riemannian}
Hiroyuki Kasai, Hiroyuki Sato, and Bamdev Mishra.
\newblock Riemannian stochastic variance reduced gradient on {G}rassmann
  manifold.
\newblock \emph{arXiv:1605.07367}, 2016.

\bibitem[Kawaguchi(2016)]{kawaguchi2016deep}
Kenji Kawaguchi.
\newblock Deep learning without poor local minima.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  586--594, 2016.

\bibitem[Khachiyan(1980)]{khachiyan1980polynomial}
Leonid~G Khachiyan.
\newblock Polynomial algorithms in linear programming.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  20\penalty0 (1):\penalty0 53--72, 1980.

\bibitem[Lessard et~al.(2016)Lessard, Recht, and Packard]{lessard2016analysis}
Laurent Lessard, Benjamin Recht, and Andrew Packard.
\newblock Analysis and design of optimization algorithms via integral quadratic
  constraints.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (1):\penalty0
  57--95, 2016.

\bibitem[Liu et~al.(2017)Liu, Shang, Cheng, Cheng, and
  Jiao]{liu2017accelerated}
Yuanyuan Liu, Fanhua Shang, James Cheng, Hong Cheng, and Licheng Jiao.
\newblock Accelerated first-order methods for geodesically convex optimization
  on {R}iemannian manifolds.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4875--4884, 2017.

\bibitem[Meyer(1989)]{meyer1989toponogov}
Wolfgang Meyer.
\newblock Toponogov's theorem and applications.
\newblock \emph{SMR}, 404:\penalty0 9, 1989.

\bibitem[Mishra and Sepulchre(2016)]{mishra2016}
Bamdev Mishra and Rodolphe Sepulchre.
\newblock Riemannian preconditioning.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (1):\penalty0
  635--660, 2016.

\bibitem[Nemirovsky and Yudin(1983)]{nemirovsky1983problem}
Arkadi{\u\i}~Semenovich Nemirovsky and David~Borisovich Yudin.
\newblock \emph{Problem complexity and method efficiency in optimization}.
\newblock Wiley, 1983.

\bibitem[Nesterov(1983)]{nesterov1983method}
Yurii Nesterov.
\newblock A method of solving a convex programming problem with convergence
  rate ${O}(1/k^2)$.
\newblock In \emph{Soviet Mathematics Doklady}, volume 27(2), pages 372--376,
  1983.

\bibitem[Nesterov(2004)]{nesterov2004introductory}
Yurii Nesterov.
\newblock \emph{Introductory lectures on convex optimization}, volume~87.
\newblock Springer Science \& Business Media, 2004.

\bibitem[Polyak(1963)]{Polyak1963}
B.T. Polyak.
\newblock Gradient methods for the minimisation of functionals.
\newblock \emph{{USSR} Computational Mathematics and Mathematical Physics},
  3\penalty0 (4):\penalty0 864--878, January 1963.

\bibitem[Reddi et~al.(2016)Reddi, Hefny, Sra, P{\'{o}}czos, and Smola]{Reddi16}
Sashank~J. Reddi, Ahmed Hefny, Suvrit Sra, Barnab{\'{a}}s P{\'{o}}czos, and
  Alexander~J. Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In \emph{Proceedings of the 33nd International Conference on Machine
  Learning, {ICML}}, pages 314--323, 2016.

\bibitem[Schmidt et~al.(2013)Schmidt, Roux, and Bach]{schmidt2013minimizing}
Mark Schmidt, Nicolas~Le Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{arXiv:1309.2388}, 2013.

\bibitem[Shamir(2015)]{shamir2015stochastic}
Ohad Shamir.
\newblock {A Stochastic PCA and SVD Algorithm with an Exponential Convergence
  Rate}.
\newblock In \emph{International Conference on Machine Learning (ICML-15)},
  pages 144--152, 2015.

\bibitem[Su et~al.(2014)Su, Boyd, and Candes]{su2014differential}
Weijie Su, Stephen Boyd, and Emmanuel Candes.
\newblock A differential equation for modeling {N}esterovâ€™s accelerated
  gradient method: Theory and insights.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2510--2518, 2014.

\bibitem[Sun et~al.(2017)Sun, Qu, and Wright]{sun2017complete}
Ju~Sun, Qing Qu, and John Wright.
\newblock Complete dictionary recovery over the sphere {I}: Overview and the
  geometric picture.
\newblock \emph{IEEE Transactions on Information Theory}, 63\penalty0
  (2):\penalty0 853--884, 2017.

\bibitem[Udriste(1994)]{udriste1994convex}
Constantin Udriste.
\newblock \emph{Convex functions and optimization methods on Riemannian
  manifolds}, volume 297.
\newblock Springer Science \& Business Media, 1994.

\bibitem[Wibisono et~al.(2016)Wibisono, Wilson, and
  Jordan]{wibisono2016variational}
Andre Wibisono, Ashia~C Wilson, and Michael~I Jordan.
\newblock A variational perspective on accelerated methods in optimization.
\newblock \emph{Proceedings of the National Academy of Sciences}, page
  201614734, 2016.

\bibitem[Zhang and Sra(2016)]{zhang2016first}
Hongyi Zhang and Suvrit Sra.
\newblock First-order methods for geodesically convex optimization.
\newblock In \emph{29th Annual Conference on Learning Theory (COLT)}, pages
  1617--1638, 2016.

\bibitem[Zhang et~al.(2016)Zhang, J.~Reddi, and Sra]{zhang2016riemannian}
Hongyi Zhang, Sashank J.~Reddi, and Suvrit Sra.
\newblock Riemannian {SVRG}: Fast stochastic optimization on {R}iemannian
  manifolds.
\newblock In \emph{Advances in Neural Information Processing Systems 29}, 2016.

\end{thebibliography}
