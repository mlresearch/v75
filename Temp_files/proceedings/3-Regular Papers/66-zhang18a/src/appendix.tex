\appendix

\section{Constant step scheme}
\begin{lemma}\label{thm:constant-step-scheme}
	Pick $\beta_k\equiv \beta > 0$. 
	If in Algorithm \ref{alg:riemannian-ag} we set \[ h_k\equiv h, \forall k\ge 0, \qquad \gamma_0\equiv\gamma = \frac{\sqrt{\beta^2+4(1+\beta)\mu h}-\beta}{\sqrt{\beta^2+4(1+\beta)\mu h}+\beta}\cdot \mu, \]
	then we have 
	\begin{equation}
	\alpha_k\equiv \alpha = \frac{\sqrt{\beta^2+4(1+\beta)\mu h}-\beta}{2}, \qquad \overline{\gamma}_{k+1}\equiv (1+\beta)\gamma, \qquad \gamma_{k+1}\equiv \gamma, \qquad \forall k\ge 0.
	\end{equation}
\end{lemma}
\begin{proof}
	Suppose that $\gamma_k=\gamma$, then from Algorithm \ref{alg:riemannian-ag} we have $\alpha_k$ is the positive root of
	\[ \alpha_k^2-(\mu-\gamma)h\alpha_k - \gamma h = 0. \]
	Also note
	\begin{equation} \label{eq:mu-and-gamma-0}
	\mu - \gamma = \frac{\beta\alpha}{(1+\beta)h}, \text{~~~~and~~~~} \gamma = \frac{\alpha^2}{(1+\beta)h},
	\end{equation}
	hence
	\begin{align*}
	\alpha_k = &~ \frac{(\mu-\gamma)h + \sqrt{(\mu-\gamma)^2h^2 + 4\gamma h}}{2} \\
	= &~ \frac{\beta\alpha}{2(1+\beta)} + \frac{1}{2}\sqrt{\frac{\beta^2\alpha^2}{(1+\beta)^2} + \frac{4\alpha^2}{1+\beta}}  \\
	= &~ \alpha
	\end{align*}
	Furthermore, we have
	\begin{align*}
	\overline{\gamma}_{k+1} = &~ (1-\alpha_k)\gamma_k + \alpha_k\mu = (1-\alpha)\gamma + \alpha\mu \\
	= &~ \gamma + (\mu-\gamma)\alpha = \gamma + \beta\frac{\alpha^2}{(1+\beta)h} \\
	= &~ (1+\beta)\gamma
	\end{align*}
	and $\gamma_{k+1} = \frac{1}{1+\beta}\overline{\gamma}_{k+1} = \gamma$. Since $\gamma_k=\gamma$ holds for $k=0$, by induction the proof is complete.
\end{proof}

\section{Proof of Lemma \ref{thm:estimate-sequence-construction}} \label{prf:estimate-sequence-construction}
\begin{proof}
	The proof is similar to~\citep[Lemma 2.2.2]{nesterov2004introductory} except that we introduce $\overline{\Phi}_{k+1}$ as an intermediate step in constructing $\Phi_{k+1}(x)$. In fact, to start we have $\Phi_0(x)\le (1-\lambda_0)f(x) + \lambda_0\Phi_0(x)\equiv\Phi_0(x)$. Moreover, assume (\ref{eq:weak-estimate-sequence-definition}) holds for some $k\ge 0$, i.e. $\Phi_k(x^*)-f(x^*)\le \lambda_k(\Phi_0(x^*)-f(x^*))$, then
	\begin{align*}
	\Phi_{k+1}(x^*) - f(x^*) \le &~ \overline{\Phi}_{k+1}(x^*) - f(x^*) \\
	\le &~ (1-\alpha_k)\Phi_k(x^*) + \alpha_k f(x^*) - f(x^*) \\
	= &~ (1-\alpha_k)(\Phi_k(x^*)-f(x^*)) \\
	\le &~ (1-\alpha_k)\lambda_k(\Phi_0(x^*)-f(x^*)) \\
	= &~ \lambda_{k+1}(\Phi_0(x^*)-f(x^*)),
	\end{align*}
	where the first inequality is due to our construction of $\Phi_{k+1}(x)$ in (\ref{eq:phi-less-overline-phi}), the second inequality due to strong convexity of $f$.
	By induction we have $\Phi_k(x^*)\le (1-\lambda_k)f(x^*) + \lambda_k\Phi_0(x^*)$ for all $k\ge 0$. It remains to note that condition \ref{eq:alpha-k-not-summable} ensures $\lambda_k\to 0$.
\end{proof}

\section{Proof of Lemma \ref{thm:complete-square}} \label{prf:complete-square}
\begin{proof}
	We prove this lemma by completing the square:
	\begin{align*}
	\overline{\Phi}_{k+1}(x) = &~\left(1 - \alpha_k\right)\left(\Phi_{k}^* + \frac{\gamma_k}{2}\|\Exp_{y_k}^{-1}(x) - \Exp_{y_k}^{-1}(v_k)\|^2\right) \\
	&~ + \alpha_k \left(f(y_k) + \langle \nabla f(y_k), \Exp_{y_k}^{-1}(x)\rangle + \frac{\mu}{2}\|\Exp_{y_k}^{-1}(x)\|^2\right) \\
	= &~ \frac{\overline{\gamma}_{k+1}}{2}\|\Exp_{y_k}^{-1}(x)\|^2 + \left\langle \alpha_k\nabla f(y_k) - \left(1-\alpha_k \right)\gamma_k \Exp_{y_k}^{-1}(v_k), \Exp_{y_k}^{-1}(x)\right\rangle  \\
	&~ + \left(1 - \alpha_k\right) \left(\Phi_k^* + \frac{\gamma_k}{2}\|\Exp_{y_k}^{-1}(v_k)\|^2 \right) + \alpha_k f(y_k) \\
	= &~ \frac{\overline{\gamma}_{k+1}}{2}\left\|\Exp_{y_k}^{-1}(x) - \left(\frac{(1-\alpha_k)\gamma_k}{\overline{\gamma}_{k+1}} \Exp_{y_k}^{-1}(v_k) - \frac{\alpha_k}{\overline{\gamma}_{k+1}} \nabla f(y_k) \right) \right\|^2 + \Phi_{k+1}^* \\
	= &~ \Phi_{k+1}^* +  \frac{\overline{\gamma}_{k+1}}{2}\left\|\Exp_{y_k}^{-1}(x) - \Exp_{y_k}^{-1}(v_{k+1}) \right\|^2
	\end{align*}
	where the third equality is by completing the square with respect to $\Exp_{y_k}^{-1}(x)$ and use the definition of $\Phi_{k+1}^*$ in (\ref{eq:phi-k+1-star}), the last equality is by the definition of $y_k$ in Algorithm \ref{alg:riemannian-ag}, and $\overline{\Phi}_{k+1}(x)$ is minimized if and only if $x= \Exp_{y_k}\left(\frac{(1-\alpha_k)\gamma_k}{\overline{\gamma}_{k+1}} \Exp_{y_k}^{-1}(v_k) - \frac{\alpha_k}{\overline{\gamma}_{k+1}} \nabla f(y_k)\right) = v_{k+1}$.
\end{proof}

\section{Proof of Lemma \ref{thm:x-k-bound}} \label{prf:x-k-bound}
\begin{proof}
	For $k=0$, $\Phi_k^*\ge f(x_k)$ trivially holds. Assume for iteration $k$ we have $\Phi_k^*\ge f(x_k)$, then from definition (\ref{eq:phi-k+1-star}) we have
	\begin{align*}
	\Phi_{k+1}^* \ge &~ \left(1 - \alpha_k\right) f(x_k) + \alpha_k f(y_k) - \frac{\alpha_k^2}{2\overline{\gamma}_{k+1}}\|\nabla f(y_k)\|^2 + \frac{\alpha_k(1-\alpha_k)\gamma_k}{\overline{\gamma}_{k+1}}\langle \nabla f(y_k), \Exp_{y_k}^{-1}(v_k)\rangle \\
	\ge &~ f(y_k) - \frac{\alpha_k^2}{2\overline{\gamma}_{k+1}}\|\nabla f(y_k)\|^2 + (1-\alpha_k)\left\langle \nabla f(y_k), \frac{\alpha_k\gamma_k}{\overline{\gamma}_{k+1}}\Exp_{y_k}^{-1}(v_k) + \Exp_{y_k}^{-1}(x_k)\right\rangle \\
	= &~ f(y_k) - \frac{\alpha_k^2}{2\overline{\gamma}_{k+1}}\|\nabla f(y_k)\|^2 \\
	= &~ f(y_k) - \frac{h_k}{2}\|\nabla f(y_k)\|^2,
	\end{align*}
	where the first inequality is due to $\Phi_k^*\ge f(x_k)$, the second due to $f(x_k)\ge f(y_k) + \langle\nabla f(y_k), \Exp_{y_k}^{-1}(x_k)\rangle$ by g-convexity, and the equalities follow from Algorithm \ref{alg:riemannian-ag}. On the other hand, we have the bound
	\begin{align*}
	f(x_{k+1}) \le &~ f(y_k) + \langle\nabla f(y_k), \Exp_{y_k}^{-1}(x_{k+1})\rangle + \frac{L}{2}\|\Exp_{y_k}^{-1}(x_{k+1})\|^2 \\
	= &~ f(y_k) - h_k\left(1 - \frac{L h_k}{2}\right)\|\nabla f(y_k)\|^2 \\
	\le &~ f(y_k) - \frac{h_k}{2}\|\nabla f(y_k)\|^2 \le \Phi_{k+1}^*,
	\end{align*}
	where the first inequality is by the $L$-smoothness assumption, the equality from the definition of $x_{k+1}$ in Algorithm \ref{alg:riemannian-ag} Line \ref{eq:x-k+1}, and the second inequality from the assumption that $h_k\le \frac{1}{L}$. Hence by induction, $\Phi_k^*\ge f(x_k)$ for all $k\ge 0$. 
\end{proof}

\section{Proof of Lemma \ref{thm:hyperbolic-squared-distance-distortion}}
\begin{lemma} \label{thm:large-c-hyperbolic}
	Let $a,b,c$ be the side lengths of a geodesic triangle in a hyperbolic space with constant sectional curvature $-1$, and $A$ is the angle between sides $b$ and $c$. Furthermore, assume $b\le\frac{1}{4},c\ge\frac{1}{2}$. Let $\triangle\bar{a}\bar{b}\bar{c}$ be the comparison triangle in Euclidean space, with $\bar{b}=b,\bar{c}=c,\bar{A}=A$, then
	\begin{equation}
	a^2\le (1+2b^2)\bar{a}^2
	\end{equation}
\end{lemma}
\begin{proof}
	We first apply \citep[Lemma 5]{zhang2016first} with $\kappa=-1$ to get 
	\begin{equation*}
	a^2 \le \frac{c}{\tanh(c)}b^2 + c^2 - 2bc\cos(A).
	\end{equation*}
	We also have
	\[ \bar{a}^2 = b^2 + c^2 - 2bc\cos(A). \]
	Hence we get
	\[ a^2-\bar{a}^2\le \left(\frac{c}{\tanh(c)}-1\right)b^2. \]
	It remains to note that for $b\le\frac{1}{4},c\ge\frac{1}{2}$,
	\[ 2a^2\ge 2(c-b)^2\ge 2\left(c-\frac{1}{4}\right)\ge\frac{c}{\tanh(1/2)}-1\ge\frac{c}{\tanh(c)}-1, \]
	which implies $a^2\le(1+2b^2)\bar{a}^2$.
\end{proof}

\begin{lemma} \label{thm:small-c-hyperbolic}
	Let $a,b,c$ be the side lengths of a geodesic triangle in a hyperbolic space with constant sectional curvature $-1$, and $A$ is the angle between sides $b$ and $c$. Furthermore, assume $b\le\frac{1}{4},c\le\frac{1}{2}$. Let $\triangle\bar{a}\bar{b}\bar{c}$ be the comparison triangle in Euclidean space, with $\bar{b}=b,\bar{c}=c,\bar{A}=A$, then
	\begin{equation}
	a^2\le (1+b^2)\bar{a}^2
	\end{equation}
\end{lemma}
\begin{proof}
	Recall the law of cosines in Euclidean space and hyperbolic space:
	\begin{align}
	\label{eq:euclidean_law_of_cosines} \bar{a}^2 = &~ \bar{b}^2 + \bar{c}^2 - 2\bar{b}\bar{c}\cos\bar{A}, \\
	\label{eq:hyperbolic_law_of_cosines} \cosh a = &~ \cosh b \cosh c - \sinh b \sinh c \cos A,
	\end{align}
	and the Taylor series expansion:
	\begin{align} 
		\label{eq:taylor_hyperbolic}
		\cosh x = &~ \sum_{n=0}^{\infty} \frac{1}{(2n)!}x^{2n},  & \sinh x = &~ \sum_{n=0}^{\infty} \frac{1}{(2n+1)!}x^{2n+1}.
	\end{align}
	We let $\bar{b}=b,\bar{c}=c,\bar{A}=A$, from Eq. (\ref{eq:euclidean_law_of_cosines})  we have
	\begin{equation} \label{eq:cosh_a_bar}
	\cosh\bar{a} =  \cosh\left(\sqrt{b^2 + c^2 - 2bc\cos A}\right)
	\end{equation}
	It is widely known that $\bar{a}\le a$.
	Now we use Eq. (\ref{eq:taylor_hyperbolic}) to expand the RHS of Eq. (\ref{eq:hyperbolic_law_of_cosines}) and Eq. (\ref{eq:cosh_a_bar}), and compare the  coefficients for each corresponding term $b^ic^j$ in the two series. Without loss of generality, we assume $i\ge j$; the results for condition $i<j$ can be easily obtained by the symmetry of $b,c$. We expand Eq. (\ref{eq:hyperbolic_law_of_cosines}) as
	\begin{align*}
	\cosh a = &~\left(\sum_{n=0}^{\infty} \frac{1}{(2n)!}b^{2n}\right)\left(\sum_{n=0}^{\infty} \frac{1}{(2n)!}c^{2n}\right) \\
	&~ - \left(\sum_{n=0}^{\infty} \frac{1}{(2n+1)!}b^{2n+1}\right)\left(\sum_{n=0}^{\infty} \frac{1}{(2n+1)!}c^{2n+1}\right)\cos A 
	\end{align*}
	where the coefficient $\alpha(i,j)$ of $b^ic^j$ is 
	\begin{equation}
	\alpha(i,j) = \left\{\begin{array}{rl}
	\frac{1}{(2p)!(2q)!}, & \text{if~} p,q\in\mathbb{N} \text{~and~} i=2p,j=2q, \\
	\frac{\cos A}{(2p+1)!(2q+1)!}, & \text{if~} p,q\in\mathbb{N} \text{~and~} i=2p+1,j=2q+1, \\
	0, & \text{otherwise.}
	\end{array}\right.
	\end{equation}
	Similarly, we expand Eq. (\ref{eq:cosh_a_bar}) as
	\begin{align*}
	\cosh\bar{a} = &~ \sum_{n=0}^{\infty} \frac{1}{(2n)!} \left(b^2+c^2-2bc\cos A\right)^n
	\end{align*}
	where the coefficient $\bar{\alpha}(i,j)$ of $b^ic^j$ is
	\begin{equation}
	\bar{\alpha}(i,j) = \left\{\begin{array}{rl}
	\frac{\sum_{k=0}^{q} \binom{p+q}{p-k, q-k, 2k}(2\cos A)^{2k}}{(2p+2q)!} , & \text{if~} p,q\in\mathbb{N} \text{~and~} i=2p,j=2q, \\
	\frac{\sum_{k=0}^{q} \binom{p+q+1}{p-k, q-k, 2k+1}(2\cos A)^{2k+1}}{(2p+2q+2)!} , & \text{if~} p,q\in\mathbb{N} \text{~and~} i=2p+1,j=2q+1, \\
	0, & \text{otherwise.}
	\end{array}\right.
	\end{equation}
	We hence calculate their absolute difference
	\begin{align*}
	& |\alpha(i,j) - \bar{\alpha}(i,j)|  \\ 
	= & \left\{\begin{array}{rl}
	\frac{\sum_{k=0}^{q} \binom{p+q}{p-k, q-k, 2k}2^{2k}\left(1-(\cos A)^{2k}\right)}{(2p+2q)!} , & \text{if~} p,q\in\mathbb{N} \text{~and~} i=2p,j=2q, \\
	\frac{\sum_{k=0}^{q} \binom{p+q+1}{p-k, q-k, 2k+1}2^{2k+1}\left(1-(\cos A)^{2k}\right)|\cos A|}{(2p+2q+2)!} , & \text{if~} p,q\in\mathbb{N} \text{~and~} i=2p+1,j=2q+1, \\
	0, & \text{otherwise.}
	\end{array}\right. \\
	\le & \left\{\begin{array}{rl}
	\frac{\sum_{k=0}^{q} \binom{p+q}{p-k, q-k, 2k}2^{2k}k}{(2p+2q)!} \sin^2 A, & \text{if~} p,q\in\mathbb{N} \text{~and~} i=2p,j=2q, \\
	\frac{\sum_{k=0}^{q} \binom{p+q+1}{p-k, q-k, 2k+1}2^{2k+1}k}{(2p+2q+2)!} \sin^2 A, & \text{if~} p,q\in\mathbb{N} \text{~and~} i=2p+1,j=2q+1, \\
	0, & \text{otherwise.}
	\end{array}\right. \\
	\le & \left\{\begin{array}{rl}
	\frac{q\sum_{k=0}^{q} \binom{p+q}{p-k, q-k, 2k}2^{2k}}{(2p+2q)!} \sin^2 A, & \text{if~} p,q\in\mathbb{N} \text{~and~} i=2p,j=2q, \\
	\frac{q\sum_{k=0}^{q} \binom{p+q+1}{p-k, q-k, 2k+1}2^{2k+1}}{(2p+2q+2)!} \sin^2 A, & \text{if~} p,q\in\mathbb{N} \text{~and~} i=2p+1,j=2q+1, \\
	0, & \text{otherwise.}
	\end{array}\right. \\
	\numberthis \label{eq:alpha-absolute-difference} = & \left\{\begin{array}{rl}
	\frac{q}{(2p)!(2q)!} \sin^2 A, & \text{if~} p,q\in\mathbb{N} \text{~and~} i=2p,j=2q, \\
	\frac{q}{(2p+1)!(2q+1)!} \sin^2 A, & \text{if~} p,q\in\mathbb{N} \text{~and~} i=2p+1,j=2q+1, \\
	0, & \text{otherwise.}
	\end{array}\right.
	\end{align*}
	where the two equalities are due to Lemma \ref{thm:multinomial-identities}, the first inequality due to the following fact
	\begin{align*}
		1 - (\cos A)^{2m} = &~ \left(1-(\cos A)^2\right)\left(1+(\cos A)^2+(\cos A)^4+\cdots+(\cos A)^{2(m-1)}\right) \\
	= &~ \sin^2 A \left(1+(\cos A)^2+(\cos A)^4+\cdots+(\cos A)^{2(m-1)}\right) \le m\sin^2 A
	\end{align*}
	By setting $q=0$, we see that in the Taylor series of $\cosh a - \cosh\bar{a}$, any term that does not include a factor of $c^2$ cancels out. By the symmetry of $b,c$, any term that does not include a factor of $b^2$ also cancels out. The term with the lowest order of power is thus $\frac{1}{4}b^2c^2\sin^2A$. Since we have $c\le \frac{1}{2}, b\le \frac{1}{4}$, the terms $|\alpha(i,j) - \bar{\alpha}(i,j)|b^ic^j$ must satisfy
	\begin{align*}
	\sum_{i,j}|\alpha(i,j) - \bar{\alpha}(i,j)|b^ic^j \le &~ \left(\frac{1}{4}+\sum_{\substack{i+j=2k, \\i,j\ge 2, k\ge 3}} \frac{i+j}{2(i!)(j!)}\frac{1}{2^{2k-4}}\right) b^2c^2\sin^2A \\
	\le &~ \left(\frac{1}{4} + \sum_{k\ge 3} \frac{1}{2^{2k-3}}\right) b^2c^2\sin^2A
	\le \frac{1}{2} b^2c^2\sin^2A \\
	= &~ \frac{1}{2} b^2\bar{a}^2\sin^2C
	\le \frac{1}{2}\bar{a}^2b^2
	\end{align*}
	where the first inequality follows from Eq. (\ref{eq:alpha-absolute-difference}) and is due to $\min(p,q)\le \frac{i+j}{2}$, the second inequality is due to $\sum_{\substack{i+j=2k\\i\ge 2,j\ge 2}}\frac{i+j}{(i!)(j!)} \le \frac{(2k)^2}{(k!)^2} \le 1$ for $k\ge 3$ and the last equality is due to Euclidean law of sines. We thus get
	\begin{equation}
	\cosh a - \cosh\bar{a} \le \sum_{i,j}|\alpha(i,j) - \bar{\alpha}(i,j)|b^ic^j\sin^2A 
	\le \frac{1}{2} b^2\bar{a}^2
	\end{equation}
	On the other hand, from the Taylor series of $\cosh$ we have
	\[ \cosh a - \cosh\bar{a} = \sum_{n=0}^{\infty}\frac{a^{2n}-\bar{a}^{2n}}{(2n)!}\ge\frac{1}{2}(a^2-\bar{a}^2), \]
	hence $a^2\le (1+b^2)\bar{a}^2$.
	%	Take $p=0,q=0$, we have $\alpha(0,0) = \bar{\alpha}(0,0) = 1$ and $\alpha(1,1) = \bar{\alpha}(1,1) = \cos A$. Take $p=1,q=0$, we have $\alpha(2,0) = \bar{\alpha}(2,0) = \frac{1}{2}$ and $\alpha(3,1) = \bar{\alpha}(3,1) = \frac{1}{6}\cos A$.
	
	%	To simplify notation, in the following derivation we use $\delta$ to denote any suitable constants in $(0,1)$ that make the equalities hold. Readers should bear in mind that $\delta$ in different places probably refer to \emph{different} numbers, and cannot be taken as the same. For example, $\delta + \delta = 2\delta$ should be interpreted as ``for certain $\delta_1, \delta_2\in (0,1)$, there exists $\delta_3\in (0,1)$ such that $\delta_1 + \delta_2 = 2\delta_3$ holds''.
	%	
	%	Assuming $a,b,c\le 1$, then the remainder form of Taylor's expansion in Eq. (\ref{eq:taylor_remainder}) holds, i.e.
	%	\begin{align*}
	%		\left(1 + \frac{a^2}{2} + \frac{a^4}{24} + \frac{\delta a^6}{360}\right) = &~ \left(1 + \frac{b^2}{2} + \frac{b^4}{24} + \frac{\delta b^6}{360}\right)\left(1 + \frac{c^2}{2} + \frac{c^4}{24} + \frac{\delta c^6}{360}\right) \\
	%		&~ - \left(b + \frac{b^3}{6} + \frac{\delta b^5}{60} \right) \left(c + \frac{c^3}{6} + \frac{\delta c^5}{60} \right)\cos A
	%	\end{align*}
	%	We thus have
	%	\begin{align*}
	%		a^2\left(1+\frac{a^2}{12} + \frac{\delta a^4}{180}\right) = &~ b^2 + c^2 - 2bc\cos A + \frac{1}{12}\left(b^4+c^4+6b^2c^2\right) -\frac{1}{3}bc\left(b^2+c^2\right)\cos A\\
	%			&~  + \frac{\delta b^6}{90} + \frac{b^2c^2}{12} \left(b^2+c^2\right) + \frac{\delta c^6}{90} - \frac{b^3c^3}{9}\cos A - \frac{\delta bc}{15}\left(b^4+c^4\right)\cos A \\
	%			= &~ b^2 + c^2 - 2bc\cos A + \frac{1}{12}\left(b^4+c^4+6b^2c^2\right) -\frac{1}{3}bc\left(b^2+c^2\right)\cos A \pm \frac{\delta}{2}b^6
	%	\end{align*}
	%	One can verify that if $x<1$ and $x(1+x/12+\delta x^2/180) = t$ for some $\delta\in (0,1)$, i.e. $x+x^2/12\le t\le x+x^2/12 + x^3/180$, then $t-t^2/12\le x \le t-t^2/12 + t^3/72$, i.e. $x = t-t^2/12 + \delta t^3/72$ for some $\delta\in (0,1)$. We thus substitute $x$ with $a^2$ and $t$ with the RHS above, and get
	%	\begin{align*}
	%		a^2 = b^2 + c^2 - 2bc\cos A + \frac{1}{3}b^2c^2\sin^2 A \pm 3\delta b^6
	%	\end{align*}
	%
\end{proof}
\begin{lemma}[Two multinomial identities] \label{thm:multinomial-identities}
	For $p,q\in\mathbb{N}, p\ge q$, we have
	\begin{align}
	\frac{(2p+2q)!}{(2p)!(2q)!} = &~ \sum_{k=0}^{q} \binom{p+q}{p-k, q-k, 2k}2^{2k} \\
	\frac{(2p+2q+2)!}{(2p+1)!(2q+1)!} = &~ \sum_{k=0}^{q} \binom{p+q+1}{p-k, q-k, 2k+1}2^{2k+1}
	\end{align}
\end{lemma}
\begin{proof}
	We prove the identities by showing that the LHS and RHS correspond to two equivalent ways of counting the same quantity. For the first identity, consider a set of $2p+2q$ balls $b_i$ each with a unique index $i = 1,\dotsc,2p+2q$, we count how many ways we can put them into boxes $B_1$ and $B_2$, such that $B_1$ has $2p$ balls and $B_2$ has $2q$ balls. The LHS is obviously a correct count. To get the RHS, note that we can first put balls in pairs, then decide what to do with each pair. Specifically, there are $p+q$ pairs $\{b_{2i-1},b_{2i}\}$, and we can partition the counts by the number of pairs of which we put one of the two balls in $B_2$. Note that this number must be even. If there are $2k$ such pairs, which gives us $2k$ balls in $B_2$, we still need to choose $2(q-k)$ pairs of which both balls are put in $B_2$, and the left are $p-k$ pairs of which both balls are put in $B_1$. The total number of counts given $k$ is thus 
	\[ \binom{p+q}{p-k, q-k, 2k}2^{2k} \]
	because we can choose either ball in each of the $2k$ pairs leading to $2^{2k}$ possible choices. Summing over $k$ we get the RHS. Hence the LHS and the RHS equal. The second identity can be proved with essentially the same argument.
\end{proof}

%\section{Proof of Theorem \ref{thm:squared-distance-ratio-bound}}

\section{Proof of Theorem \ref{thm:convergence-induction}} \label{prf:convergence-induction}
\begin{proof} 
	\emph{The base case.} First we verify that $y_0, y_1$ is sufficiently close to $x^*$ so that the comparison inequality (\ref{eq:base-change-assumption}) holds at step $k=0$. In fact, since $y_0=x_0$ by construction, we have 
	\begin{equation} \label{eq:xstar-y0-}
	\|\Exp_{y_0}^{-1}(x^*)\| =\|\Exp_{x_0}^{-1}(x^*)\| \le  \frac{1}{4\sqrt{K}}, \qquad 5K\|\Exp_{y_0}^{-1}(x^*)\|^2 \le \frac{1}{80}\left(\frac{\mu}{L}\right)^{\frac{3}{2}} \le  \beta.
	\end{equation}
	To bound $\|\Exp_{y_1}^{-1}(x^*)\|$, observe that $y_1$ is on the geodesic between $x_1$ and $v_1$. So first we bound $\|\Exp_{x_1}^{-1}(x^*)\|$ and $\|\Exp_{v_1}^{-1}(x^*)\|$. Bound on $\|\Exp_{x_1}^{-1}(x^*)\|$ comes from strong g-convexity:
	\begin{align*}
	\|\Exp_{x_1}^{-1}(x^*)\|^2\le &~ \frac{2}{\mu}(f(x_1)-f(x^*))\le \frac{2}{\mu}(f(x_0)-f(x^*))+\frac{\gamma}{\mu}\|\Exp_{x_0}^{-1}(x^*)\|^2 \\
	\le &~ \frac{L+\gamma}{\mu}\|\Exp_{x_0}^{-1}(x^*)\|^2, 
	\end{align*}
	whereas bound on $\|\Exp_{v_1}^{-1}(x^*)\|$ utilizes the tangent space distance comparison theorem. First, from the definition of $\overline{\Phi}_1$ we have
	$$\|\Exp_{y_0}^{-1}(x^*)-\Exp_{y_0}^{-1}(v_1)\|^2 = \frac{2}{\gamma}(\overline{\Phi}_1(x^*)-\Phi_1^*)\le \frac{2}{\gamma}(\Phi_0(x^*)-f(x^*))\le \frac{L+\gamma}{\gamma}\|\Exp_{x_0}^{-1}(x^*)\|^2$$
	Then note that (\ref{eq:xstar-y0-}) implies that the assumption in Theorem \ref{thm:squared-distance-ratio-bound} is satisfied when $k=0$, thus we have
	$$\|\Exp_{v_1}^{-1}(x^*)\|^2\le  (1+\beta)\|\Exp_{y_0}^{-1}(x^*)-\Exp_{y_0}^{-1}(v_1)\|^2\le \frac{2(L+\gamma)}{\gamma}\|\Exp_{x_0}^{-1}(x^*)\|^2.$$
	Together we have 
	\begin{align}
	\nonumber\|\Exp_{y_1}^{-1}(x^*)\|\le ~& \|\Exp_{x_1}^{-1}(x^*)\| + \frac{\alpha\gamma}{\gamma+\alpha\mu}\|\Exp_{x_1}^{-1}(v_1)\|\\  \nonumber\le ~& \|\Exp_{x_1}^{-1}(x^*)\| + \frac{\alpha\gamma}{\gamma+\alpha\mu}\left(\|\Exp_{x_1}^{-1}(x^*)\| + \|\Exp_{v_1}^{-1}(x^*)\|\right) \\
	\nonumber\le ~& \sqrt{\frac{L+\gamma}{\mu}}\|\Exp_{x_0}^{-1}(x^*)\| + \frac{\alpha\gamma}{\gamma+\alpha\mu}\left(\sqrt{\frac{L+\gamma}{\mu}}+\sqrt{\frac{2(L+\gamma)}{\mu}}\right)\|\Exp_{x_0}^{-1}(x^*)\| \\
	\nonumber\le ~& \left(1 + \frac{1+\sqrt{2}}{2}\right)\sqrt{\frac{L+\gamma}{\mu}}\|\Exp_{x_0}^{-1}(x^*)\| \\
	\le ~& \frac{1}{10\sqrt{K}}\left(\frac{\mu}{L}\right)^{\frac{1}{4}} 
	\le \frac{1}{4\sqrt{K}} \label{eq:base-xstar-y-}
	\end{align}
	which also implies
	\begin{equation}
	5K\|\Exp_{y_1}^{-1}(x^*)\|^2 \le \frac{1}{20}\sqrt{\frac{\mu}{L}} \le \beta \label{eq:base-beta-}
	\end{equation}
	By (\ref{eq:base-xstar-y-}), (\ref{eq:base-beta-}) and Theorem \ref{thm:squared-distance-ratio-bound} it is hence guaranteed that 
	$$\gamma \| \Exp_{y_1}^{-1}(x^*) -\Exp_{y_1}^{-1}(v_1)\|^2 \le \overline{\gamma} \|\Exp_{y_0}^{-1}(x^*)-\Exp_{y_0}^{-1}(v_1)\|^2.$$
	\emph{The inductive step.}
	Assume that for $i=0,\dots,k-1$, (\ref{eq:base-change-assumption}) hold simultaneously, i.e.:
	$$\gamma \| \Exp_{y_{i+1}}^{-1}(x^*) -\Exp_{y_{i+1}}^{-1}(v_{i+1})\|^2 \le \overline{\gamma}\|\Exp_{y_i}^{-1}(x^*)-\Exp_{y_i}^{-1}(v_{i+1})\|^2, \forall i=0,\dots,k-1$$
	and also that $\|\Exp_{y_k}^{-1}(x^*)\|\le \frac{1}{10\sqrt{K}}\left(\frac{\mu}{L}\right)^{\frac{1}{4}}$. 
	To bound $\|\Exp_{y_{k+1}}^{-1}(x^*)\|$, observe that $y_{k+1}$ is on the geodesic between $x_{k+1}$ and $v_{k+1}$. So first we bound $\|\Exp_{x_{k+1}}^{-1}(x^*)\|$ and $\|\Exp_{v_{k+1}}^{-1}(x^*)\|$.
	Note that due to the sequential nature of the algorithm, statements about any step only depend on its previous steps, but not any step afterwards. 
	Since (\ref{eq:base-change-assumption}) hold for steps $i=0,\dots,k-1$, the analysis in the previous section already applies for steps $i=0,\dots,k-1$. Therefore by Theorem  \ref{thm:main-theorem-general-scheme} and the proof of Lemma \ref{thm:x-k-bound} we know 
	\begin{align*}
	f(x^*)\le &~ f(x_{k+1})\le\Phi_{k+1}^*\le\Phi_{k+1}(x^*)
	\le f(x^*)+(1-\alpha)^{k+1}(\Phi_0(x^*)-f(x^*)) \\
	\le &~ \Phi_0(x^*) = f(x_0)+\frac{\gamma}{2}\|\Exp_{x_0}^{-1}(x^*)\|^2
	\end{align*}
	Hence we get $f(x_{k+1})-f(x^*)\le\Phi_0(x^*)-f(x^*)$ and $\frac{\gamma}{2}\|\Exp_{y_k}^{-1}(x^*)-\Exp_{y_k}^{-1}(v_{k+1})\|^2\equiv\overline{\Phi}_{k+1}(x^*)-\Phi_{k+1}^*\le\Phi_0(x^*)-f(x^*)$. Bound on $\|\Exp_{x_{k+1}}^{-1}(x^*)\|$ comes from strong g-convexity:
	\begin{align*}
	\|\Exp_{x_{k+1}}^{-1}(x^*)\|^2\le &~ \frac{2}{\mu}(f(x_{k+1})-f(x^*))\le \frac{2}{\mu}(f(x_0)-f(x^*))+\frac{\gamma}{\mu}\|\Exp_{x_0}^{-1}(x^*)\|^2 \\
	\le &~ \frac{L+\gamma}{\mu}\|\Exp_{x_0}^{-1}(x^*)\|^2, 
	\end{align*} 
	whereas bound on $\|\Exp_{v_{k+1}}^{-1}(x^*)\|$ utilizes the tangent space distance comparison theorem. First, from the definition of $\overline{\Phi}_{k+1}$ we have
	$$\|\Exp_{y_k}^{-1}(x^*)-\Exp_{y_k}^{-1}(v_{k+1})\|^2 = \frac{2}{\gamma}(\overline{\Phi}_{k+1}(x^*)-\Phi_{k+1}^*)\le \frac{2}{\gamma}(\Phi_0(x^*)-f(x^*))\le \frac{L+\gamma}{\gamma}\|\Exp_{x_0}^{-1}(x^*)\|^2$$
	Then note that the inductive hypothesis implies that
	\begin{align*}
	\|\Exp_{v_{k+1}}^{-1}(x^*)\|^2\le (1+\beta)\|\Exp_{y_k}^{-1}(x^*)-\Exp_{y_k}^{-1}(v_{k+1})\|^2\le \frac{2(L+\gamma)}{\gamma}\|\Exp_{x_0}^{-1}(x^*)\|^2
	\end{align*}
	Together we have
	\begin{align*}
	\|\Exp_{y_{k+1}}^{-1}(x^*)\|\le ~& \|\Exp_{x_{k+1}}^{-1}(x^*)\| + \frac{\alpha\gamma}{\gamma+\alpha\mu}\|\Exp_{x_{k+1}}^{-1}(v_{k+1})\|\\  \le ~& \|\Exp_{x_{k+1}}^{-1}(x^*)\| + \frac{\alpha\gamma}{\gamma+\alpha\mu}\left(\|\Exp_{x_{k+1}}^{-1}(x^*)\| + \|\Exp_{v_{k+1}}^{-1}(x^*)\|\right) \\
	\le ~& \sqrt{\frac{L+\gamma}{\mu}}\|\Exp_{x_0}^{-1}(x^*)\| + \frac{\alpha\gamma}{\gamma+\alpha\mu}\left(\sqrt{\frac{L+\gamma}{\mu}}+\sqrt{\frac{2(L+\gamma)}{\mu}}\right)\|\Exp_{x_0}^{-1}(x^*)\| \\
	\le ~& \left(1 + \frac{1+\sqrt{2}}{2}\right)\sqrt{\frac{L+\gamma}{\mu}}\|\Exp_{x_0}^{-1}(x^*)\| \\
	\le ~& \frac{1}{10\sqrt{K}}\left(\frac{\mu}{L}\right)^{\frac{1}{4}} 
	\le \frac{1}{4\sqrt{K}}
	\end{align*}
	which also implies that
	$$ 5K\|\Exp_{y_{k+1}}^{-1}(x^*)\|^2 \le \frac{1}{20}\sqrt{\frac{\mu}{L}} \le \beta$$
	By the two lines of equations above and Theorem \ref{thm:squared-distance-ratio-bound} it is guaranteed that $\|\Exp_{y_{k+1}}^{-1}(x^*)\|\le \frac{1}{10\sqrt{K}}\left(\frac{\mu}{L}\right)^{\frac{1}{4}}$ and also
	$$\gamma \| \Exp_{y_{k+1}}^{-1}(x^*) -\Exp_{y_{k+1}}^{-1}(v_{k+1})\|^2 \le \overline{\gamma} \|\Exp_{y_k}^{-1}(x^*)-\Exp_{y_k}^{-1}(v_{k+1})\|^2.$$
	i.e. (\ref{eq:base-change-assumption}) hold for $i=0,\dots,k$. This concludes the inductive step.\\
	By induction, (\ref{eq:base-change-assumption}) hold for all $k\ge 0$, hence by Theorem \ref{thm:main-theorem-general-scheme}, Algorithm \ref{alg:constant-step} converges, with
	$$\alpha_i\equiv \alpha=\frac{\sqrt{\beta^2+4(1+\beta)\mu h}-\beta}{2} = \frac{\sqrt{\mu h}}{2}\left(\sqrt{\frac{1}{25}+4\left(1+\frac{\sqrt{\mu h}}{5}\right)} - \frac{1}{5}\right)\ge \frac{9}{10}\sqrt{\frac{\mu}{L}}.$$
\end{proof}


%\section{Bounds on $b_{k+1}$}
%
%
%\begin{lemma}[$b_{k+1}$ is not effectively controlled by step size $h$] \label{thm:b-bound}
%	Assuming $G\ge\max_k\|\nabla f(y_k)\|$, assumption (\ref{eq:base-change-assumption}) holds with $\epsilon\le\frac{1}{2}$, and $b_{k+1}$ defined as in Theorem \ref{thm:squared-distance-ratio-bound}, we have for all $k\ge 0$,
%	\begin{equation} \label{eq:b-k+1}
%	b_{k+1} \le \min\left\{\frac{2G}{\mu},\frac{3(k+1)hG}{\alpha}\right\}
%	\end{equation}
%\end{lemma}
%\begin{proof}
%	We denote $\overline{d}_{k+1}=\|\Exp_{y_k}^{-1}(v_{k+1})\|, d_k=\|\Exp_{y_k}^{-1}(v_k)\|$. From Line \ref{eq:v-k+1} and Line \ref{eq:alpha-k} in Algorithm \ref{alg:riemannian-ag} and triangle inequality we have $\overline{d}_{k+1}\le \frac{1-\alpha}{1+\epsilon}d_k + \frac{\alpha}{\overline{\gamma}_0}\|\nabla f(y_k)\| \le \frac{1-\alpha}{1+\epsilon}d_k + \frac{h}{\alpha}G$. In addition, from Line \ref{eq:y-k} and Line \ref{eq:x-k+1} in Algorithm \ref{alg:riemannian-ag} and using triangle inequality we have $d_{k+1}\le \|\Exp_{x_{k+1}}^{-1}(v_{k+1})\|\le\overline{d}_{k+1}+\|\Exp_{y_k}^{-1}(x_{k+1})\|\le \overline{d}_{k+1}+hG$. In combination we have for all $k\ge 0$
%	\[ d_{k+1}\le \frac{1-\alpha}{1+\epsilon}d_k + \frac{1+\alpha}{\alpha}hG, \]
%	or by rearrangement,
%	\[ d_{k+1}-\frac{(1+\epsilon)(1+\alpha)}{\alpha(\alpha+\epsilon)}hG \le  \frac{1-\alpha}{1+\epsilon}\left(d_k-\frac{(1+\epsilon)(1+\alpha)}{\alpha(\alpha+\epsilon)}hG\right). \]
%	Applying recursion on $k$ we have
%	\begin{equation} \label{eq:d-k-bound}
%	d_{k+1}-\frac{(1+\epsilon)(1+\alpha)}{\alpha(\alpha+\epsilon)}hG \le \left(\frac{1-\alpha}{1+\epsilon}\right)^{k+1}\left(d_0-\frac{(1+\epsilon)(1+\alpha)}{\alpha(\alpha+\epsilon)}hG\right)	\le 0,
%	\end{equation} 
%	and since $d_0=0$, we get the 
%	\begin{align}
%	\nonumber d_k \le &~  \left(1-(1-k\alpha)(1-k\epsilon)\right)\frac{(1+\epsilon)(1+\alpha)}{\alpha(\alpha+\epsilon)}hG \le k\frac{(1+\epsilon)(1+\alpha)}{\alpha}hG
%	\end{align}
%	where the first inequality is due to $(1-\alpha)^k\ge 1-k\alpha$ for $k\ge 0, \alpha\in(0,1]$ and $(1+\epsilon)^{-k}\ge 1-k\epsilon$ for $k\ge 0, \epsilon\in(0,1/2]$, the second inequality is due to $(1-k\alpha)(1-k\epsilon)\ge 1-k(\alpha+\epsilon)$. Note that $\alpha(\alpha+\epsilon)=(1+\epsilon)\mu h$, $1+\epsilon\le 3/2$ and $1+\alpha\le 2$, the above bounds hence simplify into $d_k\le \min\{\frac{2G}{\mu},\frac{3khG}{\alpha}\}$.
%	
%	Also note that $\overline{d}_{k+1}\le \frac{1-\alpha}{1+\epsilon}d_k + \frac{h}{\alpha}G \le \min\{\frac{2G}{\mu},\frac{3(k+1)hG}{\alpha}\}$, so we have $b_{k+1} = \max\{\overline{d}_{k+1}, d_{k+1}\} \le \min\{\frac{2G}{\mu},\frac{3(k+1)hG}{\alpha}\}$
%\end{proof}