Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Kawanabe2005b,
author = {{M. Kawanabe}},
file = {:C$\backslash$:/Users/yanshuo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kawanabe - 2005 - Linear dimension reduction based on the fourth-order cumulant tensor.pdf:pdf},
journal = {Proc. of Artifical Neural Networks -- ICANN 2005},
pages = {151--156},
title = {{Linear dimension reduction based on the fourth-order cumulant tensor}},
year = {2005}
}
@article{Brubaker2008b,
abstract = {We present an extension of principal component analysis (PCA) and a new algorithm for clustering points in R{\textless}sup{\textgreater}n{\textless}/sup{\textgreater} based on it. The key property of the algorithm is that it is affine-invariant. When the input is a sample from a mixture of two arbitrary Gaussians, the algorithm correctly classifies the sample assuming only that the two components are separable by a hyperplane, i.e., there exists a halfspace that contains most of one Gaussian and almost none of the other in probability mass. This is nearly the best possible, improving known results substantially. For k{\&}gt;2 components, the algorithm requires only that there be some (k-1)-dimensional subspace in which the ``overlap'' in every direction is small. Our main tools are isotropic transformation, spectral projection and a simple reweighting technique. We call this combination isotropic PCA.},
archivePrefix = {arXiv},
arxivId = {0804.3575},
author = {Brubaker, S. Charles and Vempala, Santosh S.},
eprint = {0804.3575},
file = {:C$\backslash$:/Users/yanshuo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brubaker, Vempala - 2008 - Isotropic PCA and affine-invariant clustering.pdf:pdf},
journal = {Proceedings - Annual IEEE Symposium on Foundations of Computer Science, FOCS},
pages = {551--560},
title = {{Isotropic PCA and affine-invariant clustering}},
year = {2008}
}
@article{Sasaki2016b,
abstract = {Non-Gaussian component analysis (NGCA) is aimed at identifying a linear subspace such that the projected data follows a non-Gaussian distribution. In this paper, we propose a novel NGCA algorithm based on log-density gradient estimation. Unlike existing methods, the proposed NGCA algorithm identifies the linear subspace by using the eigenvalue decomposition without any iterative procedures, and thus is computationally reasonable. Furthermore, through theoretical analysis, we prove that the identified subspace converges to the true subspace at the optimal parametric rate. Finally, the practical performance of the proposed algorithm is demonstrated on both artificial and benchmark datasets.},
archivePrefix = {arXiv},
arxivId = {1601.07665},
author = {Sasaki, Hiroaki and Niu, Gang and Sugiyama, Masashi},
eprint = {1601.07665},
file = {:C$\backslash$:/Users/yanshuo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sasaki, Niu, Sugiyama - 2016 - Non-Gaussian Component Analysis with Log-Density Gradient Estimation.pdf:pdf},
journal = {International Conference on Artificial Intelligence and Statistics},
pages = {1--20},
title = {{Non-Gaussian Component Analysis with Log-Density Gradient Estimation}},
volume = {51},
year = {2016}
}
@article{Huber1985b,
abstract = {Projection pursuit is concerned with "interesting" projections of high dimensional data sets, with finding such projections by machine, and with using them for nonparametric fitting and other data-analytic purposes. This survey attempts to put the fascinating problems and ramifications of projection pursuit--which range from principal components, multidimensional scaling, factor analysis, nonparametric regression, density estimation and deconvolution of time series to computer tomography and problems in pure mathematics--into a coherent perspective},
author = {Huber, Peter J},
file = {:C$\backslash$:/Users/yanshuo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huber - 1985 - Projection Pursuit.pdf:pdf},
journal = {The Annals of Statistics},
number = {2},
pages = {435--475},
pmid = {18396487},
title = {{Projection Pursuit}},
volume = {13},
year = {2007}
}
@article{Kawanabe2007b,
abstract = {In this article, we consider high-dimensional data which contains a low-dimensional non-Gaussian structure contaminated with Gaussian noise. Motivated by the joint diagonalization algorithms, we propose a linear dimension reduction procedure called joint low-dimensional approximation (JLA) to identify the non-Gaussian subspace. The method uses matrices whose non-zero eigen spaces coincide with the non-Gaussian subspace. We also prove its global consistency, that is the true mapping to the non-Gaussian subspace is achieved by maximizing the contrast function defined by such matrices. As examples, we will present two implementations of JLA, one with the fourth-order cumulant tensors and the other with Hessian of the characteristic functions. A numerical study demonstrates validity of our method. In particular, the second algorithm works more robustly and efficiently in most cases. ?? 2007 Elsevier B.V. All rights reserved.},
author = {Kawanabe, Motoaki and Theis, Fabian J.},
file = {:C$\backslash$:/Users/yanshuo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kawanabe, Theis - 2007 - Joint low-rank approximation for extracting non-Gaussian subspaces(2).pdf:pdf;:C$\backslash$:/Users/yanshuo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kawanabe, Theis - 2007 - Joint low-rank approximation for extracting non-Gaussian subspaces.pdf:pdf},
journal = {Signal Processing},
keywords = {Characteristic function,Fourth-order cumulant tensor,Joint low-rank approximation,Linear dimension reduction,Non-Gaussian subspace},
number = {8},
pages = {1890--1903},
title = {{Joint low-rank approximation for extracting non-Gaussian subspaces}},
volume = {87},
year = {2007}
}
@article{Yu2015,
abstract = {The Davis–Kahan theorem is used in the analysis of many statistical procedures to bound the distance between subspaces spanned by population eigenvectors and their sample versions. It relies on an eigenvalue separation condition between certain population and sample eigenvalues. We present a variant of this result that depends only on a population eigenvalue separation condition, making it more natural and convenient for direct application in statistical contexts, and provide an improvement in many cases to the usual bound in the statistical literature. We also give an extension to situations where the matrices under study may be asymmetric or even non-square, and where interest is in the distance between subspaces spanned by corresponding singular vectors.},
archivePrefix = {arXiv},
arxivId = {arXiv:1405.0680v1},
author = {Yu, Y. and Wang, T. and Samworth, R. J.},
eprint = {arXiv:1405.0680v1},
file = {:C$\backslash$:/Users/yanshuo/Google Drive/Resources/Research Papers/Mathematics/Wang, Samworth - A useful variant of Davis-Kahan.pdf:pdf},
journal = {Biometrika},
keywords = {Davis-Kahan theorem,Eigendecomposition,Matrix perturbation,Singular value decomposition},
number = {2},
pages = {315--323},
title = {{A useful variant of the Davis-Kahan theorem for statisticians}},
volume = {102},
year = {2015}
}
@article{Kawanabe2006b,
abstract = {In this article, we consider high-dimensional data which contains a low-dimensional non-Gaussian structure contaminated with Gaussian noise and propose a new method to identify the non-Gaussian subspace. An algorithm based on the fourth-order cumulant tensor was proposed in our previous research. Although it works well for sub-Gaussian structures, the performance is not satisfactory for super-Gaussian data due to outliers. To overcome this problem, we construct an alternative by using hessian of characteristic functions which was applied to (multidimensional) independent component analysis. A numerical study demonstrates the usefulness of our method.},
author = {Kawanabe, Motoaki and Theis, Fabian J.},
file = {:C$\backslash$:/Users/yanshuo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kawanabe, Theis - 2006 - Estimating non-Gaussian subspaces by characteristic functions.pdf:pdf},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {157--164},
title = {{Estimating non-Gaussian subspaces by characteristic functions}},
volume = {3889 LNCS},
year = {2006}
}
@book{Billingsley1995b,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Billingsley, Patrick},
booktitle = {Wiley-Interscience Publication},
pages = {608},
pmid = {15003161},
title = {{Probability and Measure - Third Edition}},
year = {1995}
}
@article{Diederichs2010b,
abstract = {Non-Gaussian component analysis (NGCA) introduced in offered a method for high-dimensional data analysis allowing for identifying a low-dimensional non-Gaussian component of the whole distribution in an iterative and structure adaptive way. An important step of the NGCA procedure is identification of the non-Gaussian subspace using principle component analysis (PCA) method. This article proposes a new approach to NGCA called sparse NGCA which replaces the PCA-based procedure with a new the algorithm we refer to as convex projection.},
archivePrefix = {arXiv},
arxivId = {0904.0430},
author = {Diederichs, Elmar and Juditsky, Anatoli and Spokoiny, Vladimir and Sch{\"{u}}tte, Christof},
eprint = {0904.0430},
file = {:C$\backslash$:/Users/yanshuo/Google Drive/Resources/Research Papers/Learning/Diedrichs 1.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
keywords = {Convex projection,Model reduction,Principle component analysis (PCA),Reduction of dimensionality,Sparsity structural adaptation,Variable selection},
number = {6},
pages = {3033--3047},
title = {{Sparse non-Gaussian component analysis}},
volume = {56},
year = {2010}
}
@article{Sasaki2014,
abstract = {Mean shift clustering finds the modes of the data probability density by identifying the zero points of the density gradient. Since it does not require to fix the number of clusters in advance, the mean shift has been a popular clustering algorithm in various application fields. A typical implementation of the mean shift is to first estimate the density by kernel density estimation and then compute its gradient. However, since good density estimation does not necessarily imply accurate estimation of the density gradient, such an indirect two-step approach is not reliable. In this paper, we propose a method to directly estimate the gradient of the log-density without going through density estimation. The proposed method gives the global solution analytically and thus is computationally efficient. We then develop a mean-shift-like fixed-point algorithm to find the modes of the density for clustering. As in the mean shift, one does not need to set the number of clusters in advance. We empirically show that the proposed clustering method works much better than the mean shift especially for high-dimensional data. Experimental results further indicate that the proposed method outperforms existing clustering methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.5028v1},
author = {Sasaki, Hiroaki and Hyv{\"{a}}rinen, Aapo and Sugiyama, Masashi},
eprint = {arXiv:1404.5028v1},
file = {:C$\backslash$:/Users/yanshuo/Google Drive/Resources/Research Papers/Learning/Sasaki - Clustering via log-density gradient.pdf:pdf},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Clustering,High-Dimensional Data,Log-Density Gradient Estimation,Mean Shift},
number = {PART 3},
pages = {19--34},
title = {{Clustering via mode seeking by direct estimation of the gradient of a log-density}},
volume = {8726 LNAI},
year = {2014}
}
@book{Cnlar2011b,
  title={Probability and Stochastics},
author={{\c{C}}{\i}nlar, E.},
lccn={2011921929},
series={Graduate Texts in Mathematics},
year={2011},
publisher={Springer New York}
}
@article{Comon1994,
abstract = {The independent component analysis (ICA) of a random vector consists of searching for a linear transformation that minimizes the statistical dependence between its components. In order to define suitable search criteria, the expansion of mutual information is utilized as a function of cumulants of increasing orders. An efficient algorithm is proposed, which allows the computation of the ICA of a data matrix within a polynomial time. The concept of ICA may actually be seen as an extension of the principal component analysis (PCA), which can only impose independence up to the second order and, consequently, defines directions that are orthogonal. Potential applications of ICA include data analysis and compression, Bayesian detection, localization of sources, and blind identification and deconvolution. {\textcopyright} 1994.},
author = {Comon, Pierre},
file = {:C$\backslash$:/Users/yanshuo/Google Drive/Resources/Research Papers/Learning/Comon - Independent component analysis.pdf:pdf},
journal = {Signal Processing},
keywords = {array processing,blind identification,data analysis,entropy,high-order statistics,independent,information,mixture,noise reduction,principal components,random variables,separation,source,statistical independence},
number = {3},
pages = {287--314},
pmid = {19419239},
title = {{Independent component analysis, A new concept?}},
volume = {36},
year = {1994}
}
@incollection{Vershynin2011b,
abstract = {This is a tutorial on some basic non-asymptotic methods and concepts in$\backslash$nrandom matrix theory. The reader will learn several tools for the analysis of$\backslash$nthe extreme singular values of random matrices with independent rows or$\backslash$ncolumns. Many of these methods sprung off from the development of geometric$\backslash$nfunctional analysis since the 1970's. They have applications in several fields,$\backslash$nmost notably in theoretical computer science, statistics and signal processing.$\backslash$nA few basic applications are covered in this text, particularly for the problem$\backslash$nof estimating covariance matrices in statistics and for validating$\backslash$nprobabilistic constructions of measurement matrices in compressed sensing.$\backslash$nThese notes are written particularly for graduate students and beginning$\backslash$nresearchers in different areas, including functional analysts, probabilists,$\backslash$ntheoretical statisticians, electrical engineers, and theoretical computer$\backslash$nscientists.},
address = {Cambridge},
archivePrefix = {arXiv},
arxivId = {1011.3027},
author = {Vershynin, Roman},
booktitle = {Compressed Sensing},
editor = {Eldar, Yonina C. and Kutyniok, Gitta},
eprint = {1011.3027},
file = {:C$\backslash$:/Users/yanshuo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vershynin - 2011 - Introduction to the non-asymptotic analysis of random matrices.pdf:pdf},
keywords = {matrices,probability,random},
pages = {210--268},
publisher = {Cambridge University Press},
title = {{Introduction to the non-asymptotic analysis of random matrices}},
year = {2011}
}
@article{Arora2015,
abstract = {We present a new algorithm for Independent Component Analysis (ICA) which has provable performance guarantees. In particular, suppose we are given samples of the form y = Ax+$\eta$ where A is an unknown n×n matrix and x is a random variable whose components are independent and have a fourth moment strictly less than that of a standard Gaussian random variable and $\eta$ is an n-dimensional Gaussian random variable with unknown covariance $\Sigma$: We give an algorithm that provable recovers A and $\Sigma$ up to an additive  and whose running time and sam- ple complexity are polynomial in n and 1/. To accomplish this, we introduce a novel “quasi-whitening” step that may be useful in other contexts in which the covariance of Gaussian noise is not known in advance. We also give a general framework for finding all local optima of a function (given an oracle for approx- imately finding just one) and this is a crucial step in our algorithm, one that has been overlooked in previous attempts, and allows us to control the accumulation of error when we find the columns of A one by one via local search.},
archivePrefix = {arXiv},
arxivId = {1206.5349},
author = {Arora, Sanjeev and Ge, Rong and Moitra, Ankur and Sachdeva, Sushant},
eprint = {1206.5349},
file = {:C$\backslash$:/Users/yanshuo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arora et al. - 2015 - Provable ICA with Unknown Gaussian Noise, and Implications for Gaussian Mixtures and Autoencoders.pdf:pdf},
journal = {Algorithmica},
keywords = {Cumulants,Independent component analysis,Method of moments,Mixture models},
number = {1},
pages = {215--236},
title = {{Provable ICA with Unknown Gaussian Noise, and Implications for Gaussian Mixtures and Autoencoders}},
volume = {72},
year = {2015}
}
@article{Vempala2011,
abstract = {We present a generalization of the well-known problem of learning k-juntas in R{\^{}}n, and a novel tensor algorithm for unraveling the structure of high-dimensional distributions. Our algorithm can be viewed as a higher-order extension of Principal Component Analysis (PCA). Our motivating problem is learning a labeling function in R{\^{}}n, which is determined by an unknown k-dimensional subspace. This problem of learning a k-subspace junta is a common generalization of learning a k-junta (a function of k coordinates in R{\^{}}n) and learning intersections of k halfspaces. In this context, we introduce an irrelevant noisy attributes model where the distribution over the "relevant" k-dimensional subspace is independent of the distribution over the (n-k)-dimensional "irrelevant" subspace orthogonal to it. We give a spectral tensor algorithm which identifies the relevant subspace, and thereby learns k-subspace juntas under some additional assumptions. We do this by exploiting the structure of local optima of higher moment tensors over the unit sphere; PCA finds the global optima of the second moment tensor (covariance matrix). Our main result is that when the distribution in the irrelevant (n-k)-dimensional subspace is any Gaussian, the complexity of our algorithm is T(k,$\backslash$epsilon) + $\backslash$poly(n), where T is the complexity of learning the concept in k dimensions, and the polynomial is a function of the k-dimensional concept class being learned. This substantially generalizes existing results on learning low-dimensional concepts.},
archivePrefix = {arXiv},
arxivId = {1108.3329},
author = {Vempala, Santosh S. and Xiao, Ying},
eprint = {1108.3329},
file = {:C$\backslash$:/Users/yanshuo/Google Drive/Resources/Research Papers/Learning/Vempala, Xiao - Learning subspace juntas.pdf:pdf},
journal = {arXiv:1108.3329},
title = {{Structure from Local Optima: Learning Subspace Juntas via Higher Order PCA}},
year = {2011}
}
@article{Diederichs2013b,
abstract = {Sparse non-Gaussian component analysis (SNGCA) is an unsupervised method of extracting a linear structure from a high dimensional data based on estimating a low-dimensional non-Gaussian data component. In this paper we discuss a new approach to direct estimation of the projector on the target space based on semidefinite programming which improves the method sensitivity to a broad variety of deviations from normality. We also discuss the procedures which allows to recover the structure when its effective dimension is unknown.},
archivePrefix = {arXiv},
arxivId = {1106.0321},
author = {Diederichs, Elmar and Juditsky, Anatoli and Nemirovski, Arkadi and Spokoiny, Vladimir},
eprint = {1106.0321},
file = {:C$\backslash$:/Users/yanshuo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Diederichs et al. - 2013 - Sparse non Gaussian component analysis by semidefinite programming.pdf:pdf},
journal = {Machine Learning},
keywords = {Dimension reduction,Feature extraction,Non-Gaussian components analysis},
number = {2},
pages = {211--238},
title = {{Sparse non Gaussian component analysis by semidefinite programming}},
volume = {91},
year = {2013}
}
@article{Blanchard2006b,
abstract = {Finding non-Gaussian components of high-dimensional data is an important preprocessing step for efficient information processing. This article proposes a new linear method to identify the ``non-Gaussian subspace'' within a very general semi-parametric framework. Our proposed method, called NGCA (Non-Gaussian Component Analysis), is essentially based on a linear operator which, to any arbitrary nonlinear (smooth) function, associates a vector which belongs to the low dimensional non-Gaussian target subspace up to an estimation error. By applying this operator to a family of different nonlinear functions, one obtains a family of different vectors lying in a vicinity of the target space. As a final step, the target space itself is estimated by applying PCA to this family of vectors. We show that this procedure is consistent in the sense that the estimaton error tends to zero at a parametric rate, uniformly over the family, Numerical examples demonstrate the usefulness of our method.},
author = {Blanchard, Gilles and Kawanabe, Motoaki and Sugiyama, Masashi and Spokoiny, Vladimir and M{\"{u}}ller, Klaus-Robert},
file = {:C$\backslash$:/Users/yanshuo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blanchard et al. - 2006 - In Search of Non-Gaussian Components of a High-Dimensional Distribution.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {computational,information theoretic learning with statistics,learning,statistics {\&} optimisation,theory {\&} algorithms},
number = {2},
pages = {247--282},
title = {{In Search of Non-Gaussian Components of a High-Dimensional Distribution}},
volume = {7},
year = {2006}
}
@inproceedings{Goyal2014a,
abstract = {Fourier PCA is Principal Component Analysis of the covariance matrix obtained after reweighting a distribution with a random Fourier weighting. It can also be viewed as PCA applied to the Hessian of the logarithm of the characteristic function. Using this technique and its extension to higher derivatives, we derive the following results: (1) a polynomial-time algorithm for general ICA, not requiring the component distributions to be discrete ordistinguishable from Gaussian in their fourth moment (as in previous work) (2) the first polynomial-time algorithm for underdetermined ICA, where the number of components can be arbitrarily higher than the dimension and (3) an alternative algorithm for learning mixtures of spherical Gaussians with linearly independent means. These results also hold in the presence of Gaussian noise.The proof includes new bounds for anti-concentration of polynomials and perturbation bounds for complex diagonalizable matrices, which might be of interest in other contexts.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1306.5825},
author = {Goyal, Navin and Vempala, Santosh and Xiao, Ying},
booktitle = {Proceedings of the 46th Annual ACM Symposium on Theory of Computing - STOC '14},
eprint = {1306.5825},
file = {:C$\backslash$:/Users/yanshuo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goyal, Vempala, Xiao - 2014 - Fourier PCA and robust tensor decomposition.pdf:pdf},
number = {c},
pages = {584--593},
publisher = {ACM Press},
title = {{Fourier PCA and robust tensor decomposition}},
year = {2014}
}
@article{Frieze1996,
abstract = {We present a polynomial time algorithm to learn (in Valiant's PAC model) cubes in n? space (with general sides -not necessarily axes parallel) given uniformly distributed samples from the cube. In fact, we solve a more general problem of learning in polynomial time linear transformations in n?space. I.e., suppose x is an n?vector whose coordinates are mutually independent random variables with unknown (possibly diierent) probability distributions and A is an unknown nonsingular nn matrix. Then given polynomially many samples of y = Ax, we are able to learn the columns of A approximately. Geometrically, this is equivalent to learning a parallelepiped given uniformly distributed samples from it. Actually, we will only need a weak 4-way independence which we will describe later; also we will handle the case when y = Ax + b where b is an unknown vector. We rst show that using some standard Linear Algebra, we can learn parallelepipeds upto rotations. This only involves analyzing the matrix of second moments of the $\backslash$observed" variables y. The central problem is determining the rotation. We rst prove that certain fourth moments of y determine the rotation; we actually show that the maxima (and minima) of the fourth moment function give us the columns of A. Then we show a constructive (polynomial time) version of this result; i.e., we show that the maxima and minima can be found approximately by a nonlinear (fourth degree) optimization algorithm. While our primary motivation comes from Learning Theory, the problem has some similar-ities to problems in Factor Analysis, a branch of Statistics. There, no assumption is made about the independence of the x i , so the problem is more general; but also the results are weaker in that one only nds A upto rotations. Then one uses heuristics to nd a $\backslash$pleasing" rotation of A. (See for example 6].) The paper closes with some generalizations of the result and open problems.},
author = {Frieze, A. and Jerrum, M. and Kannan, Ravi},
file = {:C$\backslash$:/Users/yanshuo/Google Drive/Resources/Research Papers/Learning/Frieze, Jerrum, Kannan - Learning Linear Transformations.pdf:pdf},
journal = {Proceedings of 37th Conference on Foundations of Computer Science},
pages = {359--368},
title = {{Learning linear transformations}},
year = {1996}
}
@article{Tan2016,
author = "Tan, Yan Shuo",
fjournal = "Electronic Communications in Probability",
journal = "Electron. Commun. Probab.",
pages = "12 pp.",
pno = "43",
publisher = "The Institute of Mathematical Statistics and the Bernoulli Society",
title = "Energy optimization for distributions on the sphere and improvement to the Welch bounds",
volume = "22",
year = "2017"
}
@article{Hyvarinen1997,
abstract = {Independent component analysis (ICA) is a statistical method for transforming an observed multidimensional random vector into components that are statistically as independent from each other as possible. We use a combination of two different approaches for linear ICA: Comon's information theoretic approach and the projection pursuit approach. Using maximum entropy approximations of differential entropy, we introduce a family of new contrast functions for ICA. These contrast functions enable both the estimation of the whole decomposition by minimizing mutual information, and estimation of individual independent components as projection pursuit directions. The statistical properties of the estimators based on such contrast functions are analyzed under the assumption of the linear mixture model, and it is shown how to choose contrast functions that are robust and/or of minimum variance. Finally, we introduce simple fixed-point algorithms for practical optimization of the contrast functions},
author = {Hyvarinen, Aapo},
file = {:C$\backslash$:/Users/yanshuo/Google Drive/Resources/Research Papers/Learning/Hyvarinen - Fast and robust fixed-point algorithms.pdf:pdf},
journal = {IEEE Transactions on Neural Networks},
month = {may},
number = {3},
pages = {626--634},
pmid = {18252563},
publisher = {IEEE Comput. Soc. Press},
title = {{Fast and robust fixed-point algorithms for independent component analysis}},
volume = {10},
year = {1999}
}
@article{Comon1994,
abstract = {The independent component analysis (ICA) of a random vector consists of searching for a linear transformation that minimizes the statistical dependence between its components. In order to define suitable search criteria, the expansion of mutual information is utilized as a function of cumulants of increasing orders. An efficient algorithm is proposed, which allows the computation of the ICA of a data matrix within a polynomial time. The concept of ICA may actually be seen as an extension of the principal component analysis (PCA), which can only impose independence up to the second order and, consequently, defines directions that are orthogonal. Potential applications of ICA include data analysis and compression, Bayesian detection, localization of sources, and blind identification and deconvolution. {\textcopyright} 1994.},
author = {Comon, Pierre},
file = {:C$\backslash$:/Users/yanshuo/Google Drive/Resources/Research Papers/Learning/Comon - Independent component analysis.pdf:pdf},
journal = {Signal Processing},
keywords = {array processing,blind identification,data analysis,entropy,high-order statistics,independent,information,mixture,noise reduction,principal components,random variables,separation,source,statistical independence},
number = {3},
pages = {287--314},
pmid = {19419239},
title = {{Independent component analysis, A new concept?}},
volume = {36},
year = {1994}
}
@article{Davis1970a,
abstract = {When a Hermitian linear operator is slightly perturbed, by how much can its invariant subspaces change? Given some approximations to a cluster of neighboring eigenvalues and to the corresponding eigenvectors of a real symmetric matrix, and given an estimate for the gap that separates the cluster from all other eigenvalues, how much can the subspace spanned by the eigenvectors differ from the subspace spanned by our approximations? These questions are closely related; both are investigated here. The difference between the two subspaces is characterized in terms of certain angles through which one subspace must be rotated in order most directly to reach the other. These angles unify the treatment of natural geometric, operator-theoretic and error-analytic questions concerning those subspaces. Sharp bounds upon trigonometric functions of these angles are obtained from the gap and from bounds upon either the perturbation (1st question) or a computable residual (2nd question). An example is included.},
author = {Davis, C. and Kahan, W. M.},
file = {:C$\backslash$:/Users/yanshuo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Davis, Kahan - 1970 - The rotation of eigenvectors by a perturbation. III.pdf:pdf},
journal = {SIAM Journal on Numerical Analysis},
number = {1},
pages = {1--46},
title = {{The rotation of eigenvectors by a perturbation. III}},
volume = {7},
year = {1970}
}
@article{Kawanabe2006,
author = {Kawanabe, Motoaki and Sugiyama, Masashi and Blanchard, Gilles and M{\"{u}}ller, Klaus-Robert},
file = {:C$\backslash$:/Users/yanshuo/Google Drive/Resources/Research Papers/Learning/Kawanabe - NGCA with radial kernel functions.pdf:pdf},
journal = {Annals of the Institute of Statistical Mathematics},
number = {1},
pages = {57--75},
title = {{A new algorithm of non-Gaussian component analysis with radial kernel functions}},
volume = {59},
year = {2006}
}
