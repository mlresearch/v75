@InProceedings{barbier18,
author = {Jean Barbier and Florent Krzakala and Nicolas Macris and L{\'e}o Miolane and Lenka Zdeborov{\'a}},
title = {Optimal Errors and Phase Transitions in High-Dimensional Generalized Linear Models},
pages = {<leave empty>},
abstract = {Generalized linear models (GLMs) arise in high-dimensional
  machine learning, statistics, communications and signal processing.
%
  In this paper we analyze GLMs when the data matrix is random, as
  relevant in problems such as compressed sensing, error-correcting codes
  or benchmarks models in neural networks.
%
  We evaluate the mutual information (or ``free entropy'') from which we deduce the Bayes-optimal inference and generalization errors. 
  Our analysis applies to the high-dimensional limit where both the
  number of samples and dimensions are large and their ratio is fixed.
%
  Non-rigorous predictions for the optimal inference and
  generalization errors existed for special cases of GLMs, e.g. for
  the perceptron in the field of statistical physics based on
  the so-called replica method. Our present paper rigorously establishes those
  decades old conjectures and brings forward their
  algorithmic interpretation in terms of performance of the
  generalized approximate message-passing algorithm.
%
  Furthermore, we tightly characterize, for many learning problems,
  regions of parameters for which this algorithm achieves the optimal
  performance, and locate the associated sharp phase transitions
  separating learnable and non-learnable regions.}
}


