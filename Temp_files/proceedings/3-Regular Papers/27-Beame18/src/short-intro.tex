\section{Introduction}

In supervised learning from labelled examples, the question of the sample complexity required to obtain good generalization has been of considerable interest and research.   However, another
important parameter is how much information from these samples needs to be
kept in memory in order to learn successfully.
There has been a line of work improving the memory efficiency of
learning algorithms, and the question of the limits of such improvement has
begun to be tackled relatively recently.
\cite{DBLP:conf/nips/Shamir14}~and 
\cite{DBLP:conf/colt/SteinhardtVW16}~both
obtained constraints on the space required for certain learning problems and
in the latter paper, the authors  asked whether one could obtain strong
tradeoffs for learning from random samples that yields a superlinear threshold
for the space required for efficient learning.
\cite{DBLP:conf/focs/Raz16}~showed that
even given exact information, if the space of a learning algorithm is
bounded by a sufficiently small quadratic function of the number of input bits, then the
problem of online of learning parity functions given exact answers on random samples requires an
exponential number of samples even to learn parity functions approximately.

\begin{sloppypar}
More precisely, we consider problems of online learning from uniform random samples, in which an
unknown concept $x$ is chosen uniformly from a set $X$ of (multivalued) concepts and a learner is given a stream of samples 
$(a^{(1)},b^{(1)},(a^{(2)},b^{(2)}),\cdots$ where each $a^{(t)}$ is chosen
uniformly at random from $A$ and $b^{(t)}=\fun(a^{(t)},x)$ for labelling
function $\fun$ which maps
each pair $(a,x)$ to the outcome (or label) of the value of concept $x\in X$ when
given $a\in A$.  
The learner's goal is either that of identification ``find $x$'' or prediction ``predict $\fun(a,x)$ for randomly chosen $a$ with significant advantage over random
guessing.'' 
In the case of learning parities, $X=A=\coltset{0,1}^n$ and $L(a,x)=a\cdot x\pmod 2$.
With high probability $n+1$ uniformly random samples suffice to span
$\{0,1\}^n$ and one can learn parities using
Gaussian elimination with $(n+1)^2$ space.
Alternatively, an algorithm with only $O(n)$ space can wait for a specific
basis of vectors $a$ to appear (for example the standard basis) and store the 
resulting values; however, this takes $\Omega(2^n)$ time.
\cite{DBLP:conf/focs/Raz16}~showed that either $\Omega(n^2)$ space 
or $2^{\Omega(n)}$ time is essential: even if the space is bounded by $n^2/25$,
$2^{\Omega(n)}$ queries are required to learn $x$ correctly with any
probability that is $2^{-o(n)}$.
In follow-on work, \cite{DBLP:conf/stoc/KolRT17} showed that the same
lower bound applies even if the input $x$ is sparse.
\end{sloppypar}

We can view $x$ as a linear function over $\mathbb{F}_2$,
and, from this perspective,
parity learning identifies a linear function from evaluations over
uniformly random inputs.
A natural generalization asks if a similar lower bound exists when we learn
higher degree polynomials with bounded space. 
As a motivating example, consider homogeneous quadratic functions over
$\mathbb{F}_2$.
Let $\hypothesisdim=\binom{\sampledim+1}{2}$ and $X=\{0,1\}^\hypothesisdim$, which we identify with the space of
homogeneous quadratic polynomials in $\mathbb{F}_2[z_1,\ldots,z_\sampledim]$
or, equivalently, the space of upper triangular Boolean matrices.
This learning algorithm has $A=\coltset{0,1}^\sampledim$ and $X=\{0,1\}^\hypothesisdim$, 
and the learning function $\fun(a,x)=x(a) =\sum_{i\leq j} x_{ij}a_i a_j\bmod 2$,
or equivalently $\fun(a,x)=a^T x a$ when $x$ is viewed as an upper triangular
matrix.

Given $a\in \{0,1\}^\sampledim$ and $x\in \{0,1\}^\hypothesisdim$, we can view evaluating $x(a)$
as computing $aa^T\cdot x\bmod 2$ where we interpret $aa^T$ as an element
of $\{0,1\}^\hypothesisdim$.
For $O(\hypothesisdim)$ randomly chosen $a\in \{0,1\}^\sampledim$, the vectors $aa^T$ almost
surely span $\{0,1\}^\hypothesisdim$ and hence we can store $\hypothesisdim$ samples of
the form $(a,b)$ and apply Gaussian elimination to determine $x$.
This time, we only need $\sampledim+1$ bits to store each sample for a total space
bound of $O(\sampledim \hypothesisdim)$.  
An alternative algorithm using $O(\hypothesisdim)$ space and time $2^{O(\sampledim)}$ would be to look
for a specific basis such as the basis consisting of the
upper triangular parts of $\{e_i e_i^T\mid 1\le i\le \sampledim\}\cup
\{(e_i+e_j)(e_i+e_j)^T\mid 1\le i<j \le \sampledim\}$.
Previous lower bounds for learning do not apply to this problem\footnote{Note that in~\cite{DBLP:conf/stoc/KolRT17} the lower bound
applies in a dual case when the unknown $x$ is sparse, and hence $|X|\ll |A|$.}
because $|A|\ll |X|$. 
Our results imply that this tradeoff between $\Omega(\sampledim \hypothesisdim)$ space or
$2^{\Omega(\sampledim)}$ time
is inherently required to learn $x$ with probability $2^{-o(\sampledim)}$ or predict its output with at least $2^{-o(\sampledim)}$ advantage.

%Another view of the problem of learning homogenous quadratic functions (or
%indeed any low degree polynomial learning problem) is to
%consider it as parity learning with a smaller sample space of tests. 
%That is,
%we still want to learn $x\in \{0,1\}^n$ with samples $\{(a^{(t)},b^{(t)})\}_{t}$
%such that $b^{(t)}= a^{(t)}\cdot x\bmod 2$,
%but now $a^{(t)}$ is not chosen uniformly at random from $\{0,1\}^n$; 
%instead, we choose $c^{(t)}\in \{0,1\}^m$ uniformly at random and set $a^{(t)}$
%to be the upper triangular part of $c^{(t)} (c^{(t)})^T$.
%Then the size of the space $A$ of tests is $2^{m}$ which is $2^{O(\sqrt{n})}$
%and hence is much smaller than the size $2^n$ space $X$.


The techniques in \cite{DBLP:conf/focs/Raz16} and \cite{DBLP:conf/stoc/KolRT17}
were based on fairly ad-hoc simulations of the original space-bounded
learning algorithm by a restricted form of linear branching program
for which one can measure progress at learning $x$ using the dimension of the
consistent subspace. 
More recent papers, by \cite{DBLP:conf/colt/MoshkovitzM17} using graph mixing properties
and by \cite{DBLP:conf/focs/Raz17}
using an analytic approach, considered more general 
tests and used a measure of progress based on 2-norms.
While the method of \cite{DBLP:conf/colt/MoshkovitzM17} was not strong
enough to reproduce the bound in \cite{DBLP:conf/focs/Raz16} for the
case of parity learning, the methods of \cite{DBLP:conf/focs/Raz17} and
the later improvement~\citep{DBLP:conf/innovations/MoshkovitzM18} of~\cite{DBLP:conf/colt/MoshkovitzM17}
reproduced the parity learning bound and more. 

In particular, \cite{DBLP:conf/focs/Raz17} defined a $\pm 1$
matrix $M$ that is indexed by $A\times X$.
It is natural to see $M(a,x)$ as $(-1)^{\fun(a,x)}$ for a labelling function
$\fun$ that has labels in $\coltset{0,1}$.
The lower bound is governed by the (expectation) matrix norm of $M$, which is 
a function of the largest singular value of $M$, and the progress is analyzed
by bounding the impact of applying the matrix to probability distributions with
small expectation $2$-norm.
This method works if $|A|\ge |X|$ - i.e., the sample space of inputs is at least as
large as the concept class - but it fails completely if $|A|\ll |X|$, which
is precisely the situation for learning quadratic functions.
Indeed, none of the prior approaches works in this case.

In our work we extend the analytic approach to capture \emph{general} discrete
problems of learning from uniform random samples in which (1) the sample space
of inputs can be much smaller than the concept class and (2) members of the concept class can have values from an arbitrary finite set, which we identify with
$\coltset{0,1,\ldots,r}$ for convenience.
Our extensions come from two different directions.

We define a property of matrices $M$ that allows us to refine the
notion of the largest singular value and extend the method
of~\cite{DBLP:conf/focs/Raz17} to the cases that $|A|\ll |X|$.
This property, which we call the
{\em norm amplification curve} of the matrix on the positive orthant,
analyzes more precisely
how $\|M\cdot p\|_2$ grows as a function of $\|p\|_2$ for probability
vectors $p$ on $X$. The key reason that this is not simply governed by the
singular values is that the interior of the positive orthant can contain at most one singular vector.
We give a simple condition on the 2-norm amplification curve of $M$ that
is sufficient to ensure that there is a time-space tradeoff showing
that any learning algorithm for $M$ with success probability at least $2^{-\varepsilon \sampledim}$ for some $\varepsilon>0$ either requires space $\Omega(\hypothesisdim\sampledim)$ or
time $2^{\Omega(\sampledim)}$.

For any fixed learning problem given by a matrix $M$, the natural way to
express the amplification curve at any particular value of
$\|p\|_2$ yields an optimization problem given by a quadratic program with
constraints on $\|p\|^2_2$, $\|p\|_1$ and $p\ge 0$, and with objective function
$\|Mp\|^2_2=\langle M^TM,pp^T\rangle$ that seems difficult to solve.
Instead, we relax the quadratic program to a semi-definite program where we
replace $pp^T$ by a positive semidefinite matrix $U$ with the analogous
constraints.   We can then obtain an upper
bound on the amplification curve by moving to the SDP dual and evaluating the
dual objective at a particular Laplacian determined by the properties of
$M^T M$.

In order to handle concepts that are more than binary-valued\footnote{The
formalization of \cite{DBLP:conf/colt/MoshkovitzM17,DBLP:conf/innovations/MoshkovitzM18}
does include the case of multivalued outcomes, though they do not apply it to
any examples and their mixing condition does not hold in the case of small
input sample spaces},
we move to matrices whose entries are complex $r$-th roots of unity.
Indeed, a single matrix
$M$ does not suffice for $r>3$ and we instead work with a family of complex
matrices $M^{(1)},\ldots,M^{(r-1)}$, each associated with a different
root of unity.  We use the natural generalization of the norm amplification
curve to complex matrices and also generalize the semi-definite relaxation
method to bound these curves using $(\mj)^*\mj$ instead of $M^TM$. 
We then show how the overall analytic approach can be carried through with
a modest number of changes from the binary-valued case.

\begin{sloppypar}
Our lower bound shows that if the 2-norm amplification curve for $M$ has
(or, in the case of $r$-valued labels, matrices $M^{(1)},\ldots, M^{(r-1)}$ have)
the required property, then to achieve learning success probability for
$M$ of at least $|A|^{-\varepsilon}$ for
some $\varepsilon>0$, either space
$\Omega(\log |A|\cdot \log_r |X|)$ or time $|A|^{\Omega(1)}$ is required.
This matches the natural upper bounds asymptotically over a 
wide range of learning problems.
\end{sloppypar}

As applications, we focus on problems of learning
polynomials of varying degrees over finite fields.
For matrices $M$ associated with polynomials over $\mathbb{F}_2$,
the property of the matrices $M^T M$ required to bound the amplication
curves for $M$ correspond to properties of the weight distribution 
of Reed-Muller codes over $\mathbb{F}_2$.
For quadratic polynomials, this weight distribution is known
exactly.  In the case of higher degree polynomials, bounds on the weight
distribution of such codes, or more precisely on the bounds on the bias of
random degree $d$ polynomials over $\mathbb{F}_2$
of \cite{DBLP:journals/cc/Ben-EliezerHL12}~are sufficient to let us
show that learning polynomials of degree at most $d$
over $\mathbb{F}^\sampledim_2$ from random inputs with probability $2^{-\Omega(\sampledim/d)}$
either requires space $\Omega(\sampledim\hypothesisdim/d)$ or time $2^{\Omega(\sampledim/d)}$.

%This kind of bound is consistent even with what we know for very small sample
%spaces of tests: for example, if $X$ is the space of linear functions over
%$\mathbb{F}_2$ and $A$ is the standard basis $\coltset{e_1,\ldots,e_n}$ then,
%even for exact identification, space $O(n)$ and time $O(n\log n)$ are 
%necessary and sufficient by a simple coupon-collector analysis.

We also analyze learning problems for the case of prime fields $\mathbb{F}_p$
for $p$ odd using our multivalued techniques involving complex matrices.  
For $\mathbb{F}_p$, even the cases of linear and affine polynomials are new.
We relate the norm amplification curves of the associated matrices to bounds on
the bias of random degree $d$ polynomials over $\mathbb{F}_p$.
We also give a precise analysis of the bias of the set of quadratic
polynomials over $\mathbb{F}^\sampledim_p$ to derive tight time-space tradeoff lower
bounds for learning them.  For larger degrees we apply bounds on the bias that we
recently proved in a companion paper~\citep{bogy:reedmuller-bias}.

%Independent of the specific applications to learning from random examples
%that we obtain, the measures of matrices that we introduce, the 2-norm
%amplification curve on the positive orthant, and semi-definite relaxation
%approach seem likely to have significant applications in other contexts
%outside of learning.

\begin{sloppypar}\paragraph{Related work:}
Independently and contemporaneously with our preliminary version~\citep{bogy:quadlearn-tr}, \cite{grt:extractor-learn} 
proved closely related results to ours for the case of binary labels.   
The fundamental techniques are similarly grounded in the approach 
of~\cite{DBLP:conf/focs/Raz17}.
At the very high-level, they prove very similar structural properties of the matrix $M$, namely, they show that it is an ``$L_2$ two-source extractor'' which can be seen to be equivalent to bounding our norm amplification curve for learning matrices.
More precisely, their ``almost orthogonality property'' essentially corresponds to upper bounding $W_\kappa(M^*M)$ for some threshold $\kappa$  (see Definition \ref{d-W} and Lemma \ref{c-SDP-lemma}). However, since we use duality explicitly, our proof seems more amenable to extensions, particularly, when we have more structure in the learning matrix $M$.
Subsequently, \cite{grt:extractor-learn-stoc} were also
able to allow multivalued labels by extending the extractor approach to permit 
correlations between the sample inputs and the concept.  Our full paper appears as~\cite{bogy:learning-coltfull-tr}.
\end{sloppypar}


%though their method is based on viewing the
%matrices associated with learning problems as 2-source
%extractors rather than on bounding the SDP relaxations of their 2-norm
%amplification curves.  
%They do not consider
%applications to learning polynomials over larger finite fields.  
%Overall, the 
%extractor-based approach has very nice flexibility, though its power seems 
%incomparable to that of our approach based on SDP relaxations.
