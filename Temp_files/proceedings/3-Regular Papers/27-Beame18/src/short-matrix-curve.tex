\newpage
\section{Norm amplification by matrices on the positive orthant}

By definition, for $\mathbb{P}\in \Delta_X$, and $M\in \mathbb{C}^{A\times X}$,
$\|M\cdot \mathbb{P}\|_2^2= \E_{a\in_R A} [|(M\cdot \mathbb{P})(a)|^2]$.
Observe that for $\mathbb{P}=\mathbb{P}_{x|v}$ and $M=\mj$ for
$j\in \coltset{1,\ldots,\cp-1}$, the values
$|(\mj\cdot \mathbb{P}_{x|v})(a)|$ are the quantities that we test to determine
whether an edge labelled $a$ is a high bias edge that causes the truncation
of the computation path.  Therefore $\|\mj\cdot \mathbb{P}_{x|v}\|_2^2$ is
the expected square of this bias value for uniformly random inputs at $v$.

If we have not learned the concept $x$, we would not expect to be able to predict
its value on a random input; moreover, since any path that 
would follow a high bias input is truncated, it is essential to argue
that $\|\mj\cdot \mathbb{P}_{x|v}\|_2$ remains small at any node $v$ where there
has not been significant progress.

In~\cite{DBLP:conf/focs/Raz17} there is a single $\pm 1$ matrix $M$ and
$\|M\cdot \mathbb{P}_{x|v}\|_2$ is
bounded using the matrix norm $\|M\|_2$ given by
$\|M\|_2=\sup_{\substack{f:X\rightarrow \mathbb{R}\\f\ne 0}}\|M\cdot f\|_2/\|f\|_2$,
where the numerator is an expectation $2$-norm over $A$ and the denominator
is an expectation $2$-norm over $X$.
Thus $\|M\|_2=\sqrt{|X|/|A|}\cdot \sigma_{\max}(M)$,
where $\sigma_{\max}(M)$ is the largest singular value of $M$ and $\sqrt{|X|/|A|}$ is a normalization factor.

In the case of the matrix $M$ associated with parity learning, $|A|=|X|=2^n$
and
all the singular values are equal to $\sqrt{|X|}$ so
$\|M\|_2=\sqrt{|X|}=2^{n/2}$.  With this bound, if $v$ is not a node of
significant progress then $\|\mathbb{P}_{x|v}\|_2\le 2^{-(1-\delta/2) n}$
and hence $\|M\cdot \mathbb{P}_{x|v}\|_2 \le 2^{-(1-\delta) n/2}$ which is
$1/|A|^{(1-\delta)/2}$ and hence small.

However, even in the case of learning quadratic functions over $\mathbb{F}_2$,
the largest singular value of the matrix $M$ is still $\sqrt{|X|}$  (the uniform
distribution on $X$ is a singular vector) and so $\|M\|_2=|X|/\sqrt{|A|}$. 
But in that case, when $\|\mathbb{P}_{x|v}\|_2$ is $|X|^{-(1-\delta/2)}$ we
conclude that $\|M\|_2 \cdot\|\mathbb{P}_{x|v}\|_2$ is at most
$|X|^{\delta/2}/\sqrt{|A|}$ which is
much larger than 1 and hence a useless bound on $\|M\cdot \mathbb{P}_{x|v}\|_2$.

Indeed, the same kind of problem occurs in using the method
of~\cite{DBLP:conf/focs/Raz17} for any learning
problem for which $|A|$ is $|X|^{o(1)}$:
If $v$ is a child of the root of the branching program at which the more
likely outcome $b$ of a single randomly chosen input $a\in A$ is remembered, then
$\|\mathbb{P}_{x|v}\|_2 \le \sqrt{2}/|X|$. 
However, in this case $|(M\cdot \mathbb{P}_{x|v})(a)|=1$ and so
$\|(M\cdot \mathbb{P}_{x|v})\|_2\ge |A|^{-1/2}$. 
It follows that $\|M\|_2\ge |X|/(2|A|)^{1/2}$ and when $|A|$ is $|X|^{o(1)}$
the derived upper bound on $\|M\cdot \mathbb{P}_{x|v'}\|_2$ at nodes $v'$
where $\|\mathbb{P}_{x|v'}\|_2\ge 1/|X|^{1-\delta/2}$ will be larger
than 1 and therefore useless.

We need a more precise way to bound $\|M\cdot \mathbb{P}\|_2$ as a function
of $\|\mathbb{P}\|_2$ than using the single number $\|M\|_2$.   
To do this we will need to use the fact that $\mathbb{P}\in \Delta_X$ -- it has
a fixed $\ell_1$ norm and (more importantly) it is non-negative and therefore
lies in the positive orthant.

\begin{defn}
For $M\in \mathbb{C}^{A\times X}$ the 2-\emph{norm amplification curve} of $M$, 
$\tau_M:[0,1]\rightarrow \mathbb{R}$ is
given by\\[+1ex]
\centerline{ $\displaystyle\tau_M(\delta)=\sup_{\substack{\mathbb{P}\in \Delta_X\\ \|\mathbb{P}\|_2\le 1/|X|^{1-\delta/2}}} \log_{|A|} (\|M\cdot \mathbb{P}\|_2)$.}
\end{defn}

In other words, whenever $\|\mathbb{P}\|_2$ is at
most $|X|^{-(1-\delta/2)}$, $\|M\cdot \mathbb{P}\|_2$ is at most
$|A|^{\tau_M(\delta)}$.
To prove our lower bounds we will bound the norm amplification curves
$\tau_{\mj}$ for all $j\in \coltset{1,\ldots,\cp-1}$.

