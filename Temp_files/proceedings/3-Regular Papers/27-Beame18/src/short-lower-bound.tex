% !TEX root = short-paper.tex
\section{Lower Bound for Learning Finite Functions from Samples}

The full proof of Theorem~\ref{c-thm:mainlb} is given in the full paper.
We sketch the main ideas here.
For the  $0<\delta'<1$ given in the theorem
we define the significance threshold parameter
$\delta=\delta'/6$, the bias threshold to $|A|^{-\gamma}$ for
$\gamma=\min_j (-\tau_{\mj}(\delta')/2)>0$, and consider any branching program $B$
of length $<|A|^\beta$ and success probability at least $|A|^{-\beta/2}$ for
$\beta=\min(\delta,\gamma)/8$.
We prove that there must be at least
$|X|^{\Omega(\delta\gamma\logr)}=|A|^{\Omega(\delta\gamma\log_\cp |X|)}$
significant nodes in $B$.
We do so by proving that\\
(1) truncated paths reach significant vertices with
probability at least $|A|^{-O(\beta)}$
but\\
(2) for any particular significant vertex
$s$ in $B$, the probability that a random truncated computation path taken by a
random concept $x$ ends at $s$ with probability
$|X|^{-\Omega(\delta\gamma\logr)}$.

The proof of (1) is easy and relies on the fact that prior to reaching a
significant vertex, $\|\mathbb{P}_{x|v}\|_2<|X|^{-(1-\delta)/2}$ and 
$\tau_{\mj}(\delta)\le -2\gamma<0$, so $\|\mj\cdot \mathbb{P}_{x|v}\|_2$ is
small and hence the bias for a random input $a$ is overwhelming likely to
be smaller than $|A|^{-\gamma}$.

(2) follows according to a delicate progress argument where we
measure the progress
towards $s$ at a vertex $v$ (or by following an edge $e$) as
\begin{equation}
\rho(v)=\frac{\langle \mathbb{P}_{x|v},\mathbb{P}_{x|s}\rangle}{\langle \mathbb{P}_{x|s},\mathbb{P}_{x|s}\rangle},\qquad\mbox{respectively }
\rho(e)=\frac{\langle \mathbb{P}_{x|e},\mathbb{P}_{x|s}\rangle}{\langle \mathbb{P}_{x|s},\mathbb{P}_{x|s}\rangle}.
\end{equation}
Clearly $\rho(s)=1$ and we can show that the start vertex $v_0$ of $B$
has $\rho(v_0)\le |X|^{-\delta }$.
We denote the set of vertices of $B$ in the $t$-th layer by $V_t$. We overload
this notation by identifying it with a probability distribution that gives each
vertex $v\in V_t$ the probability that the random truncated computation path
reaches $v$ in layer $t$ or $\bot$ if it never reaches the $t$-th layer.
The progress at layer $t$ is then measured as
\begin{equation}
\Phi_t=\E_{v\sim V_t}[(\rho(v))^{\gamma \logr}]
\end{equation}
where we extend $\rho$ to define $\rho(\bot)=0$.
By definition $\Phi_0=|X|^{-\delta \gamma\logr}$.
Since any path that reaches (and therefore ends at) $s$ will contribute $\rho$
value 1, we obtain the probability bound by showing that a high moment of
$\rho$ for random $v\sim V_t$ grows slowly with $t$ and so is still extremely
small.  
More precisely, for every $t$ with $1\le t\le |A|^{\beta }-1$,
we show
\begin{equation}
\Phi_t \le \Phi_{t-1}\cdot(1+|A|^{-2\beta }) +|X|^{-\gamma \logr},
\end{equation}
which yields the claimed lower bound.
The argument requires an intermediate notion of progress in transferring from
vertices to edges and back to vertices. If $E_t$ is the set of edges from
$V_{t-1}$ to $V_t$ (together with its overloaded probability distribution),
we can define an analogous potential
$\Phi'_t=\E_{e\sim E_t}[(\rho(e))^{\gamma \logr}]$.  Merging edges into
vertices loses information and we easily obtain $\Phi_t\le \Phi'_t$ and it
remains to measure the growth from vertices $\Phi_t$ to edges $\Phi'_{t+1}$,
which of course is where new information is learned.
This follows from the key progress lemma which shows that
for $v\in V_{t-1}$,
\begin{equation}
\sum_{e\in \Gamma_{out}(v)} \Pr[e|v]\cdot 
\langle \mathbb{P}_{x|e},\mathbb{P}_{x|s}\rangle^{\gamma \logr}
\le (1+|A|^{-2\beta })\cdot 
\langle \mathbb{P}_{x|v},\mathbb{P}_{x|s}\rangle^{\gamma \logr} +|X|^{-\gamma \logr}.\label{maineqn}
\end{equation}
The second term in the sum covers the case when
$\langle \mathbb{P}_{x|e},\mathbb{P}_{x|s}\rangle$ shows correlation 
worse than the uniform distribution, which could be improved dramatically
in trivial ways and it is the $(1+|A|^{-2\beta})$ multiplicative factor bound
that is the important case.
In this case, we show that up to a factor biased by a small amount away
from 1, if edge $e$ is associated with test $a$ then
$\langle \mathbb{P}_{x|e},\mathbb{P}_{x|s}\rangle$ is essentially
a factor $(1+\sum_{j=1}^{r-1} |(\mj\cdot\mathbb{P}_f)(a)|)$ larger
than $\langle \mathbb{P}_{x|v},\mathbb{P}_{x|s}\rangle$ where $\mathbb{P}_f$
is a distribution that (essentially) gives points $x'$ probability mass given
by their proportional contribution of $\mathbb{P}_{x|v}(x')\mathbb{P}_{x|s}(x')$ to
$\langle \mathbb{P}_{x|v},\mathbb{P}_{x|s}\rangle$.
One can show that $\|\mathbb{P}_f\|_2$ itself is small relative to
$\|\mathbb{P}_{x|s}\|_2$, which can't yet be too large since $s$ is barely
significant, and so again we use the fact that the norm amplification 
$\tau_{\mj}(\delta)$ is small and deduce that $\|\mj\cdot \mathbb{P}_f\|_2$ is
tiny, which implies that each $|(\mj\cdot \mathbb{P}_f)(a)|$ is almost
surely tiny.  Plugging these in to the formula for the increase in 
the expected correlation overall all edges $e\in \Gamma_{out}(v)$ 
and analyzing the expectation of its higher
power yields~(\ref{maineqn}) and hence the bound on $\Phi'_{t+1}$ and
therefore Theorem~\ref{c-thm:mainlb}.
\newpage
