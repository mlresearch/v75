% !TEX root = colt.tex
\section{Theorems}
\label{sec:multivalued}
\label{sec:theorems}


Our general lower bound for learning problems over arbitrary finite label sets
is given by following theorem.

\begin{theorem}
\label{c-thm:mainlb}
There are constants $c_1,c_2,c_3>0$ such that the follow holds.
Let $\fun:A\times X\rightarrow \coltset{0,1,\ldots, \cp-1}$ be a labelling
function 
and for $j=1,\cdots,\cp-1$ define the matix $\mj\in \mathbb{C}^{A\times X}$
by $\mj(a,x)=\ur^{j\cdot \fun(a,x)}$ where $\ur=e^{2\pi i/\cp}$ and 
assume\footnote{We could write the statement of the theorem to apply to all $A$ and $X$ by
replacing each occurrence of $|A|$ in the lower bounds with $\min(|A|,|X|)$.
When $|A|\ge |X|$ and $\cp=2$, we can use $\|M\|_2$ to
bound $\tau_M(\delta')$ which yields
the bound given in~\cite{DBLP:conf/focs/Raz17}} that $|A|\leq |X|$.
%Let $n=\log_2 |X|$, $m=\log_2 |A|$ and
%assume that $m\le n$.
Suppose that for $0<\delta'<1$ we have
 $\tau_{\mj}(\delta')\le -\gamma'<0$
for all $j\in \{1,\cdots,\cp-1\}$.
Then, for $\varepsilon\ge c_1\, \min(\delta',\gamma')>0$,
$\beta\ge c_2\, \min(\delta',\gamma')>0$, and $\eta\ge c_3\,\delta'\, \gamma'>0$,
any algorithm that solves the learning problem for $\fun$ with
success probability at least $|A|^{-\varepsilon}$ or advantage
$\ge |A|^{-\varepsilon/2}$ either requires
space at least $\eta \log_2|A|\log_{\cp}|X|$ or time at least $|A|^{\beta}$.
\end{theorem}

%(We could write the statement of the theorem to apply to all $A$ and $X$ by
%replacing each occurrence of $|A|$ in the lower bounds with $\min(|A|,|X|)$. 
%When $|A|\ge |X|$ and $\cp=2$, we can use $\|M\|_2$ to
%bound $\tau_M(\delta')$ which yields
%the bound given in~\cite{DBLP:journals/eccc/Raz17}.)
\paragraph{Applications to learning polynomials}

There are many potential applications of the above theorem but for this paper
we focus learning polynomials from
their evaluations over finite fields of various sizes.  The bounds are 
derived using the semidefinite programming approach given in
Section~\ref{c-sec:sdp} together with analyses for polynomials given
in the full paper~\citep{bogy:learning-coltfull-tr}.

\paragraph{Learning polynomials over $\mathbb{F}_2$}

We first consider the case of polynomials over $\mathbb{F}_2$ which yield 
a binary labelling set.
In this case $\omega=-1$ and there is only one matrix $M$ whose entries
are $M(a,x)=(-1)^{\fun(a,x)}$ as in~\cite{DBLP:conf/focs/Raz17}.

The case of linear functions over $\mathbb{F}_2$ is just the parity learning problem.  
For learning higher degree polynomials over $\mathbb{F}_2$ we obtain the following bounds on the norm amplification curves of their
associated matrices:

\begin{theorem} The following properties hold:\\
(a) For all $\delta\in [0,1]$, the matrix $M$ for learning quadratic functions over
$\mathbb{F}_2^\sampledim$ satisfies\\ \centerline{$\tau_M(\delta)\le \frac{-(1-\delta)}8+\frac{5+\delta}{8\sampledim}$.}
(b) For any $\zeta>0$, there are constants $\delta,\gamma$ with $0<\delta<1/2$ and
$\gamma>0$
such that for $d\le (1-\zeta) \sampledim$ the matrix $M$ for learning functions of degree $\le d$ over $\mathbb{F}_2^\sampledim$  
satisfies $\tau_M(\delta)\le -\gamma/d$.
\end{theorem}

The case for quadratic polynomials over $\mathbb{F}_2$
follows from properties of the weight distribution of Reed-Muller codes $RM(n,2)$ shown by~\cite{DBLP:journals/tit/SloaneB70} and~\cite{mceliece1967linear}.
The case for higher degree polynomials over $\mathbb{F}_2$ follows
from tail bounds on the bias of $\mathbb{F}_2$ polynomials given by
\cite{DBLP:journals/cc/Ben-EliezerHL12}.

Using these bounds together with Theorem~\ref{c-thm:mainlb} yields the following:

\begin{theorem}
\label{cor:quadratic} There are constants $\varepsilon,\zeta>0$ such that the following hold: \\
(a) Let $\hypothesisdim=\binom{\sampledim+1}2$ for positive integer
$\sampledim$. 
Any algorithm for learning quadratic functions over
$\mathbb{F}_2^\sampledim$ that succeeds with probability at least
$2^{-\varepsilon \sampledim}$ requires space $\Omega(\sampledim\hypothesisdim)$ or time $2^{\Omega(\sampledim)}$.\\
(b)
Let $n>0$ and $d>0$ be integers such that 
$d\le (1-\zeta) \cdot \sampledim$ and let $\hypothesisdim=\sum_{i=0}^d \binom{\sampledim}i$.
Any algorithm for learning polynomial functions of degree at most $d$ over
$\mathbb{F}_2^\sampledim$ that succeeds with probability at least
$2^{-\varepsilon \sampledim/d}$ requires space $\Omega(\sampledim\hypothesisdim/d)$ or
time $2^{\Omega(\sampledim/d)}$.
\end{theorem}

These bounds are tight for constant $d$ since they match the resources used by the natural learning
algorithms described in the introduction up to constant
factors in the space bound and in the exponent of the time bound.


\paragraph{Learning polynomials over $\mathbb{F}_p$ for odd prime $p$.}


The following theorem bounds the norm amplification curves for
polynomials of various degrees over odd prime fields.

\begin{theorem}
\label{thm:linear-complex}
Let $p$ be an odd prime. 
For all $\delta\in (0,1)$ and for all
$j\in \mathbb{F}_p^*$,\\
(a) the matrices $\mj$ for learning
linear functions over $\mathbb{F}_p^\sampledim$ satisfy $\tau_{\mj}(\delta)\le -\frac{1-\delta}2$,\\
(b) the matrices $\mj$ for
learning affine functions over $\mathbb{F}_p^\sampledim$ satisfy
$\tau_{\mj}(\delta)\le -\frac{1-\delta}2 +\frac{\delta}{2\sampledim}$,\\
(c) the matrices $\mj$ for learning quadratic functions over
$\mathbb{F}_p^\sampledim$ satisfy $\tau_{\mj}(\delta)\le \frac{-(1-\delta)}4+\frac 2 \sampledim$, and\\
(d) for any $0<\zeta<1/2$, there are $\delta,\gamma$ with $0<\delta<1/2$ and $\gamma>0$
such that for $d\le \zeta \sampledim$, the matrices $\mj$ for learning functions of degree
$\le d$ over $\mathbb{F}_p^\sampledim$ satisfy
$\tau_{\mj}(\delta)\le -\gamma/d$. 
\end{theorem}

Parts (a) and (b) of this theorem are
immediate from matrix norm bounds.   
The proof of part (c) involves a tight structural
characterization of quadratic polynomials over $\mathbb{F}_p$
and is in the full paper.  
The proof of part (d) for 
$d\ge 3$ uses tail bounds on the bias of polynomials of degree at most
$d$ over $\mathbb{F}_p$ recently proved by the authors in a companion paper \citep{bogy:reedmuller-bias}.

Using the above bounds on the norm amplification curves
together with Theorem~\ref{c-thm:mainlb} we immediately obtain the time-space tradeoff lower bounds in following theorem.

\begin{theorem}
Let $p$ be an odd prime.  There is an $\varepsilon>0$ such that the
following hold:\\
(a) Any
algorithm for learning linear or affine
functions over $\mathbb{F}_p^\sampledim$ from their evaluations
that succeeds with probability at least $p^{-\varepsilon \sampledim}$ requires 
time $p^{\Omega(\sampledim)}$ or space $\Omega(\sampledim^2\log p)$.\\
%
%For quadratic polynomials over $\mathbb{F}_p$ we derive the
%following property of the norm amplification curves of the matrices associated
%with learning them from their evaluations.
%
%\begin{sloppypar}
%\begin{theorem}
%\label{thm:quadcurve-complex}
%Let $\fun$ be the function for learning  quadratic functions from their
%evaluations over
%$\mathbb{F}_p[z_1,\ldots, z_\sampledim]$ for $p$ an odd prime,
%and let  $\mj$ be the matrices associated with $\fun$ for $j=1,\cdots,p-1$.
%Then $\tau_{\mj}(\delta)\le \frac{-(1-\delta)}4+\frac 2 \sampledim$ for all 
%$\delta\in [0,1]$ and $j\in \{1,\cdots,p-1\}$.
%\end{theorem}
%\end{sloppypar}
%
%
%This gives us a lower bound of learning quadratic functions over $\mathbb{F}_p$.
%
%\begin{corollary}
(b) 
Let  $\hypothesisdim=\binom{\sampledim+2}2$. 
Any algorithm for learning quadratic functions over
$\mathbb{F}_p^\sampledim$ that succeeds with probability at least
$p^{-\varepsilon \sampledim}$ requires space $\Omega(\sampledim\hypothesisdim \log p)$ or time $p^{\Omega(\sampledim)}$.\\
%
%
%
% For general polynomials,
% the bounds of \cite{DBLP:journals/cc/Ben-EliezerHL12} are not known to extend
% to $\mathbb{F}_p$ for $p>2$.
% The closest related work is that of \cite{DBLP:journals/corr/0001L15} which also analyzes the
% conditions under which small deviations of the sort we wish to bound 
% occur, but do not provide a bound on the fraction of such occurrences.
%
%For general polynomials over $\mathbb{F}_p$,
%we have the following bound on the norm amplification curve
%for the matrix $M$ associated with learning such polynomials.
%
%
(c)
There are constants $\zeta,\varepsilon >0$ such that for 
$3\le d\le (1-\zeta) \cdot \sampledim$ and for $\hypothesisdim$ equal to the number of monomials of degree at most $d$ over
$\mathbb{F}_p^\sampledim$,
any algorithm for learning polynomial functions of degree at most $d$ 
over
$\mathbb{F}_p^\sampledim$ that succeeds with probability at least
$p^{-\varepsilon \sampledim/d}$ requires space $\Omega(\log p \cdot \sampledim\hypothesisdim/d)$ or time $p^{\Omega(\sampledim/d)}$.
\end{theorem}

%This follows  from the following theorem for which we use the bound on the bias of $\mathbb{F}_p$ polynomials from very recent work of
%\cite{bogy:reedmuller-bias}.

%\begin{theorem}
%\label{thm:F2curve-complex}
%For any $0<\zeta<1/2$, there are constants $\delta,\gamma$ with $0<\delta<1/2$ and
%$\gamma>0$
%such that the following holds.
%Let $d\le \zeta \sampledim$ and 
%$\mj$ be the matrices  for learning functions of degree
%$\le d$ over $\mathbb{F}_p[z_1,\ldots, z_\sampledim]$ for $j=1,\cdots,p-1$.
%Then $\tau_{\mj}(\delta)\le -\gamma/d$ for all 
%$j\in \{1,\cdots,p-1\}$.
%\end{theorem}



