% !TEX root = complex-paper.tex
\newcommand{\realm}{N}
\section{An SDP Relaxation for Norm Amplification on the Positive Orthant}
\label{c-sec:sdp}

For matrix $M\in \mathbb{C}^{A\times X}$, 
 is, $\tau_M(\delta)=\frac12 \log_{|A|} OPT_{M,\delta}$ where $OPT_{M,\delta}$ is the
optimum of the following quadratic program:
\begin{equation}
\begin{aligned}\label{c-original}
\text{Maximize}  &\quad \|M\cdot \mathbb{P}\|_2^2=\langle M\cdot \mathbb{P},M\cdot \mathbb{P}\rangle, \\
\text{subject to:}&\\
&\quad \sum_{i\in X} \mathbb{P}_i=1,\\
&\quad \sum_{i\in X} \mathbb{P}_i^2\le |X|^{\delta-1},\\
&\quad \mathbb{P}_i\geq 0 \qquad \text{for all }i\in X.
\end{aligned}
\end{equation}
Instead of attempting to solve \eqref{c-original}, presumably a difficult
quadratic program, we consider natural semidefinite programming (SDP) relaxation
\begin{equation} \label{c-SDP-relaxation}
\begin{aligned}
\text{Maximize}&\quad \langle M^{*} M,U\rangle\cdot |X|^2/|A| \\
\text{subject to:} &\\
[V]&\quad U\succeq 0,\\
[w]&\quad \sum_{i,j\in X} U_{ij}=1,  \\
[z]&\quad \sum_{i\in X} U_{ii}\le |X|^{\delta-1},\\
&\quad U_{ij}\in \mathbb{R},U_{ij}\geq 0 \qquad\text{for all } i,j\in X.
\end{aligned}
\end{equation}
where $M^{*}$ is the conjugate transpose of $M$ and
the $|X|^2/|A|$ factor
accounts for the difference in scaling factors based on the dimensions for the
two expectation inner products.  $M^*M$ in general is Hermitian and, since
$U$ must be symmetric, we know that we obtain the same optimum if we replace
$M^* M$ by it real part $N$.
Since we will only need to upper bound the value $OPT_{M,\delta}$,
we consider solutions to its dual program which can be conveniently expressed as
\begin{equation} \label{c-dual-SDP1}
\begin{aligned}
\text{Minimize}  &\quad w+z\cdot |X|^{\delta}\cdot |X|^{-1} &\\
\text{subject to:}&&\\
&\quad V\succeq 0,&\\
&\quad z I + w J\geq V+ \realm/|A|,\\
&\quad z\ge 0.
\end{aligned}
\end{equation}
where $I$ is the identity matrix and $J$ is the all 1's matrix over $X\times X$.

To simplify the complexity of analysis we restrict ourselves to considering
semidefinite matrices $V$ that are suitably chosen Laplacian matrices.
For any set $S$ in $X\times X$ and any $\alpha:S\rightarrow \mathbb{R}_+$
the Laplacian matrix associated with $S$ and $\alpha$
is defined by $L_{(S,\alpha)}:=\sum_{(i,j)\in S}\alpha(i,j) L_{ij}$ where
$L_{ij}=(e_i-e_j)(e_i-e_j)^T$ for the standard basis $\{e_i\}_{i\in X}$ .
Intuitively, in the dual SDP \eqref{c-dual-SDP1}, by adding matrix $V=L_{S,\alpha}$ for
suitable $S$ and $\alpha$ depending on $M$ we can shift weight from the
off-diagonal
entries of $N$ to the diagonal where they can be covered by the
$z+w$ entries on the diagonal rather than being covered by the $w$ values in
the off-diagonal entries.  
Analyzing the choice where we shift all but the last $\kappa$ of each
off-diagonal to the diagonal, we define
$W_\kappa(N)=\max_{i\in X} \sum_{j\in X:\ N_{i,j}> \kappa} (N_{i,j}-\kappa)$
and use the dual SDP to prove that
\begin{equation}
OPT_{M,\delta}\le (\kappa+W_\kappa(N)\cdot |X|^{\delta-1})/|A|.
\end{equation}
It may not be easy to bound $W_\kappa(N)$ directly,
since the real part of $M^{*}M$ may be awkward to work with.  However,
$$W_\kappa(N)\le \tilde W_\kappa(M^*M)=\max_{i\in X}\ \sum_{j\in X:\ |(M^{*}M)_{i,j}|> \kappa} (|(M^{*}M)_{i,j}|-\kappa)$$
which suffices to upper bound $OPT_{M,\delta}$.  Details are in the full paper in the appendix.

