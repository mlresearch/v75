% !TEX root = stoc-paper.tex
\section{Theorems}
\label{sec:multivalued}
\label{sec:theorems}


Our general lower bound for learning problems over arbitrary finite label sets
is given by following theorem.

\begin{theorem}
\label{c-thm:mainlb}
There are constants $c_1,c_2,c_3>0$ such that the follow holds.
Let $\fun:A\times X\rightarrow \coltset{0,1,\ldots, \cp-1}$ be a labelling
function 
and for $j=1,\cdots,\cp-1$ define the matix $\mj\in \mathbb{C}^{A\times X}$
by $\mj(a,x)=\ur^{j\cdot \fun(a,x)}$ where $\ur=e^{2\pi i/\cp}$ and 
assume\footnote{We could write the statement of the theorem to apply to all $A$ and $X$ by
replacing each occurrence of $|A|$ in the lower bounds with $\min(|A|,|X|)$.
When $|A|\ge |X|$ and $\cp=2$, we can use $\|M\|_2$ to
bound $\tau_M(\delta')$ which yields
the bound given in~\cite{DBLP:conf/focs/Raz17}} that $|A|\leq |X|$.
%Let $n=\log_2 |X|$, $m=\log_2 |A|$ and
%assume that $m\le n$.
Suppose that for $0<\delta'<1$ we have
 $\tau_{\mj}(\delta')\le -\gamma'<0$
for all $j\in \{1,\cdots,\cp-1\}$.
Then, for $\varepsilon\ge c_1\, \min(\delta',\gamma')>0$,
$\beta\ge c_2\, \min(\delta',\gamma')>0$, and $\eta\ge c_3\,\delta'\, \gamma'>0$,
any algorithm that solves the learning problem for $\fun$ with
success probability at least $|A|^{-\varepsilon}$ or advantage
$\ge |A|^{-\varepsilon/2}$ either requires
space at least $\eta \log_2|A|\log_{\cp}|X|$ or time at least $|A|^{\beta}$.
\end{theorem}

%(We could write the statement of the theorem to apply to all $A$ and $X$ by
%replacing each occurrence of $|A|$ in the lower bounds with $\min(|A|,|X|)$. 
%When $|A|\ge |X|$ and $\cp=2$, we can use $\|M\|_2$ to
%bound $\tau_M(\delta')$ which yields
%the bound given in~\cite{DBLP:journals/eccc/Raz17}.)
\paragraph{Applications to learning polynomials}

There are many potential applications of the above theorem but for this paper
we focus on natural families of problems based on learning polynomials from
their evaluations over finite fields of various sizes.  The bounds are 
derived using the semidefinite programming approach given in
Section~\ref{c-sec:sdp} together with analyses for polynomials given
in Appendix B.

\paragraph{Learning polynomials over $\mathbb{F}_2$}

We first consider the case of polynomials over $\mathbb{F}_2$ which yield 
a binary labelling set.
In this case $\omega=-1$ and there is only one matrix $M$ whose entries
are $M(a,x)=(-1)^{\fun(a,x)}$ as in~\cite{DBLP:conf/focs/Raz17}.

The case of linear functions over $\mathbb{F}_2$ is just the parity learning
problem.  For quadratic polynomials over $\mathbb{F}_2$
we have a good understanding of their structure, and
obtain the following bound on the amplification properties of the matrix $M$
associated with learning quadratic functions over $\mathbb{F}_2$.

\begin{theorem}
\label{thm:quadcurve}
Let $M$ be the matrix for learning quadratic functions over
$\mathbb{F}_2[z_1,\ldots, z_\sampledim]$.
Then $\tau_M(\delta)\le \frac{-(1-\delta)}8+\frac{5+\delta}{8\sampledim}$ for all 
$\delta\in [0,1]$.
\end{theorem}

The following corollary is then immediate.

\begin{corollary}
\label{cor:quadratic}
Let $\sampledim$ be a positive integer and $\hypothesisdim=\binom{\sampledim+1}2$. For some $\varepsilon>0$,
any algorithm for learning quadratic functions over
$\mathbb{F}_2[z_1,\ldots, z_\sampledim]$ that succeeds with probability at least
$2^{-\varepsilon \sampledim}$ requires space $\Omega(\sampledim\hypothesisdim)$ or time $2^{\Omega(\sampledim)}$.
\end{corollary}

This bound is tight since it matches the resources used by the learning
algorithms for quadratic functions given in the introduction up to constant
factors in the space bound and in the exponent of the time bound.

For $d>3$, we obtain a somewhat weaker bound on the norm amplification curve
for the matrix $M$ associated with learning polymomials of degree $\le d$.

\begin{theorem}
\label{thm:smalld}
There are constants $\zeta,\varepsilon >0$ such that for positive integer
$d\le (1-\zeta) \cdot \sampledim$ and $\hypothesisdim=\sum_{i=0}^d \binom{\sampledim}i$,
any algorithm for learning polynomial functions of degree at most $d$ over
$\mathbb{F}_2[z_1,\ldots, z_\sampledim]$ that succeeds with probability at least
$2^{-\varepsilon \sampledim/d}$ requires space $\Omega(\sampledim\hypothesisdim/d)$ or
time $2^{\Omega(\sampledim/d)}$.
\end{theorem}

This follows immediately from the following theorem which we derive using
a semidefinite programming relaxation and reducing the problem to
analyzing of the bias of $\mathbb{F}_2$ polynomials which was bounded in
\cite{DBLP:journals/cc/Ben-EliezerHL12}.

\begin{theorem}
\label{thm:F2curve}
For any $\zeta>0$, there are constants $\delta,\gamma$ with $0<\delta<1/2$ and
$\gamma>0$
such that the following holds.
Let $d\le (1-\zeta) \sampledim$ and $M$ be the matrix for learning functions of degree
$\le d$ over $\mathbb{F}_2[z_1,\ldots, z_\sampledim]$.  
Then $\tau_M(\delta)\le -\gamma/d$.
\end{theorem}

\paragraph{Learning polynomials over $\mathbb{F}_p$ for odd prime $p$.}

The application to the case of learning linear functions over
$\mathbb{F}_p$ for prime $p$ can follow directly using the matrix norm.
Nonetheless, for consistency we state them using our framework.

\begin{proposition}
\label{thm:linear-complex}
Let $p$ be an odd prime. 
For all $\delta\in (0,1)$,
for $j\in \mathbb{F}_p^*$, the matrices $\mj$ for
learning affine functions over $\mathbb{F}_p$ satisfy
$\tau_{\mj}(\delta)\le -\frac{1-\delta}2 +\frac{\delta}{2\sampledim}$, and those
for linear functions satisfy $\tau_{\mj}(\delta)\le -\frac{1-\delta}2$.
\end{proposition}

\begin{corollary}
Let $p$ be an odd prime.  There is an $\varepsilon>0$ such that any
algorithm for learning linear or affine
functions in $n$ variables over $\mathbb{F}_p$ from their evaluations
that succeeds with probability at least $2^{-\varepsilon n}$ requires 
time $2^{\Omega(n)}$ or space $\Omega(n^2)$.
\end{corollary}

For quadratic polynomials over $\mathbb{F}_p$ we derive the
following property of the norm amplification curves of the matrices associated
with learning them from their evaluations.

\begin{sloppypar}
\begin{theorem}
\label{thm:quadcurve-complex}
Let $\fun$ be the function for learning  quadratic functions from their
evaluations over
$\mathbb{F}_p[z_1,\ldots, z_\sampledim]$ for $p$ an odd prime,
and let  $\mj$ be the matrices associated with $\fun$ for $j=1,\cdots,p-1$.
Then $\tau_{\mj}(\delta)\le \frac{-(1-\delta)}4+\frac 2 \sampledim$ for all 
$\delta\in [0,1]$ and $j\in \{1,\cdots,p-1\}$.
\end{theorem}
\end{sloppypar}


This gives us a lower bound of learning quadratic functions over $\mathbb{F}_p$.

\begin{corollary}
\label{cor:quadratic-complex}
Let $\sampledim$ be a positive integer and $\hypothesisdim=\binom{\sampledim+2}2$. For some $\varepsilon>0$,
any algorithm for learning quadratic functions over
$\mathbb{F}_p[z_1,\ldots, z_\sampledim]$ that succeeds with probability at least
$p^{-\varepsilon \sampledim}$ requires space $\Omega(\sampledim\hypothesisdim)$ or time $p^{\Omega(\sampledim)}$.
\end{corollary}


% For general polynomials,
% the bounds of \cite{DBLP:journals/cc/Ben-EliezerHL12} are not known to extend
% to $\mathbb{F}_p$ for $p>2$.
% The closest related work is that of \cite{DBLP:journals/corr/0001L15} which also analyzes the
% conditions under which small deviations of the sort we wish to bound 
% occur, but do not provide a bound on the fraction of such occurrences.

For general polynomials over $\mathbb{F}_p$,
we have the following bound on the norm amplification curve
for the matrix $M$ associated with learning such polynomials.

\begin{theorem}
\label{thm:smalld-complex}
There are constants $\zeta,\varepsilon >0$ such that for positive integer
$d\le (1-\zeta) \cdot \sampledim$ and $\hypothesisdim$ be the number of monomials of degree at most $d$ over
$\mathbb{F}_p[z_1,\ldots, z_\sampledim]$,
any algorithm for learning polynomial functions of degree at most $d$ 
$\mathbb{F}_2[z_1,\ldots, z_\sampledim]$ that succeeds with probability at least
$p^{-\varepsilon \sampledim/d}$ requires space $\Omega(\log p \cdot \sampledim\hypothesisdim/d)$ or
time $p^{\Omega(\sampledim/d)}$.
\end{theorem}

This follows  from the following theorem for which we use the bound on the bias of $\mathbb{F}_p$ polynomials from very recent work of
\cite{bogy:reedmuller-bias}.

\begin{theorem}
\label{thm:F2curve-complex}
For any $0<\zeta<1/2$, there are constants $\delta,\gamma$ with $0<\delta<1/2$ and
$\gamma>0$
such that the following holds.
Let $d\le \zeta \sampledim$ and 
$\mj$ be the matrices  for learning functions of degree
$\le d$ over $\mathbb{F}_p[z_1,\ldots, z_\sampledim]$ for $j=1,\cdots,p-1$.
Then $\tau_{\mj}(\delta)\le -\gamma/d$ for all $j\in \{1,\cdots,p-1\}$.
\end{theorem}



