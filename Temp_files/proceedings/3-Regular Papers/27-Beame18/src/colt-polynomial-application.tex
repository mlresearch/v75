\newcommand{\w}{\textrm{weight}}
\newpage\section{Applications to Learning Polynomial Functions over Finite Fields}

In this section, we prove all the lower bounds on learning polynomials discussed in Section~\ref{sec:theorems}, from Theorem~\ref{thm:quadcurve} to Theorem~\ref{thm:quadcurve-complex}. We use the strategy in Section \ref{c-sec:sdp} by studying the $W_\kappa$ function for matrices associated with learning polynomials over finite fields.   We show that the values of this function are determined by the weight distribution and expected bias
of these polynomials.

\subsection{The Bias of $\mathbb{F}_2$ Polynomials and the Weight Distribution of Reed-Muller Codes}
\label{sec:f2-reed-muller}

Let $d\ge 2$ be an integer.  For any integer $\sampledim\ge d$, consider the learning
problem for $\mathbb{F}_2$ polynomials in $\sampledim$ variables of degree at most $d$, 
%we have $n=\sum_{i=1}^d \binom{m}{d}$. 
That is $A=\mathbb{F}_2^\sampledim$ and, expressing polynomials by their coefficients,
we have $X=\mathbb{F}_2^\hypothesisdim$ where $\hypothesisdim=\sum_{i=0}^d \binom{\sampledim}{d}$ and
for $a\in A$ and $x\in X$,
$\fun (a,x)=x(a)=\sum_{S: 0\leq |S|\leq d} x_S \prod_{i\in S} a_i$ over
$\mathbb{F}_2$.

Recall that since the range of $\fun$ is $\coltset{0,1}$,
we have a $N=M^T\cdot M$ 
where $M(a,x)=(-1)^{\fun(a,x)}=(-1)^{x(a)}$.
Let $M_x$ denote the $x$-th column of $M$ where $x\in \coltset{0,1}^n$.
Then $N_{xy}=2^\sampledim \cdot \langle M_x,M_y\rangle$.
%Recall that for $a\in \mathbb{F}_2^m$ and $x\in \mathbb{F}_2^n$,
%$x(a)=\sum_{S: 0\leq |S|\leq d} x_S \prod_{i\in S} a_i$ over $\mathbb{F}_2$.

\begin{proposition}
\label{prop:equalrows}
Let $\mathbf{0}=0^\hypothesisdim$.
Then $\langle M_x,M_y\rangle=\langle M_{\mathbf{0}},M_{x+y}\rangle$.
\end{proposition}

\begin{proof}
\begin{align*}
\langle M_x,M_y\rangle&=\E_{a\in \mathbb{F}_2^\sampledim} M_x(a)M_y(a)
=\E_{a\in \mathbb{F}_2^\sampledim} (-1)^{x(a)}(-1)^{y(a)}
=\E_{a\in \mathbb{F}_2^\sampledim} (-1)^{x(a)+y(a)}\\
&=\E_{a\in \mathbb{F}_2^\sampledim} (-1)^{(x+y)(a)}
=\E_{a\in \mathbb{F}_2^\sampledim} M_{\mathbf{0}}(a)M_{x+y}(a)
=\langle M_{\mathbf{0}},M_{x+y}\rangle
\end{align*}
\end{proof}

Since the mapping $y\mapsto x+y$ for $x\in \mathbb{F}_2^\hypothesisdim$ is
1-1 on $\mathbb{F}_2^\hypothesisdim$, 
every row of $N_x$ for $x\in X$ contains the same multi-set of values.
Therefore, in order to analyze the function $W_\kappa(N)$,
we only need to examine the fixed row $N_\mathbf{0}$ of $N$,
where each entry
$$N_{\mathbf{0}x}=\sum_{a\in \mathbb{F}_2^\sampledim} M(a,x)=\sum_{a\in \mathbb{F}_2^\sampledim}(-1)^{x(a)}.$$
For $x\in X$, define $\w(x)=|\coltset{a\in \mathbb{F}_2^\sampledim\ :\ x(a)=1}|$.
By definition, for $x\in \mathbb{F}_2^\hypothesisdim$, 
$N_{\mathbf{0}x}=\sum_{a\in \mathbb{F}_2^\sampledim}(-1)^{x(a)}=2^\sampledim-2\cdot \w(x)$.
Thus, understanding the function $W_\kappa(N)$ that we use to derive our
bounds via Theorem~\ref{thm:quadcurve} reduces to understanding the distribution of
$\w(x)$ for $x\in X$.
In particular, our goal of showing that for some $\kappa$ for which
$(\kappa+W_\kappa(N))/2^\sampledim$ is at most $2^{2\tau \sampledim}$ for some $\tau<0$ 
follows by showing that the distribution of $\w(x)$ is tightly concentrated
around $2^{\sampledim}/2$.

We can express this question in terms of 
%(a small variation of)
the Reed-Muller error-correcting code $RM(d,\sampledim)$ over $\mathbb{F}_2$
(see, e.g.~\cite{berlekamp-book}).

\begin{defn}
The Reed-Muller code $RM(d,\sampledim)$ over $\mathbb{F}_2$ is the set of vectors
$\coltset{G\cdot x\mid x\in \coltset{0,1}^\hypothesisdim}$ where $G$ is the $2^\sampledim\times \hypothesisdim$ matrix
for $\hypothesisdim=\sum_{t=0}^d \binom{\sampledim}{t}$ over $\mathbb{F}_2$
with rows indexed by vectors $a\in \coltset{0,1}^\sampledim$ and columns indexed by
subsets $S\subseteq [\sampledim]$ with $|S|\le d$ given by $G(a,S)=\prod_{i\in S} a_i$.
\end{defn}

%Evaluating $\w(x)$ for all $x\in \coltset{0,1}^n$ is almost exactly that of

Evaluating $\w(x)$ for all $x\in \coltset{0,1}^\hypothesisdim$ is that of
understanding the distribution of Hamming weights of the vectors in $RM(d,\sampledim)$,
a question with a long history.

%Because we assumed that the constant term of our polynomials is 0 (in order to
%view the learning problem as a variant of the parity learning problem with a
%smaller set of test vectors), we need to make a small change in the
%Reed-Muller code.
%Consider the subcode $RM'(d,m)$ of $RM(d,m)$ having the 
%generator matrix $G'$ that is the same as $G$ but with the (all 1's) column
%indexed by $\emptyset$ removed.
%In this case, for each $x\in \coltset{0,1}^m$, by definition, $\w(x)$ is 
%precisely the Hamming weight of the vector $G'\cdot x$ in $RM'(d,m)$.

%Now by definition
%$$RM(d,m)=\coltset{y\mid y\in RM'(d,m)\mbox{ or }\overline{y}\in RM'(d,m)}$$
%where $\overline{y}$ is the same as $y$ with every bit flipped.
%In particular, $RM'(d,m)\subset RM(d,m)$ and the distribution of the
%weights for $RM(d,m)$
%is symmetric about $2^{m-1}$, whereas the distribution of weights in
%$RM'(d,m)$ is not necessarily symmetric.

\paragraph{Quadratic polynomials over $\mathbb{F}_2$}
For the special case that $d=2$, \cite{DBLP:journals/tit/SloaneB70} derived an exact
enumeration of the number of vectors of each weight in $RM(2,\sampledim)$.

\begin{proposition}[\cite{DBLP:journals/tit/SloaneB70}]
\label{lem:RM(2,m)-count}
The weight of every codeword of $RM(2,\sampledim)$ is of the form $2^{\sampledim-1}\pm 2^{\sampledim-i}$
for some integer $i$ with $1\le i\le \lceil \sampledim/2\rceil$ or precisely $2^{\sampledim-1}$
and the number of codewords of weight $2^{\sampledim-1}+2^{\sampledim-i}$ or $2^{\sampledim-1}-2^{\sampledim-i}$ 
is precisely
$$2^{i(i+1)} \prod_{j=0}^{i-1} \frac{2^{\sampledim-2j}(2^{\sampledim-2j-1}-1)}{2^{2(j+1)}-1}.$$
\end{proposition}

(Though the original proof used other methods, a simpler alternative
proof by \cite{mceliece1967linear} follows from
a lemma of  \cite{dickson:book} giving a normal form theorem for quadratic polynomials over
$\mathbb{F}_{2^t}$.  
We will use a similar approach when we analyze
quadratic polynomials over $\mathbb{F}_{p^t}$.)

%\begin{proposition}\label{main_counting}
%Let $M$ be the matrix for learning quadratic functions over 
%$\mathbb{F}_2[z_1,\ldots,z_m]$ and let $N=M^T\cdot M$. 
%\begin{enumerate}
%\item Every row of $N_x$ for $x\in X$ contains the same multi-set of values.
%\item $N_{xx}=2^m$ and $N_{xy}\in \{\pm 2^{m-1},\pm 2^{m-2},\cdots,\pm 2^{\lceil \frac m 2\rceil},0\}$ for $x\neq y\in \coltset{0,1}^n$.
%\item For $i>0$, let $c_i$ be the number of entries equal to $2^{m-i}$ in each
%row of $N$;
%then 
%\[c_i=(2^{2i-1}+2^{i-1})\frac{\prod_{j=0}^{2i-1}(2^m-2^j)}{\prod_{j=1}^i2^{2j-1}(2^{2j}-1)}\leq 2^{2im}\]
%\end{enumerate}
%\end{proposition}

%Given Proposition~\ref{main_counting} we can derive
%Theorem~\ref{thm:quadcurve}.

\begin{proof}{\bfseries\upshape of Theorem~\ref{thm:quadcurve}\ }
Let the threshold $\kappa=2^{\sampledim-k}$ for some integer $k$ to be determined later.
By Lemma~\ref{c-SDP-lemma} with $X=\{0,1\}^\hypothesisdim$, for \eqref{c-original}, we have
$OPT_{M,\delta}\le (\kappa+W_\kappa(N) 2^{(\delta-1)\hypothesisdim})/2^\sampledim$ where
$N=M^T\cdot M$.
By definition for all $x\in X$ we have $N_{\mathbf{0}x}=2^\sampledim-2\cdot \w(x)$
and by Proposition~\ref{lem:RM(2,m)-count}, we know that if $2^\sampledim-2\cdot \w(x)>0$
then it is $2^{\sampledim-i+1}$ for some $1\le i\le \lceil \sampledim/2\rceil$.
Also by Proposition~\ref{lem:RM(2,m)-count}, the number, $c_i$, of $x\in X$
such that $N_{\mathbf{0}x}=2^{\sampledim-i+1}$ is at most
$$2^{i(i+1)} \prod_{j=0}^{i-1} \frac{2^{\sampledim-2j}(2^{\sampledim-2j-1}-1)}{2^{2(j+1)}-1}
\le 2^{2(i-1)\sampledim}.$$
Therefore, by definition
of $W_\kappa$ and Proposition~\ref{prop:equalrows}, for any $x\in X$ 
we have 
\begin{equation*}
W_\kappa(N)
\le \sum_{y\in X: N_{xy}> 2^{\sampledim-k}} N_{xy}
=\sum_{i=1}^{k}  c_i\cdot 2^{\sampledim-i+1} 
\le \sum_{i=1}^{k} 2^{2(i-1)\sampledim}\cdot 2^{\sampledim-i+1}
=\sum_{i=1}^{k} 2^{(2\sampledim-1)(i-1)+\sampledim}
< 2^{(2\sampledim-1)k}.
\end{equation*}
Thus for any $k$,
$$OPT_{M,\delta}\le (2^{\sampledim-k}+2^{(2\sampledim-1)k+(\delta-1)\hypothesisdim})/2^\sampledim\le 2^{-k}+2^{(2\sampledim-1)k-(1-\delta)\sampledim(\sampledim+1)/2-\sampledim}.$$
The first term is larger for $k\le (1-\delta) \sampledim/4 + (3-\delta)/4$ so to balance
them as much as possible we choose
$k=\lfloor (1-\delta) \sampledim/4 + (3-\delta)/4\rfloor\ge (1-\delta)\sampledim/4 -(1+\delta)/4$.
Hence $OPT_{M,\delta}\le 2\cdot 2^{-k} \le 2^{-\frac {1-\delta}4 \sampledim +\frac{5+\delta}4}$
Therefore,
$\tau_M(\delta)=\frac12 \log_{2^\sampledim} OPT_{M,\delta}\le -\frac{(1-\delta)}8+\frac{(5+\delta)}{8\sampledim}$ as required. 
\end{proof}

%The proof of Proposition~\ref{main_counting} is in the appendix but in the
%next section we outline the connection to the weight distribution of
%Reed-Muller codes over $\mathbb{F}_2$, and show how bounds on the weight
%distribution of such codes allow us to derive time-space tradeoffs for
%learning larger degree $\mathbb{F}_2$ polynomials as well.

%The exact enumeration for the values of the weight function $\w$ for $RM'(2,m)$
%that corresponds to the values in
%Proposition~\ref{main_counting} are only slightly different from those for
%$RM(2,m)$ in Proposition~\ref{lem:RM(2,m)-count} and 
%Proposition~\ref{lem:RM(2,m)-count} gives the same asymptotic 
%bound we used in the proof of Theorem~\ref{thm:quadcurve} since words of weight
%$2^{m-1}-2^{m-i}$ correspond to entries of value $2^{m-i+1}$ in the matrix $N$.
%We give a proof of the exact counts of
%Proposition~\ref{main_counting} in the appendix.

\paragraph{Polynomials of degree $d>2$ over $\mathbb{F}_2$}
For the case that $d>2$, the minimum distance, the smallest 
weight of a non-zero codeword, in $RM(d,\sampledim)$ is known to be $2^{\sampledim-d}$ but
for $2<d<\sampledim-2$, no exact enumeration of the weight distribution of the code
$RM(d,\sampledim)$ is known.
It was a longstanding problem even to approximate the
number of codewords of different weights in $RM(d,\sampledim)$.  
Relatively recently, bounds on
these weights (or more precisely the associated biases) that are good enough
for our purposes were shown by
\cite{DBLP:journals/cc/Ben-EliezerHL12}.

\begin{proposition}
\label{prop:behl-bias}
For $\varepsilon>0$ there are constants $c_1, c_2$ with $0<c_1,c_2<1$ such that if $p$ is a uniformly random degree $d$ polynomial over
$\mathbb{F}^\sampledim_2$ and $d\le (1-\varepsilon)\sampledim$ then 
$$\Pr[|\E_{a\in \coltset{0,1}^\sampledim} (-1)^{p(a)}|>2^{-c_1 \sampledim/d}]\le 2^{-c_2 \sum_{i=0}^d \binom{\sampledim}{i}}.$$
\end{proposition}

From this form we can obtain the bound on the norm amplification curve
of the associated matrix fairly directly.
\medskip

\begin{proof}{\bfseries\upshape of Theorem~\ref{thm:F2curve}\ }
Fix $\varepsilon>0$ and let $0<c_1, c_2<1$ be the constants depending on
$\varepsilon$ from Proposition~\ref{prop:behl-bias}.
Let $\delta=c_2/2$ so $0<\delta<1/2$.
Let $M$ be the $2^\sampledim\times 2^\hypothesisdim$ matrix associated with learning 
polynomials of degree at most $d$ over $\mathbb{F}_2$, let $N=M^T\cdot M$ and
Setting $\kappa=2^{(1-c_1/d)\sampledim}$, by Proposition~\ref{prop:behl-bias} at
most $2^{(1-c_2)\hypothesisdim}$ 
polynomials $p$ have entries $N_{0p}$ larger than $\kappa$.
Each such entry has value at most $2^\sampledim$ so 
$W_\kappa(N)\le 2^\sampledim\cdot 2^{(1-c_2)\hypothesisdim}$.
by Lemma~\ref{c-SDP-lemma} with 
$X=\coltset{0,1}^\hypothesisdim$ we have
$$OPT_{M,\delta}\le (\kappa+W_\kappa(N)\cdot 2^{(\delta-1)\hypothesisdim})/2^\sampledim
\le 2^{-c_1 \sampledim/d}+2^{(\delta - c_2)\hypothesisdim+1}\le 2^{-c_1 \sampledim/d}+2^{1-\delta \hypothesisdim}$$
which is at most $2^{-c' \sampledim/d}$ for some constant $c'>0$. 
Hence $\tau_M(\delta)\le -c'/d$.
\end{proof}

% !TEX root = stoc-paper.tex
\newcommand{\val}{\textrm{val}}
\newcommand{\monomial}[3]{\mathcal{M}_{#1}(#2,#3)}
\newcommand{\mjs}{M^{(j^*)}}
\newcommand{\biasj}{\mathrm{bias}_j}
\newcommand{\biasjs}{\mathrm{bias}_{j^*}}




\subsection{The Bias of $\mathbb{F}_p$ Polynomials for Odd Prime $p$}
\label{sec:complex-quadratic}

Let $d\ge 1$ be an integer and $p$ be an odd prime.
For any integer $\sampledim\ge d$, consider the learning
problem for $\mathbb{F}_p$ polynomials in $\sampledim$ variables of degree at most $d$.
Unlike the case over $\mathbb{F}_2$, the monomials are not necessarily
multilinear but can have degree at most $p-1$ in each variable.
Let $\monomial{p}{d}{\sampledim}$ be the set of monomials in $\sampledim$ variables of
total degree at most $d$ and degree at most $p-1$ in each
variable.
That is $A=\mathbb{F}_p^\sampledim$ and, expressing polynomials by their coefficients,
we have $X=\mathbb{F}_p^\hypothesisdim$ where $\hypothesisdim=|\monomial{p}{d}{\sampledim}|$ is the number of
monomials of total degree at most $d$ and degree at most $p-1$ in each
variable.
As in the case of $\mathbb{F}_2$, $\hypothesisdim$ is the dimension of a Reed-Muller code
$RM_p(d,\sampledim)$ over $\mathbb{F}_p$, 
and for $a\in A$ and $x\in X$,
$\fun (a,x)=x(a)\in \mathbb{F}_p$.
For $d\ge p$ there is no convenient closed form known for $|\monomial{p}{d}{\sampledim}|$
but the following is known:

\begin{proposition}
For $d<p$, $|\monomial{p}{d}{\sampledim}|=\binom{\sampledim+d}{d}$ and
for $2<p\le d\le \sampledim$,
$\sum_{i=0}^d \binom{\sampledim}{d}\le |\monomial{p}{d}{\sampledim}|\le \binom{\sampledim+d}{d}$.
\end{proposition}

\begin{sloppypar}
Since $p>2$, the learning problem for $\mathbb{F}_p$ polynomials is governed
by $p-1$ complex matrices $M^{(1)},\ldots,M^{(p-1)}$ where
$\mj(a,x)=\omega^{j\cdot x(a)}$ and $\omega=e^{2\pi i/p}$.   
We need to bound the norm amplification curves of all these matrices.
We will relate these curves to the values of $\biasj(x)$ for $j\in \coltset{1,\ldots,p-1}$ and $x\in X$, where
$$\biasj(x)=\E_{a\in_R A} \omega^{j\cdot x(a)}.$$
\end{sloppypar}

\begin{sloppypar}
Fix an arbitrary $j^*\in \coltset{1,\ldots,p-1}$,
For $N=(\mjs)^*\cdot \mjs$, the $(x,y)$ entry of $N$ is $p^\sampledim \langle \mjs_x,\mjs_y\rangle$
where $\langle \cdot,\cdot\rangle$ is the complex inner product.
\end{sloppypar}

\begin{proposition}
\label{c-prop:equalrows}
Let $\mathbf{0}=0^\hypothesisdim$.
Then for $x,y\in X$,
$\langle \mjs_x,\mjs_y\rangle=\langle \mjs_{\mathbf{0}},\mjs_{y-x}\rangle$.
\end{proposition}

\begin{proof}
\begin{align*}
\langle \mjs_x,\mjs_y\rangle&=\E_{a\in \mathbb{F}_p^\sampledim} \overline {\mjs_x(a)}\mjs_y(a)
=\E_{a\in \mathbb{F}_p^\sampledim} \omega^{-j^*\cdot x(a)}\omega^{j^*\cdot y(a)}
=\E_{a\in \mathbb{F}_p^\sampledim} \omega^{-j^*\cdot x(a)+j^*\cdot y(a)}\\
&=\E_{a\in \mathbb{F}_p^\sampledim} \omega^{j^*(y-x)(a)}
=\E_{a\in \mathbb{F}_p^\sampledim} \mjs_{\mathbf{0}}(a)\mjs_{y-x}(a)
=\langle \mjs_{\mathbf{0}},\mjs_{y-x}\rangle
\end{align*}
\end{proof}

Since the mapping $y\mapsto y-x$ for $x\in \mathbb{F}_p^\hypothesisdim$ is
1-1 on $\mathbb{F}_p^\hypothesisdim$, 
every row of $N_x$ for $x\in X$ contains the same multi-set of values.
Therefore, in order to analyze the function $\tilde W_\kappa(N)$,
we only need to examine the fixed row $N_\mathbf{0}$ of $N$.
where each entry
$$N_{\mathbf{0}x}=\sum_{a\in \mathbb{F}_p^\sampledim}\omega^{j^*\cdot x(a)}=p^\sampledim\cdot \biasjs(x).$$
Therefore we have shown the following:

\begin{lemma}
\label{lem:matrix-bias}
Let $j^*\in \coltset{1,\ldots,p-1}$.  For every $v\in \mathbb{C}$, the number of
entries in each row of $N=(\mjs)^*\cdot \mjs$ equal to $v$ is precisely
the number of polynomials $x\in X$ such that $p^\sampledim\cdot \biasjs(x)=v$. 
\end{lemma}

Therefore, to bound $\tilde W_\kappa(N)$ it suffices to bound the numbers of
polynomials $x\in X$ such that $|\biasjs(x)|$ is large.

\paragraph{Affine Functions over $\mathbb{F}_p$}

For $d=1$, an $x\in X=\mathbb{F}_p^{\sampledim+1}$ yields the function
$x(a)=x_0+\sum_{i=1}^\sampledim x_i a_i$.  Unless $x_1=\cdots=x_\sampledim=0$, for every 
$k\in \mathbb{F}_p$ we have exactly $p^{\sampledim-1}$ values $a\in \mathbb{F}_p^\sampledim$
for which $x(a)=k$ and hence $\biasjs(x)=0$.
For each of the remaining $p$ inputs with $x_1=\cdots=x_\sampledim=0$
and different values for $x_0$, we get $\biasjs(x)=\omega^{j^*\cdot x_0}$
and hence $|\biasjs(x)|=1$.
In this case we choose $\kappa=0$ and observe that $\tilde W_0(N)=p^{\sampledim+1}$.
Therefore for any $\delta$ with $0\le \delta\le 1$, we have
$$OPT_{\mjs,\delta}\le p^{\sampledim+1} |X|^{\delta-1}/|A|=p^{1+(\delta-1)(\sampledim+1)}=(p^\sampledim)^{-(1-\delta)+\delta/\sampledim},$$
so $\tau_{\mjs}(\delta)=\frac12 \log_{|A|} OPT_{\mjs,\delta}\le -\frac{1-\delta}2+\frac{\delta}{2\sampledim}$.
(Note that if we only took linear functions instead of affine functions
all non-zero $x$ would be balanced and the term $\frac{\delta}{2\sampledim}$ would not
appear. This is the analog of the parity learning bound for higher moduli.)

\paragraph{Quadratic Polynomials over $\mathbb{F}_p$}


\begin{lemma}\label{main_counting_quadratic_complex}
Let $p$ be an odd prime and $\sampledim\ge 2$ be an integer.
Let $X$ be the set of quadratic polynomials over $A=\mathbb{F}_p^\sampledim$.
Then for $j^*\in \coltset{1,\ldots,p-1}$,
\begin{enumerate}
\item For any $x\in X$,  $\biasjs(x)=0$ or $|\biasjs(x)|\in \coltset{p^{-\sampledim/2},p^{(\sampledim-1)/2},\cdots, p^{-1/2},1}$.
\item For $0\le k\le \sampledim$ the number of $x\in X$ such that
$|\biasjs(x)|=p^{-k/2}$ is less than $p^{k\sampledim+2k+1}$.
\end{enumerate}
\end{lemma}

To prove Lemma~\ref{main_counting_quadratic_complex} we start with the
following structure lemma for quadratic polynomials
over fields of odd characteristic.  This lemma is an easier analog of
Dickson's Lemma for characteristic 2~\citep{dickson:book} and is well
known but we include a proof for completeness.

\begin{lemma}\label{lem:structure-quadratic}
Let $p$ be an odd prime and integer $t\ge 1$.
For every quadratic polynomial $q$ over $\mathbb{F}_{p^t}$ 
in variables $z=(z_1,\ldots, z_\sampledim)$,
there is an invertible affine transformation $T$ over $\mathbb{F}_{p^t}$ such
that for $z'=T(z)$,
there is a unique $k\le \sampledim$, and $(c_1,\ldots,c_k)\in \coltset{1,\cdots,p-1}^k$,
and an affine form $\ell$ over $\mathbb{F}_{p^t}$ 
in $\sampledim-k$ variables such that:
\begin{align*}
q(z)&=\sum_{i=1}^k c_i{z'_i}^2+\ell(z'_{k+1},\cdots,z'_\sampledim)
\end{align*}
\end{lemma}

\begin{proof}
We show this by induction on $\sampledim$. The statement is clearly true when $\sampledim=0$.
Assume that this is true for any polynomial in $\sampledim-1$ variables.
We have several cases when $q$ has $\sampledim$ variables:\\
{\sc Case 1:} $q$ is affine: Then the statement is true with $k=0$.\\
{\sc Case 2:} $q$ contains some square term $b_i\cdot z_i^2$:
In this case we can write $q$ as
$b_i\cdot z_i^2+ \ell_i\cdot z_i+q'$,
where $\ell_i$ is affine, $q'$ is a quadratic polynomial,
and neither of them involves $z_i$.
Then we can define
$$z_i'=z_i+2^{-1}b_i^{-1}\cdot \ell_i$$
since $b_i^{-1}$ and $2^{-1}$ are defined in
field $\mathbb{F}_{p^t}$ because $b_i\ne 0$ and the characteristic $p$ is odd.
Also define $q''=q'-2^{-2}b_i^{-1}\ell_i^2$. 
Thus 
\begin{align*}
&b_i(z_i')^2+q''\\
&=b_i(z_i')^2+q'-2^{-2}b_i^{-1}\ell_i^2\\
&=b_i(z_i +2^{-1}b_i^{-1}\cdot \ell_i)^2+q'-2^{-2} b_i^{-1}\ell_i^2\\
&=b_i(z_i^2 + b_i^{-1}\ell_i \cdot z_i + 2^{-2}b_i^{-2}\ell_i^2)
+q'-2^{-2} b_i^{-1}\ell_i^2\\
&=b_i\cdot z_i^2 + \ell_i \cdot z_i +q' =q.
\end{align*}
Define $T_i$ to be the map which sets $z_j'=z_j$ for
$j\ne i$ and replaces $z_i$ with $z_i'$ according to the above
formula.
Clearly by the definition of $z_i'$, $T_i$ is an affine map; moreover,
it is invertible, with $T_i^{-1}$ setting $z_i=z_i' -2^{-1}b_i^{-1}\cdot \ell_i$
and leaving all other $z_j$ for $j\ne i$ unchanged.
By definition, $q''$ is a quadratic form defined only on the $m-1$ variables
$z_1,\ldots,z_{i-1},z_{i+1},\ldots,z_m$, a property inherited from $q'$ and
$\ell_i$.  Let $P_{i\sampledim}$ be the permutation that swaps positions $i$ and
$\sampledim$ and leaves the rest alone and define $q'''=P_{i\sampledim}(q'')$.

We now can apply the inductive hypothesis to $q'''$ and derive that there
is an invertible affine mapping $T'$ on the $\sampledim-1$ variables (excluding $z_i$)
and some $k'$ together with constants $a'_{1},\ldots,a'_{k'}\in \mathbb{F}_p^*$,
yielding variables $z''_1,\ldots,z''_{\sampledim-1}$ as affine functions of the
previous values such that 
$$q'''=\sum_{j=1}^{k'} c_i{z_{j}''}^2+\ell''(z''_{k'+1},\ldots,z''_{\sampledim-1}).$$
We can extend $T'$ to an affine transformation $T''$ on $\sampledim$ variables 
by keeping the $\sampledim$-th variable unchanged.  

\begin{sloppypar}
Finally, define $k=k'+1$, $c_k=b_i$ and
the invertible affine transformation,
$T=P_{\sampledim k}\circ T'' \circ P_{i\sampledim}\circ T_i$ where
$P_{\sampledim k}$ is the permutation that swaps positions $k$ and $\sampledim$.  
Then $T(z)=(z''_1,\ldots,z''_{k-1},z'_i,z''_{k+1},\ldots,z''_{\sampledim-1},z''_{k})$.
$$T(q)=\sum_{j=1}^{k-1} c_i{z_{j}''}^2+c_k(z_i')^2+\ell''(z''_{k+1},\ldots,z''_{\sampledim-1},z''_k)$$
which is of the required form.
\end{sloppypar}

{\sc Case 3:} $q$ has no squared terms and is not affine.
Then $q$ must contain some cross term $b_{ij}\cdot z_iz_j$ for $i\ne j$.
Here we can use the identity
\[
z_iz_j=2^{-2}\cdot ((z_i+z_j)^2-(z_i-z_j)^2)
\]
and let $S_{ij}$ be the affine mapping that leaves all other variables unchanged
and assigns $z_i'=2^{-1}(z_i+z_j)$ and
$z_j'=2^{-1}(z_i-z_j)$ which exists since $2$ is invertible over $\mathbb{F}_{p^t}$.
$S_{ij}$ is clearly invertible since $z_i=z'_i+z'_j$ and $z_j=z'_i-z'_j$.
Hency, for $z'=S_{ij}(z)$, we have $q(z)=q_{ij}(z')$ for some quadratic
$q_{ij}$ that
has two squared terms $(z'_i)^2$ and $(z'_j)^2$ and hence is covered by Case 2
above.
Let $T_2$ be the resulting affine transformation derived for $q_{ij}$.
It follows that $T=T_2\circ S_{ij}$ is the required transformation for $q$.
\end{proof}

Lemma~\ref{lem:structure-quadratic} provides a clean way of studying the bias
of quadratic polynomials.
For any invertible affine mapping $T$ on $\mathbb{F}_p^\sampledim$, for $y\in X$
and $x(z)=y(T(z))$, we have $x\in X$ and
\begin{align*}
\biasjs(x)=\E_{a\in \mathbb{F}_p^{\sampledim}} \omega^{j^*\cdot x(a)}=\E_{a\in \mathbb{F}_p^{\sampledim}} \omega^{j^*\cdot y(T(a))}=\E_{b\in \mathbb{F}_p^\sampledim}\omega^{j^* y(b)}=\biasjs(y)
\end{align*}
since $T$ is a bijection on $\mathbb{F}_p^\sampledim$.

We therefore first analyze the polynomials of the normal form in
Lemma~\ref{lem:structure-quadratic}.
Let $y(z)=\sum_{i=1}^k c_iz_i^2+\ell(z_{k+1},\ldots, z_{\sampledim})$ where
each $c_1,\ldots,c_k \ne 0$.
Write $\ell=c_0+\sum_{i=k+1}^\sampledim c_iz_i$.
If there is any $j$ with $k+1\le j\le \sampledim$ such that $c_j\ne 0$ then
then $\biasjs(y)=0$ just as in the affine case.
Therefore it remains to consider 
\begin{equation}
y(z)=\sum_{i=1}^k c_iz_i^2+c_0\quad\mbox{ for }c_1,\ldots,c_k\in \F_p^*,\ c_0\in \F_p. \label{y-form}
\end{equation}
Observe that the number of such $y$ is $(p-1)^k p< p^{k+1}$.
Furthermore,
\begin{equation*}
p^\sampledim\cdot |\biasjs(y)|=|\sum_{a\in \F_p^\sampledim}\omega^{\,j^*\cdot \sum_{i=1}^k c_ia_i^2+c_0}|
=p^{\sampledim-k}\cdot\prod_{i=1}^k|\sum_{a_i=0}^{p-1}\omega^{j^*\cdot c_ia_i^2}|
\end{equation*}
The term $\sum_{a_i=0}^{p-1}\omega^{j^*\cdot c_ia_i^2}$ in the product
is called a \emph{quadratic Gauss sum} and has been studied previously.
For our purpose, we need the following result:

\begin{sloppypar}
\begin{proposition}[Proposition 6.3.2 in \cite{ireland2013classical}]
Let $p$ be an odd prime.
For $c\in \{1,\cdots,p-1\}$,
\[
|\sum_{j=0}^{p-1}\omega^{\,cj^2}|=\sqrt{p}.
\]
\end{proposition}
\end{sloppypar}

Therefore setting $c=c_i\cdot j^*$ for the $i$-th term, we have $|\biasjs(y)|=p^{-k/2}$.  We now put things together to
prove Lemma~\ref{main_counting_quadratic_complex}.
\medskip

\begin{proof}{\bfseries\upshape of Lemma~\ref{main_counting_quadratic_complex}\ }
By Lemma~\ref{lem:structure-quadratic}, since $\biasjs$ is preserved under
invertible linear transformations $T$ of the inputs, 
it follows that every polynomial $x$ such that $\biasjs(x)\ne 0$ must have
$|\biasjs(x)|=p^{-k/2}$ for some non-negative integer $k\le \sampledim$.  
Moreover, the number of polynomials $x$ whose normal form $y$ of the form
(\ref{y-form})
is at most
the number of affine transformations that define $z'_1,\ldots,z'_k$ in
terms of $z_1,\ldots,z_k$ which is $(p^{\sampledim+1})^k$ since there are precisely
$p^{\sampledim+1}$ affine functions on $\mathbb{F}_p^\sampledim$. 
Therefore the total number of $x$ such that $\biasjs(x)=p^{-k/2}$ is less than 
$p^{(\sampledim+1))k}\cdot p^{k+1}=p^{\sampledim k+2k+1}$.
\end{proof}

Now we can use Proposition \ref{main_counting_quadratic_complex} to prove Theorem \ref{thm:quadcurve-complex}.
\medskip

\begin{proof}{\bfseries\upshape of Theorem \ref{thm:quadcurve-complex}\ }
Proposition \ref{main_counting_quadratic_complex} implies that
for $N=(\mjs)*\cdot \mjs$,
\[
W_{p^{\sampledim-k/2}}(N)\leq \tilde W_{p^{\sampledim-k/2}}(N)\leq \sum_{t=0}^{k-1}p^{\sampledim-t/2}\cdot p^{t\sampledim+2t+1}=p^{\sampledim+1}\cdot \frac{p^{k\sampledim+3k/2}-1}{p^{\sampledim+3/2}-1}\leq p^{k\sampledim+3k/2}
\]
Therefore if we set $k=\lfloor \frac {1-\delta} 2 \sampledim\rfloor\geq \frac {1-\delta} 2 \sampledim-1$,
then by Lemma~\ref{c-SDP-lemma}, since $|X|=p^{\binom{\sampledim+2}2}$
we have
\[
OPT_{\mjs,\delta}\leq p^{-k}+p^{-\sampledim}\cdot p^{k\sampledim+3k/2+(\delta-1) \binom{\sampledim+2}2}\leq 2p^{-k}\leq p^{-\frac {1-\delta} 2 \sampledim+2}
\]
Therefore, 
\[
\tau_{\mjs}(\delta)=\frac 1 2\log_{p^\sampledim}OPT_{\mjs,\delta}\leq \frac {1-\delta}4+\frac 1 \sampledim
\]
Since $j^*$ was an arbitrary fixed element of $\coltset{1,\ldots,p-1}$, the 
theorem follows.
\end{proof}



\paragraph{Polynomials of degree $d>2$ over $\mathbb{F}_p$}
Similar to the $\mathbb{F}_2$ case,
we need to understand
the weight distribution of Reed-Muller codes over $\mathbb{F}_p$.
Very recently,
\cite{bogy:reedmuller-bias} gave the following estimate,
which is already enough for our purposes.

\begin{proposition}
\label{prop:complex-bias}
For $0<\varepsilon<1/2$,
for all $j\in \mathbb{F}_p^*$,
there are constants $c_1, c_2$ depending on $\varepsilon$ with $0<c_1,c_2<1$ such that if $f$ is a uniformly random degree $d$ polynomial over
$\mathbb{F}^\sampledim_p$ and $d\le \varepsilon \sampledim$ then 
$$\Pr[|\biasjs(f)|>p^{-c_1 \sampledim/d}]\le p^{-c_2 \hypothesisdim}.$$
\end{proposition}

From this form we can obtain the bound on the norm amplification curve
of the associated matrix fairly directly.
\medskip

\begin{proof}{\bfseries\upshape of Theorem~\ref{thm:F2curve-complex}\ }
Fix $\varepsilon>0$, $j\in \mathbb{F}_P^*$,
and let $0<c_1, c_2<1$ be the constants depending on
$\varepsilon$ from Proposition~\ref{prop:behl-bias}.
Let $\delta=c_2/2$ so $0<\delta<1/2$.
For $N=(\mjs)^*\cdot \mjs$,
when we set $\kappa=p^{(1-c_1/d)\sampledim}$,  Proposition~\ref{prop:complex-bias} implies that at
most $p^{(1-c_2)\hypothesisdim}$ 
polynomials $f$ satisfy $|N_{0f}|>\kappa$.
The norm of each entry is at most $p^\sampledim$ so 
$\tilde W_\kappa(N)\le p^\sampledim\cdot p^{(1-c_2)\hypothesisdim}$.
by Lemma~\ref{c-SDP-lemma} with 
$X=\mathbb{F}_p^\hypothesisdim$ we have
$$OPT_{M,\delta}\le (\kappa+W_\kappa(N)\cdot p^{(\delta-1)\hypothesisdim})/p^\sampledim
\le p^{-c_1 \sampledim/d}+p^{(\delta - c_2)\hypothesisdim+1}\le p^{-c_1 \sampledim/d}+p^{1-\delta \hypothesisdim}$$
which is at most $p^{-c' \sampledim/d}$ for some constant $c'>0$. 
Hence $\tau_{\mjs}(\delta)\le -c'/d$.
\end{proof}

