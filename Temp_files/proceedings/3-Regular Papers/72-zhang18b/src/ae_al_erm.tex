\documentclass[final,12pt]{colt2018} % Anonymized submission
%\usepackage{fullpage}
%\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[usenames,dvipsnames]{pstricks}
%\usepackage{epsfig}
\usepackage{pst-grad} % For gradients
\usepackage{pst-plot} % For axes
\usepackage{commath}
\usepackage{booktabs}

%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}{Lemma}
%\newtheorem{corollary}{Corollary}
\newtheorem{assumption}{Assumption}
\newtheorem{claim}{Claim}
%\newtheorem{definition}{Definition}

\renewcommand\algorithmicrequire{\textbf{input:}}
\renewcommand\algorithmicensure{\textbf{output:}}
\def\calF{\mathcal{F}}
\def\calG{\mathcal{G}}
\def\R{\mathbb{R}}
\def\E{\mathbb{E}}
\def\P{\mathbb{P}}
\def\calX{\mathcal{X}}
\def\calY{\mathcal{Y}}
\def\calH{\mathcal{H}}
\def\calO{\mathcal{O}}
\def\calA{\mathcal{A}}
\def\calW{\mathcal{W}}
\def\calZ{\mathcal{Z}}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\B}{B}
\DeclareMathOperator{\HT}{P}
\DeclareMathOperator{\Alt}{Alt}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\err}{err}
\DeclareMathOperator{\polylog}{polylog}
\DeclareMathOperator{\poly}{poly}
\def\Explore{\textsc{Explore}}
\def\Refine{\textsc{Refine}}

\def\cf{C_3}
\def\ct{C_5}
\def\ce{C_4}
\def\cs{C_6}
\def\cse{C_7}
\def\cfo{C_8}
\def\cn{C_9}

\title[Efficient active learning of sparse halfspaces]{Efficient active learning of sparse halfspaces}
\usepackage{times}

\coltauthor{\Name{Chicheng Zhang} \Email{chicheng.zhang@microsoft.com}
\\
\addr Microsoft Research \\
641 6th Avenue \\
New York, NY, 10011 \\
USA
}

\begin{document}

\maketitle

\begin{abstract}
We study the problem of efficient PAC active learning of homogeneous linear classifiers (halfspaces) in $\R^d$, where the goal is to learn
a halfspace with low error using as few label queries as possible.
Under the extra assumption that there is a $t$-sparse halfspace that
performs well on the data ($t \ll d$),
we would like our active learning algorithm to be {\em attribute efficient}, i.e. to have label requirements sublinear in $d$.
In this paper, we provide a computationally efficient algorithm that achieves this goal.
Under certain distributional assumptions on the data, our algorithm achieves a label complexity of $O(t \cdot \polylog(d, \frac 1 \epsilon))$.
In contrast, existing algorithms in this setting are either computationally inefficient, or subject to label requirements
polynomial in $d$ or $\frac 1 \epsilon$.
\end{abstract}
%In this setting, existing algorithms either achieve

%that the unlabeled distribution is log-concave, and the distribution satisfies

%While the non-sparse version of this problem is well studied, only a few works have made progress in
%this setting.
\input{intro}
\input{literature}
\input{notation}
\input{algorithm}
\input{guarantees}
\input{conclusions}

\section*{Acknowledgments}
I am grateful to Daniel Hsu for suggesting this research direction to me, and many insightful discussions along this line. I would also like to thank Pranjal Awasthi, Jie Shen and Hongyang Zhang for helpful initial conversations about the results in this paper. I thank the anonymous COLT reviewers for their thoughtful comments. Special thanks to Yue Liu, who provided unconditional support throughout this research project.

%\bibliographystyle{plain}
\bibliography{alsearch}

 %Oftentimes we assume that
 %$u^*$ is sparse, that is, there are at most $k$ nonzero elements ($k \ll d$) in vector $u$.
\newpage
\appendix
\input{appendix}




\end{document}
