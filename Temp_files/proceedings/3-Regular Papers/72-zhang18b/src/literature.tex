\section{Related work}

%al
\paragraph{Attribute efficient active learning of halfspaces.}
There is a rich body of theoretical literature on active learning of general concept classes
in the PAC setting~\citep{D11, H14}. For the problem of active halfspace learning, sharp distribution-dependent label complexity results are known,
in terms of e.g. the splitting index~\citep{D05}, or the disagreement coefficient~\citep{H07}.
Direct applications of these results (without taking advantage of sparsity assumptions)
yield algorithms with label complexities at least $\Omega(d \ln \frac 1 \epsilon)$~\citep{KMT93}.
To make these algorithms attribute efficient, a natural modification is to consider concept class
$\calH_t$, the set of $t$-sparse linear classifiers.
It is well known that $\calH_t$ has VC dimension
$O(t \ln d)$. In conjunction with existing results in the active learning
literature, this observation immediately yields attribute efficient active
learning algorithms. For example, when the unlabeled distribution is isotropic log-concave,
an application of~\cite{ZC14}'s algorithm with $\calH_t$ yields a label complexity
of $O(t \ln d \ln \frac 1 \epsilon)$ in the $t$-sparse realizable setting, and gives
$O(t \ln d \cdot (\ln \frac 1 \epsilon+\frac{\nu^2}{\epsilon^2}))$ and
$O(\frac{t\ln d}{(1-2\eta)^2} \ln \frac 1 \epsilon)$
label complexities in the $t$-sparse $\nu$-adversarial noise and $t$-sparse $\eta$-bounded noise settings.\footnote{To see this, note that the $\phi(\cdot,\cdot)$ function
defined in~\cite{ZC14} with respect to $\calH_t$ can be bounded as: $\phi(r,\xi) \leq O(r \ln \frac{r}{\xi})$, as $\calH_t$ is a subset of $\calH$. Theorem 4 of \cite{ZC14} now applies.}
However, these algorithms require solving
empirical 0-1 loss minimization subject to sparsity constraints, which is computationally intractable in general~\citep{N95}.
The only attribute and computationally efficient PAC active learning algorithms we are aware of are in~\cite{ABHZ16}.  Specifically, under the $t$-sparse $\Omega(\epsilon)$-adversarial noise setting, \cite{ABHZ16} gives an efficient algorithm with label complexity $\tilde{O}(\frac{t}{\epsilon^2}\polylog(d,\frac 1 \epsilon))$. Under the $t$-sparse $\eta$-bounded noise setting, ~\cite{ABHZ16} gives an efficient algorithm with label complexity $\tilde{O}((\frac t \epsilon)^{O(\frac 1 {(1-2\eta)^2})})$.


%It is implicit in~\cite{D05} that the Splitting algorithm therein with input hypothesis
%class $\calH_t$ results in an inefficient algorithm
%with $\tilde O(t (\ln d + \ln \frac 1 \epsilon))$ label complexity in the $t$-sparse realizable setting.
%~\cite{H14} proposes an general splitting index based algorithm that


%efficient al
%For example, ~\cite{DKM05,BBZ07} achieves a label complexity of $O(d \ln \frac 1 \epsilon)$ in the realizable setting;
%~\cite{ABL17} achieves a label complexity of $\tilde{O}(d\ln \frac 1 \epsilon)$ in the $\nu$-adversarial noise setting, where $\nu = \Omega(\epsilon)$;
%~\cite{ABHU15} achieves a label complexity of $\tilde{O}(d\ln \frac 1 \epsilon)$ in the $\eta$-bounded noise setting, where $\eta$ is at most a constant.

%It combines the technique of margin-based active learning, polynomial regression~\cite{KKMS08} and $\ell_1/\ell_\infty$ Rademacher complexity bounds, and gives efficient algorithms with label complexity polynomial in $t$ and $\ln d$.

%On the other hand, many computationally efficient active halfspace learning algorithms have been proposed in the literature, with different degrees of noise tolerance~\cite{DKM05,BBZ07, ABL17, ABHU15, ABHZ16, YZ17}.
%However, all of the algorithms above have a label complexity at least $\Omega(d)$ and is thus not attribute efficient.

The notion of attribute efficient learning algorithms is initially studied in the pioneering works of~\cite{L87,B90}.
\cite{L87} considers attribute efficient online
learning of linear classifiers, with an application to learning disjunctions that depends on only $t$ attributes.
The algorithm incurs a mistake bound of $O(t \ln d)$, which can be of substantially lower order than $O(d)$ when $t$ is small.
\cite{B90} considers an online learning model where the feature space is infinite dimensional,
and each instance shown has a bounded number of nonzero attributes.
%In addition, there is an underlying concept that relies on
%only $t$ of the attributes.
It gives efficient algorithms that learn $k$-CNFs and disjunctions
with finite mistake bounds in this setting.
\cite{S00, KS06, STT12} study attribute efficient learning of decision lists and analyzes the
tradeoff between running time and mistake bound.
\cite{LS07} shows that, if the unlabeled distribution is unconcentrated over $\{-1,1\}^d$, then there
is an algorithm that learns $t$-sparse linear classifiers with a sample complexity of $\poly(t, \ln d, 2^{O(\epsilon^{-2})})$. \cite{F07} gives algorithms for attribute efficient learning parity and DNFs
in the membership query model.


%\cite{LS07} proposes an efficient algorithm that learns halfspaces
%over $t$ variables using $\poly(t, \ln d))$ samples to achieve a constant error.
%Winnow is attribute efficient, in the sense that
%when the instances and the classifiers are all $\ell_\infty$ bounded by a constant, and there is
%a $t$-sparse linear separator that separates the examples by a margin, then the mistake bound is
%$O(t \ln d)$, which only has a logarithmic dependence on the dimension.


\paragraph{One-bit compressed sensing.} The line of work on one-bit compressed sensing~\citep{BB08} is closely related to our problem setup. In this setting,
there is a unknown $t$-sparse vector $u \in \R^d$, and the algorithm can make measurements of $u$ using vectors $x \in \R^d$ and receives (possibly noisy) values of $\sign(u \cdot x)$.
Note that different from standard compressed sensing~\citep{CT06,D06}, the measurement results of one-bit compressed sensing are {\em quantized} versions of $(u \cdot x)$'s (i.e. they lie in $\{-1,+1\}$ as opposed to $\R$).
The goal is to approximately recover $u$ up to scaling with a few (ideally, $O(t \ln d)$) measurements.
 In the non-adaptive setting, the measurement vector
$x$'s are chosen at the beginning, while in the adaptive setting, the measurement vector $x$'s can be chosen sequentially,
based on past observations.
The problem of adaptive one-bit compressed sensing is therefore equivalent to attribute efficient
active halfspace learning in the membership query model~\citep{A88}.
We remark that active learning in the PAC model is more challenging than in the membership model, in that the learner has to query the labels of the unlabeled examples it has drawn.

%The crucial difference between the membership query model and the PAC model
%is that, in the PAC model, the , and is thus more challenging from the
%viewpoint of algorithm design.
%in the data
%stream literature
%This algorithm, in conjunction with
%a set of $O(\frac k \epsilon)$ nonadaptive Gaussian measurements, results in an nonadaptive 1-bit compressed sensing algorithm with
%$O(k (\ln d + \frac 1 \epsilon))$ measurements
%This, in conjunction with efficient noise tolerant learning algorithm
%working in the support of $w^*$, gives an algorithm that uses $O(k (\ln d + \frac{1}{(1-2\eta)^2 \epsilon})$ measurements.

~\cite{JLBB13} gives an algorithm that has robust recovery guarantees, however it is based on computationally-intractable $\ell_0$ minimization. Inspired by the count sketch data structure~\citep{CCF02}, ~\cite{HB11} proposes an efficient procedure that recovers the support of $u$ using $O(t \ln d)$ queries, and has strong noise tolerance properties. In conjunction with efficient full-dimensional active halfspace learning algorithms~\citep{DKM05,ABL17,CHK17,YZ17}, this procedure
 yields efficient algorithms that have label complexities of $O(t (\ln d + \ln \frac 1 \epsilon ))$
(resp. $O(t (\ln d + \ln \frac 1 \epsilon))$, $O(\frac{t}{(1-2\eta)^2} (\ln d + \ln \frac 1 \epsilon ))$) in the $t$-sparse realizable setting (resp. $t$-sparse $\Omega(\epsilon)$-adversarial noise setting, $t$-sparse $\eta$-bounded noise setting).
~\cite{GNJN13, ABK17} gives upper and lower bounds for {\em universal} one-bit compressed sensing, that is, the same set of measurements can be used to approximately recover {\em any} underlying $t$-sparse signal. In this setting,~\cite{ABK17} shows that, perhaps surprisingly, the number of measurements necessary and sufficient for support recovery is $\tilde{\Theta}(t^2 \ln d)$, as opposed to $\Theta(t \ln d)$ in the non-universal setting.
 ~\cite{PV13a} proposes a linear programming based algorithm that works in the $t$-sparse realizable setting, and has a measurement complexity of $\tilde{O}(\frac{t}{\epsilon^5})$,
based on a new tool named random hyperplane tessellations. ~\cite{L16} gives a support recovery algorithm that tolerates bounded noise, %double check this result - why don't they have a 1-2eta dependency?
using $\alpha$-stable random projections.
\cite{PV13b} proposes a convex programming based algorithm that works in the $t$-sparse $\Omega(\epsilon^2)$-adversarial noise model,
and has a measurement complexity of $\tilde{O}(\frac{t}{\epsilon^{12}})$.


Works on one-bit compressed sensing under the symmetric noise condition has been studied in the literature~\citep{PV13b, ZYJ14, CB15, ZG15}. In this model, it is assumed that there is a known function $g$, such that for all $x$, $\E[y|x] = g(u \cdot x)$. This assumption captures some realistic scenarios, but is nevertheless strong: it requires any two examples that have the same projection on $u$ to have the same conditional label distribution. In contrast, the $t$-sparse adversarial noise and the $t$-sparse bounded noise conditions allow heterogeneous noise levels, even among examples that have the same projection on $u$.
In this setting, the state of the art result of \cite{ZYJ14} gives an nonadaptive algorithm with $O(\frac{t \ln d}{ \epsilon^2})$. It also proposes an adaptive algorithm that works in same setting, achieving a label complexity bound of $O(\min(\frac{t \ln d}{\epsilon^2}, \frac{t\sqrt{d} \ln d}{\epsilon}))$, which is sometimes lower than that of the nonadaptive algorithm.
The special case of Gaussian noise before quantization has been studied extensively, i.e. given $x$, the label $y$ is generated by the formula $y = \sign(u \cdot x + n)$, where $n$ is a Gaussian random variable. \cite{GNR10} shows that when $u$ has a large dynamic range (the absolute value of the ratio between $u$'s largest and smallest nonzero elements in magnitude), adaptive approaches require fewer measurements to identify the support of $u$ than nonadaptive approaches.

%The most popular noise model is
%the generalized linear model, i.e. . The analysis crucially relies on the parameter $\lambda:=\E_{x \sim N(0,1)}[x g(x)]$; for example, $\lambda = \sqrt{\frac 2 \pi}(1 - 2\eta)$ when every label is flipped with probability $\eta$.
%\cite{PV13b} proposes a convex programming based algorithm that has a measurement complexity of $O(\frac{t \ln d}{\lambda^2 \epsilon^4})$. \cite{ZYJ14} gives a one-pass algorithm that has a measurement complexity of $O(\frac{t \ln d}{\lambda^2 \epsilon^2})$; see also \cite{ZG15} for refinements of logarithmic factors. \cite{CB15, ALPV14} studies
%subgaussian measurements; the recovery error bounds has an non-diminishing term under this general setting.
%depending on the Gaussianity of the measurements.
% whereas it tends to zero if the measurement distribution is sufficiently close to Gaussian.

%In classical compressed sensing, adaptive measurements have proven useful to reduce the measurement requirement~\cite{HBCN12}, or to reduce the signal to noise ratio requirement~\cite{MN14}. Similar results lie in 1-bit compressed sensing. ; \cite{ZYJ14} gives an adaptive algorithm that works in the parametric noise model, giving a measurement complexity of $O(\min(\frac{t \ln d}{\lambda^2\epsilon^2}, \frac{s\sqrt{d} \ln d}{\lambda^2\epsilon}))$, improving over the $O(\frac{t \ln d}{\lambda^2 \epsilon^2})$ bound in the nonadaptive setting.

%Attribute efficient active learning in the membership query model can be seen as an instance of adaptive one-bit compressed sensing.


%\cite{SSSHZ14} generalizes the analysis of \cite{L87}, and provide sample complexity bounds
%of order $\frac{t \ln d}{\epsilon^2}$ for ERM based algorithms.


%It proposes the Winnow algorithm, that achieves a mistake bound of $O(t \ln d)$ when the data is
%separable by a $t$-disjunction.
%the mistake bound has only a logarithmic dependence on the dimension of the data.




%achieving a mistake bound of
%$O(\frac{\|w^*\|_1^2 X^2 \ln d}{\gamma^2})$, under the assumption that the data is $\ell_\infty$ bounded by $X$,
%and is separable by $w^*$ with margin $\gamma$. Winnow is attribute efficient, in the sense
%that if $w^*$ is $t$-sparse and


% A straightforward application of~\cite{D05} yields an computationally inefficient algorithm that achieve a label
%complexity of $O(t (\ln d + \ln \frac 1 \epsilon))$ in this setting. On the other hand, if computational efficiency is
%required, the best known label complexity bounds are much worse, for instance, $O(\frac t {\epsilon^2})$~\cite{ABHZ16} and
%$O(d \ln \frac 1 \epsilon)$~\cite{DKM05, BBZ07}. As we will see in this note, we develop a computationally efficient algorithm that has a near-optimal label complexity
%bound of $O( t (\ln d)^3 \ln \frac 1 \epsilon )$, advancing the state of the art.
We provide a detailed comparison between our work and the results most closely related to ours in Tables~\ref{tab:comp-r}, \ref{tab:comp-an}, and \ref{tab:comp-bn}.



\begin{table}[t]
\centering
\begin{tabular}{llll}
\toprule
Algorithm & Model & Label complexity & Efficient? \\
\midrule
\begin{tabular}{@{}c@{}}\cite{HB11}\\ with \cite{DKM05} \end{tabular} & MQ & $\tilde{O}(t (\ln d + \ln \frac 1 \epsilon))$ & Yes \\
\cite{D05} & PAC & $\tilde{O}(t (\ln d + \ln \frac 1 \epsilon))$ & No \\
\cite{ABHZ16} & PAC & $\tilde{O}(\frac{t}{\epsilon^2} \polylog(d,\frac 1 \epsilon) )$ & Yes \\
Our work & PAC & $\tilde{O}(t \polylog(d,\frac 1 \epsilon) )$ & Yes \\
\bottomrule
\end{tabular}
\caption{A comparison of algorithms for active learning of halfspaces in the $t$-sparse realizable setting (Definition~\ref{def:r}); all the PAC algorithms above work under isotropic log-concave distributions.}
\label{tab:comp-r}
\end{table}


\begin{table}[t]
\centering
\begin{tabular}{lllll}
\toprule
Algorithm & Model & Noise tolerance  & Label complexity & Efficient? \\
\midrule
\begin{tabular}{@{}c@{}}\cite{HB11}\\ with \cite{ABL17} \end{tabular}& MQ & $\nu = \Omega(\epsilon)$ & $\tilde{O}(t (\ln d + \ln \frac 1 \epsilon))$ & Yes \\
\cite{ZC14} & PAC & $\nu = \Omega(\epsilon)$ & $\tilde{O}(t \ln d \ln \frac 1 \epsilon))$ & No \\
\cite{PV13b} & PAC  & $\nu = \Omega(\epsilon^2)$ & $\tilde{O}(\frac{t\ln d}{\epsilon^{12}})$ & Yes \\
\cite{ABHZ16} & PAC & $\nu = \Omega(\epsilon)$ & $\tilde{O}(\frac{t}{\epsilon^2} \polylog(d,\frac 1 \epsilon) )$ & Yes \\
Our work & PAC & $\nu = \Omega(\epsilon)$ & $\tilde{O}(t \polylog(d,\frac 1 \epsilon) )$ & Yes \\
\bottomrule
\end{tabular}
\caption{A comparison of algorithms for active learning of halfspaces in the $t$-sparse $\nu$-adversarial noise setting (Definition~\ref{def:an});  all the PAC algorithms above work under isotropic log-concave distributions.}
\label{tab:comp-an}
\end{table}

\begin{table}[t]
\centering
\begin{tabular}{lllll}
\toprule
Algorithm & Model & Noise tolerance & Label complexity & Efficient? \\
\midrule
\begin{tabular}{@{}c@{}}\cite{HB11}\\ with \cite{CHK17} \end{tabular} & MQ & $\eta \in [0,\frac 1 2)$ & $\tilde{O}(\frac{t}{(1-2\eta)^2} (\ln d + \ln \frac 1 \epsilon))$ & Yes \\
\cite{ZC14} & PAC & $\eta \in [0,\frac 1 2)$ & $\tilde{O}(\frac{t}{(1-2\eta)^2} \ln d \ln \frac 1 \epsilon))$ & No \\
\cite{ABHZ16} & PAC & $\eta \in [0,\frac 1 2)$ & $\tilde{O}((\frac{t}{\epsilon})^{O(\frac{1}{(1-2\eta)^2})} )$ & Yes \\
Our work & PAC & $\eta \in [0, \Omega(1))$ & $\tilde{O}(t \polylog(d,\frac 1 \epsilon) )$ & Yes \\
\bottomrule
\end{tabular}
\caption{A comparison of algorithms for active learning of halfspaces in the $t$-sparse $\eta$-bounded noise setting (Definition~\ref{def:bn}); all the PAC algorithms above work under isotropic log-concave distributions.}
\label{tab:comp-bn}
\end{table}
