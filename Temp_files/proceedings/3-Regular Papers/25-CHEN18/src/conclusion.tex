\section{Conclusion}
We study the problem of incentivizing exploration with heterogeneous
user preferences.
We proposed an algorithm that achieves expected cumulative regret
$O(\ARMNUM \e^{2/\MinProb} + \ARMNUM \log^3(T))$,
using expected cumulative payments of $O(\ARMNUM^2 \e^{2/\MinProb})$.
It is possible to improve these bounds to polynomial (in \ARMNUM and
$1/\MinProb$) when \MinProb is known or the preference distribution is
discrete.
In fact, we conjecture that this should be possible even in the full
generality of our model.
As a first step towards such a polynomial bound, we can obtain an exponential dependence on
$1/(\MinProb \ARMNUM)$ by changing the probability threshold to be $\frac{1}{\ARMNUM\log(s)}$
\footnote{This will lead to a different dependence on $\ARMNUM$
in the regret bound as well as the payment bound}, which gives polynomial dependence unless some
arm has a much smaller fraction of the population preferring it.

Taking this goal one step further, we would like to 
develop algorithms that do not require all arms to be preferred by a
strictly positive fraction of agents.
An alternate algorithm might only incentivize an arm if its estimated
attribute vector is close enough to a Pareto frontier.
The regret will then be $\Omega(\log(T))$ when at least one arm falls
below the Pareto frontier, as we no longer have free exploration of
all arms. 
It is likely that a bound will deteriorate as the number of such
unpreferred arms increases.

Finally, it would be desirable to generalize to utility
functions beyond inner products.
We believe that similar results hold for arbitrary
Lipschitz-continuous utility functions of the arm's attribute vector,
and that only minor modifications are necessary to the algorithm and
proofs.
