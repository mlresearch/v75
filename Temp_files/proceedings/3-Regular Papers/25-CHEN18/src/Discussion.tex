\subsection{Tighter Bounds Under Additional Assumptions}
%: Discrete Preferences and Known \MinProb}

The proofs of Theorem~\ref{rst:budget} and Theorem~\ref{rst:regret}
incur exponential (in \ARMNUM) payment and regret in the initial
phases because the threshold $1/\log(s)$ required for incentivization
decreases slowly. 
This slow decrease is needed to bound the regret in the later phases
when the concentration inequality kicks in as in
Equation~(\ref{equ:small_regret_bound}).
In this section, we discuss two restrictions under which we can
modify the algorithm slightly and provide stronger bounds,
avoiding this exponential dependence.
Both problem settings are special cases of the more general
setting previously considered. 

\subsubsection{Discrete Preferences}
\label{subsec:discrete}
Discrete preferences by agents are captured by the following
strengthening of Assumption~\ref{A1},
which states that the agents who are close to tied between
two arms have measure 0:

\begin{assumption}[Discrete Preferences]
\label{a:discrete}
There is a positive $\hat{z}$ such that
$\AlmostTied{\hat{z}} = 0$.
\end{assumption}

When Assumption~\ref{a:discrete} holds,
we restrict the payment-eligibility criterion by only incentivizing
arms with much smaller probability to be pulled:
an arm $i$ is payment-eligible at time $t$ in phase $s$ when both of
the following are true: 

\begin{itemize}
\item $i$ has been pulled at most $s$ times up to time $t$,
i.e., $\NumPull{t}{i} \leq s$.
\item Based on the current estimates \ArmEV{t}{i'} of all arms' attribute vectors,
the probability of pulling arm $i$ is less than $1/s$
(compared to $1/\log(s)$ in the general algorithm).
\end{itemize}

We refer to this modified version of
Algorithm~\ref{alg:basic-incentivizing} as the
\emph{Discrete-Preference Algorithm.}
We outline a proof of the following result for this algorithm under
Assumption~\ref{a:discrete} in Appendix~\ref{sec:discussion-proof1}.

\begin{theorem}
\label{rst:discrete}
Under Assumption~\ref{a:discrete}, the Discrete-Preference Algorithm
has expected payment bounded above by 
$O \left(\ARMNUM^2/\MinProb \right)$
and expected regret bounded above by $O(\ARMNUM / \MinProb)$.
\end{theorem}

\subsubsection{Known \MinProb}
An alternative useful assumption is that \MinProb
(or a strictly positive lower bound on \MinProb) is known. 

\begin{assumption}[Known $\MinProb$]
\label{a:known-p}
A strictly positive lower bound on \MinProb is known in advance.
\end{assumption}

When this assumption holds, we choose a different modification in the
definition of payment eligibility.
Let $s_0 = \exp(2/\MinProb)$.
An arm $i$ is \emph{payment-eligible} at time $t$ (in phase $s$)
if both:
\begin{itemize}
\item $i$ has been pulled at most
$s$ times up to time $t$, i.e., $\NumPull{t}{i} \leq s$.
\item Based on the current estimates \ArmEV{t}{i'} of all arms' attribute vectors,
the probability of pulling arm $i$ is less than $1/\log(s+s_0)$.
\end{itemize}

This knowledge of \MinProb allows the algorithm to incentivize
significantly fewer arm pulls.
We refer to this modified version of
Algorithm~\ref{alg:basic-incentivizing} as the \emph{Known-\MinProb
Algorithm.}
We outline a proof of the following result for this algorithm under
Assumption~\ref{a:known-p} in Appendix~\ref{sec:discussion-proof2}.

\begin{theorem}
\label{rst:known-p}
Under Assumption~\ref{a:known-p}, the Known-\MinProb Algorithm has
expected payment  bounded above by  
$O(\ARMNUM^2 \cdot \max(1,(\TieDensity/\MinProb)^{5/2}))$ 
and expected regret bounded above by
%\begin{align*}
$O \left( \frac{\ARMNUM^2}{\MinProb^2}
%  + \frac{\ARMNUM^2}{\hat{z}^2}+\frac{\ARMNUM}{\hat{z}^3}
  +\frac{\ARMNUM \TieDensity \log^3(T)}{\MinProb}\right)$.
%  \end{align*}
\end{theorem}

