\section{Proof Sketch of Theorem~\ref{rst:discrete}}
\label{sec:discussion-proof1}

\begin{rtheorem}{Theorem}{\ref{rst:discrete}}
Under Assumption~\ref{a:discrete}, the Discrete-Preference Algorithm
has expected payment bounded above by 
$O \left(\ARMNUM^2/\MinProb \right)$
and expected regret bounded above by $O(\ARMNUM / \MinProb)$.
\end{rtheorem}

\begin{proof}
The proof of the expected payment bound is the same as the proof of
Theorem~\ref{rst:budget},
except that we now define
$h(\ell) := \max \left( \frac{2}{\MinProb},
\left( \frac{24 \sigma \ell \Diam d}{\TieCutoff} \right)^{5/2} \right)$.
%\pfedit{and $s_1 = \max(2,\frac{30\sigma^3}{x^3}, \frac2{\MinProb})$}.

To prove the bound on the expected regret,
one can first prove tightened versions of Lemmas~\ref{lem:no-incentives} and \ref{lem:numP},
which replace the $\exp(2/\MinProb)$ term with $2/\MinProb$ and use $L=0$.
In return, the length of phase $s$ is now bounded only by $\ARMNUM s$
instead of $\ARMNUM \log(s)$,
and the expected number of times steps in which a payment is made is bounded by
$O(\ARMNUM/\MinProb + \ARMNUM^2)$.
Substituting these changes into the proof of Theorem~\ref{rst:regret},
we obtain the bound $O(\MAXR(\ARMNUM/\MinProb + \ARMNUM^2) + \ARMNUM \MAXR +
\ARMNUM \MAXR) = O(\ARMNUM/\MinProb)$ since $\MinProb \le 1/\ARMNUM$.
\end{proof}

% \pfcomment{
% The calculation I did for the bound on the expected payment is,
% \begin{align*}
% &O \left(\ARMNUM^2 d \cdot (\MAXR + \Diam d \sigma) \cdot h(1) \right) \\ 
% &=O \left(\ARMNUM^2  d \cdot (\MAXR + \Diam d \sigma)\cdot
%     \max\bigg\{\frac{2}{p},
%     \left( \frac{\sigma \Diam d}{\TieCutoff} \right)^{5/2}
%     \bigg\}
% \right)
% \end{align*}
% }
% \pfcomment{
% The calculation I did for the bound on the regret is:

% \begin{itemize}
% \item regret from times when we pay is $RN(N + 1/p)$
% \item regret from phases with $\gamma(s) >= \hat{z}$ is $O(NR D^6 d^6 \sigma^6 / \hat{z}^6)$
% \item regret from pulls with small regret is 0
% \item regret from pulls with big regret is $O(NR)$
% \end{itemize}
% }

\section{Proof Sketch of Theorem~\ref{rst:known-p}}
\label{sec:discussion-proof2}


\begin{rtheorem}{Theorem}{\ref{rst:known-p}}
% Under Assumption~\ref{a:known-p}, the Known-\MinProb Algorithm has an expected payment bounded above by 
% $\pfedit{O\left(N^2 \max\{1, (L/p)^{5/2}\}\right)}$.
% and an expected regret bounded above by
% \bcedit{
% \begin{align*}
% O\left(\frac{\ARMNUM^2}{\MinProb^2}
%     + \frac{\ARMNUM^2}{\hat{z}^2} + \frac{\ARMNUM}{\hat{z}^3}
%     + \frac{NL\log^3(T)}{p}    
%     \right)
% \end{align*}
% }
Under Assumption~\ref{a:known-p}, the Known-\MinProb Algorithm has
expected payment bounded above by  
$O(\ARMNUM^2 \cdot \max(1,(\TieDensity/\MinProb)^{5/2}))$ 
and expected regret bounded above by
%\begin{align*}
$O \left( \frac{\ARMNUM^2}{\MinProb^2}
%  + \frac{\ARMNUM^2}{\hat{z}^2}+\frac{\ARMNUM}{\hat{z}^3}
  +\frac{\ARMNUM \TieDensity \log^3(T)}{\MinProb}\right)$.
%  \end{align*}
\end{rtheorem}

\begin{proof}
The proof of the expected payment bound follows Lemma~\ref{rst:budget},
except we define $h(\ell)$ without including the $\exp(2/\MinProb)$ term,
instead using
$h(\ell) := \max \left(
  \left(\frac{24\sigma \ell\Diam d}{\TieCutoff} \right)^{5/2},
  \left( \frac{48\sigma \ell\TieDensity \Diam d}{\MinProb} \right)^{5/2}
\right)$.
The resulting bound on the expected payment is
\begin{align*}
O\left( \ARMNUM^2 d \cdot (\MAXR+\Diam d\sigma) \cdot
  \max \left(
    \left(\frac{\sigma \Diam d}{\TieCutoff} \right)^{5/2},
    \left( \frac{\sigma \TieDensity \Diam d}{\MinProb} \right)^{5/2}
    \right) \right)
& = O\left(\ARMNUM^2 \max (1, (\TieDensity/\MinProb)^{5/2}) \right).
\end{align*}

The proof of the expected regret bound first establishes tightened versions of Lemmas~\ref{lem:no-incentives} and \ref{lem:numP},
proving the following upper bound on the number of time steps in which a payment is made:
    
\begin{align*}
O\left(\frac{\ARMNUM^2 \TieDensity^3 \Diam^3 d^3 \sigma^3}{\MinProb^2}
    + \frac{\ARMNUM^2 d^3 \sigma^2 \Diam^2}{\TieCutoff^2} + \frac{\ARMNUM \Diam^3 d^3 \sigma^3}{\TieCutoff^3}
  \right)
& = O \left( \frac{\ARMNUM^2}{\MinProb^2} \right).
\end{align*}

The proof of this result follows that of Lemma~\ref{lem:numP},
but the less aggressive incentivization allows us to define
$\EvenLaterPhase = \max(2, \frac{30 \sigma^3}{x^3})$
since $\frac{1}{\log(s+s_0)} \leq \frac{\MinProb}{2}$ is true for all $s$.

Using this tighter bound on the number of incentivizations,
and the fact that phases now last at most $\ARMNUM \log(s+s_0)$ steps in expectation,
we can bound the regret in Equation~\eqref{equ:small_regret_bound} by
%$71.11 \ARMNUM\Diam^2 d^2\sigma^2 \TieDensity\log(T)(\log(T)+1)\log(T+s_0)$
$O\left(\frac{\ARMNUM\Diam^2 d^2\sigma^2 \TieDensity \cdot (\log(T))^3}{\MinProb}\right)$.
\end{proof}
