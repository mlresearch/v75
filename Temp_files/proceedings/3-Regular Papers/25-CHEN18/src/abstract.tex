We consider the problem of incentivizing exploration with heterogeneous agents.
In this problem, \ARMNUM bandit arms provide vector-valued outcomes equal to an unknown
arm-specific attribute vector,
perturbed by independent noise.
Agents arrive sequentially and choose arms to pull based on their own
private and heterogeneous linear utility functions over attributes
and the estimates of the arms' attribute vectors derived from
observations of other agents' past pulls.
Agents are myopic and selfish and thus would choose the arm with
maximum estimated utility.
A principal, knowing only the distribution from which agents'
preferences are drawn, but not the specific draws,
can offer arm-specific incentive payments 
to encourage agents to explore underplayed arms.
The principal seeks to minimize the total expected cumulative regret
incurred by agents relative to their best arms,
while also making a small expected cumulative payment.

We propose an algorithm that incentivizes arms played
  infrequently in the past
whose probability of being played in the next round would be small
without incentives.
Under the assumption that each arm is preferred by at
least a fraction $\MinProb > 0$ of agents,
we show that this algorithm achieves expected
cumulative regret of $O (\ARMNUM \e^{2/\MinProb} + \ARMNUM \log^3(T))$,
using expected cumulative payments of $O(\ARMNUM^2 \e^{2/\MinProb})$.
If \MinProb is known or the distribution over agent
preferences is discrete,
the exponential term $\e^{2/\MinProb}$ can be replaced with suitable
polynomials in \ARMNUM and $1/\MinProb$.
For discrete preferences, the regret's dependence on $T$ can be
eliminated entirely, giving constant (depending only polynomially on
\ARMNUM and $1/\MinProb$) expected regret and payments.
This constant regret stands in contrast to the $\Theta(\log(T))$ dependence of
regret in standard multi-armed bandit problems.
It arises because even unobserved heterogeneity in agent preferences
causes exploitation of arms to also explore arms fully;
succinctly, heterogeneity provides free exploration.
