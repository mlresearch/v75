\section{Overview of Results and Discussion}

Our main algorithm is presented as
Algorithm~\ref{alg:basic-incentivizing}
in Section~\ref{sec:ub}.
Our main result is the following pair of theorems%
\footnote{Recall that we omit the dependence on parameters other than
  \ARMNUM, \MinProb, $T$ unless making a particular point.},
analyzing the payments and regret of the algorithm.

\begin{theorem} \label{rst:budget}
The expected total payment of
Algorithm~\ref{alg:basic-incentivizing} is at most
$O \left(\ARMNUM^2 \cdot \e^{2/\MinProb} \right)$.
\end{theorem}

\begin{theorem} \label{rst:regret}
For any time horizon $T$, the expected cumulative regret for
Algorithm~\ref{alg:basic-incentivizing} up to time $T$ is bounded
above by 
$O \left(\ARMNUM \cdot \e^{2/\MinProb} + \TieDensity \ARMNUM \log^3(T) \right)$.
\end{theorem}

When $\TieDensity = 0$, the bound of Theorem~\ref{rst:regret} is
constant in $T$;
thus, the algorithm achieves constant regret using constant
expected payments. 
As discussed in Section~\ref{sec:prob},
the case $\TieDensity = 0$ arises, for instance, for
discrete agent distributions with finite support.
In fact, when $\TieDensity = 0$, one can also modify the algorithm to
reduce the dependence on \MinProb from exponential to polynomial.
Theorem~\ref{rst:discrete} (given later) states that the modified
algorithm achieves expected regret
$O \left(\ARMNUM/\MinProb \right)$
with expected payments of
$O \left(\ARMNUM^2/\MinProb \right)$.
  
The fact that constant regret can be achieved with constant payment
(independent of $T$) when $\TieDensity = 0$ suggests aiming for a
constant bound more generally, i.e., for $\TieDensity > 0$.
That such a bound is unachievable is shown in Appendix~\ref{sec:lb},
where we show a lower bound of $\Omega(\log(T))$ on
the expected regret of any algorithm.
The instance is simple: it has two arms, one with known attributes; 
in addition, one draw from the other arm is observed in
each step $t$ even when it is not pulled.
While the probability of pulling the wrong arm decreases over time, it
does not do so fast enough, causing the stated regret.

The exponential dependence on $1/\MinProb$ implies an exponential
dependence on \ARMNUM (because $\MinProb \leq 1/\ARMNUM$).
This exponential dependence arises from a need to continue to
incentivize arm pulls to ensure that nearly tied agents learn their
best arms quickly.
Aside from the assumption that $\TieDensity = 0$,
another assumption allows us to eliminate this exponential dependence.
Namely, when \MinProb (or a lower bound on it) is known ahead of time,
the algorithm can be modified to incentivize arms less aggressively.
As shown in Theorem~\ref{rst:known-p},
the modified algorithm has expected regret at most
$O \left(\frac{\ARMNUM}{\MinProb^3}
+\frac{\ARMNUM \TieDensity \log^3(T)}{\MinProb} \right)$,
with expected payments of at most
$O \left(\frac{\ARMNUM^2}{\MinProb^{5/2}} \right)$.

We compare these bounds to those for standard bandits,
focusing on the dependence on $T$.  
The standard bandit setting is the case when the agent types \AgV
are concentrated on a single point, and agents pull arms at the
principal's direction without requiring payment.
(This setting violates our Assumption~\ref{A3},
so our bounds do not apply to it.)
Then, the payment is $0$ and the expected regret scales as
$\Theta(\log(T))$ \cite[Theorem 2.1]{bubeck2012regret}.

Our algorithm's payment is constant in $T$,
while its regret is $O(\log^3(T))$ in general with a lower bound of
$\Omega(\log(T))$;
when preferences are discrete, our algorithm's regret is
constant in $T$.
Thus, viewed solely in terms of the dependence on $T$,
the best performance achievable seems comparable to that in a standard
multi-armed bandit problem;
but when preferences are discrete, the constant regret
surpasses the $\Theta(\log(T))$ achievable in the standard multi-armed
bandit setting.
This may seem surprising, because the principal in our setting has
both less control and less information than in the standard bandit setting.
The result arises because heterogeneity in preferences provides
free exploration, and allows all of the arms to be pulled infinitely
often without incurring regret once estimates are accurate enough.

While heterogeneity in preferences enables this free exploration,
heterogeneity alone is not always sufficient for enabling performance
improvements compared to the standard bandit setting.
Indeed, suppose that agents are still heterogeneous,
but the principal pulls arms directly.
Unless the principal can also observe the agents' types,
she will be unable to correctly choose each agent's preferred arm,
even with infinite exploration of arm attributes.
Regret will then grow as $\Omega(T)$. 

Thus, reaping the benefits of (unobserved) heterogeneous preferences
requires the principal to give up direct control of the arms,
providing agents the autonomy they need to express their private
information about their own preferences.
Our results show that simple arm-based incentives are sufficient
to overcome the apparent challenges created by this abdication of
control.
