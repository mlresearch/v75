\section{Lower Bound Results for Estimating $k$-Nearest Neighbor Algorithms}
\label{sec:lower}
Our lower bound results in this section are stronger in the sense that the estimator has query access to $f$, knows the distribution to be the uniform distribution $\calU$ over a finite ground set $X$ and is only supposed to work on some fixed tie-breaking mechanism. Moreover, we don't require the estimator to have success probability at least $\frac{2}{3}$ for \emph{any} $S$; instead, the success probability is calculated over the random draw of $S$ and the internal randomness of the estimator. 

\begin{theorem}
\label{thm:pthpowerlower}Let $\calU$ be the uniform distribution over a finite ground set $X$. There exists a positive constant $c$ such that for any fixed $p\geq 1$, $\epsilon\in(0,\frac{1}{6\sqrt{e}})$ and oracle $M$ using any fixed tie-breaking mechanism, $\tsoft_{\calU}(f\qu,\epsilon,S,k)$ for $p$th-power loss requires at least $c\cdot\frac{p}{\epsilon^2}$ queries in the worst case over all finite metric spaces $(X,\D)$.
\end{theorem}

%Our goal of the testing algorithm is to approximate the accuracy of the $k$-nearest majority vote algorithm given $X,d,f$ and $N$. We assume that the distribution $\calD$ is unknown to the algorithm and the function $f$ is given by active access. 

%A natural question to ask is whether there is a better algorithm achieving less query complexity. By the following reduction theorem, we'll show a negative answer to this question under the assumption that the ``naive'' algorithm for $\aga$ cannot be improved in query complexity. The theorem also leads to an unconditional sample complexity lower bound for approximating the accuracy of $k$-nearest majority vote indicating the linear growth with respect to $k$ in the sample complexity is unimprovable.



%$\aga(\mathbf A, \gamma,\epsilon)$ has a simple algorithm requiring $O(\frac{1}{\gamma^2\epsilon^2}\log\frac{1}{\epsilon})$ queries as follows. The algorithm randomly picks $O(\frac{1}{\epsilon^2})$ arms. For each of the picked arms, the algorithm queries each arm $O(\frac{1}{\gamma^2}\log\frac{1}{\epsilon})$ times and think of it as ``good'' if more than half of the results are positive and ``bad'' otherwise. The algorithm outputs the fraction of the ``good'' arms among the picked arms.

%The following is the reduction theorem. Note that the theorem is stronger in the sense that the algorithm $A$ has query access and knows the distribution to be the uniform distribution $\calU$.
%The following theorem states a lower bound $\Omega(\frac{k}{\epsilon\log\frac{1}{\epsilon}})$ for the query complexity of $\thard$, implying that the linear growth with respect to $k$ in the query complexity of $\thard$ can't be improved.
\begin{theorem}
\label{thm:mv}
There exists a positive constant $c$ such that for any fixed $k\in\mathbb{N}^*,\epsilon\in(0,\frac{1}{4})$ and oracle $M$ using any fixed tie-breaking mechanism, $\thard_{\calU}(f\qu,\epsilon,S,k)$ requires at least $c\cdot\frac{k}{\epsilon\log\frac{1}{\epsilon}}$ queries in the worst case.
\end{theorem}
%\begin{Theorem}

%\end{Theorem}
Before we prove the above theorems in Sections \ref{subsec:proofpthpowerlower} and \ref{subsec:proofmv}, we first show some related definitions and results in the stochastic multi-armed bandit setting that will be useful in the proofs of the theorems.

\subsection{Counting and Approximating the Number of Good Arms}
\label{subsec:arm}
To show query complexity lower bound results for estimating the performance of $k$-Nearest Neighbor algorithms, we show reductions from two related problems in the stochastic multi-armed bandit setting: counting the number of good arms ($\cga$) and approximating the number of good arms ($\aga$).

The setting of stochastic multi-armed bandit problems \citep{R85} is as follows. The algorithm is given $n$ arms, denoted by $\mathbf A=(A_1,A_2,\cdots,A_n)$. Each arm is a distribution over $\mathbb R$ unknown to the algorithm. The algorithm adaptively accesses these arms to receive values independently sampled according to the distributions.

In this paper, we only consider arms with Bernoulli distributions. When given $\gamma\in(0,\frac{1}{2}]$, we define good arms to be arms with mean at least $\frac{1}{2}+\gamma$ and bad arms to be arms with mean at most $\frac{1}{2}-\gamma$.

The problem of $\cga(\mathbf A,\gamma)$ is, when given $\mathbf A$ in which every $A_i$ is either good or bad, to output the number of good arms among the given $n$ arms. The algorithm should output the correct answer with probability at least $\frac{2}{3}$.

The problem of $\aga(\mathbf A,\gamma,\epsilon)$ is a similar task to $\cga(\mathbf A,\gamma)$, except that we only need to approximate the correct answer up to an additive error $\epsilon n$.

The following lemma is developed by \citet{KCG16} as a useful tool for proving lower bounds in the stochastic multi-armed bandit setting.
\begin{lemma}[Change of measure]
\label{lm:changeofdistribution}
Suppose $\mathbf A=(A_1,A_2,\cdots,A_n)$ and $\mathbf A'=(A_1',A_2',\cdots,A_n')$ are two sequences of arms. Suppose algorithm $\calA$ takes $n$ arms as input. Suppose $\mathcal E$ is an event in the $\sigma$-field $\mathcal{F}_T$ for some almost-surely finite stopping time $T$ with respect to the filtration $\{\mathcal{F}_t\}_{t\geq 0}$. Suppose $\tau_i$ is the number of queries on $A_i$ made by the algorithm. Then,
$$\sum\limits_{i=1}^n\mathbb E_{\calA,\mathbf A}[\tau_i]\mathrm{KL}(A_i,A_i')\geq D(\Pr_{\calA,\mathbf A}[\calE],\Pr_{\calA,\mathbf A'}[\calE]).\footnote{$\mathrm{KL}(X,Y)$ denotes the Kullback-Leibler divergence from distribution $Y$ to distribution $X$. If the two distributions $X$ and $Y$ are Bernoulli with means $x$ and $y$, their Kullback-Leibler divergence is the relative entropy $D(x,y)=x\log\frac{x}{y}+(1-x)\log\frac{1-x}{1-y}$.}$$
\end{lemma}



A simple special case ($n=1$) of the lemma is that to distinguish a coin with mean $\mu_1$ from a coin with mean $\mu_2$ with success probability at least $1-\delta$, an algorithm needs at least $\frac{D(1-\delta,\delta)}{D(\mu_1,\mu_2)}=\Omega(\frac{1}{D(\mu_1,\mu_2)}\log\frac{1}{\delta})$ queries in expectation for $\mu_1\neq \mu_2$ and $0<\delta\leq\frac{2}{5}$.

\subsection{Proof of Theorem \ref{thm:pthpowerlower}}
\label{subsec:proofpthpowerlower}
\begin{proof}(of Theorem \ref{thm:pthpowerlower})
We define $\epsilon'=6\sqrt{e}\epsilon$. Note that $D(\frac{1-\epsilon'}{2p},\frac{1}{2p})=O(\frac{{\epsilon'}^2}{p})$ for $p\geq 1$ and $\epsilon'\in(0,1)$. Therefore, we only need to show that a  $\tsoft_{\calU}(f\qu,\epsilon,S,k)$ estimator implies an algorithm that distinguishes a coin of mean $\frac{1-\epsilon'}{2p}$ from a coin of mean $\frac{1}{2p}$ with success probability at least $\frac{3}{5}$ using at most the same number of queries. We construct the algorithm in the following way.

The algorithm first constructs a $\knnsoft$ instance with ground set $X$ and distance metric $\D$. We first choose $k=\lceil\frac{c'p^2}{\epsilon^2}\rceil, b=\lceil\frac{6}{\epsilon}\rceil,N=\lceil c''\cdot (1+b)k\rceil$ and $m=\lceil \frac{c'''N^2}{1+b}\rceil\geq\frac{6N}{(1+b)\epsilon}$. Here, $c',c''$ and $c'''$ are sufficiently large constants. $X$ consists of a star with $m$ centers and $bm$ leaves. Each center $C$ has a distance $\D_C\in(1,2)$ to every leaf in the star and different centers have different values of $\D_C$ to avoid ties. The distance between each pair of leaves is 2 and the distance between each pair of centers is 1. 

The algorithm then simulates the estimator $\tsoft_{\calU}(f\qu,\epsilon,S,k)$ on this $\knnsoft$ instance without knowing $f$ beforehand. Every time the estimator queries the label of a new example, it simulates the result as follows. If the example being queried is a leaf, the result is 1. If the example being queried is a center, the result is obtained to be the same result of an independent toss of the coin we want to distinguish. Finally, if the output of $\tsoft_{\calU}(f\qu,\epsilon,S,k)$ is above $\frac{1}{2}[(1-\frac{1}{2p})^p+(1-\frac{1-\epsilon'}{2p})^p]$, the algorithm then guesses the coin to have mean $\frac{1-\epsilon'}{2p}$. Otherwise, the algorithm guesses the coin to have mean $\frac{1}{2p}$.

Now we show that the above algorithm correctly distinguishes the coins with success probability at least $\frac{3}{5}$. The process of the algorithm, by interchanging the randomness of the labels (coin tosses) and the internal randomness of the $\tsoft_{\calU}(f\qu,\epsilon,S,k)$ estimator, can be viewed in the way that the true labels $f$ are determined before we run the $\tsoft_{\calU}(f\qu,\epsilon,S,k)$ estimator. The leaves all have label 1 and each center is independently labeled 0 or 1 according to the result of a toss of the coin. After the labels $f$ are decided, the $\tsoft_{\calU}(f\qu,\epsilon,S,k)$ estimator is then simulated  to approximate the $p$th-power loss of the $\knnsoft$ instance up to additive error $\epsilon$ with success probability at least $\frac{2}{3}$. 

Suppose the coin to be distinguished has mean $\mu$. Note that the total number of points in the ground set is $(1+b)m=\Omega(N^2)$, therefore we can make sure with probability at least $1-\frac{1}{40}$ that no two unlabeled examples lie on the same point. Because each random example has probability $\frac{1}{1+b}$ to lie in the centers and $N\geq c''\cdot (1+b)k$, therefore by choosing a sufficiently large $c''$, we can make sure with probability at least $1-\frac{1}{40}$ that in the unlabeled sample pool, there are at least $k$ examples lying at the centers. These two events happen at the same time with probability at least $1-\frac{1}{20}$ by the Union Bound. Conditioned on these two events happening, by a sufficiently large choice of $c'$, among those unlabeled examples lying at the centers, we can make sure that with probability at least $1-\frac{1}{20}$, the average of the labels of the $k$ examples with smallest $\D_C$ is contained in $(\mu-\frac{\epsilon}{6p},\mu+\frac{\epsilon}{6p})$. All these events happen at the same time with probability at least $(1-\frac{1}{20})^2\geq 1-\frac{1}{10}$, and in this case, every leaf outside the unlabeled pool $S$ has $L^1$ error in $(1-\mu-\frac{\epsilon}{6p},1-\mu+\frac{\epsilon}{6p})$ and thus has $p$th-power loss in $((1-\mu)^p-\frac{\epsilon}{6},(1-\mu)^p+\frac{\epsilon}{6})$. The total number of leaves in the unlabeled pool $S$ and centers is upper bounded by the size $N$ of the pool plus $m$, which contributes only a $\frac{N+m}{(b+1)m}\leq\frac{\epsilon}{3}$ fraction of the total number of points. Therefore, with probability at least $1-\frac{1}{10}$, the average $p$th-power loss of all points is contained in $((1-\mu)^p-\frac{\epsilon}{2},(1-\mu)^p+\frac{\epsilon}{2})$. %Finally, by Markov's Inequality, we know if we only consider the randomness of the labels, with probability at least $1-\frac{1}{10}$, the expected $p$th power loss of all points is contained in $((1-\mu)^p-\frac{\epsilon}{2},(1-\mu)^p+\frac{\epsilon}{2})$, which completes the proof.

Note that $(1-\frac{1-\epsilon'}{2p})^p-(1-\frac{1}{2p})^p>3\epsilon$, therefore the algorithm correctly guesses the mean of the coin with probability at least $(1-\frac{1}{10})\cdot\frac{2}{3}=\frac{3}{5}$.
\end{proof}

\subsection{Proof of Theorem \ref{thm:mv}}
\label{subsec:proofmv}
\begin{lemma}
\label{thm:reductionfrombandit}There exists a positive constant $c$ such that for any fixed $k\in \mathbb{N}^*$, $\epsilon\in (0,\frac{1}{4})$ and oracle $M$ using any fixed tie-breaking mechanism, if there is a $\thard_{\calU}(f\qu,\epsilon,S,k)$ estimator using at most $q$ queries in the worst case, then there is an $\aga(\mathbf A,\gamma,2\epsilon)$ algorithm using at most $O(q)$ queries in the worst case where $\gamma=\min\left\{\frac{1}{2},c\cdot \sqrt{\frac{\log\frac{1}{\epsilon}}{k}}\right\}$.
\end{lemma}
%Let's first consider the average error rate for points in satisfied stars.  , and in this case, all the leaves of each of these stars that are not in the unlabeled pool $S$ has error rate 1 if the star is good and 0 if the star is bad. The total number of the leaves in the unlabeled pool $S$ and the centers is upper bounded by the size $N$ of the pool plus $m$, which contributes only a $\frac{N+m}{(b+1)m}<\frac{\epsilon}{3}$ fraction of the total number of points. Therefore, with probability at least $1-\frac{1}{10}$, the average $p$th power loss of all points is contained in $((1-\mu)^p-\frac{\epsilon}{2},(1-\mu)^p+\frac{\epsilon}{2})$. 

%To show the claim, given the fraction of good arms to be $\xi$, we only need to show that if the labels of the centers are chosen independently at random such that each center of a star whose corresponding arm has mean $p$ has probability $p$ being labeled 1 and probability $1-p$ being labeled 0, then with probability $\frac{9}{10}=\frac{\frac{3}{5}}{\frac{2}{3}}$, the $\mv$ instance has error rate in $[\xi-\epsilon,\xi+\epsilon]$. In fact, with probability at least $1-\frac{\epsilon^2}{120}$, each star satisfies that there are more than $k$ training data located in its center (by the choice of $N$ and $c'$), more than half of the $k$-nearest are labeled 1 for good stars and less than half of the $k$-nearest are labeled 1 for bad stars (by the choice of $\gamma$ and $c$). By Markov's Inequality, with probability $1-\frac{\epsilon}{20}$, more than $1-\frac{\epsilon}{6}$ fraction of the stars satisfy the previous property. Conditioned on this happening, the instance has error rate in $[\xi-\frac{\epsilon}{2},\xi+\frac{\epsilon}{2}]$ because $b\geq\frac{6}{\epsilon}$ and $N\leq\frac{\epsilon}{6}\cdot mnb$. Then by Markov's inequality, with probability at least $1-\frac{1}{10}$ of the labeling, with probability at least $1-\frac{\epsilon}{2}$ of the sampling has error rate in $[\xi-\frac{\epsilon}{2},\xi+\frac{\epsilon}{2}]$, which means that with probability at least $\frac{9}{10}$ of the labeling, the error rate of the instance is in $[\xi-\epsilon,\xi+\epsilon]$. 

The above lemma shows that a query complexity lower bound for $\aga(\mathbf A, \gamma,\epsilon)$ can imply a query complexity lower bound for $\thard_{\calU}(f\qu,\epsilon,S,k)$. $\aga(\mathbf A, \gamma,\epsilon)$ has a simple algorithm requiring $O(\frac{1}{\gamma^2\epsilon^2}\log\frac{1}{\epsilon})$ queries as follows. The algorithm randomly picks $O(\frac{1}{\epsilon^2})$ arms. For each of the picked arms, the algorithm queries it $O(\frac{1}{\gamma^2}\log\frac{1}{\epsilon})$ times and thinks of it as ``good'' if more than half of the results are positive and ``bad'' otherwise. The algorithm outputs the fraction of ``good'' arms among the picked arms.

If we assume the simple $O(\frac{1}{\gamma^2\epsilon^2}\log\frac{1}{\epsilon})$ query complexity for $\aga$ is not improvable, then Lemma \ref{thm:reductionfrombandit} implies that the $O(\frac{k}{\epsilon^2})$ query complexity for $\thard$ is also not improvable. In other words, if for every sequences $\epsilon_n\rightarrow 0$ and $\gamma_n\rightarrow 0$, there exists a positive constant $c$ such that $\aga(\mathbf A,\epsilon_i,\gamma_i)$ needs at least $c\cdot \frac{1}{\gamma_i^2\epsilon_i^2}\log\frac{1}{\epsilon_i}$ queries in the worst case, then according to Lemma \ref{thm:reductionfrombandit}, we know for any sequences $\{k_n\},\{\epsilon_n\}$ such that $\epsilon_n\rightarrow 0,\frac{k_n}{\log\frac{1}{\epsilon_n}}\rightarrow\infty$, there exists a positive constant $c'$ such that the estimator $\thard_{\calU}(f\qu,\epsilon,S,k)$ for $\mv$ algorithms needs at least $c'\cdot\frac{k_i}{\epsilon_i^2}$ queries in the worst case.

\begin{proof}(of Lemma \ref{thm:reductionfrombandit})
Since the success probability can be boosted by repetition, we only show an $\aga(\mathbf A,\gamma,2\epsilon)$ algorithm with success probability at least $\frac{3}{5}$. Given any instance of $\aga(\mathbf A,\gamma,2\epsilon)$ with total number of arms equal to $n$, the algorithm constructs a ground set $X$ and the distance metric $\D$ on it to form a $\mv$ instance in the following way. We first choose $b=\lceil\frac{3}{\epsilon}\rceil,N=\lceil c'\cdot (1+b)n(k+\log\frac{1}{\epsilon})\rceil$ and $m=\lceil\frac{c''N^2}{(1+b)n}\rceil\geq\frac{3N}{(1+b)n\epsilon}$. Here, $c'$ and $c''$ are sufficiently large constants. $X$ consists of $n$ identical stars, each corresponds to an arm, with the distances between stars to be very large. Each star consists of $m$ centers and $bm$ leaves. Each center $C$ has a distance $\D_C\in(1,2)$ to every leaf in the same star and different centers have different values of $\D_C$ to avoid ties. The distance between each pair of leaves in the same star is 2 and the distance between each pair of centers in the same star is 1. 

The algorithm then simulates the estimator $\thard_{\calU}(f\qu,\epsilon,S,k)$ on this $\mv$ instance without knowing $f$ beforehand. Every time the estimator queries the label of a new example, it simulates the result as follows. If the example being queried is a leaf, the result is 0. If the example being queried is a center, the result is obtained to be the same result of an independent query to the corresponding arm. Finally, the algorithm outputs $\hat\alpha n$ when the $\thard_{\calU}(f\qu,\epsilon,S,k)$ estimator outputs $\alpha$.

%We run algorithm $A$ on ground set $X$ with distance metric $\D$ without knowing the labels $f$ beforehand. Each time $A$ queries the label of a new example, we answer in the following manner. If the example being queried is a leaf, we always answer 0, and if the example being queried is a center of the $i$th star, we query the $i$th arm and answer the result of the coin toss. Finally we output what $A$ outputs. We claim that this is an $\aga(\mathbf A,\gamma,2\epsilon)$ algorithm.
Now we show that the above is an $\aga(\mathbf A,\gamma,2\epsilon)$ algorithm with success probability at least $\frac{3}{5}$. The process of the algorithm, by interchanging the randomness of the labels (arms) and the internal randomness of the $\thard_{\calU}(f\qu,\epsilon,S,k)$ estimator, can be viewed in the way that the true labels $f$ are determined before we run the $\thard_{\calU}(f\qu,\epsilon,S,k)$ estimator. The leaves all have labels 0 and each center is independently labeled 0 or 1 according to the result of a query to the corresponding arm. After the labels $f$ are decided, the $\thard_{\calU}(f\qu,\epsilon,S,k)$ estimator is then simulated  to approximate the error rate of the $\mv$ instance up to additive error $\epsilon$ with success probability at least $\frac{2}{3}$. 

Let's say a star is good (bad) if it corresponds to a good (bad) arm. Suppose there are $\xi n$ good arms, and thus $\xi n$ good stars. Note that there are $(1+b)mn=\Omega(N^2)$ points in the ground set, we can make sure with probability at least $1-\frac{1}{20}$ that no two unlabeled examples lie on the same point, on which the following discussion is conditioned. Let's first fix a star $R$ whose corresponding arm has mean $\mu$. Because each random example has probability $\frac{1}{(1+b)n}$ to lie in the centers of $R$ and $N\geq c'\cdot (1+b)n(k+\log\frac{1}{\epsilon})$, therefore by choosing a sufficiently large $c'$, we can make sure with probability at most $\frac{\frac{\epsilon}{120}}{1-\frac{1}{20}}$ that in the unlabeled sample pool, there are less than $k$ examples lying at the centers of $R$. Therefore, by a sufficiently large choice of $c$, among those unlabeled examples lying at the centers of $R$, we can make sure that with probability at least $(1-\frac{\frac{\epsilon}{120}}{1-\frac{1}{20}})(1-\frac{\epsilon}{200})\geq 1-\frac{\frac{\epsilon}{60}}{1-\frac{1}{20}}$, the average of the labels of the $k$ examples with smallest $\D_C$ is contained in $(\mu-\gamma,\mu+\gamma)$, or $R$ is \emph{satisfied}. By Markov's Inequality, with probability at least $1-\frac{\frac{1}{20}}{1-\frac{1}{20}}$, or $1-\frac{1}{10}$ if we unwrap the conditional probability of $1-\frac{1}{20}$, at least a $(1-\frac{\epsilon}{3})$ fraction of all the $n$ stars are satisfied. In a satisfied star, any leaf that is not in the unlabeled pool has $L^1$ error 1 if the star is good and $L^1$ error 0 if the star is bad. Note that there are at most $N$ leaves in the unlabeled pool, contributing at most an $\frac{N}{(1+b)mn}\leq\frac{\epsilon}{3}$ fraction of the total number of points. Also there are only $mn$ centers in total, contributing at most an $\frac{mn}{(1+b)mn}\leq\frac{\epsilon}{3}$ fraction of the total number of points. Therefore, with probability at least $1-\frac{1}{10}$, the average error of all points is contained in $[\xi-\epsilon,\xi+\epsilon]$, which implies that with probability at least $(1-\frac{1}{10})\cdot\frac{2}{3}=\frac{3}{5}$, $\hat\alpha\in[\xi-2\epsilon,\xi+2\epsilon]$.
 \end{proof}
%Though we are not able to show an $\Omega(\frac{1}{\gamma^2\epsilon^2}\log\frac{1}{\epsilon})$ lower bound for approximating the fraction of good arms, by considering it's special case, \emph{counting good arms}, we can easily get an $\Omega(\frac{1}{\gamma^2\epsilon})$ lower bound, which implies by Theorem \ref{thm:reductionfrombandit} an $\Omega(\frac{k}{\epsilon\log\frac{1}{\epsilon}})$ lower bound for approximating the accuracy of $\mv$. Specifically, we show the following theorem.




 
 Before proving Theorem \ref{thm:mv}, we first show a query complexity lower bound for $\cga$.
\begin{lemma}
\label{lm:counting}
There exists a universal constant $c$ such that for any fixed $\gamma\in(0,\frac{1}{2}]$ and $n\in\mathbb N^*$, $\cga(\mathbf A,\gamma)$ requires at least $c\cdot\frac{n}{\gamma^2}$ queries in the worst case, where $n$ is the number of arms in $\mathbf A$.
\end{lemma}

\begin{proof}(of Lemma \ref{lm:counting})
We use $G$ to denote the good arm with mean $\frac{1}{2}+\gamma$ and $B$ to denote the bad arm with mean $\frac{1}{2}-\gamma$. Let's first consider the case where each of the $n$ arms is independently chosen to be $G$ or $B$ uniformly at random. Note that we require the probability of success to be at least $\frac 23$, so $\cga(\mathbf A,\gamma)$ can't always make less than $n$ queries because the probability of success is at most $\frac 12$ in this case. Therefore, $n$ is an obvious query complexity lower bound and in the rest of the proof we can assume $\gamma<\frac 14$.

We claim a stronger fact that for any $0\leq q\leq n$ and any instance consisting of $q$ $G$'s and $n-q$ $B$'s, $\cga(\mathbf A,\gamma)$ needs at least $c\cdot\frac{1}{\gamma^2}$ queries on \emph{every} of the $n$ arms. By symmetry between ``good'' and ``bad'', we only show that every $G$ arm needs to be queried at least $c\cdot\frac{1}{\gamma^2}$ times. The reason is as follows. Suppose $\mathbf A=(A_1,A_2,\cdots,A_n)$ in which $A_i=G$ for $1\leq i\leq q$ and $A_i=B$ otherwise. We define $\mathbf A'=(A_1',A_2',\cdots,A_n')$ in which $A'_i=G$ for $1\leq i\leq p-1$ and $A'_i=B$ otherwise. The only difference between $\mathbf A$ and $\mathbf A'$ is that $A_p=G$ while $A'_p=B$. We use $\calE$ to denote the event that $\cga(\mathbf A,\gamma)$ outputs $p$. By Lemma \ref{lm:changeofdistribution} and $\mathrm{KL}(G,B)=O(\gamma^2)$, we know $\mathbb E[\tau_p]\cdot O(\gamma^2)\geq D(\frac{2}{3},\frac{1}{3})=\Omega(1)$ and thus $\mathbb E[\tau_p]=\Omega(\frac{1}{\gamma^2})$. For similar reasons, we can show for all $1\leq i\leq p$ that $\mathbb E[\tau_i]=\Omega(\frac{1}{\gamma^2})$, which completes the proof.
\end{proof}

\begin{proof}(of Theorem \ref{thm:mv})
Lemma \ref{lm:counting} immediately implies the existence of a positive constant $c'$ such that for any fixed $\epsilon\in(0,\frac{1}{2})$ and $\gamma\in(0,\frac{1}{2}]$, $\aga(\mathbf A, \gamma,\epsilon)$ requires at least $c'\cdot\frac{1}{\gamma^2\epsilon}$ queries in the worst case by choosing $n=\lceil\frac{1}{2\epsilon}\rceil-1$. Then, by Lemma \ref{thm:reductionfrombandit}, we get an $\Omega(\frac{1}{
\left(\min\left\{\frac{1}{2},\sqrt{\frac{
	\log\frac{1}{\epsilon}
}{k}}\right\}\right)^2
}\cdot\frac{1}{2\epsilon})=\Omega(\frac{k}{\epsilon\log\frac{1}{\epsilon}})$ lower bound for $\thard_{\calU}(f\qu,\epsilon,S,k)$ for $k\in\mathbb N^*$ and $\epsilon\in(0,\frac{1}{4})$.
\end{proof}
%Before introducing the reduction, we first describe the setting for the problem of approximating the fraction of good arms. Suppose we are given $n$ arms $A_1,A_2,\cdots, A_n$, each $A_i$ as a 0-1 coin with mean $\mu_i$. We are also given a parameter $\gamma\in(0,\frac{1}{2}]$. We assume that the $n$ arms are either ``good'', i.e., $\mu_i\geq \frac{1}{2}+\gamma$, or ``bad'', i.e., $\mu_i\leq\frac{1}{2}-\gamma$. The algorithm can choose to query each arm for multiple times to receive the results of independent coin tosses. The goal of the algorithm is to approximate the fraction of good arms among the $n$ arms within additive error $\epsilon$ with success probability at least $\frac{2}{3}$.