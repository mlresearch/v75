\section{Our Results and Methods}
\label{sec:results}
\paragraph{Tolerant testing:}
We show (Theorem \ref{thm:main}) that in the active testing model, there is a tolerant tester that approximates the distance of a function to the class of unions of $d$ intervals on the line up to an additive error $\epsilon$ using $O(\frac{1}{\epsilon^6}\log\frac{1}{\epsilon})$ label queries on $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ unlabeled examples.
%, even when the data distribution is unknown to the algorithm.

We begin by assuming data is drawn from the uniform distribution $\calU$ over $[0,1]$ and then later generalize to arbitrary distributions $\calD$. Our tester evenly partitions $[0,1]$ into $m=\Theta(\epsilon d)$ segments, and focuses on $l=O(\poly(\epsilon^{-1}))$ segments chosen uniformly at random (without replacement). On the union of the chosen segments, we test how close is the function to the class of (roughly) $\frac{dl}{m}=O(\poly(\epsilon^{-1}))$ intervals by a proper agnostic learning algorithm.

One challenge we face is that the number of intervals on each segment might vary drastically, so that the sample of segments is unable to capture the entire information on the whole domain $[0,1]$. To address this challenge, we observe that on each segment,  $t=\Theta(\frac 1{\epsilon^2})$ intervals are sufficient to approximate the distance within an additive error $O(\epsilon)$, and we change the class of unions of $d$ intervals to the class of unions of $d$ intervals \emph{truncated} by $t$ in our algorithm based on this observation. This gives us a (roughly) $(\epsilon,1+\epsilon)$-bi-criteria tester for the class of unions of intervals, i.e., we estimate the distance up to additive error $\epsilon$ and approximate the number of intervals up to a factor of $1+\epsilon$. \citet{BBBY12} showed that any function that is a union of $(1+\epsilon)d$ intervals has distance $O(\epsilon)$ to a union of $d$ intervals, implying that any function that is $\alpha$-close to a union of $(1+\epsilon)d$ intervals is $\alpha+O(\epsilon)$ close to a union of $d$ intervals,  leading to the uni-criterion tester desired.

Our algorithm implements a reduction that if we can do tolerant testing when $d$ is small ($\poly(\epsilon^{-1})$), we can do tolerant testing for any $d$, with query complexity independent of $d$. We abstract this reduction as the composition lemma (Lemma \ref{thm:additive}), which may be useful for tolerant testing for other classes. Indeed, the reduction works also for $(\epsilon,1+\epsilon)$-bi-criteria tolerant testing for surface area for arbitrary $\epsilon>0$, where the class consists of functions $f:\mathbb{R}^n\rightarrow\{0,1\}$ satisfying $f^{-1}(1)$ has a small surface area (see Section \ref{subsec:surfacearea} for related work). For example, consider the class of functions with \emph{Gaussian surface area} (see \citep{KOS08} for definition) at most $S$ with respect to the standard Gaussian distribution over $\mathbb{R}^n$. We can first use $m=\Theta(\epsilon S)$ hyperplanes to evenly partition $\mathbb{R}^n$ into $2m$ parts and focus on $l=O(\poly(\epsilon^{-1}))$ random parts. If we could do tolerant testing on the $l$ parts for $S=O(\poly(\epsilon^{-1}))$, then we could do bi-criteria tolerant testing for general $S$ with query complexity independent of $S$. However, the difficulty for tolerant testing surface area is that we do not know how to do this even when $S$ is small. \citet{KOS08} have shown an agnostic learning algorithm using $n^{O(S^2)}$ samples for the class of concepts with Gaussian surface area at most $S$ over Gaussian distributions on $\mathbb{R}^n$, but their algorithm is \emph{improper}, not being able to imply a tolerant testing algorithm directly.

%We note that the tester relies little on the particular class of unions of intervals, except for the last step from bi-criteria to uni-criterion. Therefore, from the tester for the class of unions of intervals, we abstract a bi-criteria tester for the class of \emph{compositions of additive properties}. The definition of the class of compositions of additive properties is over a distribution $\calD$ evenly partitioned into $m$ pieces, and on each piece $p_i$ there is a class of functions having some property $d_i\geq 0$, and the class consists of functions with $\sum\limits_id_i\leq d$ for some $d\geq 0$. We also define the class of compositions of additive properties \emph{truncated} by $t$ by additionally requiring that $\forall i,d_i\leq t$. We provide an abstraction of our testing algorithm for the class of unions of intervals in the composition lemma, which shows that tolerant testing for the class of additive properties can be done via tolerant testing for the class of additive properties truncated by $t$ over a small (independent of $m$) sub-union of pieces.

%Our tester for the class of compositions of additive properties might be applied to tolerant testing for other classes, for example testing surface area \citep{KNOW13}. Indeed, the whole algorithm works for bi-criteria tolerantly testing the class of concepts with surface area at most $S$ with query complexity independent of $S$ as long as we are able to partition the whole space evenly using hyperplanes satisfying that the area for each hyperplane has a constant upper bound, and we can do tolerant testing when $S$ is small (roughly $O(\poly(\epsilon^{-1}))$). The first condition can be satisfied easily, especially for Gaussian surface area where the (Gaussian) area for any hyperplane does not exceed an absolute constant $\sqrt{\frac{2}{\pi}}$ \citet{KOS08}, but the second condition seems not to be satisfied as easily as the case of unions of intervals. \citet{KOS08} have shown an agnostic learning algorithm using $n^{O(S^2)}$ samples for the class of concepts with Gaussian surface area at most $S$ over Gaussian distributions on $\mathbb{R}^n$, but their algorithm is \emph{improper}, not being able to imply a tolerant testing algorithm directly.[[TO BE CHECKED MORE CAREFULLY]]

%To achieve this result, we propose the notion of \emph{compositions of additive properties} (Section \ref{subsec:composition}) and prove the Composition Lemma (Lemma \ref{thm:additive}) that to approximate the distance to any composition of $m$ additive properties on a semi-uniform distribution up to an additive error $\epsilon$, we only need a distance approximation oracle for compositions of only $O(\frac{1}{\epsilon\mu^2}+\frac{1}{\epsilon^2})$ additive properties, though this may produce a bi-criteria approximation that depends on $\mu$. See Section \ref{subsec:composition} for definitions.

%The Composition Lemma implies an $(\epsilon,\mu)$-bi-criteria distance approximation algorithm for unions of $d$ intervals on the uniform distribution over $[0,1]$ using $O((\frac{1}{\epsilon^3\mu^3}+\frac{1}{\epsilon^4\mu})\log\frac{1}{\epsilon})$ queries on $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ unlabeled examples; in particular, we estimate the error to an additive $\pm \epsilon$ and the number of intervals to a multiplicative factor $1+\mu$. We then show (Lemma \ref{lm:reductiontobicriteria}) how to remove the approximation in number of intervals and get a uni-criterion distance approximation algorithm.

To generalize our tester for the class of unions of intervals from the uniform distribution on $[0,1]$ to arbitrary unknown distributions, we show a general relationship between active testing and query testing for arbitrary distributions in Lemmas \ref{thm:unlabeled} and \ref{thm:reductiontoquery}, which also improves a previous result in \citep{BBBY12} by showing that the unlabeled sample complexity of non-tolerant property testing for unions of $d$ intervals on arbitrary unknown distributions can be reduced to $O(\frac{d}{\epsilon}\log\frac{1}{\epsilon})$, from $O(\frac{d^2}{\epsilon^6})$ (implicit) in their original paper. We also generalize the result in \citep{BBBY12} for actively testing the class of unions of testable functions to the tolerant case in Appendix \ref{sec:union}.
\paragraph{Algorithm estimation:}
For the $k$-Nearest Neighbor (\knn) algorithm with soft predictions and $p$th-power loss (the prediction on a point $x$ is the average label of the $k$ nearest examples to $x$ in a random sample of size $N$, and we use the $p$th-power loss to penalize mistakes) we show in Theorem \ref{thm:pth} that this loss can be estimated up to an additive error $\epsilon$ using $O(\frac{p}{\epsilon^2})$ queries on $N+O(\frac{1}{\epsilon^2})$ unlabeled examples, even when the data distribution is unknown to the estimator. The same result also holds for Weighted Nearest Neighbor algorithms, where the prediction on a point $x$ is a weighted average of the labels of all the examples depending on their distances to $x$ (see Appendix \ref{sec:pthproof}). For the $O(\frac{p}{\epsilon^2})$ query complexity upper bound, we show a matching lower bound (Theorem \ref{thm:pthpowerlower}). In the case where $k$ is a quantity to be optimized, we show an algorithm that finds an approximately-best choice of $k$ up to an additive error $\epsilon$ using roughly 
%$O(\frac{p^2\log N}{\epsilon^3}(\log\log N+\log p+\log\frac{1}{\epsilon}))$
$O(\frac{p^2\log N}{\epsilon^3})$ queries on roughly $N+O(\frac{p\log N}{\epsilon^3})$ unlabeled examples (Theorem \ref{thm:bestk}).  For $\knn$ with hard predictions (the prediction is a strict majority vote over the $k$ nearest neighbors), we show that it's impossible to estimate the performance with query complexity independent of $k$
%: by a reduction (Theorem \ref{thm:reductionfrombandit}) from \emph{approximating the number of good arms} ($\aga$, see Section \ref{subsec:arm} for definition) in the stochastic multi-armed bandit setting, we show that the query complexity cannot be improved beyond $O(\frac{k}{\epsilon\log\frac{1}{\epsilon}})$ 
(Theorem \ref{thm:mv} in Appendix \ref{sec:lower}).%In Section \ref{sec:preli}, we introduce important definitions and related previous results. In Section \ref{sec:additive}, we prove the Composition Lemma (Lemma \ref{thm:additive}). In Section \ref{sec:interval}, we prove Theorem \ref{thm:main} of active tolerant testing for unions of intervals. In Section \ref{sec:neighbor}, we consider problems of estimating the performance of $\knn$. In Appendix \ref{sec:union}, we extend the result of testing disjoint unions of properties from non-tolerant property testing \citep{BBBY12} to tolerant testing.


%[[regarding the 3 slightly different settings:]]

We note that there are three natural but somewhat different ways to model the task of estimating the error rate of algorithm $\calA$ trained on dataset $S$. Let $\error(h_S)$ denote the error rate of hypothesis $h_S$ with respect to distribution $\calD$, and let $\hat\alpha$ be the output of the estimator $\calE$ that estimates $\error(h_S)$. In the first model, we require that $\hat\alpha$ be a good estimate of $\error(h_S)$ with probability at least $\frac23$ for {\em any} training set $S$, even sets $S$ not drawn from $\calD$.  In the second model, we only require that $\calE$ be accurate when $S$ is drawn from $\calD$ (that is, the $\frac23$ probability is over both the internal randomness in $\calE$ and in the draw of $S$).  Finally, in the third model, $S$ is drawn from $\calD$ but $\calE$ does not have access to it: instead, $\calE$ has the ability to draw (a polynomial number of) fresh unlabeled examples and to query points from them.  That is,
%could take $S$ as its input (as in the first two models) or not (as in the third model). There are two types of randomness: the randomness of $S$ (iid from $\calD$) and the internal randomness of the tester $\calT$. 
\begin{enumerate}
\item In the first model, we require that $\forall S,\Pr_{\calE(S)}[|\hat\alpha-\error(h_S)|\leq\epsilon]\geq \frac{2}{3}$. 
\item In the second model, we require that $\Pr_{S,\calE(S)}[|\hat\alpha-\error(h_S)|\leq \epsilon]\geq \frac{2}{3}$. 
\item In the third model, we require that $\Pr_{\calE}[|\hat\alpha-\mathbb E_S[\error(h_S)]|\leq\epsilon]\geq \frac{2}{3}$. 
\end{enumerate}
Roughly, the first model is the hardest while the third model is the easiest.  
All our upper bounds and lower bounds in this paper apply to all three models with slight modifications, though for simplicity of presentation we focus on the second model throughout the paper.%first model for upper bounds (Sections \ref{subsec:knnsoft}, \ref{subsec:bestk}, \ref{subsec:mv}) and the second model for lower bounds (Section \ref{subsec:lower}) because the first model is more restraining than the second model.

%Though in this paper, we present our upper bound and lower bound results in the second model, they can be naturally generalized to all models with slight modifications.



