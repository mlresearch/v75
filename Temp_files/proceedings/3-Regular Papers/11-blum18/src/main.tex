\documentclass[final,12pt]{colt2018} % Anonymized submission
% \documentclass{colt2017} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

\title{Active Tolerant Testing}
\usepackage{times}
 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % Two authors with the same address
  % \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
  %  \Name{Author Name2} \Email{xyz@sample.com}\\
  %  \addr Address}

 % Three or more authors with the same address:
 % \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
 %  \Name{Author Name2} \Email{an2@sample.com}\\
 %  \Name{Author Name3} \Email{an3@sample.com}\\
 %  \addr Address}


 % Authors with different addresses:
 \coltauthor{\Name{Avrim Blum} \Email{avrim@ttic.edu}\\
 \addr {Toyota Technological Institute at Chicago, Chicago, IL, USA}
 \AND
 \Name{Lunjia Hu} \Email{hulj14@mails.tsinghua.edu.cn}\\
 \addr {Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China}
 }


\newcommand{\dintervals}{{\mathcal{I}(d)}}
\newcommand{\dist}{{\mathrm{dist}}}
\newcommand{\interval}{{\mathcal{I}}}
\newcommand{\standardinterval}{{\widetilde{\mathcal{I}}}}
\newcommand{\calC}{{\mathcal{C}}}
\newcommand{\calD}{{\mathcal{D}}}
\newcommand{\calS}{{\mathcal{S}}}
\newcommand{\property}{{\mathcal{P}}}
\newcommand{\D}{{\mathrm{d}}}
\newcommand{\knn}{{\text{$k$-NN}}}
\newcommand{\mv}{{\text{$k$-NN}^{\mathrm{hard}}}}
\newcommand{\thard}{\mathcal{E}^{\mathrm{hard}}}
\newcommand{\tsoft}{\mathcal{E}^{\mathrm{soft}}}
\newcommand{\knnsoft}{{\text{$k$-NN}^{\mathrm{soft}}}}
\newcommand{\loss}{{\mathrm{loss}}}
\newcommand{\calE}{{\mathcal E}}
\newcommand{\udi}{{\mathrm{Int}}}
\newcommand{\pt}{{\mathrm{PT}}}
\newcommand{\tot}{{\mathrm{TT}}}
\newcommand{\da}{{\mathrm{DA}}}
\newcommand{\qu}{_{\mathrm{query}}}
\newcommand{\ac}{_{\mathrm{active}}}
\newcommand{\pa}{_{\mathrm{passive}}}
\newcommand{\ap}{{\mathrm{Comp}}}
\newcommand{\aga}{{\mathrm{AGA}}}
\newcommand{\cga}{{\mathrm{CGA}}}
\newcommand{\calU}{{\mathcal{U}}}
\newcommand{\best}{{\mathrm{Best}}}
\newcommand{\calA}{{\mathcal{A}}}
\newcommand{\error}{{\mathrm{error}}}
\newcommand{\calT}{{\mathcal{T}}}
\newcommand{\err}{{\mathrm{err}_1}}
\newcommand{\calB}{{\mathcal B}}
\newcommand{\poly}{{\mathrm{poly}}}


\newtheorem{claim}[theorem]{Claim}


\begin{document}

\maketitle

\begin{abstract}
In this work, we show that for a nontrivial hypothesis class $\calC$, we can estimate the distance of a target function $f$ to $\calC$ (estimate the error rate of the best $h\in \calC$) using substantially fewer labeled examples than would be needed to actually {\em learn} a good $h \in \calC$.   Specifically, we show that for the class $\calC$ of unions of $d$ intervals on the line, in the active learning setting in which we have access to a pool of unlabeled examples drawn from an arbitrary underlying distribution $\calD$, we can estimate the error rate of the best $h \in \calC$ to an additive error $\epsilon$ with a number of label requests that is {\em independent of $d$} and depends only on $\epsilon$.  In particular, we make $O(\frac{1}{\epsilon^6}\log \frac{1}{\epsilon})$ label queries to an unlabeled pool of size $O(\frac{d}{\epsilon^2}\log \frac{1}{\epsilon})$.  This task of estimating the distance of an unknown $f$ to a given class $\calC$  is called {\em tolerant testing} or {\em distance estimation} in the testing literature, usually studied in a membership query model and with respect to the uniform distribution.  Our work extends that of  \citet{BBBY12} who solved the {\em non}-tolerant testing problem for this class (distinguishing the zero-error case from the case that the best hypothesis in the class has error greater than $\epsilon$).  
%In this work, we give the first algorithms for tolerant testing of nontrivial classes in the active model: estimating the distance of a target function to a hypothesis class $\calC$ with respect to some arbitrary distribution $\calD$, using only a small number of label queries to a polynomial-sized pool of unlabeled examples drawn from $\calD$.   Specifically, we show that for the class $\calC$ of unions of $d$ intervals on the line, we can estimate the error rate of the best hypothesis in the class to an additive error $\epsilon$ from only $O(\frac{1}{\epsilon^6}\log \frac{1}{\epsilon})$ label queries to an unlabeled pool of size $O(\frac{d}{\epsilon^2}\log \frac{1}{\epsilon})$.  The key point here is the number of labels needed is independent of the VC-dimension of the class. This extends the work of  \citet{BBBY12} who solved the {\em non}-tolerant testing problem for this class (distinguishing the zero-error case from the case that the best hypothesis in the class has error greater than $\epsilon$).  

We also consider the related problem of estimating the performance of a given learning algorithm $\calA$ in this setting.  That is, given a large pool of unlabeled examples drawn from distribution $\calD$, can we, from only a few label queries, estimate how well $\calA$ would perform if the entire dataset were labeled and given as training data to $\calA$?   We focus on $k$-Nearest Neighbor style algorithms, and also show how our results can be applied to the problem of hyperparameter tuning (selecting the best value of $k$ for the given learning problem).
\end{abstract}

\begin{keywords}
property testing, agnostic learning, algorithm estimation
\end{keywords}

%\section{Introduction}
%
%This is where the content of your paper goes.  Remember to:
%\begin{itemize}
%\item Limit the main text (without references and appendices) to 12 PMLR-formatted pages (i.e., using this template).
%\item Include, either in the main text or the appendices, all details, proofs
%  and derivations required to substantiate the results.
%\item Include {\em in the main text} enough details, including proof
%  details, to convince the reviewers of the contribution, novelty and significance of the submissions.
%\item Not include author names (this is done automatically), and to
%  the extent possible, avoid directly identifying the authors.  You
%  should still include all relevant references, including your own,
%  and any other relevant discussion, even if this might allow a
%  reviewer to infer the author identities.
%  \end{itemize}

\input{introduction}
\input{model}
\input{results}
\input{preliminaries}
\input{relationship}
\input{composition}
\input{interval}
\input{neighbor}
% Acknowledgments---Will not appear in anonymized version
\acks{This work was supported in part by the National Science Foundation under grants CCF-1525971 and CCF-1800317.}


\bibliography{references}


\appendix
\input{refutation}
\input{union_proof}
\input{composition_proof}
\input{interval_proof}
\input{knn_proof}
\input{tuning_proof}
\input{lowerbound_proof}
\end{document}