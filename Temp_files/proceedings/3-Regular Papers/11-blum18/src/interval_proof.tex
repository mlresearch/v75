\section{Proof of Theorem \ref{thm:main}}
\label{sec:proofmain}
\begin{proof}(of Theorem \ref{thm:main})
We use the definitions of $\interval(d)$ and $\interval_\calD(d,\alpha)$ in Section \ref{subsec:relatedinterval}. As pointed out in Section \ref{sec:interval}, we only need to consider $\calD$ as the uniform distribution over $[0,1]$ and we omit it for simplicity.  %\begin{claim}
%\label{lm:reductiontobicriteria}
%$\forall \epsilon\in(0,\frac{1}{2}),\forall \alpha\in[0,1],\forall d>\frac{2}{\epsilon},\interval((1+\frac{\epsilon}{2})d,\alpha-\epsilon)\subseteq\interval(d,\alpha)$.
%\end{claim}
%\begin{proof}
%$\forall f\in\interval((1+\frac{\epsilon}{2})d,\alpha-\epsilon),\exists g\in\interval((1+\frac{\epsilon}{2})d)$ s.t. $\dist(f,g)\leq \alpha-\epsilon$. Assume $g$ uses $k\leq (1+\frac{\epsilon}{2})d$ intervals. Without loss of generality, we can assume that $k\geq d$. We remove $\lceil\frac{\epsilon k}{2}\rceil$ shortest intervals from the $k$ intervals. The number of remaining intervals is at most $(1-\frac{\epsilon}{2})k\leq (1-\frac{\epsilon}{2})(1+\frac{\epsilon}{2})d\leq d$. The distance increase is upper bounded by $(\frac{\epsilon k}{2}+1)\cdot\frac{1}{k}\leq \frac{\epsilon}{2}+\frac{1}{d}\leq\epsilon$.
%\end{proof}





%Now we come back and prove Theorem \ref{thm:main}.
If $d\leq\frac{8}{\epsilon}$, we can simply do agnostic learning using $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})=O(\frac{1}{\epsilon^3}\log\frac{1}{\epsilon})$ queries and unlabeled examples. So in the rest of the proof, we assume $d>\frac{8}{\epsilon}$. We pick the largest positive integer $m$ satisfying $m\leq\frac{\epsilon d}{8}$ and we define $\lambda=\frac{d}{m}=O(\frac{1}{\epsilon})$.

Since the data distribution is assumed uniform on $[0,1]$, we can assume without loss of generality that our ground set $X$ is $[0,1]$ and $f\in\{0,1\}^X$. We evenly cut $X$ into $m$ pieces: $X_1=[0,\frac{1}{m}],X_2=(\frac{1}{m},\frac{2}{m}],X_3=(\frac{2}{m},\frac{3}{m}],\cdots,X_m=(\frac{m-1}{m},1]$. $\forall 1\leq i\leq m,\forall k\in\mathbb{N}$, we define $\calC_i^k$ to be the class of binary functions $f$ on $X_i$ such that $f^{-1}(1)$ is a union of at most $k$ intervals. Note that $\calC_i^0\neq\emptyset$. Therefore, we can define $\property$, the composition of $m$ additive properties as in Section \ref{sec:composition}. 

Note that for any $d'>0$ and any truncation $t>0$, the concept class $\property^{t}(d')$ has VC-dimension at most $2d'$. Therefore, according to the VC Theory for agnostic learning, for any $\mu\in(0,\frac{1}{2}),\epsilon'=\frac\epsilon 4,l=O(\frac{1}{\epsilon'\mu^2}+\frac{1}{\epsilon'^2})$, we have a $((1+\frac{\mu}{2})(1+\frac{\epsilon}{8})\lambda l,l,\frac{2(1+\frac{\epsilon}{8})\lambda}{\epsilon'},\epsilon')$ distance approximation oracle using $O(\frac{(1+\frac{\mu}{2})(1+\frac{\epsilon}{8})\lambda l}{{\epsilon'}^2}\log\frac{1}{\epsilon'})=O(\frac{l}{\epsilon'^2\epsilon}\log\frac{1}{\epsilon'})=O((\frac{1}{\epsilon'^3\epsilon\mu^2}+\frac{1}{\epsilon'^4\epsilon})\log\frac{1}{\epsilon'})=O((\frac 1{\epsilon^4\mu^2}+\frac 1{\epsilon^5})\log\frac 1\epsilon)$ queries and unlabeled examples simply by empirical risk minimization. By the Composition Lemma (Lemma \ref{thm:additive}), we have an algorithm that outputs $\hat\alpha$ such that $\forall f$,
\begin{enumerate}
\item $\forall \alpha$ s.t. $f\in\property((1+\frac{\epsilon}{8})\lambda m,\alpha)$, it holds with probability at least $\frac{2}{3}$ that $\hat\alpha\leq\alpha+2\epsilon'(=\alpha+\frac\epsilon 2)$;
\item $\forall \alpha$ s.t. $f\notin\property((1+\mu)(1+\frac{\epsilon}{8})\lambda m,\alpha)$, it holds with probability at least $\frac{2}{3}$ that $\hat\alpha>\alpha-2\epsilon'(=\alpha-\frac\epsilon 2)$.
\end{enumerate}

Choose $1+\mu=\frac{1+\frac{\epsilon}{4}}{1+\frac{\epsilon}{8}}$ and note that $\lambda m=d,\interval(d,\alpha)\subseteq\property(d+m,\alpha)\subseteq\property((1+\frac{\epsilon}{8})d,\alpha)$ and $\property((1+\frac{\epsilon}{4})d,\alpha)\subseteq \interval((1+\frac{\epsilon}{4})d,\alpha)$, we have $\forall f$,
\begin{enumerate}
\item $\forall \alpha$ s.t. $f\in\interval(d,\alpha)$, it holds with probability at least $\frac{2}{3}$ that $\hat\alpha\leq\alpha+\frac\epsilon 2$;
\item $\forall \alpha$ s.t. $f\notin\interval((1+\frac{\epsilon}{4})d,\alpha)$, it holds with probability at least $\frac{2}{3}$ that $\hat\alpha>\alpha-\frac \epsilon 2$.
\end{enumerate}

This is an $(\frac \epsilon 2,1+\frac{\epsilon}{4})$-bi-criteria tester for unions of $d$ intervals. According to the Composition Lemma (Lemma \ref{thm:additive}), the query complexity and the unlabeled sample complexity of the algorithm are $O((\frac{1}{\epsilon^4\mu^2}+\frac{1}{\epsilon^5})\log\frac{1}{\epsilon})=O(\frac{1}{\epsilon^6}\log\frac{1}{\epsilon})$ and $O((\frac{l}{\epsilon'^2\epsilon}\log\frac{1}{\epsilon'})\cdot\frac{m}{l})=O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$.

%Now we define $\epsilon'=\frac{\epsilon}{2}$ and by rewriting the second statement in an equivalent way, we get $\forall \alpha$,
%\begin{enumerate}
%\item $\forall f\in\interval(d,\alpha)$, it holds with probability at least $\frac{2}{3}$ that $\hat\alpha\leq\alpha+\frac{\epsilon}{2}<\alpha+\epsilon$;
%\item $\forall f\notin\interval((1+\frac{\epsilon}{4})d,\alpha-\frac{\epsilon}{2})$, it holds with probability at least $\frac{2}{3}$ that $\hat\alpha>(\alpha-\frac{\epsilon}{2})-\frac{\epsilon}{2}=\alpha-\epsilon$.
%\end{enumerate}

Finally, note that \citet{BBBY12} revealed a basic property of unions of $d$ intervals that $\interval((1+\frac\epsilon 4)d)\subseteq\interval(d,\frac\epsilon 2)$, implying $\interval((1+\frac\epsilon 4)d,\alpha)\subseteq \interval(d,\alpha+\frac\epsilon 2)$, which completes the proof.


%approximates the distance from $f$ to $\property((1+\frac{\epsilon}{8})\lambda m)$
%Also, we can define $\property(d,\alpha)$ as in Section \ref{subsec:composition}. An easy observation is that $\forall \delta>0,\forall\alpha\in[0,1],\interval(d,\alpha)\subseteq\property(d+m,\alpha)\subseteq\property((1+\frac{\epsilon}{8})d,\alpha)$ and $\property((1+\delta)(1+\frac{\epsilon}{8})d,\alpha)\subseteq \interval((1+\delta)(1+\frac{\epsilon}{8})d,\alpha)$.

%Therefore, the algorithm only needs to distinguish $f\in\property((1+\frac{\epsilon}{8})d,\alpha))=\property((1+\frac{\epsilon}{8})\lambda m,\alpha)$ and $f\notin \property((1+\frac{\epsilon}{4})d,\alpha+\frac{\epsilon}{2})=\property((1+\frac{\epsilon}{4})\lambda m,\alpha+\frac{\epsilon}{2})$. Now, if we define $\lambda'=(1+\frac{\epsilon}{8})\lambda,\delta'=\frac{1+\frac{\epsilon}{4}}{1+\frac{\epsilon}{8}}-1=\frac{\epsilon}{8+\epsilon},\epsilon'=\frac{\epsilon}{2}$, then we can rewrite our task as distinguishing $f\in\property(\lambda' m,\alpha)$ and $f\notin\property((1+\delta')\lambda' m,\alpha+\epsilon')$. Note that we have a $((1+\frac{\delta'}{2})\lambda' l,l,\frac{\lambda'}{\epsilon'},\alpha+\frac{\epsilon'}{3},\alpha+\frac{\epsilon'}{2})$-partial tester using $O(\frac{\lambda' l}{\epsilon'^2}\log\frac{1}{\epsilon'})=O(\frac{l}{\epsilon^3}\log\frac{1}{\epsilon})$ queries and unlabeled samples by agnostic learning. Also note that $O(\frac{1}{\epsilon'\delta'^2}+\frac{1}{\epsilon'^2})=O(\frac{1}{\epsilon^3})$. Therefore, by Theorem \ref{thm:additive}, there is an algorithm that can handle the task using $O(\frac{1}{\epsilon^6}\log\frac{1}{\epsilon})$ queries on $O(\frac{m}{\epsilon^3}\log\frac{1}{\epsilon})=O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ unlabeled samples.
\end{proof}

