\section{Distance Approximation for Disjoint Unions of Properties}
\label{sec:union}
In this section, we extend the theorem of \citet{BBBY12} that disjoint unions of testable properties are testable from non-tolerant testing to tolerant testing. 
%\subsection{Active Distance Approximation}
%Distance approximation is a natural generalization of tolerant testing. Given a distribution $\calD$ and a concept class $\calC$ on ground set $X$, the distance of a function $f\in\{0,1\}^X$ to $\calC$ is defined to be $\dist_{\calD}(f,\calC)=\inf\limits_{g\in\calC}\dist_{\calD}(f,g)$. A distance approximation algorithm takes a function $f\in\{0,1\}^X$ and a parameter $\epsilon$ as input, and outputs an estimation $\widehat\dist_{\calD}(f,\calC)$ which is in $[\dist_{\calD}(f,\calC)-\epsilon,\dist_{\calD}(f,\calC)+\epsilon]$ with probability at least $\frac{2}{3}$. The parameter $\epsilon$ is called the \emph{additive error}.

%For an active distance approximation algorithm, similar to an active tolerant testing algorithm, the input function $f$ is given through active access. The success probability can be boosted to $1-\delta$ for any $\delta$ by repeating the algorithm $O(\log\frac{1}{\delta})$ times and choosing the median.


We first introduce the definition of disjoint unions of properties in \citep{BBBY12}. Suppose the ground set $X$ is partitioned as a disjoint union $\bigcup\limits_{i=1}^m X_i$. On every $X_i$, there is a property (concept class) $\calC_i\neq\emptyset$. The disjoint union of these properties is defined to be $\calC=\{f\in\{0,1\}^X:\forall 1\leq i\leq m,f|_{X_i}\in\calC_i\}$.

Let $\calD$ be a distribution over $X$. Suppose the conditional distribution of $\calD$ on $X_i$ is denoted by $\calD_i$ and the probability $\Pr_{x\sim\calD}[x\in X_i]$ is denoted by $p_i$.

\begin{theorem}
Suppose $\epsilon\in(0,\frac{1}{2})$. Suppose for every $1\leq i\leq m$, there is an active tolerant tester $\calA$ for $\calC_i$ over $\calD_i$ with additive error $\frac\epsilon 2$ using at most $q$ queries on $N$ unlabeled examples. Then, there is an active tolerant tester $\calB$ for $\calC$ over $\calD$ with additive error $\epsilon$ using at most $O(\frac{q}{\epsilon^2}\log\frac{1}{\epsilon})$ queries on $O(\frac{mN}{\epsilon}\log\frac{1}{\epsilon})$ unlabeled examples. If tester $\calA$ can perform on unknown distributions, then tester $\calB$ can also perform on unknown distributions, though we need extra $O(\frac{1}{\epsilon^2})$ unlabeled examples.


%Suppose there is an $\frac{\epsilon}{2}$-distance approximation oracle using at most $q$ queries on at most $U$ unlabeled samples. Then, the distance $\dist_{\mathcal{D}}(f,\mathcal{C})$ can be approximated with success probability at least $\frac{2}{3}$ within additive error $\epsilon+\delta$ using $O(\frac{q}{\delta^2}\log\frac{1}{\delta})$ queries on $O(\frac{mU}{\delta}\log\frac{1}{\delta})$ unlabeled samples. Furthermore, if $\forall 1\leq i\leq m,p_i=\frac{1}{m}$, then the unlabeled sample complexity can be reduced to $O(mU\log\frac{1}{\delta})$.
\end{theorem}



\begin{proof}
Tester $\calB$ is constructed as follows. The tester chooses $s=O(\frac{1}{\epsilon^2})$, receives an unlabeled pool of size $O(\frac{mN}{\epsilon}\log s)$ and independently chooses $s$ indices $i_1,i_2,\cdots,i_s$ from $\{1,2,\cdots,m\}$ according to distribution $\{p_i\}_{1\leq i\leq m}$. This can be achieved by looking at on which $X_i$'s the extra $s$ unlabeled examples are, when the distribution $\calD$ is unknown. Then for each $1\leq j\leq s$, if there are enough $(O(N\log s))$ unlabeled examples lying in $X_{i_j}$, the tester repeats $\calA$ for $O(\log s)$ times to calculate an estimator $\widehat\dist_{i_j}$ of the distance from $f$ to $\calC$ on $\calD_{i_j}$ up to an additive error $\frac{\epsilon}{2}$ with success probability at least $1-\frac{1}{9s}$;\footnote{Repeat tester $\calA$ $O(\log s)$ times and take the median to boost its success probability to at least $1-\frac{1}{9s}$.} otherwise, define $\widehat\dist_{i_j}=0$. The final output of tester $\calB$ is $\frac{1}{s}\cdot\sum\limits_{j=1}^s\widehat\dist_{i_j}$.

To prove the correctness of the above tester, we first define $\dist_i:=\inf\limits_{g\in\calC}\dist_{\calD_i}(f,g)$ and $\dist:=\inf\limits_{g\in\calC}\dist_{\calD}(f,g)$. Note that $\dist=\sum\limits_{i=1}^mp_i\dist_{i}$. 

For every $1\leq i\leq m$, we further define $\dist_i'=\left\{\begin{array}{ll}\dist_i,&\text{if $p_i\geq\frac{\epsilon}{4m}$}\\0,&\text{if $p_i<\frac{\epsilon}{4m}$}\end{array}\right.$ and $\dist_i''=\left\{\begin{array}{ll}\dist_i,&\text{if $p_i\geq\frac{\epsilon}{4m}$}\\1,&\text{if $p_i<\frac{\epsilon}{4m}$}\end{array}\right.$. Then $\dist-\frac{\epsilon}{4}\leq \sum\limits_{i=1}^mp_i\dist'\leq\sum\limits_{i=1}^mp_i\dist''\leq\dist+\frac{\epsilon}{4}$. By the Chernoff Bound, $s=O(\frac{1}{\epsilon^2})$ is enough to make sure with probability at least $1-\frac{1}{9}$ that $\dist-\frac{\epsilon}{2}<\frac{1}{s}\sum\limits_{j=1}^s\dist'_{i_j}\leq\frac{1}{s}\sum\limits_{j=1}^s\dist''_{i_j}<\dist+\frac{\epsilon}{2}$.

Note that the unlabeled pool has size $O(\frac{mN}{\epsilon}\log s)$, which is enough to make sure that with probability at least $1-\frac{1}{9}$, for every $i_j$ with $p_{i_j}\geq\frac{\epsilon}{4m}$, there are enough ($O(N\log s)$) unlabeled examples lying in $X_{i_j}$. Therefore, with probability at least $(1-\frac{1}{9})(1-s\cdot\frac{1}{9s})\geq 1-\frac{2}{9}$, for all $i_j$ such that $p_{i_j}\geq \frac{p}{4m}$, it holds that $|\widehat \dist_{i_j}-\dist_{i_j}|\leq\frac{\epsilon}{2}$.

Finally, by the Union Bound, we know with probability at least $1-\frac{1}{3}$, it holds that $\dist-\epsilon<\frac{1}{s}\sum\limits_{j=1}^s\dist'_{i_j}-\frac{\epsilon}{2}\leq \frac{1}{s}\sum\limits_{j=1}^s\widehat\dist_{i_j}\leq\frac{1}{s}\sum\limits_{j=1}^s\dist''_{i_j}+\frac{\epsilon}{2}<\dist+\epsilon$.
\end{proof}
%Then, with probability at least $1-\frac{1}{6}$, we have $|\frac{1}{s}\sum\limits_{j=1}^s\dist_{i_j}'-\dist|<\frac{\epsilon}{2}$.
 %If $p_i<\frac{\delta}{2m}$ for some $i$, we conceptually re-define $\dist_i=0$. This will result in an additive change of $\dist_{\mathcal{D}}(f,\mathcal{C})$ by at most $\frac{\delta}{2}$. The algorithm first independently chooses $s=O(\frac{1}{\delta^2})$ indices $i_1,i_2,\cdots,i_s$ from $\{1,2,\cdots,m\}$ according to distribution $\{p_i\}_{1\leq i\leq m}$. By Chernoff Bound, with probability at least $\frac{5}{6}$, $|\dist_{\calD}(f,\mathcal{C})-\frac{1}{s}\sum\limits_{j=1}^s\dist_{i_j}|<\frac{\delta}{2}$. The algorithm then requests for $O(\frac{mU}{\delta}\log\frac{1}{\delta})$ ($O(mU\log\frac{1}{\delta})$ when all $p_i$'s are equal) unlabeled samples. By Chernoff Bound and Union Bound, with probability at least $\frac{5}{6}$, $\forall 1\leq j\leq s$, if $p_{i_j}\geq\frac{\delta}{2m}$, then there are $O(U\log\frac{1}{\delta})$ unlabeled samples in cluster $i_j$. Then, for every such $j$, with error probability at most $\frac{1}{6s}$, the algorithm can approximate $\dist_{i_j}$ by $\widehat{\dist}_{i_j}$within additive error $\epsilon$ using $O(q\log\frac{1}{\delta})$ queries on these $O(U\log\frac{1}{\delta})$ unlabeled samples. Finally, the algorithm outputs $\widehat{\dist}_{\calD}(f,\mathcal{C}):=\frac{1}{s}\sum\limits_{j=1}^s\widehat{\dist}_{i_j}$ as an approximation of $\dist_{\calD}(f,\mathcal{C})$. By Union Bound, the failure probability of the algorithm is bounded by $\frac{1}{6}+\frac{1}{6s}\cdot s=\frac{1}{3}$ and the total additive error is bounded by $\frac{\delta}{2}+\frac{\delta}{2}+\epsilon=\epsilon+\delta$.
