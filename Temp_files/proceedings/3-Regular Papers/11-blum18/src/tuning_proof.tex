\section{Proof of Theorem \ref{thm:bestk}}
\label{sec:bestkproof}
\begin{lemma}
\label{lm:bestk}
Suppose $k_1\leq k_2$ and the loss function $\loss(\cdot)$ is $L$-Lipschitz. Then, $|\loss_{k_1}-\loss_{k_2}|\leq L\cdot(1-\frac{k_1}{k_2})$.
\end{lemma}
\begin{proof}
When the test point $x$ is chosen, we use $x_1,x_2,\cdots,x_{k_2}$ to denote the closest $k_2$ points to $x$ in $S$, arranged in non-decreasing order of their distances to $x$. Each $x_i$ might be random because ties might be broken randomly. We use $e_i$ to denote $|f(x_i)-f(x)|$. Note that we have $\loss_{k_1}=\mathbb E_{x,x_1,x_2,\cdots,x_{k_1}}[\loss(\frac{1}{k_1}\sum\limits_{i=1}^{k_1}e_i)]$ and $\loss_{k_2}=\mathbb E_{x,x_1,x_2,\cdots,x_{k_2}}[\loss(\frac{1}{k_2}\sum\limits_{i=1}^{k_2}e_i)]$. Therefore,
\begin{equation}
\begin{split}
&|\loss_{k_1}-\loss_{k_2}|\\
\leq &\mathbb E_{x,x_1,x_2,\cdots,x_{k_2}}[|\loss(\frac{1}{k_1}\sum\limits_{i=1}^{k_1}e_i)-\loss(\frac{1}{k_2}\sum\limits_{i=1}^{k_2}e_i)|]\\
\leq &L\cdot \mathbb E_{x,x_1,x_2,\cdots,x_{k_2}}[|\frac{1}{k_1}\sum\limits_{i=1}^{k_1}e_i-\frac{1}{k_2}\sum\limits_{i=1}^{k_2}e_i|]\\
= &L\cdot \mathbb E_{x,x_1,x_2,\cdots,x_{k_2}}[|(\frac{1}{k_1}-\frac{1}{k_2})\sum\limits_{i=1}^{k_1}e_i-\frac{1}{k_2}\sum\limits_{i=k_1+1}^{k_2}e_i|]\\
\leq &L\cdot \mathbb E_{x,x_1,x_2,\cdots,x_{k_2}}[\max\{(\frac{1}{k_1}-\frac{1}{k_2})\sum\limits_{i=1}^{k_1}e_i,\frac{1}{k_2}\sum\limits_{i=k_1+1}^{k_2}e_i\}]\\
\leq &L\cdot\max\{(\frac{1}{k_1}-\frac{1}{k_2})\cdot k_1,\frac{1}{k_2}\cdot(k_2-k_1)\}\\
=&L\cdot(1-\frac{k_1}{k_2})
\end{split}
\end{equation}
\end{proof}
\begin{proof}(of Theorem \ref{thm:bestk})
If we apply Lemma \ref{lm:bestk} to $p$th-power loss, which is $p$-Lipschitz, we know for any $1\leq \frac{k_2}{k_1}\leq \frac{p}{p-\epsilon}$, it holds that $|\loss_{k_1}-\loss_{k_2}|\leq\epsilon$. If we define $t=\lfloor\log_{\frac{p}{p-\frac{\epsilon}{3}}}N\rfloor,k_{2i}=\lfloor(\frac{p}{p-\frac{\epsilon}{3}})^i\rfloor,k_{2i+1}=\lceil(\frac{p}{p-\frac{\epsilon}{3}})^i\rceil$ for $i=0,1,2,\cdots,t$, then we know $\exists 0\leq i\leq 2t+1$ such that $k_i$ is $\frac{\epsilon}{3}$-approximately-best. By Theorem \ref{thm:pth}, we can estimate $\loss_{k_i}$ for every $0\leq i\leq 2t+1$ up to an additive error $\frac{\epsilon}{3}$ using $O(\frac{pt\log t}{\epsilon^2})$ queries on $N+O(\frac{t\log t}{\epsilon^2})$ unlabeled examples.\footnote{Repeat the estimator $O(\log t)$ times and take the median to boost its success probability to $1-O(\frac{1}{t})$.} The $k_i$ yielding the smallest approximation of $\loss_{k_i}$ is $\epsilon$-approximately-best. Note that $t=O(\frac{p\log N}{\epsilon})$, so the query complexity is $O(\frac{p^2\log N}{\epsilon^3}(\log\log N+\log p+\log\frac{1}{\epsilon}))$ and the unlabeled sample complexity is $N+O(\frac{p\log N}{\epsilon^3}(\log\log N+\log p+\log\frac{1}{\epsilon}))$.
\end{proof}
