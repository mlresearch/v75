\section{Introduction}
Suppose you are about to embark on a project to label a large quantity of data, such as medical images or street scenes.  Your intent is to then feed this data into your favorite learning algorithm for, say, a medical diagnosis or robotic car application.  Before embarking on this project, can you, from just a few (perhaps well-chosen) labels, estimate {\em how well} your algorithm can be expected to perform when trained on the large sample?   Here, ``few'' should mean much less than the number of labeled examples needed for learning, and in particular we will be interested in cases where we can do this with a number of labels that {\em does not depend on the complexity of the target function}.
We consider this question in two related  contexts.  

\paragraph{Tolerant testing:} Here, the goal is to approximate the distance of a target function $f$ to a hypothesis class $\calC$.  Specifically, consider a hypothesis class $\calC$ of VC-dimension $d$, where $d$ should be thought of as large.  (We will generally think of $d$ as large and $\epsilon$ as constant.)   If we wish to {\em find} an $\epsilon$-best hypothesis in $\calC$, we will need roughly $O(d/\epsilon^2)$ labeled examples.
% in the (realizable) case that $f \in \calC$, or $O(d/\epsilon^2)$ labeled examples in the (agnostic) case that $f \not\in \calC$.   
However, if we just want to estimate what its error rate is without actually finding it, can we do this from less data? 


%There are two scenarios in which we want to know the accuracy of a learning algorithm beforehand. The first one is when the algorithm does Empirical Risk Minimization over a hypothesis class, where the bottleneck for accuracy is the best function in the underlying hypothesis class. The second one is when the bottleneck for accuracy is the limited amount of data.

The ``realizable'' version of this question 
%problem of determining {\em whether} one will be able to learn well using a given hypothesis class $\calC$ using substantially  less labeled data than needed to actually (attempt to) learn well using that class  
is the problem of {\em passive} and {\em active} property testing, studied by \citet{KR98} and \citet{BBBY12}.  That work considers the problem of distinguishing (a) the case that the target function $f$ belongs to class $\calC$ from (b) the case that the target function $f$ is $\epsilon$-far from any concept in $\calC$ with respect to the underlying data distribution $\calD$.   For instance, suppose our data consists of points $x$ on the real line, labeled by $f$ as positive or negative, and we are interested in learning using the class $\calC$ consisting of unions of $d$ intervals.  This class has VC-dimension $2d$ and so would require $\Omega(d)$ labeled examples to learn.  However, \citet{BBBY12} show that in the active testing framework (one can sample $poly(d)$ {\em unlabeled} examples for free and then query for the labels of a small number of those examples), one can solve the testing problem using only a constant number of label queries (when $\epsilon$ is constant), independent of $d$.

One limitation of these results, however, is that they do not guarantee to give a meaningful answer when the target function is ``almost'' in the  class $\calC$.  For instance, suppose $f$ can be perfectly described by a union of 10,000 intervals but is $\epsilon/2$-close to a union of 100 intervals.  Then we would like a tester that can say ``good enough'' at $d=100$ rather than telling us that we need $d=10,000$.  The tester of \citet{BBBY12}, unfortunately, seems to require $f$ to be $O(\epsilon^3)$-close to a union of $d$ intervals in order to guarantee an output of YES, which is much less than $\epsilon$.

In this work, we give algorithms for such {\em tolerant testing} \citep{PRR06} for the case of unions of intervals and a few related classes.  We can distinguish the case that the best function in $\calC$ has error rate $\geq 2\epsilon$ from the case that the best function in $\calC$ has error rate $\leq \epsilon$, and more generally we can estimate the error rate $\alpha$ of the best function in the class up to $\pm \epsilon$.  Thus, for the first time, from a small number of label queries, we can solve the property-testing analog of the notion of agnostic learning.

One point we wish to make up front: while the classes of functions we consider are fairly simple, such as unions of intervals on the line, we are operating in a challenging model.  We would like algorithms that work for any (unknown) underlying data distribution $\calD$, not just the uniform distribution, {\em and} we want algorithms that only query for labels  from among examples seen in a poly-sized sample of unlabeled data drawn from $\calD$ rather than querying arbitrary points in the input space.  These are important conditions for being able to use property testing for machine learning problems.  

\paragraph{Algorithm estimation:}
The second context we consider is that we are given a learning algorithm $\calA$ and a large unlabeled sample $S$ of $N$ examples drawn from distribution $\calD$.  If we were to label all $N$ examples of $S$ and feed them into algorithm $\calA$, then $\calA$ would produce some hypothesis (call it $h_S$) with some error rate $\alpha$.   What we would like to do is, by labeling only very few examples in $S$, and perhaps a few additional examples drawn from $\calD$, to estimate the value of $\alpha$ (so that we can determine whether our project of labeling all examples in $S$ is worthwhile).   

To get a feel for this problem, one algorithm $\calA$ for which this task is easy is 1-Nearest Neighbor (1-NN).  This algorithm would produce a hypothesis $h_S$ that on any given query point $x$ predicts the label of the example $x' \in S$ that is nearest to $x$.  For this algorithm, we can easily estimate the error rate of $h_S$ from just a few label queries by repeatedly drawing a random $x$ from $\calD$, finding the point $x' \in S$ that is closest to $x$, and then requesting the labels of $x$ and $x'$ to see if they agree.   We only need to repeat this process $O(1/\epsilon^2)$ times in order to estimate the error rate of $h_S$ to $\pm \epsilon$.  This works because $h_S$ is constructed, and makes predictions, in a very local way.\footnote{In contrast, note that estimating the error rate of this algorithm could require a large labeled sample if we only \emph{passively} receive labeled examples.  Specifically, suppose the distribution $\calD$ is uniform over $c$ clusters and the 1-NN algorithm aims to use $N = c\log\frac{c}{\delta}$ examples, so that with probability at least $1-\delta$, every cluster has at least one training example in it.  We want to distinguish two cases: either every cluster is pure but random so the error rate is roughly 0, or every cluster is 50/50 so the error rate is roughly $\frac{1}{2}$. To distinguish these cases, the estimator needs to see at least two labels in the same cluster, implying an $\Omega(\sqrt c)=\Omega(\sqrt {N/\log N})$ passive sample size lower bound.}  In this work, we extend this to different forms of $k$-Nearest Neighbor algorithms, where the prediction on some point $x$ depends on the $k$ nearest examples in $S$, developing estimators for which the number of queries {\em does not depend on $k$}.    This then allows us to use this for {\em hyperparameter tuning}: determining the (approximately) best value of $k \in \{1, \ldots, N\}$ for the given application.

%Moved details to the ``our results'' section
\label{sec:intro}