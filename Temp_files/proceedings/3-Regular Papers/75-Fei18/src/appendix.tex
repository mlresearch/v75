
%\section*{Acknowledgement}

%Y. Fei and Y. Chen were partially supported by the National Science
%Foundation CRII award 1657420 and grant 1704828.


\appendix
%\appendixpage

\section{Additional notations}

We define the shorthand $\error\coloneqq\norm[\Yhat-\Ystar]1$. For
a matrix $\M$, we write $\norm[\M]{\infty}\coloneqq\max_{i,j}\left|M_{ij}\right|$
as its entry-wise $\ell_{\infty}$ norm, and $\opnorm{\M}$ as its
spectral norm (maximum singular value).  We let $\I$ and $\OneMat$
be the $\num\times\num$ identity matrix and all-one matrix, respectively.
For a real number $x$, $\left\lceil x\right\rceil $ denotes its
ceiling. We denote by $\clustset a\coloneqq\left\{ i\in\left[\num\right]:\labelstar(i)=a\right\} $
the set of indices of points in cluster $a$, and we define $\size\coloneqq\left|\clustset a\right|=\frac{\num}{\numclust}$. 

\section{Proof of Theorem \ref{thm:ip_sdp}\label{sec:proof_ip_sdp}}

\subsection{Initial steps}

We assume $\error>0$ since otherwise we are done. We can write $\Adj=\C+\C\t-2\H\H\t$,
where $\H$ is a matrix whose $i$-th row is the point $\h_{i}$ and
$\C$ is a matrix where the entries in the $i$-th row are identical
and equal to $\norm[\h_{i}]2^{2}$. Since the row-sum constraint in
the program (\ref{eq:SDP1}) ensures that the matrix $\Yhat-\Ystar$
has zero row sum, we have $\left\langle \Yhat-\Ystar,\C\right\rangle =\left\langle \Yhat-\Ystar,\C\t\right\rangle =0$
which implies $\left\langle \Yhat-\Ystar,\C+\C\t\right\rangle =0$.

Let $\G\coloneqq\H-\E\H$ be a matrix of entries in $\H$ with their
means removed. We can compute
\begin{align*}
\H\H\t & =\left(\G+\E\H\right)\left(\G+\E\H\right)\t\\
 & =\G\G\t+\G\left(\E\H\right)\t+\left(\E\H\right)\G\t+\left(\E\H\right)\left(\E\H\right)\t
\end{align*}
and 
\[
\E\H\H^{\top}=\E\G\G\t+\left(\E\H\right)\left(\E\H\right)\t.
\]
Therefore 
\[
\H\H\t-\E\H\H^{\top}=\left(\G\G\t-\E\G\G\t\right)+\G\left(\E\H\right)\t+\left(\E\H\right)\G\t.
\]
Let $\U\in\real^{\num\times\numclust}$ be the matrix of the left
singular vectors of $\Ystar$. For any $\M\in\real^{\num\times\num}$,
define the projection $\PT\left(\M\right)\coloneqq\U\U\t\M+\M\U\U\t-\U\U\t\M\U\U\t$
and its orthogonal complement $\PTperp\left(\M\right)\coloneqq\M-\PT\left(\M\right)$.
The fact that $\Yhat$ is optimal and $\Ystar$ is feasible to the
program (\ref{eq:SDP1}) implies 
\begin{align*}
0 & \leq-\frac{1}{2}\left\langle \Yhat-\Ystar,\Adj\right\rangle \\
 & =\left\langle \Yhat-\Ystar,\H\H\t-\E\H\H^{\top}\right\rangle +\left\langle \Yhat-\Ystar,\E\H\H^{\top}\right\rangle \\
 & =\left\langle \Yhat-\Ystar,\G\G\t-\E\G\G\t+\G\left(\E\H\right)\t+\left(\E\H\right)\G\t\right\rangle +\left\langle \Yhat-\Ystar,\E\H\H^{\top}\right\rangle \\
 & =\left\langle \Yhat-\Ystar,\PT\left(\G\G\t-\E\G\G\t\right)\right\rangle +\left\langle \Yhat-\Ystar,\PTperp\left(\G\G\t-\E\G\G\t\right)\right\rangle \\
 & \quad+2\left\langle \Yhat-\Ystar,\G\left(\E\H\right)\t\right\rangle +\left\langle \Yhat-\Ystar,\E\H\H^{\top}\right\rangle \\
 & \eqqcolon S_{1}+S_{2}+2S_{3}+S_{4}.
\end{align*}
We may control $S_{1}$, $S_{2}$ and $S_{4}$ using the following. 
\begin{prop}
\label{prop:S1} If $\snr^{2}\geq C\left(\sqrt{\frac{\numclust\vecdim}{\num}\log\left(\num\numclust\right)}+\sqrt{\frac{\numclust}{\num}}\log\left(\num\numclust\right)\right)$
for some universal constant $C>0$, then $S_{1}\leq\frac{1}{100}\minsep^{2}\error$
with probability at least $1-\left(2\num\right)^{-2}$.
\end{prop}

\begin{prop}
\label{prop:S2} If $\snr^{2}\geq C\numclust\left(\sqrt{\frac{\vecdim}{\num}}+1\right)$
for some universal constant $C>0$, then $S_{2}\leq\frac{1}{100}\minsep^{2}\error$
with probability at least $1-2e^{-\num}$.
\end{prop}

\begin{prop}
\label{prop:S4} We have $S_{4}=-\frac{1}{2}\sum_{a\ne b}T_{ab}\minsep_{ab}^{2}\le-\frac{1}{4}\minsep^{2}\error$
where $T_{ab}\coloneqq\sum_{i\in\clustset a,j\in\clustset b}\left(\Yhat-\Ystar\right)_{ij}$. 
\end{prop}
The proofs are given in Sections \ref{sec:proof_S1}, \ref{sec:proof_S2}
and \ref{sec:proof_S4}, respectively. Combining the above propositions,
we have $S_{1}+S_{2}\le-\frac{1}{2}S_{4}$ and therefore 
\begin{equation}
0\leq S_{3}+\frac{1}{4}S_{4}\eqqcolon S_{0}\label{eq:error_S3_bound}
\end{equation}
with probability at least $1-\left(2\num\right)^{-C'}-2e^{-\num}$
for some universal constant $C'>0$.

Let $\B\coloneqq\Yhat-\Ystar$. We have 
\begin{align*}
S_{3} & =\sum_{j}\sum_{a}\sum_{i\in C_{a}}B_{ji}\left\langle \Mean_{a},\g_{j}\right\rangle \\
 & =\size\sum_{j}\sum_{a}\left\langle \Mean_{a},\g_{j}\right\rangle \left(\frac{1}{\size}\sum_{i\in\clustset a}B_{ji}\right)\\
 & =\size\sum_{j}\sum_{a\ne\labelstar(j)}\left\langle \Mean_{a}-\Mean_{\labelstar(j)},\g_{j}\right\rangle \left(\frac{1}{\size}\sum_{i\in\clustset a}B_{ji}\right)
\end{align*}
where the last step holds since $\sum_{a\ne\labelstar(j)}\left(\sum_{i\in\clustset a}B_{ji}\right)=-\sum_{i\in\clustset a:a=\labelstar(j)}B_{ji}$
for each $j\in\left[\num\right]$ which follows from the row-sum constraint
of program (\ref{eq:SDP1}). By Proposition \ref{prop:S4}, we have
\begin{align*}
S_{4} & =-\size\sum_{j}\sum_{a\ne\labelstar(j)}\frac{1}{2}\minsep_{\labelstar(j),a}^{2}\left(\frac{1}{\size}\sum_{i\in\clustset a}B_{ji}\right).
\end{align*}
Therefore, we have 
\[
S_{0}=\size\sum_{j}\sum_{a\ne\labelstar(j)}\left(\left\langle \Mean_{a}-\Mean_{\labelstar(j)},\g_{j}\right\rangle -c\minsep_{\labelstar(j),a}^{2}\right)\left(\frac{1}{\size}\sum_{i\in\clustset a}B_{ji}\right)
\]
where $c=\frac{1}{8}$.

To control $S_{0}$, we define $\beta_{ja}\coloneqq\left\langle \Mean_{a}-\Mean_{\labelstar(j)},\g_{j}\right\rangle -c\minsep_{\labelstar(j),a}^{2}$
and consider the program 
\begin{align}
\max_{\X}\  & \sum_{j}\sum_{a\ne\labelstar(j)}\beta_{ja}X_{ja}\nonumber \\
\text{s.t.}\  & 0\leq X_{ja}\leq1,\qquad\forall a\ne\labelstar(j),j\in\left[\num\right]\nonumber \\
 & \sum_{a\ne\labelstar(j)}X_{ja}\leq1,\qquad\forall j\in\left[\num\right]\label{eq: int_opt}\\
 & \sum_{j}\sum_{a\ne\labelstar(j)}X_{ja}=R,\nonumber 
\end{align}
where $R\in(0,\num]$. Let us denote by $V(R)$ the optimal value
of the above program and we let $V(R)=-\infty$ if the program is
infeasible. The constraints of program (\ref{eq:SDP1}) implies that
$\frac{\error}{2\size}\in(0,\num]$ and 
\[
\sum_{j\in\left[\num\right]}\sum_{a\ne\labelstar(j)}\left(\sum_{i\in\clustset a}B_{ji}\right)=\frac{\error}{2}.
\]
Hence, by Equation (\ref{eq:error_S3_bound}), we have 
\begin{equation}
0\leq S_{0}\leq\size\cdot V\left(\frac{\error}{2\size}\right).\label{eq:basic_ineq_upper_bound_V}
\end{equation}


\subsection{Controlling $\protect\error$ by LP}

We show that $\error$ is upper bounded by the objective value of
an LP that is related to program (\ref{eq: int_opt}). If $\error=0$
then the conclusion of Theorem \ref{thm:ip_sdp} holds trivially.
For $\error>0$, we have the following cases:
\begin{enumerate}
\item If $\frac{\error}{2\size}\in(0,1]$, it follows from Equation (\ref{eq:basic_ineq_upper_bound_V})
that the error $\error$ must satisfy 
\[
0\le V\left(\frac{\error}{2\size}\right)=\beta^{*}\frac{\error}{2\size}\le\beta^{*}\left\lceil \frac{\error}{2\size}\right\rceil =V\left(\left\lceil \frac{\error}{2\size}\right\rceil \right)
\]
where $\beta^{*}\coloneqq\max_{j\in\left[\num\right],a\ne\labelstar(j)}\beta_{ja}$.
This implies 
\begin{align*}
\frac{\error}{2\size}\le\left\lceil \frac{\error}{2\size}\right\rceil  & \le\max\left\{ R\in\{0,1,.\ldots\}:V(R)\ge0\right\} .
\end{align*}
\item If $\frac{\error}{2\size}>1$, it follows from Equation (\ref{eq:basic_ineq_upper_bound_V})
that the error $\error$ must satisfy 
\[
0\le V\left(\frac{\error}{2\size}\right)\le\max\left\{ V\left(\left\lceil \frac{\error}{2\size}\right\rceil \right),V\left(\left\lfloor \frac{\error}{2\size}\right\rfloor \right)\right\} =\max\left\{ V\left(\left\lceil \frac{\error}{2\size}\right\rceil \right),V\left(\left\lceil \frac{\error}{2\size}\right\rceil -1\right)\right\} .
\]
In other words, we have
\begin{align*}
\frac{\error}{2\size}\le\left\lceil \frac{\error}{2\size}\right\rceil  & \le\max\left\{ R\in\{0,1,.\ldots\}:V(R)\vee V(R-1)\ge0\right\} \\
 & =1+\max\left\{ R\in\{0,1,.\ldots\}:V(R)\ge0\right\} .
\end{align*}
Note that $\left\lceil \frac{\error}{2\size}\right\rceil \ge2$, and
therefore we must have $1\le\max\left\{ R\in\{0,1,.\ldots\}:V(R)\ge0\right\} $.
This implies 
\[
\frac{\error}{2\size}\le2\max\left\{ R\in\{0,1,.\ldots\}:V(R)\ge0\right\} .
\]
\end{enumerate}
Consequently, we have 
\[
\frac{\error}{2\size}\le2\max\left\{ R\in\{0,1,.\ldots\}:V(R)\ge0\right\} .
\]


\subsection{Converting LP to IP}

We are now ready to formally establish a connection between the error
of the SDP (\ref{eq:SDP1}) and that of the Oracle IP (\ref{eq:oracleIP}),
by relating $\max\left\{ R\in\{0,1,.\ldots\}:V(R)\ge0\right\} $ to
the quantity (\ref{eq:IPerror}). Note that if $R\ge0$ is an integer,
then there exists an optimal solution $\left\{ w_{ja}\right\} $ of
program (\ref{eq: int_opt}) such that $w_{ja}\in\{0,1\}$ for all
$j\in[\num],a\in[\numclust]$. Therefore, if $R\in\{0,1,\ldots\}$
is an integer, then 
\begin{equation}
V(R)=\IP_{1}(R)\coloneqq\left\{ \begin{aligned}\max_{\X}\  & \sum_{j}\sum_{a\ne\labelstar(j)}\beta_{ja}X_{ja}\\
\text{s.t.}\  & X_{ja}\in\{0,1\},\qquad\forall a\ne\labelstar(j),j\in\left[\num\right]\\
 & \sum_{a\ne\labelstar(j)}X_{ja}\leq1,\qquad\forall j\in\left[\num\right]\\
 & \sum_{j}\sum_{a\ne\labelstar(j)}X_{ja}=R
\end{aligned}
\right\} .\label{eq:IP1}
\end{equation}
Combining the last two display equations we obtain that
\begin{align}
\frac{\error}{2\size} & \le2\max\left\{ R\in\{0,1,.\ldots\}:\IP_{1}(R)\ge0\right\} \nonumber \\
 & \overset{}{=}2\cdot\left\{ \begin{aligned}\max_{R,\X}\; & R\\
\text{s.t.}\; & R\in\{0,1,\ldots\}\\
 & \sum_{j}\sum_{a\ne\labelstar(j)}\beta_{ja}X_{ja}\ge0\\
 & X_{ja}\in\{0,1\},\qquad\forall a\ne\labelstar(j),j\in\left[\num\right]\\
 & \sum_{a\ne\labelstar(j)}X_{ja}\leq1,\qquad\forall j\in\left[\num\right]\\
 & \sum_{j}\sum_{a\ne\labelstar(j)}X_{ja}=R,
\end{aligned}
\right\} \nonumber \\
 & =2\cdot\IP_{2}\coloneqq2\cdot\left\{ \begin{aligned}\max_{\X}\; & \sum_{j}\sum_{a\ne\labelstar(j)}X_{ja}\\
\text{s.t.}\; & \sum_{j}\sum_{a\ne\labelstar(j)}\beta_{ja}X_{ja}\ge0\\
 & X_{ja}\in\{0,1\},\qquad\forall a\ne\labelstar(j),j\in\left[\num\right]\\
 & \sum_{a\ne\labelstar(j)}X_{ja}\leq1,\qquad\forall j\in\left[\num\right]
\end{aligned}
\right\} .\label{eq:error_bound2}
\end{align}

Let us reparameterize the integer program $\IP_{2}$ by a change of
variable. Recall that 
\[
\mathcal{F}\coloneqq\left\{ \F\in\{0,1\}^{\num\times\numclust}:\F\one_{\numclust}=\one_{\num}\right\} 
\]
is the set of all possible assignment matrices and $\F^{*}\in\mathcal{F}$
is the true assignment matrix; that is, $F_{ja}^{*}=\indic\left\{ a=\labelstar(j)\right\} $
for all $j\in[\num],a\in[\numclust]$. Consider any feasible solution
$\X$ of $\IP_{2}$; here for each $j\in[\num]$, we may fix $X_{j,\labelstar(j)}=-\sum_{a\neq\labelstar(j)}X_{ja}$
\textemdash{} doing so does not affect the feasibility and objective
value of $\X$ w.r.t. $\IP_{2}$. Define the new variable $\F\coloneqq\F^{*}+\X\in\mathcal{F}$.
The objective value and constraints of the old variable $\X$ can
be mapped to those of $\F$; in particular, we have 
\begin{align*}
\sum_{j}\sum_{a\ne\labelstar(j)}X_{ja} & =\sum_{j}\sum_{a\ne\labelstar(j)}(F_{ja}-F_{ja}^{*})=\frac{1}{2}\norm[\F-\F^{*}]1
\end{align*}
and
\begin{align*}
\left.\begin{array}{c}
X_{ja}\in\{0,1\},\forall a\ne\labelstar(j),j\in\left[\num\right]\\
\sum_{a\ne\labelstar(j)}X_{ja}\leq1,\forall j\in\left[\num\right]\\
X_{j,\labelstar(j)}=-\sum_{a\neq\labelstar(j)}X_{ja},\forall j\in[\num]
\end{array}\right\}  & \Longleftrightarrow\F\in\mathcal{F}
\end{align*}
and
\[
\sum_{j}\sum_{a\ne\labelstar(j)}\beta_{ja}X_{ja}\overset{(i)}{=}\sum_{j}\sum_{a}\beta_{ja}X_{ja}=\sum_{j}\sum_{a}\beta_{ja}F_{ja}-\sum_{j}\sum_{a}\beta_{ja}F_{ja}^{*}\overset{(ii)}{=}\sum_{j}\sum_{a}\beta_{ja}F_{ja},
\]
where steps $(i)$ and $(ii)$ both follow from the fact that $\beta_{j,\labelstar(j)}=0,\forall j.$
It follows that $\IP_{2}$ has the same optimal value as a corresponding
integer program in terms of $\X$; in particular, we have
\[
\IP_{2}=\IP_{3}\coloneqq\left\{ \begin{aligned}\max_{\F}\; & \frac{1}{2}\norm[\F-\F^{*}]1\\
\text{s.t.}\; & \sum_{j}\sum_{a}\beta_{ja}F_{ja}\ge0\\
 & \F\in\mathcal{F}
\end{aligned}
\right\} .
\]
Combining with equation (\ref{eq:error_bound2}), we see that the
error $\error$ satisfies
\begin{equation}
\frac{\error}{2\size}\le2\cdot\IP_{3}.\label{eq:error_bound3}
\end{equation}

We further simplify the first constraint in $\IP_{3}$. Recall that
$\bar{\h}_{i}\coloneqq\Mean_{\labelstar(i)}+(2c)^{-1}\g_{i}$ for
each $i\in[\num]$. Note that $\left\{ \bar{\h}_{i}\right\} $ can
be viewed as data points generated from the Sub-Gaussian Mixture Model
but with $(2c)^{-1}$ times the standard deviation. By definition
of $\beta_{ja}$, we have 
\begin{align*}
\beta_{ja} & =\left\langle \Mean_{a}-\Mean_{\labelstar(j)},\g_{j}\right\rangle -c\minsep_{\labelstar(j),a}^{2}\\
 & =c\left(2\left\langle \Mean_{a}-\Mean_{\labelstar(j)},(2c)^{-1}\g_{j}\right\rangle -\minsep_{\labelstar(j),a}^{2}\right)\\
 & =c\left(2\left\langle \Mean_{a}-\Mean_{\labelstar(j)},(2c)^{-1}\g_{j}\right\rangle -\norm[\Mean_{a}-\Mean_{\labelstar(j)}]2^{2}\right)\\
 & =c\left(2\left\langle \Mean_{a}-\Mean_{\labelstar(j)},(2c)^{-1}\g_{j}\right\rangle -\norm[\Mean_{a}-\Mean_{\labelstar(j)}]2^{2}-\norm[(2c)^{-1}\g_{j}]2^{2}+\norm[(2c)^{-1}\g_{j}]2^{2}\right)\\
 & =c\left(-\norm[\Mean_{\labelstar(j)}-\Mean_{a}+(2c)^{-1}\g_{j}]2^{2}+\norm[(2c)^{-1}\g_{j}]2^{2}\right)\\
 & =c\left(-\norm[\bar{\h}_{j}-\Mean_{a}]2^{2}+\norm[(2c)^{-1}\g_{j}]2^{2}\right).
\end{align*}
For any $\F\in\mathcal{F}$, we then have
\begin{align*}
\sum_{j}\sum_{a}\beta_{ja}F_{ja} & =c\sum_{j}\sum_{a}\left(-\norm[\bar{\h}_{j}-\Mean_{a}]2^{2}+\norm[(2c)^{-1}\g_{j}]2^{2}\right)F_{ja}\\
 & =c\left(-\sum_{j}\sum_{a}\norm[\bar{\h}_{j}-\Mean_{a}]2^{2}F_{ja}+\sum_{j}\norm[(2c)^{-1}\g_{j}]2^{2}\sum_{a}F_{ja}\right)\\
 & \overset{(i)}{=}c\left(-\sum_{j}\sum_{a}\norm[\bar{\h}_{j}-\Mean_{a}]2^{2}F_{ja}+\sum_{j}\norm[(2c)^{-1}\g_{j}]2^{2}\sum_{a}F_{ja}^{*}\right)\\
 & =c\left(-\sum_{j}\sum_{a}\norm[\bar{\h}_{j}-\Mean_{a}]2^{2}F_{ja}+\sum_{j}\sum_{a}\norm[(2c)^{-1}\g_{j}]2^{2}F_{ja}^{*}\right)\\
 & =c\left(-\sum_{j}\sum_{a}\norm[\bar{\h}_{j}-\Mean_{a}]2^{2}F_{ja}+\sum_{j}\sum_{a}\norm[\bar{\h}_{j}-\Mean_{\labelstar(j)}]2^{2}F_{ja}^{*}\right)\\
 & \overset{(ii)}{=}c\left(-\sum_{j}\sum_{a}\norm[\bar{\h}_{j}-\Mean_{a}]2^{2}F_{ja}+\sum_{j}\sum_{a}\norm[\bar{\h}_{j}-\Mean_{a}]2^{2}F_{ja}^{*}\right),
\end{align*}
where step $(i)$ holds because $\sum_{a}F_{ja}=1=\sum_{a}F_{ja}^{*},\forall j$,
and step $(ii)$ holds because $F_{ja}^{*}=1$ only if $a=\labelstar(j)$.
Again recall the shorthand
\[
\eta(\F)\coloneqq\sum_{j}\sum_{a}\norm[\bar{\h}_{j}-\Mean_{a}]2^{2}F_{ja}.
\]
We have the more compact expression
\begin{equation}
\sum_{j}\sum_{a}\beta_{ja}F_{ja}=c\left(\eta(\F^{*})-\eta(\F)\right)\label{eq:eta_func_equivalence}
\end{equation}
It follows that for any $\F\in\mathcal{F}$, the first constraint
in $\IP_{3}$ is satisfied if and only if 
\[
\eta(\F)\le\eta(\F^{*}).
\]
Combining with the (\ref{eq:error_bound3}), we obtain that
\[
\frac{\error}{2\size}\le2\cdot\IP_{3}=2\cdot\left\{ \begin{aligned}\max_{\F}\; & \frac{1}{2}\norm[\F-\F^{*}]1\\
\text{s.t.}\; & \eta(\F)\le\eta(\F^{*})\\
 & \F\in\mathcal{F}
\end{aligned}
\right\} .
\]
Rearranging terms, we have the bound
\begin{equation}
\error\le2\size\cdot\max\left\{ \norm[\F-\F^{*}]1:\eta(\F)\le\eta(\F^{*}),\F\in\mathcal{F}\right\} .\label{eq:error_bound3a}
\end{equation}
The result follows from the fact that $\norm[\Ystar]1=\num\size$
and $\norm[\F^{*}]1=\num$. 

\subsection{Proof of Proposition \ref{prop:S1} \label{sec:proof_S1}}

In this section we control $S_{1}$. We can further decompose $S_{1}$
as 
\begin{align*}
S_{1} & =\left\langle \Yhat-\Ystar,\U\U\t\left(\G\G\t-\E\G\G\t\right)\right\rangle +\left\langle \Yhat-\Ystar,\left(\G\G\t-\E\G\G\t\right)\U\U\t\right\rangle \\
 & \qquad-\left\langle \Yhat-\Ystar,\U\U\t\left(\G\G\t-\E\G\G\t\right)\U\U\t\right\rangle \\
 & \leq2\left|\left\langle \Yhat-\Ystar,\U\U\t\left(\G\G\t-\E\G\G\t\right)\right\rangle \right|+\left|\left\langle \Yhat-\Ystar,\U\U\t\left(\G\G\t-\E\G\G\t\right)\U\U\t\right\rangle \right|\\
 & \eqqcolon2T_{1}+T_{2}
\end{align*}
By the generalized Holder's inequality, we have 
\begin{align*}
T_{1} & \leq\error\cdot\norm[\U\U\t\left(\G\G\t-\E\G\G\t\right)]{\infty}
\end{align*}
and 
\begin{align*}
T_{2} & =\left|\left\langle \Yhat-\Ystar,\U\U\t\left(\G\G\t-\E\G\G\t\right)\U\U\t\right\rangle \right|\\
 & =\left|\left\langle \left(\Yhat-\Ystar\right)\U\U\t,\U\U\t\left(\G\G\t-\E\G\G\t\right)\right\rangle \right|\\
 & \leq\error\cdot\norm[\U\U\t\left(\G\G\t-\E\G\G\t\right)]{\infty}
\end{align*}
where the last inequality holds since 
\[
\norm[\left(\Yhat-\Ystar\right)\U\U\t]1\leq\norm[\Yhat-\Ystar]1=\error.
\]
Combining the above, we have 
\[
S_{1}\leq3\error\cdot\norm[\U\U\t\left(\G\G\t-\E\G\G\t\right)]{\infty}.
\]

Note that there are $m=\num\numclust$ distinct random variables in
$\U\U\t\left(\G\G\t-\E\G\G\t\right)$ and let us call them $X_{1},\ldots,X_{m}$.
For each $i$, we can see that $X_{i}$ is the average of $\size$
entries in $\G\G\t-\E\G\G\t$ and we let $\B_{i}$ be an $\num\times\num$
matrix with $\size$ entries equal to 1 and the others equal to 0
such that $\size X_{i}=\left\langle \B_{i},\G\G\t-\E\G\G\t\right\rangle $.
To proceed, we need the Hanson-Wright inequality (an extension of
Exercise 6.2.7 on pp.$\ $140 in \citet{vershynin2017high}).
\begin{lem}[Higher-dimensional Hanson-Wright inequality]
\emph{ \label{lem:hanson-wright} }Let $\x_{1},\ldots,\x_{N}$ be
independent, mean zero, sub-Gaussian random vectors in $\real^{M}$.
Let $\B$ be an $N\times N$ matrix. For every $t\geq0$ and some
universal constant $c>0$, we have 
\[
\P\left[\left|\sum_{i,j}B_{ij}\left\langle \x_{i},\x_{j}\right\rangle -\E\sum_{i,j}B_{ij}\left\langle \x_{i},\x_{j}\right\rangle \right|\geq t\right]\leq4\exp\left[-c\min\left(\frac{t^{2}}{K^{4}M\norm[\B]F^{2}},\frac{t}{K^{2}\norm[\B]{}}\right)\right]
\]
where $K\coloneqq\max_{i}\norm[\x_{i}]{\psi_{2}}.$ 
\end{lem}
The proof is given in Section \ref{sec:proof_hanson_wright}. Using
Lemma \ref{lem:hanson-wright}, we see that for any $t\ge0$ 
\[
\P\left\{ \size X_{i}\geq t\right\} =\P\left\{ \left\langle \B_{i},\G\G\t-\E\G\G\t\right\rangle \geq t\right\} \leq4\exp\left[-c\min\left(\frac{t^{2}}{K^{4}\vecdim\size},\frac{t}{K^{2}\sqrt{\size}}\right)\right].
\]
We can choose $t^{*}=DK^{2}\sqrt{\size}\left(\sqrt{\vecdim\log m}+\log m\right)$
with $K=\sgnorm$ and $D>0$ a universal constant. Apply the union
bound, we have 
\[
S_{1}\leq3\error\cdot\frac{1}{\size}\cdot t^{*}
\]
with probability at least $1-m\cdot\P\left\{ \size X\geq t\right\} \geq1-\exp\left(-C'\log m\right)=1-m^{-C'}$
where $C'>0$ is a universal constant. The result follows from the
condition of the proposition.

\subsection{Proof of Proposition \ref{prop:S2} \label{sec:proof_S2}}

In this section we control $S_{2}$. We have 
\begin{align*}
S_{2} & =\left\langle \PTperp\left(\Yhat-\Ystar\right),\G\G\t-\E\G\G\t\right\rangle \\
 & \leq\Tr\left[\PTperp\left(\Yhat-\Ystar\right)\right]\cdot\opnorm{\G\G\t-\E\G\G\t}\\
 & \le\frac{\error}{\size}\cdot\opnorm{\G\G\t-\E\G\G\t}.
\end{align*}
Let $\Var\left(g_{ij}\right)=\std^{2}$. We record a fact about the
sub-Gaussian property of columns of $\G$. 
\begin{fact}
\label{fact:satisfy_cond_gauss_choas_operator_norm_bound} Let $\x\in\real^{\num}$
be an arbitrary column of $\G$. We have 
\[
\norm[\left\langle \x,\w\right\rangle ]{\psi_{2}}\leq C\frac{\sgnorm}{\std}\sqrt{\E\left\langle \x,\w\right\rangle ^{2}}\qquad\text{for any }\w\in\real^{\num},
\]
where $C>0$ is a universal constant and $C\frac{\sgnorm}{\std}\ge1$.
\end{fact}
The proof is given in Section \ref{sec:proof_satisfy_cond}. Applying
Lemma \ref{lem:subg_cov_mat_bound} with $\rho_{0}=\frac{\sgnorm}{\std}$,
we have 
\[
\opnorm{\frac{1}{d}\G\G\t-\frac{1}{d}\E\G\G\t}\leq C_{1}\rho_{0}^{2}\left(\sqrt{\frac{2\num}{\vecdim}}+\frac{2\num}{\vecdim}\right)\opnorm{\frac{1}{d}\E\G\G\t}
\]
with probability at least $1-2e^{-\num}$. Here we let $m=\vecdim,u=\num$
and define $\x_{i}$ to be the $i$-th column of $\G$ and $\x$ to
be a vector independent of but identically distributed as each column of $\G$ (note that columns of $\G$ are identically
distributed). We also use the fact that $\E\x\x\t=\frac{1}{\vecdim}\E\G\G\t=\std^{2}\I$.
Multiplying $\vecdim$ on both sides of the above equation yields
\[
\opnorm{\G\G\t-\E\G\G\t}\leq C_{1}\left(\sqrt{\frac{2\num}{\vecdim}}+\frac{2\num}{\vecdim}\right)\vecdim\sgnorm^{2}.
\]
Hence, we have 
\[
S_{2}\le\frac{\error}{\size}\cdot C_{1}\left(\sqrt{\frac{2\num}{\vecdim}}+\frac{2\num}{\vecdim}\right)\vecdim\sgnorm^{2}=2C_{1}\error\numclust\left(\sqrt{\frac{\vecdim}{\num}}+1\right)\frac{\minsep^{2}}{\snr^{2}}
\]
The result follows from the condition of the proposition.

\subsection{Proof of Proposition \ref{prop:S4} \label{sec:proof_S4}}

We can compute 
\[
\left(\E\H\H\t\right)_{ij}=\begin{cases}
\vecdim\std^{2}+\norm[\Mean_{\labelstar(i)}]2^{2} & \text{if }i=j\\
\norm[\Mean_{\labelstar(i)}]2^{2} & \text{if }i\ne j\text{ and }\labelstar(i)=\labelstar(j)\\
\left\langle \Mean_{\labelstar(i)},\Mean_{\labelstar(j)}\right\rangle  & \text{otherwise}.
\end{cases}
\]
We partition the matrix $\Yhat-\Ystar$ into $\numclust^{2}$ of $\size\times\size$
blocks, and note that $T_{ab}$ denotes the sum of entries within
the $(a,b)$-th block. The constraints of program (\ref{eq:SDP1})
implies that 
\begin{enumerate}
\item $T_{aa}\leq0$ for each $a\in\left[\numclust\right]$ and $T_{ab}\geq0$
for each $a\ne b\in\left[\numclust\right]$;
\item $T_{ab}=T_{ba}$ for each $a,b\in\left[\numclust\right]$;
\item $-T_{aa}=\sum_{b\in\left[\numclust\right]:b\ne a}T_{ab}$ for each
$a\in\left[\numclust\right]$;
\item $-\sum_{a\in\left[\numclust\right]}T_{aa}+\sum_{a,b\in\left[\numclust\right]:a\ne b}T_{ab}=\error$
and thus $-\sum_{a\in\left[\numclust\right]}T_{aa}=\sum_{a,b\in\left[\numclust\right]:a\ne b}T_{ab}=\frac{\error}{2}$.
\end{enumerate}
Since $\Yhat-\Ystar$ has zero diagonal, we can write 
\begin{align*}
S_{4} & =\sum_{a\in\left[\numclust\right]}T_{aa}\norm[\Mean_{a}]2^{2}+2\sum_{a,b\in\left[\numclust\right]:a<b}T_{ab}\left\langle \Mean_{a},\Mean_{b}\right\rangle \\
 & =-\sum_{a,b\in\left[\numclust\right]:a<b}T_{ab}\minsep_{ab}^{2}\\
 & =-\frac{1}{2}\sum_{a,b\in\left[\numclust\right]:a\ne b}T_{ab}\minsep_{ab}^{2}\\
 & \leq-\frac{1}{2}\sum_{a,b\in\left[\numclust\right]:a\ne b}T_{ab}\minsep^{2}\\
 & =-\frac{1}{4}\minsep^{2}\error.
\end{align*}


\section{Proof of Theorem \ref{thm:ip_exp_rate}\label{sec:proof_ip_exp_rate}}

We define the shorthand 
\[
\iperror\coloneqq\max\left\{ \frac{1}{2}\norm[\F-\F^{*}]1:\eta(\F)\le\eta(\F^{*}),\F\in\mathcal{F}\right\} .
\]
It is not hard to see that $\iperror$ takes integer values in $[0,\num]$.
If $\iperror=0$ then we are done. We therefore focus on the case
$\iperror\in\left[\num\right]$.

Suppose $\iperror>3\num\numclust e^{-\snr^{2}/C_{0}^{2}}$ for a fixed
$C_{0}>D/c$. Note that 
\[
3\num\numclust e^{-\snr^{2}/C_{0}^{2}}\overset{(i)}{\le}\num\numclust\cdot\frac{1}{\numclust}\cdot e^{-\snr^{2}/\left(2C_{0}^{2}\right)}\le\num e^{-\snr^{2}/\left(2C_{0}^{2}\right)}<\num
\]
where step $(i)$ holds since we have assumed $\snr^{2}\ge\consts\numclust$
for some universal constant $\consts>0$. We record an important result
for our proof.
\begin{lem}
\label{lm:order_stats} Let $m\ge4$ and $g\ge1$ be integers. Let
$\X\in\real^{m\times g}$ be a matrix such that each $X_{ja}$ is
a sub-Gaussian random variable with its mean equal to $\lambda_{ja}$
and its sub-Gaussian norm no larger than $\rho_{ja}$, and each pair
$X_{ja}$ and $X_{ib}$ are independent for $j\ne i$ and $a,b\in\left[g\right]$.
Then for some universal constant $D>0$ and for any $\beta\in(0,m]$,
we have 
\begin{align*}
\sum_{j,a}X_{ja}M_{ja} & \le D\sqrt{\left\lceil \beta\right\rceil \left(\sum_{j,a}\rho_{ja}^{2}M_{ja}\right)\log\left(3mg/\beta\right)}+\sum_{j,a}\lambda_{ja}M_{ja},\\
 & \qquad\quad\forall\M\in\left\{ 0,1\right\} ^{m\times g}:\M\one_{g}\le\one_{m},\norm[\M]1=\left\lceil \beta\right\rceil ,
\end{align*}
with probability at least $1-\frac{1.5}{m}$.
\end{lem}
The proof is given in Section \ref{sec:proof_lm_order_stat}. Define
the set 
\[
\calM\coloneqq\left\{ \M\in\left\{ 0,1\right\} ^{\num\times\numclust}:\M\one_{\numclust}\le\one_{\num},\norm[\M]1=\iperror,M_{j,\labelstar(j)}=0\ \forall j\in\left[\num\right]\right\} .
\]
For any $\F$ feasible to $\IP_{3}$, we have 
\begin{align*}
0 & \le\frac{1}{c}\left(\eta(\F^{*})-\eta(\F)\right)\\
 & \overset{(i)}{=}\sum_{j\in[\num]}\sum_{a\in[\numclust]}\beta_{ja}F_{ja}\\
 & =\sum_{\left(j,a\right):F_{ja}=1,a\ne\labelstar(j)}\beta_{ja}\\
 & \le\max_{\M\in\calM}\sum_{j}\sum_{a\ne\labelstar(j)}\beta_{ja}M_{ja}\\
 & \overset{(ii)}{\le}\max_{\M\in\calM}\left[D\sqrt{\iperror\sgnorm^{2}\left(\sum_{j}\sum_{a\ne\labelstar(j)}\minsep_{\labelstar(j),a}^{2}M_{ja}\right)\log\left(3\num\left(\numclust-1\right)/\iperror\right)}-c\sum_{j}\sum_{a\ne\labelstar(j)}\minsep_{\labelstar(j),a}^{2}M_{ja}\right]\\
 & \le\max_{\M\in\calM}\left[D\sqrt{\iperror\sgnorm^{2}\left(\sum_{j}\sum_{a\ne\labelstar(j)}\minsep_{\labelstar(j),a}^{2}M_{ja}\right)\frac{\snr^{2}}{C_{0}^{2}}}-c\sum_{j}\sum_{a\ne\labelstar(j)}\minsep_{\labelstar(j),a}^{2}M_{ja}\right]\\
 & \le\left(\frac{D}{C_{0}}-c\right)\cdot\max_{\M\in\calM}\sum_{j}\sum_{a\ne\labelstar(j)}\minsep_{\labelstar(j),a}^{2}M_{ja}
\end{align*}
where step $(i)$ holds by Equation (\ref{eq:eta_func_equivalence}),
step $(ii)$ holds by Lemma \ref{lm:order_stats} with $g=\numclust-1$
since only $\numclust-1$ entries of $\left\{ \beta_{ja}\right\} $
are considered for each $j$ in the sum above $(ii)$, and the last
step holds since $\iperror\minsep^{2}\le\sum_{j}\sum_{a\ne\labelstar(j)}\minsep_{\labelstar(j),a}^{2}M_{ja}$.
Since $C_{0}>D/c$ and $\sum_{j}\sum_{a\ne\labelstar(j)}\minsep_{\labelstar(j),a}^{2}M_{ja}>0$,
the RHS above is negative, which is a contradiction. Hence, we must
have $\iperror\le3\num\numclust e^{-\snr^{2}/C_{0}^{2}}\le\num e^{-\snr^{2}/\left(2C_{0}^{2}\right)}$
and the result follows from the fact that $\norm[\F^{*}]1=\num$.

\section{Proof of technical results}

In this section we provide the proofs of the technical results used
in the proofs of our main theorems.

\subsection{Proof of Lemma \ref{lem:hanson-wright}\label{sec:proof_hanson_wright}}

We record the following lemma (Exercise 6.2.7 on pp.$\ $140 in \citet{vershynin2017high}).
\begin{lem}[Higher-dimensional Hanson-Wright inequality]
\emph{} \label{lem:hanson_wright_hdp} Let $\x_{1},\ldots,\x_{N}$
be independent, mean zero, sub-Gaussian random vectors in $\real^{M}$.
Let $\B=\left\{ B_{ij}\right\} $ be an $N\times N$ matrix. There
exists some universal constant $c>0$ such that for every $t\geq0$
\[
\P\left[\left|\sum_{i,j:i\ne j}^{N}B_{ij}\left\langle \x_{i},\x_{j}\right\rangle \right|\geq t\right]\leq2\exp\left[-c\min\left(\frac{t^{2}}{K^{4}M\norm[\B]F^{2}},\frac{t}{K^{2}\opnorm{\B}}\right)\right]
\]
where $K\coloneqq\max_{i}\norm[\x_{i}]{\psi_{2}}.$ 
\end{lem}
With this result, we only need to prove the same tail bound for $\P\left[\left|\sum_{i=1}^{N}B_{ii}\left(\norm[\x_{i}]2^{2}-\E\norm[\x_{i}]2^{2}\right)\right|\geq t\right]$.
To prove that, we cite another useful lemma (Theorem 2.8.2 on pp.$\ $36
in \citet{vershynin2017high}).
\begin{lem}[Bernstein's inequality for sub-exponential random variables]
\emph{}\label{lem:bernstein-subexp} Let $X_{1},\ldots,X_{N}$ be
independent, mean zero, sub-exponential random variables, and $\a\in\real^{N}$.
Then for every $t\geq0$, we have 
\[
\P\left[\left|\sum_{i=1}^{N}a_{i}X_{i}\right|\geq t\right]\leq2\exp\left[-c\min\left(\frac{t^{2}}{K_{1}^{2}\norm[\a]2^{2}},\frac{t}{K_{1}\norm[\a]{\infty}}\right)\right]
\]
where $K_{1}\coloneqq\max_{i}\norm[X_{i}]{\psi_{1}}$.
\end{lem}
Here, $\norm[\cdot]{\psi_{1}}$ denotes the sub-exponential norm;
see \citet{vershynin2017high} for more details. We work under the
premise of Lemma \ref{lem:hanson_wright_hdp}. Since $\x_{i}$ are
independent sub-Gaussian random vectors, each $\norm[\x_{i}]2^{2}-\E\norm[\x_{i}]2^{2}$
is the sum of $M$ independent, mean zero, sub-exponential random
variables with sub-exponential norm equal to $K^{2}$. Then Lemma
\ref{lem:bernstein-subexp} implies 
\[
\P\left[\left|\sum_{i=1}^{N}B_{ii}\left(\norm[\x_{i}]2^{2}-\E\norm[\x_{i}]2^{2}\right)\right|\geq t\right]\leq2\exp\left[-c\min\left(\frac{t^{2}}{K^{4}M\norm[\B]F^{2}},\frac{t}{K^{2}\opnorm{\B}}\right)\right]
\]
as required. 

\subsection{Proof of Fact \ref{fact:satisfy_cond_gauss_choas_operator_norm_bound}
\label{sec:proof_satisfy_cond}}

We prove the following equivalent statement 
\[
\norm[\left\langle \x,\w\right\rangle ]{\psi_{2}}^{2}\leq C\frac{\sgnorm^{2}}{\std^{2}}\E\left\langle \x,\w\right\rangle ^{2}\qquad\text{for any }\w\in\real^{\num},
\]
where $C>0$ is a universal constant and $C\frac{\sgnorm^{2}}{\std}\ge1.$
We first establish a relationship between $\sgnorm^{2}$ and $\Var\left(x_{1}\right)$:
Proposition 2.5.2 on pp. 24 of \citet{vershynin2017high} implies
that $\frac{C'\sgnorm^{2}}{\std^{2}}\ge\frac{1}{2}$ for some universal
constant $C'>0$. Hence, we have 
\begin{align*}
\norm[\left\langle \x,\w\right\rangle ]{\psi_{2}}^{2} & \overset{(i)}{\le}2C'\sum_{i\in\left[\num\right]}w_{i}^{2}\norm[x_{i}]{\psi_{2}}^{2}\\
 & =2C'\frac{\sgnorm^{2}}{\std^{2}}\sum_{i\in\left[\num\right]}w_{i}^{2}\std^{2}\\
 & \overset{(ii)}{=}2C'\frac{\sgnorm^{2}}{\std^{2}}\E\left\langle \x,\w\right\rangle ^{2},
\end{align*}
where $(i)$ holds according to Proposition 2.6.1 on pp. 28 of \citet{vershynin2017high},
and $(ii)$ holds since $x_{i}$ are i.i.d.$\ $and $\E x_{i}=0$.
Letting $C=2C'$ completes the proof.

\subsection{Proof of Lemma \ref{lm:order_stats} \label{sec:proof_lm_order_stat}}

We define 
\begin{align*}
L_{\M} & \coloneqq\sum_{j,a}\left(X_{ja}-\lambda_{ja}\right)M_{ja},\\
R_{\beta,\M} & \coloneqq D\sqrt{\left\lceil \beta\right\rceil \left(\sum_{j,a}\rho_{ja}^{2}M_{ja}\right)\log\left(3mg/\beta\right)},\\
\calM_{\beta} & \coloneqq\left\{ \M\in\left\{ 0,1\right\} ^{m\times g}:\M\one_{g}\le\one_{m},\norm[\M]1=\left\lceil \beta\right\rceil \right\} .
\end{align*}
To establish a uniform bound in $\beta$, we apply a discretization
argument to the possible values of $\beta$. Define the shorthand
$E\coloneqq(0,m]$. We can cover $E$ by the sub-intervals $E_{t}\coloneqq(t-1,t]$
for $t\in[m]$. For each $t\in[m]$ we define the probability

\begin{align*}
\alpha_{t} & \coloneqq\P\left\{ \exists\beta\in E_{t},\exists\M\in\calM_{\beta}:L_{\M}>R_{\beta,\M}\right\} .
\end{align*}
We bound each of these probabilities: 

\begin{align}
\alpha_{t} & \overset{(i)}{\le}\P\left\{ \exists\M\in\calM_{t}:L_{\M}>R_{t,\M}\right\} \nonumber \\
 & \leq\P\left\{ \bigcup_{\M\in\calM_{t}}\left\{ L_{\M}>R_{t,\M}\right\} \right\} \nonumber \\
 & \leq\sum_{\M\in\calM_{t}}\P\left\{ L_{\M}>R_{t,\M}\right\} ,\label{eq:double union bd on normal-1-1-2}
\end{align}
where step $(i)$ holds since $\beta\in E_{t}$ implies $\beta\le\left\lceil \beta\right\rceil =t$. 

Note that each $X_{ja}-\lambda_{ja}$ is an independent zero-mean
sub-Gaussian random variable and the squared sub-Gaussian norm of
$L_{\M}$ is at most $C_{\psi_{2}}\sum_{j,a}\rho_{ja}^{2}M_{ja}$
where $C_{\psi_{2}}>0$ is a universal constant. We apply Hoeffding
inequality (Lemma \ref{lem:hoeffding}) to bound the probability on
the RHS of (\ref{eq:double union bd on normal-1-1-2}): 
\begin{align*}
\P\left\{ L_{\M}>R_{t,\M}\right\}  & \leq\exp\left\{ -\frac{cD^{2}t\left(\sum_{j,a}\rho_{ja}^{2}M_{ja}\right)\log(3mg/t)}{C_{\psi_{2}}\sum_{j,a}\rho_{ja}^{2}M_{ja}}\right\} \\
 & \leq\exp\left\{ -4t\log(3mg/t)\right\} 
\end{align*}
where $c>0$ is a universal constant. Plugging this back to (\ref{eq:double union bd on normal-1-1-2}),
we have for each $t\in\left[m\right]$, 
\begin{align}
\alpha_{t} & \leq\sum_{\M\in\calM_{t}}\exp\left\{ -4t\log(3mg/t)\right\} \nonumber \\
 & =\binom{m}{t}g^{t}\exp\left\{ -4t\log(3mg/t)\right\} \nonumber \\
 & \leq\left(\frac{me}{t}\right)^{t}g^{t}\exp\left\{ -4t\log(3mg/t)\right\} \nonumber \\
 & \leq\exp\left\{ t\log(3mg/t)+t-4t\log(3mg/t)\right\} \nonumber \\
 & \leq\exp\left\{ -t\log(3mg/t)\right\} =\left(\frac{t}{3mg}\right)^{t},\label{eq:binom coeff bound-1-2}
\end{align}
where the last inequality follows from $t\leq t\log(3mg/t)$ for $t\in\left[m\right]$.
It follows that 
\begin{align*}
 & \quad\P\left\{ \exists\beta\in E,\exists\M\in\calM_{\beta}:L_{\M}>R_{\beta,\M}\right\} \\
 & \leq\P\left\{ \bigcup_{t=1}^{m}\left\{ \exists\beta\in E_{t},\exists\M\in\calM_{\beta}:L_{\M}>R_{\beta,\M}\right\} \right\} \\
 & \le\sum_{t=1}^{m}\alpha_{t}\\
 & \leq\sum_{t=1}^{m}\left(\frac{t}{3mg}\right)^{t}\eqqcolon P_{1}(m).
\end{align*}

It remains to show that $P_{1}(m)\leq\frac{1.5}{m}$. Since 
\begin{align*}
P_{1}(m) & \leq\sum_{t=1}^{m}\left(\frac{t}{3m}\right)^{t}\\
 & \le\frac{1}{3m}+\sum_{t=2}^{m}\left(\frac{t}{3m}\right)^{t}\\
 & \le\frac{1}{3m}+m\cdot\max_{t=2,3,\ldots,m}\left(\frac{t}{3m}\right)^{t},
\end{align*}
the proof is completed if for each integer $t=2,3,\ldots,m$, we can
show the bound $\left(\frac{t}{3m}\right)^{t}\leq\frac{1}{m^{2}}$,
or equivalently $f(t)\coloneqq t(\log3m-\log t)\geq2\log m.$ Since
$t\le m$, $f(t)$ has derivative 
\[
f'(t)=\log3m-\log t-1\ge\log3m-\log\left(\frac{3m}{3}\right)-1=\log3-1\ge0.
\]
Therefore, $f(t)$ is non-decreasing for $2\le t\le m$ and therefore
$f(t)\ge f(2)=2\log3m-2\log2\ge2\log m.$ Hence, $P_{1}(m)\le\frac{1.5}{m}$. 

\section{Proof of Theorem \ref{thm:cluster_error_rate}\label{sec:proof_cluster_error_rate}}

We only need to prove the first part of the theorem. The second part
follows immediately from the first part and Theorem \ref{cor:SDP_exp_rate}.

The proof follows similar lines as those of Theorem 17 and Lemma 18
in \citet{makarychev2016learning}. In the rest of the section, we
work under the context of Algorithms \ref{alg:apx_clustering} and
\ref{alg:est_clustering}. Recall that $\numclust'=\left|\left\{ B_{t}\right\} _{t\ge1}\right|$
and we let $\epsilon\coloneqq\norm[\Yhat-\Ystar]1/\norm[\Ystar]1$.
We have the following lemma.
\begin{lem}
\label{lem:apx_clustering} There exists a partial matching $\perm'$
between $\left[\numclust\right]$ and $\left[\numclust'\right]$ and
a universal constant $C>0$ such that 
\[
\left|\bigcup_{t=\perm'(a)}\clustset a\cap B_{t}\right|\ge\left(1-C\epsilon\right)\num.
\]
\end{lem}
The proof is given in Section \ref{sec:proof_apx_clustering}. The
next lemma concerns the quality of clustering by Algorithm \ref{alg:clustering}.
\begin{lem}
\label{lem:clustering} There exists a permutation $\perm$ on $\left[\numclust\right]$
and a universal constant $C>0$ such that 
\[
\left|\bigcup_{t=\perm(a)}\clustset a\cap U_{t}\right|\ge\left(1-C\epsilon\right)\num.
\]
\end{lem}
The proof is given in Section \ref{sec:proof_clustering}. The result
follows from combining the above lemmas and the fact that 
\[
\misrate(\LabelHat,\LabelStar)=1-\frac{1}{\num}\max_{\perm\in S_{\numclust}}\left|\bigcup_{t=\perm(a)}\clustset a\cap U_{t}\right|.
\]


\subsection{Proof of Lemma \ref{lem:apx_clustering}\label{sec:proof_apx_clustering}}

We define $\y_{a}$ to be an arbitrary row of $\Ystar$ whose index
is in $\clustset a$.
\begin{align*}
G_{a} & \coloneqq\left\{ i\in\clustset a:\norm[\Yhat_{i\bullet}-\y_{a}]1\leq\frac{\size}{8}\right\} ,\qquad\forall a\in\left[\numclust\right]\\
G & \coloneqq\bigcup_{a\in\left[\numclust\right]}G_{a},\\
H & \coloneqq\vertexset\backslash G.
\end{align*}

We construct a partial matching $\perm'$ between sets $\clustset a$
and $B_{t}$ by matching every cluster $\clustset a$ with the first
$B_{t}$ that intersects $G_{a}$, and we let $\perm'(a)=t$. Since
each $i\in\left[\num\right]$ belongs to some $B_{t}$, we are able
to match every $\clustset a$ with some $B_{t}$. The fact that we
cannot match two distinct clusters $\clustset a$ and $\clustset b$
with the same $B_{t}$ as well as the rest of the proof are given
by the following fact.
\begin{fact}
\label{fact:apx_clustering} We have 
\begin{enumerate}
\item For each $a\in\left[\numclust\right]$ and $t\in\left[\numclust'\right]$
such that $t=\perm'(a)$, we have $B_{t}\cap G_{b}=\emptyset$ for
any $b\in\left[\numclust\right]\backslash\left\{ a\right\} $ and
$B_{t}\subset G_{a}\cup H$;
\item For each $a\in\left[\numclust\right]$ and $t\in\left[\numclust'\right]$
such that $t=\perm'(a)$, we have 
\[
\left|B_{t}\cap\clustset a\right|\geq\left|G_{a}\right|-\left|B_{t}\cap H\right|.
\]
\item We have 
\[
\sum_{t=\perm'(a)}\left|B_{t}\cap\clustset a\right|\ge\left|\vertexset\right|-2\left|H\right|.
\]
\item There exists a universal constant $C>0$ such that $\left|H\right|\leq C\epsilon\num$.
\end{enumerate}
\end{fact}
The proof is given below.

\subsubsection{Proof of Fact \ref{fact:apx_clustering}\label{sec:proof_fact_apx_clustering}}
\begin{enumerate}
\item Suppose that there exist $B_{t}$ and $b\in\left[\numclust\right]$
such that $b\ne a$ and $B_{t}\cap G_{b}\ne\emptyset$. Let $u\in B_{t}\cap G_{a}$
and $v\in B_{t}\cap G_{b}$. Since $G_{a}$ and $G_{b}$ are disjoint,
we know that $u\ne v$. Let $w\in B_{t}$. Then we have 
\begin{align*}
\norm[\Yhat_{u\bullet}-\Yhat_{w\bullet}]1 & \leq\frac{\size}{4}\\
\norm[\Yhat_{v\bullet}-\Yhat_{w\bullet}]1 & \leq\frac{\size}{4}.
\end{align*}
Therefore 
\[
\norm[\Yhat_{u\bullet}-\Yhat_{v\bullet}]1\leq\norm[\Yhat_{u\bullet}-\Yhat_{w\bullet}]1+\norm[\Yhat_{v\bullet}-\Yhat_{w\bullet}]1\le\frac{\size}{2}.
\]
This implies 
\begin{align*}
\norm[\y_{a}-\y_{b}]1 & \leq\norm[\y_{a}-\Yhat_{u\bullet}]1+\norm[\Yhat_{u\bullet}-\Yhat_{v\bullet}]1+\norm[\y_{b}-\Yhat_{v\bullet}]1\\
 & \leq\frac{\size}{8}+\frac{\size}{2}+\frac{\size}{8}<\size,
\end{align*}
which is a contradiction to the fact that $\norm[\y_{a}-\y_{b}]1=2\size$.
To complete the proof, we note that for any $i\in B_{t}$ we have
either $i\in G_{a}$ or $i\in H$.
\item Fix $i\in G_{a}$ for some $a\in\left[\numclust\right]$. For any
$j\in G_{a}$ we have $j\in B(i)$ since 
\[
\norm[\Yhat_{i\bullet}-\Yhat_{j\bullet}]1\le\norm[\y_{a}-\Yhat_{i\bullet}]1+\norm[\y_{a}-\Yhat_{j\bullet}]1\le\frac{\size}{4}.
\]
Therefore, by definition 
\[
\left|B_{t}\right|\ge\left|B(i)\right|\ge\left|G_{a}\right|.
\]
We have 
\begin{align*}
\left|B_{t}\cap\clustset a\right| & \overset{(i)}{\ge}\left|B_{t}\cap G_{a}\right|\\
 & =\left|B_{t}\right|-\left|B_{t}\backslash G_{a}\right|\\
 & \overset{(ii)}{=}\left|B_{t}\right|-\left|B_{t}\cap H\right|\\
 & \ge\left|G_{a}\right|-\left|B_{t}\cap H\right|,
\end{align*}
where step $(i)$ holds since $G_{a}\subset\clustset a$ and step
$(ii)$ holds since $B_{t}\subset G_{a}\cup H$.
\item Summing the LHS of the above equation over $t=\perm'(a)$ gives 
\begin{align*}
\sum_{t=\perm'(a)}\left|B_{t}\cap\clustset a\right| & =\sum_{a\in\left[\numclust\right]}\left|G_{a}\right|-\sum_{t=\perm'(a)}\left|B_{t}\cap H\right|\\
 & \ge\sum_{a\in\left[\numclust\right]}\left|G_{a}\right|-\sum_{t\ge1}\left|B_{t}\cap H\right|\\
 & \overset{(i)}{=}\left|G\right|-\left|\vertexset\cap H\right|\\
 & =\left|\vertexset\right|-2\left|H\right|,
\end{align*}
where step $(i)$ holds since $B_{t}\cap H$ are disjoint and $\bigcup_{t\geq1}B_{t}=\vertexset$.
\item We have 
\[
\left|H\right|\cdot\frac{\size}{8}\leq\sum_{i\in H}\norm[\Yhat_{i\bullet}-\y_{\labelstar(i)}]1\leq\norm[\Yhat-\Ystar]1\leq\epsilon\norm[\Ystar]1=\epsilon\cdot\num\size
\]
where the last step follows from the fact that $\norm[\Ystar]1=\num\size$.
The result follows.
\end{enumerate}

\subsection{Proof of Lemma \ref{lem:clustering}\label{sec:proof_clustering}}

Let $\perm'$ be the partial matching between $\clustset a$ and $B_{t}$
from Lemma \ref{lem:apx_clustering}. Define $\perm(a)=\perm'(a)$
for $\perm'(a)\le\numclust$. If the resulting $\perm$ is a partial
permutation, we extend $\perm$ to a permutation defined on $\left[\numclust\right]$
in an arbitrary way. We may assume that $\left\{ U_{t}\right\} _{t\in\left[\numclust\right]}$
are $\left\{ B_{t}\right\} _{t\in\left[\numclust\right]}$ WLOG, and
that $U_{t}$ consists of $B_{t}$ and some elements from sets $B_{u}$
with $u>\numclust$. We have 
\begin{align*}
\left|\bigcup_{t=\perm(a)}\clustset a\cap U_{t}\right| & \ge\left|\bigcup_{t=\perm'(a)\le\numclust}\clustset a\cap B_{t}\right|\\
 & =\left|\bigcup_{t=\perm'(a)}\clustset a\cap B_{t}\right|-\left|\bigcup_{t=\perm'(a)>\numclust}\clustset a\cap B_{t}\right|\\
 & \ge\left(1-C'\epsilon\right)\num-\left|\bigcup_{t=\perm'(a)>\numclust}\clustset a\cap B_{t}\right|
\end{align*}
where $C'>0$ is a universal constant. Define 
\begin{align*}
T_{1} & \coloneqq\left\{ t>\numclust:t=\perm'(a)\text{ for some }a\in\left[\numclust\right]\right\} ,\\
T_{2} & \coloneqq\left\{ t\in\left[\numclust\right]:t\ne\perm'(a)\text{ for any }a\in\left[\numclust\right]\right\} .
\end{align*}
Note that $\left|T_{1}\right|=\left|T_{2}\right|$ and for any $t_{1}\in T_{1}$
and $t_{2}\in T_{2}$ we have $\left|B_{t_{1}}\right|\le\left|B_{t_{2}}\right|$.
Therefore, 
\begin{align*}
\left|\bigcup_{t=\perm'(a)>\numclust}\clustset a\cap B_{t}\right| & \le\left|\bigcup_{t\in T_{1}}B_{t}\right|\\
 & \le\left|\bigcup_{t\in T_{2}}B_{t}\right|\\
 & \le\left|\vertexset\right|-\left|\bigcup_{t=\perm'(a)}\clustset a\cap B_{t}\right|\\
 & =C'\epsilon\num.
\end{align*}
The result follows by setting $C\coloneqq2C'$.

\section{Proof of Theorem \ref{thm:mean_estimation_error}\label{sec:proof_mean_estimation_error}}

Let $\Var\left(g_{ij}\right)=\std^{2}$. For $a\in\left[\numclust\right]$,
define $\clustest a\coloneqq\left\{ i\in\left[\num\right]:\labelhat_{i}=a\right\} $
the estimated clusters encoded in $\LabelHat$, and recall that our
cluster center estimators are defined by $\Meanhat_{a}\coloneqq\size^{-1}\sum_{i\in\clustest a}\h_{i}$.
We assume $\left\{ \clustest a\right\} $ achieves the lowest clustering
error as given in Theorem \ref{thm:cluster_error_rate} WLOG. For
each $a\in\left[\numclust\right]$, we have 
\begin{align*}
\norm[\Meanhat_{a}-\Mean_{a}]2 & \le\norm[\frac{1}{\size}\sum_{i\in\clustest a}\h_{i}-\frac{1}{\size}\sum_{j\in\clustset a}\h_{j}]2+\norm[\frac{1}{\size}\sum_{j\in\clustset a}\h_{j}-\Mean_{a}]2\\
 & \eqqcolon Q_{1}+Q_{2}.
\end{align*}


\subsection{Controlling $Q_{1}$}

Define $\epsilon\coloneqq\misrate(\LabelHat,\LabelStar)$. We work
on the event that the result Theorem \ref{thm:cluster_error_rate}
is true. We have 
\[
Q_{1}=\frac{1}{\size}\norm[\sum_{i\in\clustest a\backslash\clustset a}\h_{i}-\sum_{j\in\clustset a\backslash\clustest a}\h_{j}]2
\]
Note that $\left|\clustest a\backslash\clustset a\right|=\left|\clustset a\backslash\clustest a\right|$
so we can pair each point in $\clustest a\backslash\clustset a$ with
a point in $\clustset a\backslash\clustest a$. Let us pair $i$th
point in $\clustest a\backslash\clustset a$ with $j(i)$th point
in $\clustset a\backslash\clustest a$, and define $\calM\coloneqq\left\{ \left(i,j(i)\right)\right\} $.
We have $\left|\calM\right|\le\num\epsilon$ and we can write 
\begin{align*}
Q_{1} & =\frac{1}{\size}\norm[\sum_{(i,j(i))\in\calM}\left(\h_{i}-\h_{j(i)}\right)]2\\
 & \le\frac{1}{\size}\sum_{(i,j(i))\in\calM}\norm[\h_{i}-\h_{j(i)}]2\\
 & \le\frac{1}{\size}\sum_{(i,j(i))\in\calM}\left(\minsep_{\labelstar(i),\labelstar(j(i))}+\norm[\g_{i}-\g_{j(i)}]2\right)\\
 & \le\frac{1}{\size}\sum_{(i,j(i))\in\calM}\left(C_{q}\minsep+\norm[\g_{i}-\g_{j(i)}]2\right),
\end{align*}
where the last step holds for some universal constant $C_{q}>0$ given
that $\max_{a,b\in\left[\numclust\right]}\minsep_{ab}\le C_{q}\minsep$.
By Theorem 3.1.1 on pp.$\ $41 of \citet{vershynin2017high}, $\frac{1}{\sqrt{2}\std}\norm[\g_{i}-\g_{j(i)}]2-\sqrt{\vecdim}$
is a sub-Gaussian random variable with sub-Gaussian norm at most $C_{\psi_{2}}\frac{\sgnorm^{2}}{\std^{2}}$
where $C_{\psi_{2}}>0$ is a universal constant. Then Lemma \ref{lem:hoeffding}
implies that 
\[
\P\left[\frac{1}{\sqrt{2}\std}\norm[\g_{i}-\g_{j(i)}]2-\sqrt{\vecdim}\ge C\frac{\sgnorm^{2}}{\std^{2}}\sqrt{\log\num}\right]\le\num^{-C'}
\]
for some universal constants $C,C'>2$. By the union bound and the
facts that $\left|\calM\right|\le\num$ and $\std\lesssim\sgnorm$,
we have 
\[
\max_{(i,j)\in\calM}\norm[\g_{i}-\g_{j(i)}]2\le C_{g}\left(\sgnorm\sqrt{2\vecdim}+C\sgnorm\sqrt{2\log\num}\right)
\]
with probability at least $1-n^{-C_{1}}$ where $C_{g},C_{1}>0$ are
universal constants. 

Therefore, we have 
\begin{align*}
Q_{1} & \le C_{0}\left(\minsep+\sgnorm\sqrt{\vecdim}+\sgnorm\sqrt{\log\num}\right)\cdot\numclust\exp\left[-\frac{\snr^{2}}{\conste}\right]\\
 & \le C_{0}\left(\minsep+\sgnorm\sqrt{\vecdim}+\sgnorm\sqrt{\log\num}\right)\cdot\exp\left[-\frac{\snr^{2}}{2\conste}\right]
\end{align*}
for some universal constant $C_{0},\conste>0$ with probability at
least $1-n^{-C_{1}}$, where the last step holds since $\snr^{2}\ge\numclust$.
The fact that $e^{x}\ge1+x>x$ for any $x$ implies 
\[
\exp\left[-\frac{\snr^{2}}{4\conste}\right]\le\frac{4\conste}{\snr^{2}}=\frac{\sgnorm}{\minsep}\cdot\frac{4\conste}{\snr}\le4\conste\frac{\sgnorm}{\minsep}
\]
where the last step holds since we have $\snr\ge1$ by the conditions
of Theorem \ref{thm:cluster_error_rate}. Hence, we have 
\begin{align*}
Q_{1} & \le C_{0}\sgnorm\left(4\conste+\sqrt{\vecdim}+\sqrt{\log\num}\right)\cdot\exp\left[-\frac{\snr^{2}}{4\conste}\right]\\
 & \leq C_{1}\sgnorm\left(1+\sqrt{\vecdim}+\sqrt{\log\num}\right)\cdot\exp\left[-\frac{\snr^{2}}{4\conste}\right]\\
 & \leq2C_{1}\sgnorm\left(\sqrt{\vecdim}+\sqrt{\log\num}\right)\cdot\exp\left[-\frac{\snr^{2}}{4\conste}\right]
\end{align*}
where $C_{1}>0$ is a universal constant.

\subsection{Controlling $Q_{2}$}

We have 
\[
Q_{2}=\norm[\frac{1}{\size}\sum_{j\in\clustset a}\g_{j}]2.
\]
We see that $\frac{1}{\size}\sum_{j\in\clustset a}g_{ji}$ has variance
$\frac{1}{\size}\std^{2}$. By Proposition 2.6.1 on pp.$\ $28 and
Theorem 3.1.1 on pp. 41 of \citet{vershynin2017high}, $\frac{\sqrt{\size}}{\std}\norm[\frac{1}{\size}\sum_{j\in\clustset a}\g_{j}]2-\sqrt{\vecdim}$
is a sub-Gaussian random variable with sub-Gaussian norm at most $C_{\psi_{2}}\frac{\sgnorm^{2}}{\std^{2}}$
where $C_{\psi_{2}}>0$ is a universal constant. Then Lemma \ref{lem:hoeffding}
implies that 
\[
\P\left[\frac{\sqrt{\size}}{\std}\norm[\frac{1}{\size}\sum_{j\in\clustset a}\g_{j}]2-\sqrt{\vecdim}\ge C\frac{\sgnorm^{2}}{\std^{2}}\sqrt{\log\num}\right]\le\num^{-C'}
\]
for some universal constants $C,C'>0$. Since $\std\lesssim\sgnorm$,
there exists a universal constant $C_{0}>0$ such that 
\[
Q_{2}\leq C_{0}\sgnorm\left(\sqrt{\frac{\numclust\vecdim}{\num}}+\sqrt{\frac{\numclust\log\num}{\num}}\right)
\]
with probability at least $1-\num^{-C'}$. 

\section{Technical lemmas}

The following lemma is Theorem 2.6.2 on pp.$\ $28 in \citet{vershynin2017high}.
\begin{lem}[General Hoeffding's inequality]
\emph{ \label{lem:hoeffding} }Let $X_{1},\ldots,X_{N}$ be independent,
mean zero, sub-Gaussian random variables. Then, for every $t\geq0$
we have 
\[
\P\left[\left|\sum_{i=1}^{N}X_{i}\right|\geq t\right]\leq2\exp\left[-\frac{ct^{2}}{\sum_{i=1}^{N}\norm[X_{i}]{\psi_{2}}^{2}}\right],
\]
where $c>0$ is a universal constant.
\end{lem}
The following lemma is Exercise 4.7.3 in \citet{vershynin2017high}.
\begin{lem}[Tail bound of covariance matrix of sub-Gaussians]
\emph{ }\label{lem:subg_cov_mat_bound} Let $\x$ be a sub-Gaussian
vector and let $\x_{1},\ldots,\x_{m}$ be independent samples of $\x$.
Let $m$ be a positive integer and define 
\begin{align*}
\boldsymbol{\Sigma} & \coloneqq\E\x\x\t,\\
\boldsymbol{\Sigma}_{m} & \coloneqq\frac{1}{m}\sum_{i=1}^{m}\x_{i}\x_{i}\t.
\end{align*}
Let $\rho_{0}\ge1$ be such that 
\[
\norm[\left\langle \x,\w\right\rangle ]{\psi_{2}}\leq\rho_{0}\sqrt{\E\left\langle \x,\w\right\rangle ^{2}}\qquad\text{for any }\w\in\real^{N}.
\]
For any $u\geq0$, we have for a universal constant $C>0$, 
\[
\opnorm{\boldsymbol{\Sigma}_{m}-\boldsymbol{\Sigma}}\leq C\rho_{0}^{2}\left(\sqrt{\frac{N+u}{m}}+\frac{N+u}{m}\right)\opnorm{\boldsymbol{\Sigma}}
\]
with probability at least $1-2e^{-u}$.
\end{lem}

