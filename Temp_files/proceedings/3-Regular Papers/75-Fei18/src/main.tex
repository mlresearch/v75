
\section{Main results \label{sec:main}}

In this section, we establish the connection between the estimation
error of the SDP relaxation (\ref{eq:SDP1}) and that of what we call
the Oracle Integer Program. Using this connection, we derive explicit
bounds on the error of the SDP, and explore their implications for
clustering and center estimation.

\subsection{Oracle Integer Program}

Consider an idealized setting where an oracle reveals the true cluster
centers $\left\{ \Mean_{a}\right\} _{a\in\left[\numclust\right]}$.
Moreover, we are given the data points $\left\{ \bar{\h}_{i}\right\} _{i\in[\num]}$,
where $\bar{\h}_{i}\coloneqq\Mean_{\labelstar(i)}+(2c)^{-1}\g_{i}$
for $c:=\frac{1}{8}$ and $\left\{ \g_{i}\right\} $ are the same
(realizations of the) random variables in the original SGMM. In other
words, $\left\{ \bar{\h}_{i}\right\} $ are the same as the original
data points $\left\{ \h_{i}\right\} $ generated by the SGMM, except
that the standard deviation (or more generally, the sub-Gaussian norm)
of noise $\left\{ \g_{i}\right\} $ is scaled by $(2c)^{-1}=4$. To
cluster $\left\{ \bar{\h}_{i}\right\} $ in this idealized setting,
a natural approach is to simply assign each point to the closest cluster
center, so that the total distance of the points to their assigned
centers are minimized. We may formulate this procedure as an integer
program, by representing each candidate clustering assignment using
an assignment matrix $\F\in\mathcal{F}$ as before. Then, for each
assignment matrix $\F$, the quantity 
\[
\eta(\F)\coloneqq\sum_{j}\sum_{a}\norm[\bar{\h}_{j}-\Mean_{a}]2^{2}F_{ja}
\]
is exactly the sum of the distances of each point to its assigned
cluster center. The clustering procedure above thus amounts to solving
the following ``Oracle Integer Program (IP)'':
\begin{equation}
\begin{aligned}\min_{\F}\; & \eta(\F),\qquad\text{s.t.}\;\F\in\mathcal{F}.\end{aligned}
\label{eq:oracleIP}
\end{equation}
Let $\F^{*}\in\mathcal{F}$ be the assignment matrix associated with
the true underlying clustering of the SGMM; that is, $F_{ja}^{*}=\indic\left\{ \labelstar(j)=a\right\} $
for each $j\in[\num],a\in[\numclust]$. For each assignment $\F\in\mathcal{F}$,
it is easy to see that the quantity $\frac{1}{2}\norm[\F-\F^{*}]1$
is exactly the number of nodes that are assigned differently in $\F$
and $\F^{*}$, and hence measures the clustering error of $\F$ with
respect to the ground truth $\F^{*}$.

A priori, there is no obvious connection between the estimation error
of a solution to the above Oracle IP and that of a solution to the
SDP. In particular, the latter involves a continuous relaxation whose
solutions are fractional in general, and the true centers are unknown
therein. Surprisingly, we are able to establish a formal connection
between the two, and in particular show that the error of the SDP
is bounded by the error of the IP in an appropriate sense.

\subsection{Errors of SDP relaxation and Oracle IP}

To establish the connection, we begin with the following observation:
for a solution $\F\in\mathcal{F}$ to potentially be an optimal solution
of the Oracle IP (\ref{eq:oracleIP}), it must satisfy $\eta(\F)\le\eta(\F^{*})$
since $\F^{*}$ is feasible to (\ref{eq:oracleIP}). Consequently,
the quantity 
\begin{equation}
\max\left\{ \frac{1}{2}\norm[\F-\F^{*}]1:\F\in\mathcal{F},\eta(\F)\le\eta(\F^{*})\right\} \label{eq:IPerror}
\end{equation}
is the worst-case error of a potentially optimal solution to the Oracle
IP. This quantity turns out to be an \emph{upper} bound of the error
of any optimal solution $\Yhat$ to the SDP relaxation, as is shown
in the theorem below.
\begin{thm}[IP bounds SDP]
\emph{\label{thm:ip_sdp} }Under Model~\ref{mdl:SGMM}, there exist
some universal constants $\consts>0,C\ge1$ for which the following
holds. If the SNR satisfies 
\begin{equation}
\snr^{2}\geq\consts\left(\sqrt{\frac{\numclust\vecdim}{\num}\log\num}+\numclust\sqrt{\frac{\vecdim}{\num}}+\numclust\right),\label{eq:snr_cond}
\end{equation}
then we have
\[
\frac{\norm[\Yhat-\Ystar]1}{\norm[\Ystar]1}\le2\cdot\max\left\{ \frac{\norm[\F-\F^{*}]1}{\norm[\F^{*}]1}:\eta(\F)\le\eta(\F^{*}),\F\in\mathcal{F}\right\} 
\]
with probability at least $1-\num^{-C}-2e^{-\num}$. 
\end{thm}
The proof is given in Section \ref{sec:proof_ip_sdp}, and consists
of two main steps: $(i)$ showing that with high probability the SDP
error is upper bounded by the objective value of a linear program
(LP), and $(ii)$ showing that the LP admits an \emph{integral} optimal
solution and relating this solution to the quantity (\ref{eq:IPerror}).
We note that the key step $(ii)$, which involves establishing certain
hidden integrality properties, is completely deterministic. The SNR
condition (\ref{eq:snr_cond}) is required only in the probabilistic
step $(i)$; therefore, sharper analysis in step $(i)$ will lead
to potentially more relaxed conditions on the SNR.\\

To obtain an explicit bound on the SDP error, it suffices to upper
bound the error of the Oracle IP. This turns out to be a relatively
easy task compared to directly controlling the error of the SDP. The
reason is that the Oracle IP has only finitely many feasible solutions,
allowing one to use a union-bound-like argument. In particular, our
analysis establishes that the error of Oracle IP decays \emph{exponentially}
in the SNR, as summarized in the theorem below.
\begin{thm}[Exponential rates of IP]
\label{thm:ip_exp_rate} Under Model~\ref{mdl:SGMM}, there exist
universal constants $\consts,\constgamma,\conste>0$ for which the
following holds. If $\snr^{2}\ge\consts\numclust$, then we have 
\[
\max\left\{ \frac{\norm[\F-\F^{*}]1}{\norm[\F^{*}]1}:\eta(\F)\le\eta(\F^{*}),\F\in\mathcal{F}\right\} \le\constgamma\exp\left[-\frac{\snr^{2}}{\conste}\right]
\]
with probability at least $1-\frac{3}{2}\num^{-1}$.
\end{thm}
The proof is given in Section \ref{sec:proof_ip_exp_rate}. An immediate
consequence of Theorems \ref{thm:ip_sdp} and \ref{thm:ip_exp_rate}
is that the SDP (\ref{eq:SDP1}) also achieves an exponentially decaying
error rate.
\begin{cor}[Exponential rates of SDP]
 \label{cor:SDP_exp_rate} Under the SNR condition (\ref{eq:snr_cond}),
there exist universal constants $\constmis,\conste>0$ such that 
\[
\frac{\norm[\Yhat-\Ystar]1}{\norm[\Ystar]1}\leq\constmis\exp\left[-\frac{\snr^{2}}{\conste}\right]
\]
with probability at least $1-2\num^{-1}$.
\end{cor}
Our next result concerns the explicit clustering $\LabelHat$ extracted
from $\Yhat$ using the procedure described in Section \ref{sec:setup_explict_clustering}.
In particular, we show that the number of misclassified points is
upper bounded by the error in $\Yhat$ and hence also exhibits an
exponential decay.
\begin{thm}[Clustering error]
\emph{}\label{thm:cluster_error_rate} The error rate in $\LabelHat$
is always upper bounded by the error in $\Yhat$:
\[
\misrate(\LabelHat,\LabelStar)\lesssim\frac{\norm[\Yhat-\Ystar]1}{\norm[\Ystar]1}.
\]
Consequently, under the SNR condition (\ref{eq:snr_cond}), there
exist universal constants $\constmis,\conste>0$ such that 
\[
\misrate\left(\LabelHat,\LabelStar\right)\leq\constmis\exp\left[-\frac{\snr^{2}}{\conste}\right]
\]
with probability at least $1-2\num^{-1}$.
\end{thm}
The proof is given in Section \ref{sec:proof_cluster_error_rate}.
 Note that the above bound in terms of the \emph{clustering error}
is optimal (up to a constant in the exponent) in view of the minimax
results in \citet{lu2016lloyd}. 

\subsection{Consequences}

We explore the consequences of our error bounds in Corollary \ref{cor:SDP_exp_rate}
and Theorem \ref{thm:cluster_error_rate}.
\begin{itemize}
\item \textbf{Exact recovery:} If the SNR $\snr^{2}$ satisfies the condition
(\ref{eq:snr_cond}) and moreover $\snr^{2}\gtrsim\log\num$, then
Theorem \ref{thm:cluster_error_rate} guarantees that $\misrate\left(\LabelHat,\LabelStar\right)<\frac{1}{\num}$,
which means that $\misrate\left(\LabelHat,\LabelStar\right)=0$ and
the true underlying clustering is recovered exactly. Note that these
conditions can be simplified to $\snr^{2}\gtrsim\numclust+\log\num$
when $\num\gtrsim\vecdim$. In fact, by Corollary \ref{cor:SDP_exp_rate}
we know that the SDP solution satisfies the bound $\norm[\Yhat-\Ystar]1<\frac{1}{4}$
in this case, so simply rounding $\Yhat$ \emph{element-wise }produces
the ground-truth cluster matrix $\Ystar$. Therefore, the SDP relaxation
is able to achieve exact recovery (sometimes called \emph{strong consistency
}in the literature on SBM \citep{abbe2016recent}) of the underlying
clusters when the SNR is sufficiently large. 
\end{itemize}
In fact, our results apply even in regimes with a lower SNR, for which
exact recovery of the clusters is impossible due to potential overlap
between points from different clusters. In such regimes, Corollary
\ref{cor:SDP_exp_rate} and Theorem \ref{thm:cluster_error_rate}
imply \emph{approximate recovery} guarantees for the SDP relaxation:
\begin{itemize}
\item \textbf{Almost exact recovery:} If $\snr^{2}$ satisfies the condition
(\ref{eq:snr_cond}) and $\snr^{2}=\omega\left(1\right)$, then Theorem
\ref{thm:cluster_error_rate} implies that $\misrate\left(\LabelHat,\LabelStar\right)=o\left(1\right)$.
That is, the SDP recovers asymptotically the cluster memberships of
almost all points, which is sometimes called \emph{weak consistency.}
\item \textbf{Recovery with $\delta$-error:} More generally, for any number
$\delta\in(0,1)$, Theorem \ref{thm:cluster_error_rate} implies the
following non-asymptotic recovery guarantee: If $\snr^{2}$ satisfies
the condition (\ref{eq:snr_cond}) and $\snr^{2}\gtrsim\log\frac{1}{\delta}$,
then $\misrate\left(\LabelHat,\LabelStar\right)\le\delta$. That is,
the SDP correctly recovers the cluster memberships of at least $(1-\delta)$
fraction of the points.
\end{itemize}
We compare the above results with existing ones in Section \ref{sec:compare}
to follow.


\paragraph*{Cluster center estimation: }

We may obtain an estimate of the cluster \emph{centers} using estimated
cluster labels $\LabelHat$ produced by the SDP relaxation. In particular,
we simply compute the empirical means of the points within each estimated
clusters; that is, 
\[
\Meanhat_{a}\coloneqq\frac{\numclust}{\num}\sum_{i:\labelhat_{i}=a}\h_{i}
\]
for each $a\in\left[\numclust\right]$. As a corollary of our bounds
on clustering errors, we obtain the following guarantee on center
estimation.
\begin{thm}[Cluster center estimation error]
\emph{}\label{thm:mean_estimation_error} Suppose that $\max_{a,b\in\left[\numclust\right]}\minsep_{ab}\le C_{q}\minsep$
for some universal constant $C_{q}>0$. Under the same conditions
of Theorem \ref{thm:ip_sdp}, there exist universal constants $\constmis,\conste>0$
such that 
\[
\max_{a\in\left[\numclust\right]}\min_{\perm\in\calS_{\numclust}}\norm[\Meanhat_{a}-\Mean_{\perm(a)}]2\leq\constmis\sgnorm\left(\sqrt{\frac{\numclust\vecdim+\log\num}{\num}}+\left(\sqrt{\vecdim+\log\num}\right)\cdot\exp\left[-\frac{\snr^{2}}{\conste}\right]\right)
\]
with probability at least $1-3\num^{-1}$.
\end{thm}
The proof is given in Section \ref{sec:proof_mean_estimation_error}.
Note that the error is measured again up to permutation of the cluster
labels. Our error bound consists of two terms. The first term, $\sgnorm\sqrt{\frac{\numclust\vecdim+\log\num}{\num}}$,
is the error of estimating a $\vecdim$-dimensional cluster center
vector using the $\frac{\num}{\numclust}$ data points (with standard
deviation $\sgnorm$) from that cluster. This term is unavoidable
even when the true cluster labels are known. On the other hand, the
second term captures the error due to incorrect cluster labels for
some of the points. When $\snr^{2}\gtrsim\log\num$ and $\vecdim\gtrsim\log\num$,
we achieve the minimax optimal rate $\sgnorm\sqrt{\frac{\vecdim}{\num/\numclust}}$
for center estimation. 

\subsection{Comparison with existing results\label{sec:compare}}

Table \ref{tab:compare} summarizes several most representative results
in the literature on clustering SGMM/GMM. Most of them are in terms
of SNR conditions required to achieve exact recovery of the underlying
clusters. Note that our results imply sufficient conditions for \emph{both}
exact and approximate recovery. 

Most relevant to us is the work of \citet{li2017kmeans}, which considers
similar SDP relaxation formulations. They show that exact recovery
is achieved when $\snr^{2}\gtrsim\numclust+\log\num$ and $\num\gg\vecdim^{2}\numclust^{3}\log\numclust$.
In comparison, a special case of our Corollary \ref{cor:SDP_exp_rate}
guarantees exact recovery whenever $\snr^{2}\gtrsim\numclust+\log\num$
and $\num\gtrsim\vecdim$, which is milder then the condition in \citet{li2017kmeans}.

The work in \citet{lu2016lloyd} also proves an exponentially decaying
clustering error rate, but for a different algorithm (Lloyd's algorithm).
To achieve non-trivial approximate recovery of the clusters, they
require $\snr^{2}\gg\numclust^{2}+\numclust^{3}\frac{\vecdim}{\num}$
and $\numclust^{3}\ll\frac{\num}{\log\num}$ as $\num\to\infty$.
Our SNR condition in (\ref{eq:snr_cond}) has milder dependency on
$\numclust$, though dependency on $\num$ and $\vecdim$ are a bit
more subtle. We do note that under their more restricted SNR condition,
\citet{lu2016lloyd} are able to obtain tight constants in the exponent
of the error rate.

Finally, the work of \citet{mixon2017clustering} considers the SDP
relaxation introduced by \citet{peng2007approximating} and provides
bounds on center estimation when $\snr^{2}\gtrsim\numclust^{2}$.
An intermediate result of theirs concerns errors of the SDP solutions;
under the setting of balanced clusters, their error bound can be compared
with ours after appropriate rescaling. In particular, their result
implies the error bound $\norm[\Yhat-\Ystar]F^{2}\lesssim\frac{\num^{2}}{\snr^{2}}$
when $\num$ is sufficiently large. This bound is non-trivial when
$\snr^{2}\gtrsim\numclust$ since $\norm[\Ystar]F^{2}=\frac{\num^{2}}{\numclust}$.
Under the same conditions on $\snr^{2}$ and $\num$, our results
imply the exponential error bound
\[
\norm[\Yhat-\Ystar]F^{2}\le\norm[\Yhat-\Ystar]1\lesssim\frac{\num^{2}}{\numclust}e^{-\snr^{2}},
\]
which is strictly better. 

To sum up, corollaries of our results provide more relaxed conditions
for exact or approximate recovery compared to most of the existing
results listed in Table \ref{tab:compare}. Our results are weaker
by a $\sqrt{\numclust}$ factor than the one in \citet{vempala2004spectral},
which focuses on exact recovery under spherical Gaussian mixtures;
on the other hand, our results apply to the more general sub-Gaussian
setting, and imply approximate recovery guarantees under more general
SNR conditions. 

\begin{table}[H]
\begin{centering}
\begin{tabular}{|>{\centering}m{4.5cm}|c|c|c|}
\hline 
Paper & Condition on $\text{SNR}^{2}$ & Recovery Type & Algorithm\tabularnewline
\hline 
\hline 
\citet{vempala2004spectral} & $\Omega\left(\sqrt{\numclust\log\num}\right)$ & Exact & Spectral\tabularnewline
\hline 
\citet{achlioptas2005spectral} & $\Omega\left(\numclust\log\num+\numclust^{2}\right)$ & Exact & Spectral\tabularnewline
\hline 
\citet{kumar2010clustering} & $\Omega\left(\numclust^{2}\cdot\text{polylog}\left(\num\right)\right)$ & Exact & Spectral\tabularnewline
\hline 
\citet{awasthi2012improved} & $\Omega\left(\numclust\cdot\text{polylog}\left(\num\right)\right)$ & Exact & Spectral\tabularnewline
\hline 
\multirow{2}{4.5cm}{$\quad\quad$\citet{lu2016lloyd}} & $\Omega\left(\numclust^{2}\right)$ & Approximate & \multirow{2}{*}{%
\begin{tabular}{c}
Spectral\tabularnewline
+Lloyd's\tabularnewline
\end{tabular}}\tabularnewline
\cline{2-3} 
 & $\Omega\left(\numclust^{2}+\log\num\right)$ & Exact & \tabularnewline
\hline 
\citet{mixon2017clustering} & $\Omega\left(\numclust^{2}\right)$  & For center estimation & SDP\tabularnewline
\hline 
\citet{li2017kmeans} & $\Omega\left(\numclust+\log\num\right)$ & Exact & SDP\tabularnewline
\hline 
\multirow{2}{4.5cm}{$\qquad\qquad\quad$Ours} & $\Omega\left(\numclust\right)$ & Approximate & \multirow{2}{*}{SDP}\tabularnewline
\cline{2-3} 
 & $\Omega\left(\numclust+\log\num\right)$ & Exact & \tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{Summary of existing results on cluster recovery for GMM. Here ``approximate''
means correct recovery of the memberships of at least $(1-\delta)$
fraction of the points for a fixed constant $\delta\in(0,1)$. Some
of the results listed assume that $\protect\num\gg\text{poly}(\protect\numclust,\protect\vecdim)$;
see Section \ref{sec:compare} for details. \label{tab:compare}}
\end{table}


\section{Conclusion}

In this paper, we have considered clustering problems under SGMMs
using an SDP relaxation. We have shown that the SDP performs at least
as well as an idealized IP, which achieves an exponentially decaying
error rate. As a by-product of our analysis, we have obtained an error
bound for estimating mixture centers via the SDP. 

Our work points to several interesting future directions. An immediate
problem is extending our results to the case of imbalanced clusters
and non-isotropic distributions. It is also of interest to study the
robustness of SDP relaxations for SGMMs by considering adversarial
attacks or arbitrary outliers in the generated data under various
semi-random models \citep{awasthi2017clustering}. Other directions
that are worth exploring include obtaining better constants in error
bounds, identifying sharp thresholds for different types of recovery,
and obtaining tight localized proximity conditions in the lines of
\citet{li2017kmeans}.
