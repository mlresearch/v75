\newtheorem{condition}[theorem]{Condition}

\section{Proof of Theorem~\ref{thm:lbmain}: roadmap}
\label{sec:lbmain-roadmap}
In this section, we briefly explain how the proof of Theorem~\ref{thm:lbmain}
is structured. The proof for the lower bounds is presented in
Section~\ref{sec:proof-lbmain}, whose subsections are arranged in the same
order as Section~\ref{sec:main-lbidea} in the paper. Among them,
in \ref{sec:proof-lbmain-const}, the construction of
``difficult-to-distinguish'' functions for smooth strongly convex functions is
deferred to a separate section (Section~\ref{sec:constsscvx}), because the
explanation of smooth maximum of quadratic functions took too much space due
to technicality.

Since the proof is highly involved, we focused on conveying the main idea of the proof while deferring technical details to separate sections. Sections~\ref{sec:defer-proof-lbmain} and \ref{sec:defer-proof-const} contain deferred technical proofs from Sections~\ref{sec:proof-lbmain} and \ref{sec:constsscvx}, respectively.

The roadmap for the proof of upper bound (Theorem~\ref{thm:ubmain}) can be found in \ref{sec:ubmain-roadmap}.

\paragraph{Notation for Sections~\ref{sec:proof-lbmain}--\ref{sec:defer-proof-const}.}
Throughout Sections~\ref{sec:proof-lbmain}--\ref{sec:defer-proof-const}, we use
$\ballp{u}{\delta}$ to denote an $\ell_p$ ball centered at $u \in
\reals^d$ with radius $\delta$. As is standard, a
\emph{$\delta$-packing} of $S$ with respect to the metric $\rho$ is a set
$S' \subset S$ such that for $v, v' \in S'$ with $v \neq v'$, $\rho(v, v')
\ge \delta$.  A maximal $\delta$-packing is any $\delta$-packing $S'$ of $S$
with maximum cardinality. For integers $a \leq b$, let $a:b \defeq \{a, a +
1, \ldots, b\}$.  Let $\ones \in \reals^d$ be a $d$-dimensional vector full
with ones, and $\unitvec[j]$ be the $j$th standard
basis vector.  Given two functions $f, g : \R^d \to \R$,
we define the their \emph{intersection set} by
\begin{equation*}
	\intset{f}{g} \defeq \{x \in \R^d \mid f(x) = g(x) \}.
\end{equation*}
For $A \subset \R^d$, $\cl A$ and $\interior A$ denote its closure and
interior, respectively. 
Let $\tvnorm{P_- - P_+} = \sup_G |P_-(G) - P_+(G)|$ be the usual total variation distance between two distributions $P_-$ and $P_+$.
%For any $m \in \N$ we use $[m]$ to denote the set $\{1, 2, \ldots, m\}$.

\section{Proof of Theorem~\ref{thm:lbmain}}
\label{sec:proof-lbmain}
%In this section, we describe the key step for proving Theorem~\ref{thm:lbmain},
%which is the most important contribution of our paper. 
%In Section~\ref{sec:proof-lbmain-summary}, we will summarize the function construction in Section~\ref{sec:funcConst}, and present a \emph{unified} lemma (Lemma~\ref{lem:ftnconstrunified}) that holds for both of our family of functions ($\ffamily_L$ and $\ffamily_{H,\lambda}$). 
%In the next two Sections~\ref{sec:proof-lbmain-notation} and \ref{sec:proof-lbmain-inforec}, we describe how to prove the lower bound for stochastic batched optimization, given that it is possible to construct functions that satisfy Lemma~\ref{lem:ftnconstrunified}. We note that the argument presented in Sections~\ref{sec:proof-lbmain-notation} and \ref{sec:proof-lbmain-inforec} is \emph{universal}, meaning that our argument works for any other function classes for which we can construct functions that satisfy Lemma~\ref{lem:ftnconstrunified}.
%We first define notation that are used throughout the proof in Section~\ref{sec:proof-lbmain-notation}, and describe the ``information recursion'' based on pigeonhole principle in Section~\ref{sec:proof-lbmain-inforec}.

\subsection{The outline}
\label{sec:proof-lbmain-outline}
Recall that the minimax error \eqref{eqn:minimax-risk-definition}, which we want to bound from below, is
\begin{equation*}
	\minimax_M(\ffamily, \ofamily) \defeq 
	\sup_{\orc \in \ofamily}
	\inf_{\alg \in \sfamily_M}
	\sup_{f\in \ffamily} 
	\E_f \left [f(\what{X}) - f^\star \right ].
\end{equation*}
For proof of lower bound, we are going to consider only oracles $\orc_\normal \in \ofamily$ with independent Gaussian noise with variance $\sigma^2$. With these i.i.d.\ Gaussian $\orc_\normal$, we are going to provide a lower bound for $\sup_{f \in \ffamily} \E_f[f(\what{X}) - f^\star]$ that holds for \emph{any} algorithm $\alg$, which also is a lower bound for $\minimax_M(\ffamily, \ofamily)$.
To this end, we reduce $\ffamily$ to a subset of two functions $\{f_-, f_+\} \subset \ffamily$. With any such subset and \emph{any} probabilistic event $G$,
\begin{align*}
	\sup_{f\in \ffamily} \E_f \left [f(\what X) - f\opt \right]
	&\geq \max_{v \in \{-,+\}} \E_{v} \left [ (f_v (\what X) - f_v\opt) \IND{G} \right ]\\
	&= \max_{v \in \{-,+\}} \P_{v} (G) \E_{v} \left [ f_v(\what X) - f_v\opt \mid G \right ],
	\numberthis \label{eq:finallb}
\end{align*}
where $\P_{\pm}$ and $\E_{\pm}$ are the probability and expectation given that the true function is $f_{\pm}$.

As mentioned in Section~\ref{sec:main-lbidea}, our lower bound construction begins with the Le Cam technique for proving lower bounds in statistical and stochastic optimization
problems~\citep{Tsybakov09,AgarwalBaRaWa12,Duchi17}. The idea is due to \citeauthor{AgarwalBaRaWa12}: given two convex functions $f_-$ and $f_+$,
the separation between them is
\begin{equation*}
%\label{eqn:fn-sep}
\fndist(f_-, f_+)
\defeq \inf_{x \in \domain}
\left\{f_-(x) + f_+(x) \right\}
- \inf_{x\opt \in \domain} f_-(x\opt) - \inf_{x\opt \in \domain} f_+(x\opt).
\end{equation*}
The key to this construction is that if we have a point $x$ optimizing $f_-$
to accuracy $\half \fndist(f_-, f_+)$, that is, $f_-(x) \le \inf_{x\opt \in
	\domain} f_-(x\opt) + \half\fndist(f_-, f_+)$, then $f_+(x) \ge \inf_{x\opt \in
	\domain} f_+(x\opt) + \half\fndist(f_-, f_+)$.  Thus, an argument with Markov's
inequality (see \citet[Eq.~(18)]{AgarwalBaRaWa12} or
\citet[Ch.~4.1]{Duchi17}) gives the following lemma that reduces the optimization problem to a hypothesis testing problem:
\begin{lemma}
	\label{lem:opt2hyptest}
	Let $G$ be any probabilistic event.	Then,
	\begin{equation*}
		\max_{v \in \{-,+\}} \E_{v} \left [ f_v(\what X) - f_v\opt \mid G \right ]
		\geq
		\frac{\fndist(f_-, f_+)}{4}
		\left( 1-\tvnorm{ P_-(\cdot \mid G) -P_+(\cdot \mid G) } \right).
	\end{equation*}
\end{lemma}
This is a standard technique for proving minimax lower bounds, and we will defer the proof of Lemma~\ref{lem:opt2hyptest} to the Appendix~\ref{sec:proof-opt2hyptest}.

Eq~\eqref{eq:finallb} and Lemma~\ref{lem:opt2hyptest} is the starting point of our
strategy for lower bounds.
For any given algorithm $\alg$ and function class $\ffamily$, 
we will develop methods that can construct a pair of functions $f_-,f_+ \in \ffamily$
(for which $\fndist(f_-, f_+)$ is reasonably large)
such that there exists an event $G$ with
\begin{equation}
\label{eq:outlinegoal}
\P_{v} (G) \geq \frac{1}{2\cdot 4^M} \text{ for } v \in \{-,+\},
~~\text{and}~~
\tvnorm{ P_-(\cdot \mid G) -P_+(\cdot \mid G) } \leq \half,
\end{equation}
under the sampling strategies defined by $\alg$.

In the following subsections, we will describe in details the methods outlined in Sections~\ref{sec:main-lbidea} and \ref{sec:main-inforec}.
In Section~\ref{sec:proof-lbmain-const}, we provide how to construct pairs of functions $f_-,f_+ \in \ffamily$ at multiple scales, where each scale corresponds to a round or batch of data
in the method being used for optimization.
Using these construction methods, in Section~\ref{sec:proof-lbmain-inforec} we provide a delicate induction argument based on pigeonhole principle and properties of constructed functions that enable us to choose the ``difficult'' $f_-$ and $f_+$, and prove Eq~\eqref{eq:outlinegoal} for some $G$. After these are done, we will come back to this point and finish the proof in Section~\ref{sec:proof-lbmain-finish}.

\subsection{Function construction}
\label{sec:proof-lbmain-const}

%In Section~\ref{sec:funcConst}, we constructed our specially designed Lipschitz convex functions and smooth strongly convex functions that satisfy Lemmas~\ref{lem:ftnconstrLipcvx} and \ref{lem:ftnconstrsscvx}, respectively. This was done by first constructing a hierarchy of maximal packings, and then using the points in the packings as parameters to construct functions using Algorithms~\ref{alg:ftnconstrLipsconv} and \ref{alg:ftnconstrSmthstrcvx}.

%In this section, we unify these two cases and state a more general lemma (Lemma~\ref{lem:ftnconstrunified}) that subsumes both Lemmas~\ref{lem:ftnconstrLipcvx} and \ref{lem:ftnconstrsscvx}. To do this, let us first start with construction of maximal packings. 

For construction of functions that are necessary for our proof, we start by constructing
a nested sequence of maximal packings of $\domain$ that we use to define our ``difficult'' functions.
Then using the points in the packings as parameters, we construct functions that have desirable properties.

Consider $\delta_1, \delta_2, \dots, \delta_M$, which are real valued functions $n$ which satisfy $\lim_{n\rightarrow \infty} \delta_1(n) = 0$ and $\lim_{n\rightarrow \infty}\frac{\delta_t(n)}{\delta_{t-1}(n)} = 0$. 
%The exact values will be fixed in Eq~\eqref{eq:defdeltat}, Section~\ref{sec:proof-lbmain-notation}.
Given such $\delta_t$'s, a multiplier $0<\eta<1$, and a norm $\ell_p$, 
we recursively define a hierarchy of maximal packings as follows:
\begin{description}
	\item[$1.$] Let $\Uone$ to be any maximal $2\delta_1$-packing of $[\delta_1, 1-\delta_1]^d$ with respect to $\ell_p$ norm.
	\item[$2.$] For any $u_1 \in \Uone$, let $\Utwo$ to be any maximal $2\delta_2$-packing of $\ballp{u_1}{\eta\delta_1-\delta_2}$ w.r.t.\ $\ell_p$ norm.
	\item[$3.$] For any $u_2 \in \Utwo$, let $\Utemp{2}{3}$ to be any maximal $2\delta_3$-packing of $\ballp{u_2}{\eta\delta_2-\delta_3}$ w.r.t.\ $\ell_p$ norm.\\ $\vdots$
	\item[$M.$] For any $u_{M-1} \in \Utemp{M-2}{M-1}$, let $\UM$ to be any maximal $2\delta_M$-packing of $\ballp{u_{M-1}}{\eta\delta_{M-1}-\delta_M}$ w.r.t.\ $\ell_p$ norm.
	%\item[$M+1.$] For any $u_{M} \in \UM$, let $\V = \{-1, +1\}$.
\end{description}

Let us explain in words what is going on. In the first stage, we construct a set of points $\Uone$ so that any $u_1 \in \Uone$ satisfies $\ballp{u_1}{\delta_1} \subset \domain$.
Starting from second stage, we construct maximal $2\delta_t$ packings for \emph{all} points in the previous stage;
for example, $\Utemp{1}{2}$ is defined for all $u_1 \in \Uone$ and $\Utemp{1}{2}$ depends on which $u_1$ we choose.
We continue in this recursive way until we define $\Utemp{M-1}{M}$. After that stage, we define the final set $\V \defeq \{ \pm 1\}$.
By construction we have $\ballp{u_t}{\delta_t} \subset \ballp{u_{t-1}}{\eta \delta_{t-1}}$ for $t \in 2:M$.
Also notice that for any $t \in 2:M$, $\ballp{u_{t}}{\eta \delta_{t}} \cap \ballp{\tilde u_{t}}{\eta \delta_{t}} = \emptyset$,
for different $u_t, \tilde u_t \in \Utemp{t-1}{t}$.
This means that a point $u_t \in \Utemp{t-1}{t}$ uniquely maps to all their ``ancestors'' $u_{t-1}, u_{t-2}, \dots, u_1$ in the chain,
because the neighborhoods of their ancestors never overlap with the other ancestors at the corresponding level.

Given these maximal packings, we can choose $u_1 \in \Uone$,
and then a chain of parameters
\begin{equation}
\tag{\ref{eqn:defining-sequence-of-u}}
u_t \in \mc{U}^{(t)}_{u_{t-1}}
\end{equation}
for $t \in 2:M$, and choose $v \in \V$.
For this set of parameters $u_{1:M}$ and $v$, we will define a corresponding function $\ftnuv(x)$. The functions we construct has the property that a pair of functions with ``similar parameters'' have the same value outside a small set, while they can only differ inside the set. This construction is crucial for proof of Eq~\eqref{eq:outlinegoal}, as it makes functions with similar parameter values difficult to distinguish them.

More concretely, we summarize the list of desired properties that $\ftnuv(x)$ must satisfy, in Condition~\ref{con:ftnconstrunified}.
As we will show, Condition~\ref{con:ftnconstrunified} holds for both function classes of interest---$L$-Lipschitz convex functions ($\ffamily_{L}$) and $H$-smooth $\lambda$-strongly convex functions ($\ffamily_{H,\lambda}$)---with a class-dependent set of constants $(C, \alpha, \eta, \beta, \kappa, p)$.
If the two functions share the same parameter values from $u_1$ up to $u_{t-1}$ ($t \in 2:M$),
but their parameter deviated from each other after $t$, say $u_t, \dots, u_M, v$ and $\tilde u_t, \dots, \tilde u_M, \tilde v$, 
then the two functions $\ftnuv(x)$ and $\ftnuvtildet(x)$ are completely identical far away from $u_{t-1}$, and differ only at points near $u_{t-1}$.
Similar thing happens when parameter values are the same up to level $M$ ($u_{1:M}$) but $v$ and $\tilde v$ differ.
\begin{condition}
	\label{con:ftnconstrunified}
	For a given class of functions $\ffamily$, there exist constants $(C, \alpha, \eta, \beta, \kappa, p)$, where $C>0$, $0<\alpha<1$, $0<\beta<1$, $0<\eta<1$, $\kappa \in \N$, $p \in [2, \infty]$,
	that satisfy the following statements:
	Construct the nested packing sets $\Uone, \dots, \UM$ w.r.t.\ $\ell_p$ norm and choose chains of parameters.
	For $t \in 2:M$, if we have two chains of parameters $(u_{1:t-1}, u_{t:M}, v)$ and
	$(u_{1:t-1}, \tilde u_{t:M}, \tilde v)$, the corresponding functions 
	$\ftnuv$ and $\ftnuvtildet$ satisfy
	\begin{enumerate}
		\item \label{item:ftnconstrunified-1}
		$\ftnuv(x) = \ftnuvtildet(x),~\forall x \notin \Bptmone$.
		\item \label{item:ftnconstrunified-2}
		$| \ftnuv(x) - \ftnuvtildet(x) | \leq C(1-\beta) \alpha^{t-2} \delta_{t-1}^\kappa,~\forall x \in \Bptmone$.
		\item \label{item:ftnconstrunified-3}
		$\ltwo{\nabla \ftnuv(x) - \nabla \ftnuvtildet(x)} \leq 2 \kappa C\alpha^{t-2} \delta_{t-1}^{\kappa-1},~\forall x \in \Bptmone$.
	\end{enumerate}	
	Similarly, for a chain of parameters $u_{1:M}$,
	\begin{enumerate}
		\setcounter{enumi}{3}
		\item \label{item:ftnconstrunified-4}
		$\ftnuvm(x) = \ftnuvp(x),~ \forall x \notin \BpM$.
		\item \label{item:ftnconstrunified-5}
		$| \ftnuvm(x) - \ftnuvp(x) | \leq C(1-\beta) \alpha^{M-1} \delta_M^{\kappa},~ \forall x \in \BpM$.
		\item \label{item:ftnconstrunified-6}
		$\ltwo{\nabla \ftnuvm(x) - \nabla \ftnuvp(x)} \leq 2 \kappa C\alpha^{M-1} \delta_{M}^{\kappa-1},~ \forall x \in \BpM$.
		\item \label{item:ftnconstrunified-7}
		$\fndist(\ftnuvm,\ftnuvp) = 2C\alpha^M \eta^\kappa \delta_M^\kappa$.
	\end{enumerate}
	Here, whenever the function $\ftnuv$ is non-differentiable at $x$,
	we replace $\nabla \ftnuv(x)$ with any subgradient $\grduv \in \partial \ftnuv(x)$.
\end{condition}

It now remains to show that we can construct functions $\ftnuv$ that satisfy Condition~\ref{con:ftnconstrunified}, for both $\ffamily_{L}$ and $\ffamily_{H,\lambda}$.
We present the construction $\ffamily_{L}$ in the following subsection. 
Construction for $\ffamily_{H,\lambda}$ is done in essentially the same way, but \emph{smoothing} the maximum of quadratic functions require additional technicality. To help the proof run smoothly, we separate the construction for $\ffamily_{H,\lambda}$ to a different section (Section~\ref{sec:constsscvx}).

\subsubsection{Function construction: Lipschitz convex functions}
\label{sec:multiconstlipcvx}
\label{sec:constLipcvx}

\begin{algorithm}[t]
	\caption{Construction of Lipschitz convex $\ftnuv$.}  \label{alg:ftnconstrLipsconv}
	\begin{algorithmic}[1]  % [1] is line numbering
		\STATEx Given parameters $u_1, u_2, \dots, u_M, v$, size $\delta_1, \dots, \delta_M$, and Lipschitzness parameter $L$,
		\STATE Start with $\ftnuone (x) = \ftninuone(x) \defeq L\linf{x-u_1}$.
		\FOR{ $t = 2$ to $M$ }
		\STATE $\ftninut (x) \defeq \frac{L}{3^{t-1}} \linf{x-u_{t}} + \sum_{m=1}^{t-1} \frac{L \delta_m}{2 \cdot 3^{m-1}}.$
		\STATE $\ftnut (x) \defeq \max \{ \ftnutmone (x), \ftninut (x) \}$.
		\ENDFOR
		\STATE $\ftninuv (x) \defeq \frac{L}{3^M} \linf{x-u_M-\frac{v \delta_M}{2} \ones} 
		+ \sum_{m=1}^{M} \frac{L \delta_m}{2 \cdot 3^{m-1}}$. 
		\STATE Return $\ftnuv(x) \defeq \max \{ \ftnuM(x), \ftninuv(x) \}$.
	\end{algorithmic}
\end{algorithm}

For $L$-Lipschitz convex functions, construct nested maximal packing sets $\Uone, \dots, \UM$ w.r.t.\ $\ell_\infty$ norm. For any chain $u_{1:M}$ from the packings and $v \in \V$, 
Algorithm~\ref{alg:ftnconstrLipsconv} takes those parameters as input and returns the corresponding $\ftnuv$. We begin with the base function $\ftnuone(x) \defeq L \linf{x - u_1}$. Then, recursively, we define $\ftninut(x) = a_t \linf{x - u_t} + b_t$ for appropriate scalars $a_t$ and
$b_t$, defining $\ftnut (x) \defeq \max \{ \ftnutmone (x), \ftninut (x) \}$.
For the scalars in Algorithm~\ref{alg:ftnconstrLipsconv}, we can show that the functions $\ftnuv(x)$ satisfy Condition~\ref{con:ftnconstrunified}:
\begin{lemma}
	\label{lem:ftnconstrLipcvx}
	The functions constructed by Algorithm~\ref{alg:ftnconstrLipsconv} satisfy Condition~\ref{con:ftnconstrunified} with 
	\begin{equation*}
	(C, \alpha, \eta, \beta, \kappa, p) = \left (L, \frac{1}{3}, \half, \half, 1, \infty \right ).
	\end{equation*}
\end{lemma}
The scalars $a_t$ and $b_t$ are carefully chosen so that in $\ftnut (x) \defeq \max \{ \ftnutmone (x), \ftninut (x) \}$, the $\max$ operation can only change the function for $x \in \Binftmone$; from this, we can show Condition~\ref{con:ftnconstrunified}.\ref{item:ftnconstrunified-1}. Also, since $\ftnuv(x)$ and $\ftnuvtildet(x)$ can only differ in a ball of radius $\delta_{t-1}$, their difference should be proportional to $\delta_{t-1}$, implying Condition~\ref{con:ftnconstrunified}.\ref{item:ftnconstrunified-2}. The rest of the proof of Lemma~\ref{lem:ftnconstrLipcvx} is deferred to Appendix~\ref{sec:proof-ftnconstrLipcvx}.

We also need to check whether the functions $\ftnuv(x)$ we are constructing are indeed in our function class of interest:
$L$-Lipschitz convex functions. The following lemma addresses this points, whose simple proof is deferred to Appendix~\ref{sec:proof-ftnconstrLipcvx2}.
\begin{lemma}
	\label{lem:ftnconstrLipcvx2}
	The functions constructed with Algorithm~\ref{alg:ftnconstrLipsconv} are $L$-Lipschitz and convex.
\end{lemma}

\subsubsection{Function construction: smooth strongly convex functions}
We can show that for $\ffamily_{H,\lambda}$, we can construct functions $\ftnuv$ that satisfy Condition~\ref{con:ftnconstrunified}, with $(\kappa, p) = (2,2)$. For details, please refer to Section~\ref{sec:constsscvx}.

\subsection{The information recursion}
\label{sec:proof-lbmain-inforec}

The proof depends on whether the oracle is zeroth-order ($\ofamilyz$) or first-order ($\ofamilyf$).
Let $\zeta$ denote the \emph{order of the oracle}: $\zeta = 0$ whenever we are using zeroth-order oracles, 
$\zeta = 1$ for first-order.
So, with $\kappa$ and $\zeta$, we can express the three cases presented in Theorem~\ref{thm:lbmain} into three tuples: $(\kappa, \zeta) = (1,0)$ for Lipschitz convex/zeroth-order, $(2,0)$ for smooth strongly convex/zeroth-order, and $(2,1)$ for smooth strongly convex/first-order.
Our proof strategy assumes $\kappa > \zeta$;
this means that the proof \emph{does not} apply to $(\kappa,\zeta) = (1,1)$, which corresponds to 
Lipschitz convex functions with first-order oracles.
Also, let $\probuv$ and $\exptuv$ denote the probability of an event and expectation of any quantity, respectively, based on the event that the true objective function is $\ftnuv$.

The rest of our proof is based solely on the assumption that construction of functions $\ftnuv \in \ffamily$ that satisfy Condition~\ref{con:ftnconstrunified} is possible with some constants $(C, \alpha, \eta, \beta, \kappa, p)$ for the function class $\ffamily$. 
This means that our analysis that follows can be used \emph{universally} for any other function classes once we can construct functions $\ftnuv$ that satisfy Condition~\ref{con:ftnconstrunified}.

For the information recursion step, the crucial argument is the pigeonhole principle, i.e., if there are $n$ samples and $R$ disjoint subsets of $\domain$, there must be at least one \emph{scarce-sampled subset} that contains at most $n/R$ samples.
For any constructed nested packing sets $\Uone, \dots, \UM$ and given algorithm $\alg$, we will show that we can inductively find a particular chain of parameters $u_{1:M}$ such that all $\Bpone, \dots, \BpM$ are scarce-sampled subsets (each $\Bpt$ contains at most $n/|\Ut|$ sample points) with constant probability.
For that particular $u_{1:M}$, it is difficult for $\alg$ to distinguish between $\ftnuvm$ and $\ftnuvp$, hence leading to small total variation distance between $\probuvm$ and $\probuvp$. This corresponds to proving Eq~\eqref{eq:outlinegoal}, for the event $G$ that all $\Bpone, \dots, \BpM$ are scarce-sampled.

\subsubsection{Minimax rates: information-theoretic intuition}
Let us first provide a rather heuristic argument to give intuition for the ``rates'' (in $n$) of lower bounds we prove.
Suppose a procedure $\alg$, by querying the true function $\ftnuv$, has
``identified'' $u_{1:t-1}$, but is oblivious to $u_{t:M}$.
Then, given a batch of $n$ points at which to compute function information, it is possible to
distinguish two different functions $\ftnuv$ and $\ftnuvtildet$ only if one samples a point near
$u_{t-1}$.
Consider a batch-based algorithm, querying $n$ points in computational round $t$,
attempting to find the right value for $u_t$.
As the functions are identical outside of
$\Bptmone$, we may consider a sampling scheme that without
loss of generality sample $n$ points only in the ball $\Bptmone$.
%By Lemma~\ref{lem:packingno}, $|\Ut| = \Omega((\frac{\delta_{t-1}}{\delta_{t}})^d)$.
By the pigeonhole principle, for at least one $u_t'$, the procedure can
collect a sample size of at most $n / |\Ut|$ in $\ballp{u_t'}{\delta_t}$.
Suppose for the sake of worst-case analysis that the true $u_t$ is equal to the scarce-sampled ball $u_t'$. Now, with $n$ samples in round $t$ the procedure identified $u_t$, and given $n / |\Ut|$ samples in $\Bpt$ one must seek to find the next $u_{t+1}$.

Now, consider the amount of information about $u_{t+1}$ that function evaluation queries in round $t$ can release when function values and/or (sub)gradients are perturbed by i.i.d\ mean-zero Gaussian noise (recall our assumption that we consider $\orc_\normal$). In this case, by Condition~\ref{con:ftnconstrunified}.\ref{item:ftnconstrunified-2}, we know that the difference in function values scales as $\delta_{t}^\kappa$,
and the KL-divergence
\begin{equation}
\tag{\ref{eqn:kl-heuristic}}
\dkl{\normal(\ftnuv(x), \sigma^2)}{
	\normal(\ftnuvtildetpone(x), \sigma^2)}
\asymp (\ftnuv(x) - \ftnuvtildetpone(x))^2 
\lesssim \delta_{t}^{2\kappa},
\end{equation}
when the oracle is zeroth-order ($\zeta = 0$). Similarly, we can show from Condition~\ref{con:ftnconstrunified}.\ref{item:ftnconstrunified-3} that the information we get in the case of the first-order Gaussian-noise oracle ($\zeta = 1$) is $O(\delta_{t}^{2(\kappa-1)})$.

In the typical proofs of
information-theoretic lower
bounds~\citep{Tsybakov09,AgarwalBaRaWa12,Duchi17}, the goal is to choose the
separation between distributions, $\tvnorms{\probuv - \probuvtildetpone}$ in Le Cam's
method~\eqref{eqn:le-cam-lower-bound}, to be a constant so as to guarantee a
reasonable lower bound. In this case, recalling the
KL-bound~\eqref{eqn:kl-heuristic}, we see that the ``information'' about $u_{t+1}$ revealed
in round $t$ of a sequential sampling procedure is constant over the
least-sampled region whenever
\begin{equation}
\label{eq:info-intuition-apdx}
\delta_t^{2(\kappa-\zeta)} \cdot \frac{n}{|\Ut|} = 
\frac{n \delta_t^{d+2(\kappa-\zeta)}}{\delta_{t-1}^d} = 1,
~~ \mbox{or} ~~
\delta_t = n^{-\frac{1}{d+2(\kappa-\zeta)}} \delta_{t-1}^\frac{d}{d + 2(\kappa-\zeta)},
\end{equation}
where $|\Ut| = (\frac{\delta_{t-1}}{\delta_{t}})^d$ was from a volume argument.
By inspection, beginning from $\delta_0 = 1$, this recursion \eqref{eq:info-intuition-apdx} has the
solution
\begin{equation}
\delta_M = n^{-\frac{1}{2(\kappa-\zeta)} \left(1 - \left(\frac{d}{d + 2(\kappa-\zeta)}\right)^M
	\right)}.
\label{eqn:heuristic-recursion-apdx}
\end{equation}
Note, from Lemma~\ref{con:ftnconstrunified}.\ref{item:ftnconstrunified-7}, that 
our construction satisfies $\fndist(\ftnuvm,\ftnuvp) \asymp \delta_M^\kappa$. 
Using Eqs~\eqref{eq:finallb}, \eqref{eq:outlinegoal}, and \eqref{eqn:heuristic-recursion-apdx}, together with Lemmas~\ref{lem:opt2hyptest} and \ref{con:ftnconstrunified}.\ref{item:ftnconstrunified-7}, we can check the calculated rates agree with Theorem~\ref{thm:lbmain} up to sub-polynomial factors, as desired.

In the following subsections, we introduce a set of new notation in Section~\ref{sec:proof-lbmain-notation}, and then, in Section~\ref{sec:proof-lbmain-induct}, provide the formal inductive argument that proves Eq~\eqref{eq:outlinegoal} for some $G$.% the proof of Theorem~\ref{thm:lbmain}.

\subsubsection{Notation}
\label{sec:proof-lbmain-notation}
%
%The proof depends on whether the oracle is zeroth-order or first-order.
%Let $\zeta$ denote the \emph{order of the oracle}: $\zeta = 0$ whenever we are using zeroth-order oracles, 
%$\zeta = 1$ for first-order.
%So, with $\kappa$ and $\zeta$, we can express the three cases presented in Theorem~\ref{thm:lbmain} into three tuples: $(\kappa, \zeta) = (1,0)$ for Lipschitz convex/zeroth-order, $(2,0)$ for smooth strongly convex/zeroth-order, and $(2,1)$ for smooth strongly convex/first-order.
%Our proof strategy assumes $\kappa > \zeta$;
%this means that the proof \emph{does not} apply to $(\kappa,\zeta) = (1,1)$, which corresponds to 
%Lipschitz convex functions with first-order oracles.

Using $\zeta$, for example, we can define
\begin{equation}
\label{eq:deftildeC}
\tilde C_\zeta \defeq C(1-\beta)^{1-\zeta}(2\kappa)^\zeta,
\end{equation}
with which we can simplify the notations quite a bit. For example, Condition~\ref{con:ftnconstrunified}.\ref{item:ftnconstrunified-2}--\ref{item:ftnconstrunified-3} can be now written as
\begin{align*}
	| \ftnuv(x) - \ftnuvtildet(x) | &\leq \tilde C_0 \alpha^{t-2} \delta_{t-1}^\kappa,~\forall x \in \Bptmone,\\
	\ltwo{\nabla \ftnuv(x) - \nabla \ftnuvtildet(x)} &\leq \tilde C_1 \alpha^{t-2} \delta_{t-1}^{\kappa-1},~\forall x \in \Bptmone.
\end{align*}
Next, define the ``exponent'' constants (as we saw from the intuition)
\begin{equation}
\label{eq:defgammat}
\gamma_t \defeq \frac{1}{d+2(\kappa-\zeta)} \sum_{m=0}^{t-1} \left(\frac{d}{d+2(\kappa-\zeta)}\right)^m
= 
\frac{1}{2(\kappa-\zeta)} \left(1 - \left( \frac{d}{d+2(\kappa-\zeta)} \right)^t\right),
\end{equation}
for $t \in 0:M$. % CY: is t = 0 here for some reason? should check
%Note that because $\kappa-\zeta > 0$, this is expressed as
%\begin{equation}
%\label{eq:defgammat2}
%\gamma_t =
%\frac{1}{2(\kappa-\zeta)} \left(1 - \left( \frac{d}{d+2(\kappa-\zeta)} \right)^t\right).
%\end{equation}
Using this exponent constants, define $\delta_1, \dots, \delta_M$ as
\begin{equation}
\label{eq:defdeltat}
\begin{array}{ll}
\delta_t \defeq 
D_t n^{ - \gamma_t } \exp \left( -2 \sqrt{2} \gamma_{t-1} \sqrt{\log n} \right) & \text{ for } t = 1:M-1,\\
\delta_M \defeq 
D_M n^{ -\gamma_M } \exp \left( -2 \sqrt{2} \gamma_{M-1} \sqrt{\log n} \right) \log^{-\nu/\kappa} n,
\end{array}
\end{equation}
where $\nu > 0$ is any arbitrarily small number.
Note that $\delta_t$'s have the same rates in $n$ as seen in Eq~\eqref{eq:info-intuition-apdx} up to sub-polynomial factors.
Some sub-polynomial factors appear in the process of the proof; whether they are artifact of our proof technique or not is still an open question.

The leading constants are defined recursively as
\begin{equation}
\label{eq:defDt}
\begin{array}{ll}
D_1 \defeq \left ( \frac{\sigma^2}{2 \cdot 8^{d} \tilde C_\zeta^2 } \right )^{\frac{1}{d+2(\kappa-\zeta)}}\\
D_t \defeq \left ( \frac{D_{t-1} \eta}{4} \right )^{\frac{d}{d+2(\kappa-\zeta)}} 
\left ( \frac{\sigma^2} {8e \tilde C_\zeta^2 \alpha^{2t-2}} \right)^{\frac{1}{d+2(\kappa-\zeta)}}
&\text{ for } t \in 2:M.
\end{array}
\end{equation}
Solving the recurrence of $D_t$ gives
\begin{equation}
\label{eq:defDt2}
D_t = D_1^{\left( \frac{d}{d+2(\kappa-\zeta)} \right)^{t-1} }
\left ( \frac{\eta^d \sigma^2}{8e\cdot 4^d \tilde C_\zeta^2} \right)
^{\frac{1}{d}\sum_{m=1}^{t-1} \left (\tfrac{d}{d+2(\kappa-\zeta)}\right )^m}
\alpha^{-\frac{2}{d} \sum_{m=1}^{t-1} (t-m) \left (\tfrac{d}{d+2(\kappa-\zeta)}\right )^m }.
\end{equation}

Also, define 
\begin{equation}
\label{eq:defht}
h_t \defeq \left ( \frac{\sigma}{\tilde C_\zeta \alpha^{t-1} D_t^{\kappa-\zeta}} \right )^2 
n^{2(\kappa-\zeta) \gamma_t} 
\exp \left( 4 \sqrt{2} (\kappa-\zeta) \gamma_{t-1} \sqrt{\log n} \right)
\text{ for } t= 1:M,
\end{equation}
so that
\begin{equation}
\label{eq:defht2}
\begin{array}{ll}
\tilde C_\zeta^2 \alpha^{2t-2} h_t \delta_t^{2(\kappa-\zeta)} = \sigma^2
&\text{ for } t \in 1:M-1,
\\
\tilde C_\zeta^2 \alpha^{2M-2} h_M \delta_M^{2(\kappa-\zeta)} = \sigma^2 \log^{-\frac{2\nu(\kappa-\zeta)}{\kappa}}n.
\end{array}
\end{equation}
Note that $h_t$ is an increasing quantity as $n$ grows. Also, the exponent of $n$ in $h_t$ is 
$1-(\frac{d}{d+2(\kappa-\zeta)})^t$, so we have $h_1, \dots, h_M \leq n$ for large enough $n$.

Now that we have $h_m$'s, define events $\EvSmSplm$ as
\begin{align*}
	\EvSmSplm &\defeq \left\{ \sum_{i=1}^n \IND{ X_{i}^{(m)}\in \Bpm } \leq h_m \right\} & \textup{ for } m = 1:M.
\end{align*}
These are probabilistic events that, 
%among the points sampled in the $m$-th round, the number of points within $\delta_m$ distance from $u_m$ is no greater than $h_m$.
in the pigeonhole principle argument, this event corresponds to the case that this particular hole $\Bpm$ around $u_m$ has small number of ``pigeons'' in it.
Given the definition of $\EvSmSplm$, we can see from Eq~\eqref{eq:info-intuition-apdx} that $h_m$ corresponds to $n/|\Um|$, and Eq~\eqref{eq:defht2} is a scaled version of $\delta_t^{2(\kappa-\zeta)} \times \text{(\# of samples)}=1$.

For a fixed true function $\ftnuv$, $\EvSmSplm$ is an event that only a small number $h_m$ of 
sampled points $X_i^{(m)}$ during the $m$-th round are in the region $\Bpm$,
which contains the global minimum of $\ftnuv$.
So, if this occurs, the amount of information to distinguish between $\ftnuv$ and other functions $\ftnuvtildem$ is small, so it is hard to optimize $\ftnuv$ to global optimality.
Recall that $\probuv$ is the probability measure when underlying true function is $\ftnuv$.
So, if $\probuv(\EvSmSplm)$ happens with constant probability, it means that there is some chance for
sampling strategy $\Qm \in \alg$ to fail to sample good enough amount of informative sample points.

\subsubsection{The inductive argument}
\label{sec:proof-lbmain-induct}

The key of this part is to show that, for any algorithm $\alg$,
there \emph{exist} $u_{1:M}$ such that Eq~\eqref{eq:outlinegoal} is satisfied when we substitute $\P_- \leftarrow \probuvm$, $\P_+ \leftarrow \probuvp$, and $G \leftarrow \bigcap_{m=1}^{M} \EvSmSplm$. This is proved in Lemma~\ref{lem:lblem3} at the end of a careful inductive argument. 
This means that if the true objective function is either one of $\ftnuvp$ or $\ftnuvm$,
with constant probability we do not have enough informative samples needed to 
distinguish between $\ftnuvp$ and $\ftnuvm$, which are reasonably separated with respect to $\fndist$ (by Condition~\ref{con:ftnconstrunified}.\ref{item:ftnconstrunified-7}), hence leading to non-trivial error in optimization.

Before jumping into the induction, we present a lemma that bounds the packing numbers of sets $\Uone, \Utwo, \dots, \UM$. This will prove useful later in the induction.
\begin{lemma}
	\label{lem:packingno}
	For large enough $n$, the cardinality of maximal packings satisfy
	\begin{equation*}
		| \Uone | \geq \left(\frac{1}{8\delta_1} \right)^d, \text{ and }
		| \Ut | \geq \left( \frac{\eta \delta_{t-1}}{4\delta_t} \right )^d \text{ for } t \in 2:M,
	\end{equation*}
	regardless of the choice $u_{t-1}$.
\end{lemma}
The proof of Lemma~\ref{lem:packingno} is a simple volumetric argument, which is provided in Appendix~\ref{sec:proof-packingno}.

The proof is done by mathematical induction. 
First we fix any $M$-stage procedure $\alg$ (hence the distributions $\Qone, \dots, \QM$).
Starting from $t = 1$ to $t = M$, we prove the statement 
\begin{displayquote}
	$\stmt_t$ : There exists a chain of parameters $u_1 \in \Uone, u_2 \in \Utwo, \dots, u_M \in \UM$ and $v \in \V$ such that $\probuv (\bigcap_{m=1}^t \EvSmSplm ) \geq \frac{1}{4^{t}}$ for sufficiently large $n$.
\end{displayquote}

\paragraph{Base case ($t=1$).}
Since the first stage observations are sampled without any information about the true function, the sampling strategies are all identical regardless of the true function $\ftnuv$.
This is, in other words, $\probuv(X_{i}^{(1)}\in \Bpone) = \Qone(X_{i}^{(1)}\in \Bpone)$, 
for any $u_{1:M}$ and $v$.
Recall that interiors of balls $\interior(\Bpone)$ are disjoint for different $u_1 \in \Uone$.
So, for any fixed $\Qone$,
\begin{equation}
\label{eq:smallthann}
\sum_{u_1 \in \Uone} \sum_{i=1}^n \probuv(X_{i}^{(1)}\in \Bpone)
=\sum_{i=1}^n \sum_{u_1 \in \Uone} \Qone(X_{i}^{(1)}\in \Bpone) \leq n.
\end{equation}
Now by the pigeonhole principle, there must exist at least one $u_1 \in \Uone$ such that 
\begin{equation*}
	\sum\nolimits_{i=1}^n \Qone(X_{i}^{(1)}\in \Bpone) \leq \tfrac{n}{| \Uone |}.
\end{equation*}
%Otherwise, $\sum_{i=1}^n \Qone(X_{i}^{(1)}\in \Bpone) > \frac{n}{| \Uone |}$ for all $u_1$, so summing up for all $u_1$'s will contradict Eq~\eqref{eq:smallthann}. Thus, there must exist such $u_1 \in \Uone$.
Now recall $| \Uone | \geq \left( \frac{1}{8\delta_1} \right )^d$ from Lemma~\ref{lem:packingno}.
Given $u_1$, choose the next parameters $u_{2:M}$ and $v$ arbitrarily. Then,
\begin{equation*}
	\sum_{i=1}^n \Qone(X_{i}^{(1)}\in \Bpone) = \exptuv \left [\sum_{i=1}^n \IND{ X_{i}^{(1)}\in \Bpone } \right ] \leq (8\delta_1)^d n.
\end{equation*}
Then, for those chosen $u_{1:M}$ and $v$, by Markov's inequality,
\begin{align*}
	\probuv \left ((\EvSmSplone)^c \right ) 
	&= \probuv \left (\sum_{i=1}^n \IND{ X_{i}^{(1)}\in \Bpone } > h_1 \right ) 
	\leq \frac{(8 \delta_1)^d n}{h_1} \\
	&= \frac{8^d \tilde C_\zeta^2}{\sigma^2}\delta_1^{d+2(\kappa-\zeta)} n
	= \frac{8^d \tilde C_\zeta^2}{\sigma^2} D_1^{d+2(\kappa-\zeta)} n^{-\gamma_1(d+2(\kappa-\zeta))} n
	= \half,
\end{align*}
by definition of $D_1$~\eqref{eq:defDt} and $\gamma_1$~\eqref{eq:defgammat}.
This implies $\stmt_1$.
In words, for the first-stage sampling strategy $\Qone$, there exist parameter $u_{1:M}$ and $v$ such that,
no more than $h_1$ sample points are in $\Bpone$ (the region where global optimum lies) with probability at least 1/4.

%\paragraph{Inductive step ($t =2$).}
%From the previous step, we saw that for any fixed $\Qone$, there exists a $u_1 \in \Uone$ such that
%for any choice of the next parameters $u_2 \in \Utwo, \dots, u_{M} \in \UM$ and $v \in \V$,
%$\probuv(\EvSmSplone) \geq \frac{1}{4}$.
%Note that this holds for \emph{any} choice of $u_{2:M}$ and $v$.
%Given that 

\paragraph{Inductive step ($2 \leq t \leq M$).}
%Recall that $\probuv$ is a probability measure when the ground-truth function is $\ftnuv$.
At step $t$, by the induction hypothesis $\stmt_{t-1}$ we know that there exist $u_{1:M}$ and $v$ 
such that $\probuv (\bigcap_{m=1}^{t-1} \EvSmSplm ) \geq \frac{1}{4^{t-1}}$.
This means that for the sampling strategies $\Qone, \Qtwo, \dots, \Qtmone$,
there is constant probability for them all to fail to sample sufficient amount of samples 
in $\Bpone, \Bptwo, \dots, \Bptmone$.
Note that these are the balls containing the global minimizer of $\ftnuv$.

%Suppose that the event $\bigcap_{m=1}^{t-1} \EvSmSplm$ is already given. At round $t$, the next sampling scheme $\Qt$, at best, will focus on sampling more points in $\Bptmone$.
%However, by the following two lemmas, we make pigeon hole principle work again, and find a $\tilde u_t \in \Ut$ such that $\EvSmSpltildet$ occurs; i.e.\ find a ``hole'' $\ballp{\tilde u_t}{\delta_t}$ such that small number of points are sampled with constant probability.
%The procedure of this step is as follows.
Given the chain $u_{1:M}$ and $v$ as defined by $\stmt_{t-1}$,
consider re-choosing the parameters from level $t$ and onwards.
That is, we leave the first $t-1$ parameters $u_{1:t-1}$ unchanged, and arbitrarily
re-choose the rest of them to define another chain of parameters,
say $\tilde u_t \in \Utemp{t-1}{t}, \tilde u_{t+1} \in \mathcal{U} _{\tilde u_t}^{(t+1)}, \dots, \tilde u_{M} \in \mathcal{U} _{\tilde u_{M-1}}^{(M)}$ and $\tilde v \in \V$.
Then, note by Condition~\ref{con:ftnconstrunified}.\ref{item:ftnconstrunified-1} that 
$\ftnuv(x) = \ftnuvtildet(x)$ for all $x \notin \Bptmone$.
Since the two functions $\ftnuv(x)$ and $\ftnuvtildet(x)$ look exactly the same outside $\Bptmone$
and it is known that $\Qone, \dots, \Qtmone$ are likely to 
fail to sample sufficient amount of samples in $\Bpone, \dots, \Bptmone$,
it is plausible to conjecture that the similar thing might happen to $\ftnuvtildet(x)$ as well.
The next lemma formalizes and proves this idea:
\begin{lemma}
	\label{lem:lblem1}
	Suppose there exist $u_{1:M}$ and $v$ 
	such that $\probuv (\bigcap_{m=1}^{t-1} \EvSmSplm ) \geq \frac{1}{4^{t-1}}$ for sufficiently large $n$ ($\stmt_{t-1}$).
	For \textbf{\emph{any}} $\tilde u_{t:M}$ and $\tilde v$ re-chosen as above, the following inequality holds: 
	\begin{equation}
	\label{eqn:lblem1induc}
	\probuvtildet \left (\bigcap\nolimits_{m=1}^{t-1} \EvSmSplm \right ) 
	\geq \frac{1}{2 \cdot 4^{t-1}},
	\end{equation}
	for sufficiently large $n$.
\end{lemma}
Notice that this lemma holds for \emph{any} new choice of $\tilde u_{t:M}$ and $\tilde v$.
The proof of Lemma~\ref{lem:lblem1} is presented in Appendix~\ref{sec:proof-lblem1}.

Next, consider the conditional probability of $\EvSmSpltildet$ given $\bigcap_{m=1}^{t-1} \EvSmSplm$,
for any re-chosen $\tilde u_t \in \Ut$.
Recall that the sampling strategy $\Qt$ was fixed before we start our proof, 
so the conditional distribution $\Qt(X_{1:n}^{(t)} \mid \bigcap_{m=1}^{t-1} \EvSmSplm)$ 
is also a fixed probability distribution.
Then, by the pigeonhole principle, there exists at least one $\tilde u_t \in \Ut$ such that $\ballp{\tilde u_t}{\delta_t}$ are scarce-sampled, i.e., have at most $h_t$ sample points in $\ballp{\tilde u_t}{\delta_t}$, with a constant probability.
%Given that $\bigcap_{m=1}^{t-1} \EvSmSplm$ already occurred, as described above, the distribution $\Qt(X_{1:n}^{(t)} \mid \bigcap_{m=1}^{t-1} \EvSmSplm)$ is likely to sample more points in/near $\Bptmone$.
The next lemma formalizes and proves this idea.
\begin{lemma}
	\label{lem:lblem2}
	Suppose there exist $u_{1:M}$ and $v$ 
	such that $\probuv (\bigcap_{m=1}^{t-1} \EvSmSplm ) \geq \frac{1}{4^{t-1}}$ for sufficiently large $n$ ($\stmt_{t-1}$).
	Then, there \textbf{\emph{exist}} re-chosen parameters $\tilde u_{t:M}$ and $\tilde v$ such that 
	the following lower bound is satisfied:
	\begin{equation}
	\label{eqn:lblem2induc}
	\probuvtildet \left (\EvSmSpltildet \mid \bigcap\nolimits_{m=1}^{t-1} \EvSmSplm \right ) \geq \half
	\end{equation}
\end{lemma}
The proof of Lemma~\ref{lem:lblem2} is in Appendix~\ref{sec:proof-lblem2}.
By Lemma~\ref{lem:lblem2}, there exists a function $\ftnuvtildet$ such that Eq~\eqref{eqn:lblem2induc} holds. 
Combining this with Eq~\eqref{eqn:lblem1induc}, we finish the proof of $\stmt_t$.

\paragraph{Final step.}
The proof of the final step is similar to the inductive step.
From $\stmt_M$, let $u_{1:M}$ and $v$ be the parameter values 
satisfying $\probuv (\bigcap_{m=1}^{M} \EvSmSplm ) \geq \frac{1}{4^{M}}$.
We can state another lemma, which actually is our goal \eqref{eq:outlinegoal}:
\begin{lemma}
	\label{lem:lblem3}
	Suppose there exist $u_{1:M}$ and $v$ 
	such that $\probuv (\bigcap_{m=1}^{M} \EvSmSplm ) \geq \frac{1}{4^{M}}$ for sufficiently large $n$.
	Then, for $\tilde v \neq v$, the following lower bound is satisfied for $n$ large enough:
	\begin{equation}
	\label{eqn:lblem3eq1}
	\probuvtildetfin \left (\bigcap\nolimits_{m=1}^{M} \EvSmSplm  \right ) \geq \frac{1}{2 \cdot 4^{M}}.
	\end{equation}
	Also, the total variation distance between two conditional probability $\probuvm$ and $\probuvp$ 
	given the event $\bigcap\nolimits_{m=1}^{M} \EvSmSplm$ satisfies
	\begin{equation}
	\label{eqn:lblem3eq2}
	\tvnorm{ \probuvm \left (\cdot \mid \bigcap\nolimits_{m=1}^{M} \EvSmSplm \right ) -
		\probuvp \left (\cdot \mid \bigcap\nolimits_{m=1}^{M} \EvSmSplm \right ) } \leq \frac{1}{2},
	\end{equation}
	for sufficiently large $n$.
\end{lemma}
The proof is very similar to those of Lemma~\ref{lem:lblem1} and \ref{lem:lblem2}, and is also deferred to Appendix~\ref{sec:proof-lblem3}.

\subsection{Finishing the proof}
\label{sec:proof-lbmain-finish}
Now, we are left with the final step of our proof. Recall from Eq~\eqref{eq:finallb}, Lemmas~\ref{lem:opt2hyptest} and \ref{lem:lblem3}, Condition~\ref{con:ftnconstrunified}.\ref{item:ftnconstrunified-7}, and substituting $G \leftarrow \bigcap\nolimits_{m=1}^{M} \EvSmSplm$ that 
\begin{align*}
\minimax_M(\ffamily, \ofamily) 
&\geq \frac{1}{2\cdot 4^M} \max_{v \in \V} \exptuv [ \ftnuv(\what X) - (\ftnuv)^\star \mid \EvSmSplFrmOneTo{M}]\\
&\geq \frac{\fndist(\ftnuvm, \ftnuvp)}{2\cdot 4^{M+1}}
\left( 1-\tvnorm{ \probuvm(\cdot \mid \bigcap\nolimits_{m=1}^{M} \EvSmSplm) -\probuvp+(\cdot \mid \bigcap\nolimits_{m=1}^{M} \EvSmSplm) } \right)\\
&\geq \frac{C\alpha^M \eta^\kappa D_M^\kappa}{2\cdot 4^{M+1}}n^{-\kappa \gamma_M} \exp\left ( - 2 \sqrt{2} \kappa \gamma_{M-1} \sqrt{\log n} \right ) \log^{-\nu} n,
\numberthis \label{eq:generalbd}
\end{align*}
where the last inequality used 
\begin{align*}
&\fndist(\ftnuvm,\ftnuvp) = 2C\alpha^M \eta^\kappa \delta_M^\kappa, \text{ and }\\
&\delta_M \defeq 
D_M n^{ -\gamma_M } \exp \left( -2 \sqrt{2} \gamma_{M-1} \sqrt{\log n} \right) \log^{-\nu/\kappa} n.
\end{align*}

The sub-polynomial factors such as
$\exp\left ( - 2 \sqrt{2} \kappa \gamma_{M-1} \sqrt{\log n} \right )$ and $\log^{-\nu} n$
unfortunately appear during the course of analysis (while proving lemmas),
and whether they are artifacts of analysis or inevitable factors is not clear at this moment.
To simplify the expressions a bit, note from Eq~\eqref{eq:defgammat} that
\begin{equation*}
\gamma_{M-1} =
\frac{1}{2(\kappa-\zeta)} \left (1 - \left( \frac{d}{d+2(\kappa-\zeta)} \right)^{M-1}\right ),
\end{equation*}
and that the term
\begin{equation*}
\exp\left ( \frac{\sqrt{2} \kappa}{\kappa-\zeta}  \left( \frac{d}{d+2(\kappa-\zeta)} \right)^{M-1} \sqrt{\log n} \right ) \log^{-\nu} n
\end{equation*}
in Eq~\eqref{eq:generalbd} is an increasing but sub-polynomial factor in $n$. Thus, in presenting the lower bound we can safely discard those factors, resulting in a bound
\begin{align*}
\minimax_M(\ffamily, \ofamily) 
\geq \frac{C\alpha^M \eta^\kappa D_M^\kappa}{2\cdot 4^{M+1}}n^{-\kappa \gamma_M} \exp\left ( - \frac{\sqrt{2} \kappa \sqrt{\log n}}{\kappa-\zeta}  \right ).
\numberthis \label{eq:generalbd2}
\end{align*}

\paragraph{Bounding the leading constant.}
From the leading constant $\frac{C\alpha^M \eta^\kappa D_M^\kappa}{2 \cdot 4^{M+1}}$ that appeared in Eq~\eqref{eq:generalbd2} 
and the definition of $D_M$ in Eq~\eqref{eq:defDt2},
we can check that the leading constant is dependent on
$M$, $d$, $\sigma$, $C$, $\alpha$, $\eta$, $\beta$, $\kappa$, and $\zeta$,
and has a lower bound of the form $\conthm_1 \cdot r^M$, where $\conthm_1 > 0$ and $0 < r < 1$ are independent of $M$.
Recall that our theorem statements are for $M \leq \log\log n/\log(1+\frac{2(\kappa-\zeta)}{d})$. This implies that
\begin{equation*}
\log(r^M) \geq \log \left (r^{\log\log n/\log(1+\frac{2(\kappa-\zeta)}{d})} \right ) = \frac{\log r}{\log (1+\frac{2(\kappa-\zeta)}{d})} \log \log n = \log \left( (\log n)^{-\conthm_2} \right ),
\end{equation*}
where $\conthm_2 \defeq - \log r / \log (1+\frac{2(\kappa-\zeta)}{d}) > 0$. Thus, for small $M$,
the leading constant can be bounded below by $\conthm_1 \cdot \log^{-\conthm_2} n$, where $\conthm_1, \conthm_2>0$ depend on 
$d$, $\sigma$, $C$, $\alpha$, $\eta$, $\beta$, $\kappa$, and $\zeta$, \emph{not} $M$.

Now recall from Condition~\ref{con:ftnconstrunified} and Lemmas~\ref{lem:ftnconstrLipcvx} and \ref{lem:ftnconstrsscvx} that 
$C$, $\alpha$, $\eta$, $\beta$, $\kappa$ are defined according to function classes, 
and $\zeta$ depends on oracle order.
So, in Lipschitz convex/zeroth-order case ($\ffamily_{L}$), the constants $\conthm_1$ and $\conthm_2$ depend only on $d$, $\sigma$, $L$. In smooth strongly convex case ($\ffamily_{H,\lambda}$), the constants $\conthm_1$, $\conthm_2$ (for $\zeta = 0,1$) depend only on $d$, $\sigma$, $H$, and $\lambda$.

\paragraph{Proving each cases.}
%Recall the exponent constant $\gamma_M$ \eqref{eq:defgammat}:
%\begin{equation*}
%$	\gamma_M =
%	\frac{1}{2(\kappa-\zeta)} \left (1 - \left( \frac{d}{d+2(\kappa-\zeta)} \right)^M\right ) $
%\end{equation*}
Given the general bound \eqref{eq:generalbd2}, and definition of $\gamma_{M}$ \eqref{eq:defgammat}, let us now substitute $\kappa$ and $\zeta$ to finish the proof for each case presented in Theorem~\ref{thm:lbmain}.
Recall that $\kappa = 1$ for Lipschitz convex functions, and $\kappa = 2$ for smooth strongly convex functions.
Also, $\zeta = 0$ for zeroth order oracle, and $\zeta = 1$ for first order oracle.

In the case of $L$-Lipschitz and zeroth-order $(\kappa, \zeta) = (1,0)$, for $M \leq \log\log n/\log(1+\frac{2}{d})$,
\begin{equation*}
\begin{array}{l}
	\minimax_M(\ffamily_{L}, \ofamilyz) 
	\geq 
		\conthm_1 n^{-\half \left (1 - \left( \frac{d}{d+2} \right)^M\right )  } 
		e^{- \sqrt{2 \log n}} \log^{-\conthm_2} n,
\end{array}
\end{equation*}
where $\conthm_1, \conthm_2>0$ depend only on $d$, $\sigma$, $L$.
The cases of $(\kappa, \zeta) = (2,0)$ and $(\kappa, \zeta) = (2,1)$ can be treated in similar ways.
%, where $K_L^{(0)}$ depends only on $M$, $d$, $\sigma$, $L$.
%For $H$-smooth and $\lambda$-strongly convex functions,
%\begin{equation*}
%\begin{array}{l}
%	\sup_{f\in \ffamily_{H,\lambda}} \E[f(\what X) - f^\star]
%	\geq 
%	K_{H,\lambda}^{(0)} n^{-\half \left (1 - \left( \frac{d}{d+4} \right)^M\right) } 
%	\exp\left ( - \sqrt{2\log n} \right ) \log^{-\nu} n,\\
%	\sup_{f\in \ffamily_{H,\lambda}} \E[f(\what X) - f^\star]
%	\geq 
%	K_{H,\lambda}^{(1)} n^{- \left (1 - \left( \frac{d}{d+2} \right)^M\right)  } 
%	\exp\left ( - \sqrt{8\log n} \right ) \log^{-\nu} n,
%\end{array}
%\end{equation*}
%for zeroth- and first-order oracles, respectively. 
%The leading constants $K_{H,\lambda}^{(0)}$ and $K_{H,\lambda}^{(1)}$ depend only on $M$, $d$, $\sigma$, $H$, $\lambda$.

\section{Function construction for smooth strongly convex functions}
\label{sec:constsscvx}

This section handles the construction of smooth strongly convex functions ($\ffamily_{H, \lambda}$). The high-level idea is the same as the Lipschitz case, but 
taking max of two smooth functions $f$ and $g$ can break smoothness on the \emph{intersection set} $\intset{f}{g} \defeq \{x \in \R^d \mid f(x) = g(x) \}$, so this case requires considerably more involved treatment. On the vicinity of the intersection set, we \emph{interpolate} the gradients of two functions to smooth the boundary, using suprema of infinitely many hyperplanes.
Section~\ref{sec:constsscvx-1dex} shows a simple 1-d example that illustrates the idea of ``smoothing'' maximum of two quadratic functions, and Section~\ref{sec:constsscvx-md} presents the algorithm $\algmaxsmth$ that calculates smooth maximum of two multi-dimensional quadratic functions. Finally, Section~\ref{sec:multiconstsscvx} describes recursive multi-stage construction of the function (with $\ell_2$ balls at this time) by a similar way as in Section~\ref{sec:constLipcvx}.
We also present that with suitable parameter choices, the constructed functions are in $\ffamily_{H,\lambda}$ and satisfy Condition~\ref{con:ftnconstrunified}.

\subsection{Smooth interpolation of two quadratic functions: a 1D example}
\label{sec:constsscvx-1dex}
Before we describe all the complicated details, let us start with an easy 
1D example that illustrates our key approach.
In Lipschitz convex case, the goal of getting functions that have the same values 
outside a certain set while having different values in that set was simply achieved by
taking $\max$ operations. 
We want to do the same for smooth strongly convex functions, 
but the difficulty is that the maximum of two functions $f$ and $g$ is not smooth on the intersection set $\intset{f}{g}$.
To remedy this problem, we can \emph{interpolate} or \emph{smooth} the functions near the intersection set using \emph{suprema of infinitely many hyperplanes.}

To illustrate the idea, let us start with a simple 1D example; 
consider taking the maximum of 
two quadratic functions $f_1(x) = 2x^2$ and $f_2(x) = (x-1)^2 + 14$ defined on $[0, \infty)$. 
Their intersection set is $\intset{f_1}{f_2}= \{ 3 \}$, 
which contains the only non-smooth point of $\max\{f_1, f_2\}$.
Note that $\max\{f_1, f_2\} = f_1$ for $x \geq 3$ and $\max\{f_1, f_2\} = f_2$ for $x \leq 3$.
Smoothing is done by linearly interpolating the gradient in the vicinity 
of the non-smooth point, for example $x \in [2,4]$, 
while leaving the function values outside $[2,4]$ the same.
We define constants $\dot g_-, \dot g_0, \dot g_+$:
\begin{equation*}
	\dot g_- \defeq \dot f_2(2) = 2, ~
	\dot g_0 \defeq \frac{\dot f_2(3)+\dot f_1(3)}{2} = \frac{4+12}{2} = 8, ~
	\dot g_+ \defeq \dot f_1(4) = 16,
\end{equation*}
and then define the linearly interpolated gradients
\begin{align*}
	\dot h_-(x) &\defeq \dot g_- + (\dot g_0 - \dot g_-) (x-2) = 6x-10 & \text{ for } x \in [2,3],\\
	\dot h_+(x) &\defeq \dot g_+ - (\dot g_0 - \dot g_+) (x-4) = 8x-16 & \text{ for } x \in [3,4].
\end{align*}
Since the gradient in $[2,4]$ changed, also calculate interpolated function value accordingly:
\begin{align*}
	h_-(x) & \defeq f_2(2) + \int_2^x \dot h_-(t) dt = 3x^2-10x+23 & \text{ for } x \in [2,3],\\
	h_+(x)& \defeq f_1(4) - \int_x^4 \dot h_+(t) dt = 4x^2-16x+32 & \text{ for } x \in [3,4].
\end{align*}
Note that $f_2(2) = h_-(2) = 15$, $h_-(3) = h_+(3) = 20$, and $h_+(4) = f_1(4) = 32$,
so the functions $f_2$, $h_-$, $h_+$, $f_1$ can be ``connected'' to make a continuous function.
Lastly define an infinite number of affine functions using previously calculated $\dot h_-$, $\dot h_+$, $h_-$ and $h_+$:
\begin{align*}
	f_-^\rho(x) \defeq \dot h_-(\rho)(x-\rho)+h_-(\rho) & \text{ for } \rho \in [2,3],\\
	f_+^\rho(x) \defeq \dot h_+(\rho)(x-\rho)+h_+(\rho) & \text{ for } \rho \in [3,4].
\end{align*}
Here, we are defining one affine function $f_-^\rho(x)$ for \emph{each} value of $\rho \in [2,3]$. Same applies to $f_+^\rho$ for $\rho \in [3,4]$.

Now, we can define the interpolated function, which is the supremum of the original functions
$f_1$ and $f_2$, and affine functions $f_-^\rho$ and $f_+^\rho$:
\begin{equation}
\label{eq:ftn1d}
f(x) \defeq \max \left \{ 
f_1(x), f_2(x), 
\sup_{\rho \in [2,3]} f_-^\rho(x),
\sup_{\rho \in [3,4]} f_+^\rho(x) 
\right \}.
\end{equation}
We can prove that this $f(x)$, defined as the maximum of many functions,
is actually a smooth interpolation of maximum of $f_1$ and $f_2$. We state this in 
the following lemma. 
\begin{lemma}
	\label{lem:ssftn1d}
	The function $f(x)$ defined in Eq~\eqref{eq:ftn1d} satisfies the following:
	\begin{equation*}
		f(x) = 
		\begin{cases}
			f_2(x) & \text{if } 0 \leq x \leq 2\\
			h_-(x) & \text{if } 2 \leq x \leq 3\\
			h_+(x) & \text{if } 3 \leq x \leq 4\\
			f_1(x) & \text{if } x \geq 4,
		\end{cases}
		\text{ and }
		\dot f(x) = 
		\begin{cases}
			\dot f_2(x) & \text{if } 0 \leq x \leq 2\\
			\dot h_-(x) & \text{if } 2 \leq x \leq 3\\
			\dot h_+(x) & \text{if } 3 \leq x \leq 4\\
			\dot f_1(x) & \text{if } x \geq 4.
		\end{cases}
	\end{equation*}
	Also, $f(x)$ is a $8$-smooth and $2$-strongly convex function.
\end{lemma}
The proof of Lemma~\ref{lem:ssftn1d} is a special case of Lemmas~\ref{lem:smthconstvalues} and \ref{lem:smthconstparams}, which we will omit.

By Lemma~\ref{lem:ssftn1d}, we saw that the maximum of 
two quadratic functions can be smoothly interpolated by using infinite number of hyperplanes. 
Also note that this function is $8$-smooth and $2$-strongly convex,
whereas $f_1$ and $f_2$ are in the class of $4$-smooth and $2$-strongly convex functions; the interpolation cause the ``increase'' of the smoothness constant,
which we will observe in the multi-dimensional example as well.

\subsection{Smooth interpolation of two quadratic functions: multi-dimension}
\label{sec:constsscvx-md}
\paragraph{Extending to multi-dimension.}
Now, we extend the domain to $d$-dimension, and consider taking the maximum of 
two quadratic functions $\fxone(x)$ and $\fxtwo(x)$, each minimized at $x_1$ and $x_2$, respectively, where $x_2 \in \balltwo{x_1}{\eta\delta}$:
\begin{equation*}
	\fxone(x) \defeq \ltwosqr{x-x_1 } ~~\text{and}~~
	\fxtwo(x) \defeq \alpha \ltwosqr{x-x_2} + \beta \delta^2,
\end{equation*}
where the parameters satisfy $0<\eta<1$, $\delta > 0$, $0<\alpha<1$, and $0<\beta<1$.

By solving $\fxone(x) = \fxtwo(x)$, we can see that their intersection set $\intset{\fxone}{\fxtwo} \defeq \{ x \in \R^d \mid \fxone(x) = \fxtwo(x)\}$ is a sphere:
\begin{equation*}
	\intset{\fxone}{\fxtwo} = \{ x \mid \ltwosqr {x-c} = r^2 \},
\end{equation*}
where 
\begin{align}
	c &= \frac{1}{1-\alpha} x_1 - \frac{\alpha}{1-\alpha} x_2 = x_1 - \frac{\alpha}{1-\alpha} (x_2-x_1), \label{eq:defcent}\\
	r &= \sqrt{\frac{\alpha}{(1-\alpha)^2} \ltwosqr{x_1-x_2} + \frac{\beta\delta^2}{1-\alpha}}. \label{eq:defradi}
	%	=\sqrt{\frac{(1+\eta)^2 \delta^2}{4}-\frac{\alpha}{(1-\alpha)^2} (\eta^2 \delta^2-\ltwosqr{x_1-x_2})}.
\end{align}
Since $\frac{\alpha}{(1-\alpha)^2} \ltwosqr{x_1-x_2} + \frac{\beta\delta^2}{1-\alpha} > 0$ by assumptions on parameters, this sphere exists.

As seen in the 1D example,
we need some ``margin'' for interpolation near non-smooth points.
In the 1D example the ``margin'' or what we call ``interpolation set'' was the interval $[2,4]$, on which we alter the function values to do smooth interpolation.
So, after taking the maximum between $\fxone$ and $\fxtwo$, 
we will smooth the non-smooth points in the intersection set 
$\intset{\fxone}{\fxtwo}$ by linearly interpolating the gradients
on a set $\itpset$ called ``interpolation set'':
\begin{equation*}
	\itpset \defeq \left \{ x \mid (1-\theta)r \leq \ltwo{x-c} \leq (1+\theta)r \right \},
\end{equation*}
where $0<\theta<1$ will be chosen shortly.

\paragraph{Choosing the right parameters.}
We now state a lemma that chooses the parameters in the ``right'' way 
that makes our construction of smooth maximum easier.
\begin{lemma}
	\label{lem:ssftnmdparam}
	Recall the conditions $0<\eta<1$, $\delta > 0$, $0<\alpha<1$, $0<\beta<1$, and $0<\theta<1$ on parameters of $\fxone$, $\fxtwo$, and $\itpset$.
	Choose parameters that satisfy
	\begin{align}
		&\eta+\alpha+\alpha\eta < 1,\label{eq:constraints}\\
		&\beta = \frac{(1-\alpha)(1+\eta)^2}{4} - \frac{\alpha \eta^2}{1-\alpha}, \label{eqn:smthconstbetaval}\\
		&\theta = \frac{1-\eta-\alpha-\alpha\eta}{1+\eta-\alpha-\alpha\eta}. \label{eqn:smthconstthetaval}
	\end{align}
	Then, the following statements hold: 
	\begin{align}
		&\itpset \subset \cl(\balltwo{x_1}{\eta\delta}^c \cap \balltwo{x_1}{\delta}) ~\text{ for any } x_2 \in \balltwo{x_1}{\eta\delta}, \label{eqn:itpsubset}%\\
		%&\fxone(x) \leq \fxtwo(x) ~\text{ for any } x \in \balltwo{x_1}{\eta\delta},\label{eqn:fxtwobig}\\
		%&\fxone(x) \geq \fxtwo(x) ~\text{ for any } x \notin \balltwo{x_1}{\delta}.\label{eqn:fxonebig}
	\end{align}
\end{lemma}
The proof of Lemma~\ref{lem:ssftnmdparam} is provided in Appendix~\ref{sec:proof-ssftnmdparam}.
With the parameters satisfying Eqs~\eqref{eq:constraints}--\eqref{eqn:smthconstthetaval}, we can ensure that
the interpolation set is a subset of $\cl(\balltwo{x_1}{\eta\delta}^c \cap \balltwo{x_1}{\delta})$,
so any point $x \notin \cl(\balltwo{x_1}{\eta\delta}^c \cap \balltwo{x_1}{\delta})$ will not be affected by the interpolation.

\paragraph{Smoothing the maximum of two quadratic functions.}

\begin{algorithm}[t]
	\caption{Algorithm $\algmaxsmth(\fxone, \fxtwo, \alpha, \eta, \delta)$.}  
	\label{alg:algmaxsmth}
	\begin{algorithmic}[1]  % [1] is line numbering
		\STATEx Assume $0<\alpha<1, 0<\eta<1, \eta+\alpha+\alpha\eta<1, \delta>0$. Let $\beta \defeq \frac{(1-\alpha)(1+\eta)^2}{4} - \frac{\alpha \eta^2}{1-\alpha}$.
		\STATEx Assume $\fxone(x) = s\ltwosqr{x-x_1 } + t$, $\fxtwo(x) = s\alpha \ltwosqr{x-x_2} + s\beta \delta^2 + t$, $s>0$, $x_2 \in \balltwo{x_1}{\eta\delta}$
		\STATE $\theta \defeq \frac{1-\eta-\alpha-\alpha\eta}{1+\eta-\alpha-\alpha\eta},~c \defeq x_1 - \frac{\alpha}{1-\alpha} (x_2-x_1),~ r \defeq \sqrt{\frac{\alpha}{(1-\alpha)^2} \ltwosqr{x_1-x_2} + \frac{\beta\delta^2}{1-\alpha}}$.
		\STATE For all $\rho \in [(1-\theta)r,r]$ and all unit vectors $\ltwo w = 1$, 
		\begin{align*}
			\dot h_-(\rho,w) 
			\defeq& \frac{2\alpha}{1-\alpha}(x_1-x_2) 
			-	\frac{(1-\alpha)(1-\theta)r}{\theta} w 
			+ \left ( \frac{1-\alpha}{\theta}+2\alpha \right ) \rho w,\\
			h_-(\rho,w) 
			\defeq &\frac{\alpha}{(1-\alpha)^2} \ltwosqr{x_1-x_2}
			+ \beta \delta^2 + \frac{(1-\alpha)(1-\theta)^2 r^2}{2\theta}\\
			&+ \left ( \frac{2\alpha}{1-\alpha} \< x_1-x_2, w \> - \frac{(1-\alpha)(1-\theta)r}{\theta} \right ) \rho
			+ \left ( \frac{1-\alpha}{2\theta} + \alpha \right ) \rho^2,\\
			\fhypm (x) 
			\defeq& s \< \dot h_-(\rho,w), x-(c+\rho w) \> + s h_-(\rho,w) + t.
		\end{align*}
		\STATE For all $\rho \in [r, (1+\theta)r]$ and all unit vectors $\ltwo w = 1$,
		\begin{align*}
			\dot h_+(\rho,w) 
			\defeq& \frac{2\alpha}{1-\alpha}(x_1-x_2) 
			- \frac{(1-\alpha)(1+\theta)r}{\theta} w 
			+ \left ( \frac{1-\alpha}{\theta}+2 \right ) \rho w,\\
			h_+(\rho,w) 
			\defeq &\frac{\alpha^2}{(1-\alpha)^2} \ltwosqr{x_1-x_2}
			+ \frac{(1-\alpha)(1+\theta)^2 r^2}{2\theta}\\
			&+ \left ( \frac{2\alpha}{1-\alpha} \< x_1-x_2, w \> - \frac{(1-\alpha)(1+\theta)r}{\theta} \right ) \rho
			+ \left ( \frac{1-\alpha}{2\theta} + 1 \right ) \rho^2,\\
			\fhypp (x) 
			\defeq & s \< \dot h_+(\rho,w), x-(c+\rho w) \> + s h_+(\rho,w) + t.
		\end{align*}
		\STATE Return $f(x) \defeq \max \left \{ 
		\fxone(x), \fxtwo(x), 
		\sup_{\rho \in [(1-t)r,r], \ltwo w = 1} \fhypm(x),
		\sup_{\rho \in [r,(1+t)r], \ltwo w = 1} \fhypp(x)
		\right \}.$
	\end{algorithmic}
\end{algorithm}

We now describe how the smooth interpolation of $\max\{\fxone, \fxtwo\}$ is done in $\itpset$. 
Notice that $\itpset$ can be expressed in a ``polar'' form:
\begin{equation*}
	\itpset 
	\defeq \left \{ x \mid (1-\theta)r \leq \ltwo{x-c} \leq (1+\theta)r \right \}
	= \left \{ 
	c + \rho w 
	\mid
	(1-\theta) r \leq \rho \leq (1+\theta) r, 
	\ltwo{w} = 1
	\right \},
\end{equation*}
and we will specify the new interpolated gradient and function values for all $(1-\theta) r \leq \rho \leq (1+\theta) r$ and $\ltwo{w} = 1$.
For each fixed direction $w$,
interpolated gradients $\dot h_-(\rho, w)$ and $\dot h_+(\rho, w)$ are obtained by
linearly interpolating the gradients along $w$.
After that, we obtain interpolated function values $h_-(\rho, w)$ and $h_+(\rho, w)$ 
by integrating directional derivatives along $w$, starting from $\fxtwo(c+(1-\theta)r)$ and $\fxone(c+(1+\theta)r)$, respectively.

For each fixed $w$, we define
\begin{align*}
	\dot g_-(w) &\defeq \nabla \fxtwo (c+(1-\theta)rw),\\
	\dot g_0(w) &\defeq \frac{\nabla \fxtwo (c+rw)+\nabla \fxone (c+rw)}{2},\\
	\dot g_+(w) &\defeq \nabla \fxone (c+(1+\theta)rw).
\end{align*}
and then linearly interpolate the gradients along each direction $w$:
\begin{align*}
	\dot h_-(\rho,w) 
	&\defeq \dot g_-(w) + \frac{\dot g_0(w) - \dot g_-(w)}{\theta r} (\rho-(1-\theta) r) 
	&\text{ for } \rho \in [(1-\theta)r,r], \\
	\dot h_+(\rho,w) 
	&\defeq \dot g_+(w) - \frac{\dot g_0(w) - \dot g_+(w)}{\theta r} (\rho-(1+\theta) r)
	& \text{ for } \rho \in [r,(1+\theta)r].
\end{align*}
Function values are obtained by integrating the directional derivatives along the direction $w$:
The function values after interpolation is calculated by integrating the directional derivatives, i.e.,
\begin{align*}
	h_-(\rho,w)
	& \defeq 	\fxtwo(c+(1-\theta)rw) + 
	\int_{(1-\theta)r}^\rho 
	\<
	\dot h_-(t,w), w
	\>
	dt
	& \text{ for } \rho \in [(1-\theta)r,r],\\
	h_+(\rho,w)
	& \defeq 	\fxone(c+(1+\theta)rw) -
	\int_\rho^{(1+\theta)r}
	\<
	\dot h_+(t,w), w
	\>
	dt
	& \text{ for } \rho \in [r,(1+\theta)r].
\end{align*}
Using $\dot h_-$, $\dot h_+$, $h_-$, and $h_+$ defined as above, we can define 
infinite number of hyperplanes corresponding to each point $c+\rho w$ in $\itpset$,
\begin{align*}
	\fhypm (x) &\defeq \< \dot h_-(\rho,w), x-(c+\rho w) \> +h_-(\rho,w) 
	& \text{ for } \rho \in [(1-\theta)r,r], \ltwo{w} = 1,\\
	\fhypp (x) &\defeq \< \dot h_+(\rho,w), x-(c+\rho w) \> +h_+(\rho,w) 
	& \text{ for } \rho \in [r,(1+\theta)r], \ltwo{w} = 1.
\end{align*}
Finally, we define the smoothed function
\begin{equation*}
	f(x) \defeq \max \left \{ 
	\fxone(x), \fxtwo(x), 
	\sup_{\rho \in [(1-t)r,r], \ltwo w = 1} \fhypm(x),
	\sup_{\rho \in [r,(1+t)r], \ltwo w = 1} \fhypp(x)
	\right \}.
\end{equation*}
For more details of the calculation, please refer to Appendix~\ref{sec:proof-algmaxsmth}.

We summarize the construction in Algorithm~\ref{alg:algmaxsmth}. The equations written in Algorithm~\ref{alg:algmaxsmth} are just explicit calculation of $\dot h_-$, $\dot h_+$, $h_-$, and $h_+$, using the parameters as defined in Lemma~\ref{lem:ssftnmdparam}. 
Algorithm~\ref{alg:algmaxsmth} presents the process of getting the ``smooth maximum'' of two quadratic functions,
for a slightly more general case where $\fxone(x)$ and $\fxtwo(x)$ are defined in the form
\begin{equation*}
	\fxone(x) \defeq s\ltwosqr{x-x_1 } + t ~~\text{and}~~
	\fxtwo(x) \defeq s\alpha \ltwosqr{x-x_2} + s\beta \delta^2 + t,
\end{equation*}
where $s > 0$ and $t \in \R$. That is, $s$ and $t$ are scale and translation in the range space. 

\paragraph{Correctness of smooth maximum.}
We now prove that the output of Algorithm~\ref{alg:algmaxsmth} is
indeed the smooth interpolation of $\max \{\fxone, \fxtwo\}$,
and the interpolated function values attain the maximum/suprimum as originally intended.
We prove this by the following lemma, whose technical proof is deferred to Appendix~\ref{sec:proof-smthconstvalues}.
\begin{lemma}
	\label{lem:smthconstvalues}
	Let 
	\begin{equation*}
		\fxone(x) \defeq \ltwosqr{x-x_1 } ~~\text{and}~~
		\fxtwo(x) \defeq \alpha \ltwosqr{x-x_2} + \beta \delta^2,
	\end{equation*}
	where parameters satisfy $0<\alpha<1$, $0<\eta<1$, $\eta+\alpha+\alpha\eta<1$, $\delta>0$, $\beta = \frac{(1-\alpha)(1+\eta)^2}{4} - \frac{\alpha \eta^2}{1-\alpha}$, and $x_2 \in \balltwo{x_1}{\eta\delta}$.
	Then, the output $f(x)$ of $\algmaxsmth(\fxone, \fxtwo, \alpha, \eta, \delta)$
	satisfies, for all $\rho \geq 0$ and $\ltwo w = 1$,
	\begin{align*}
		f(c+\rho w) &=
		\begin{cases}
			\fxtwo(c+\rho w) 	& \text{if } \rho \in [0, (1-\theta)r]\\
			h_-(\rho, w) 		& \text{if } \rho \in [(1-\theta)r,r]\\
			h_+(\rho, w) 		& \text{if } \rho \in [r,(1+\theta) r]\\
			\fxone(c+\rho w) 	& \text{if } \rho \in [(1+\theta) r, \infty)
		\end{cases}\\
		\nabla f(c+\rho w) &=
		\begin{cases}
			\nabla \fxtwo(c+\rho w) 	& \text{if } \rho \in [0, (1-\theta)r]\\
			\dot h_-(\rho, w) 		& \text{if } \rho \in [(1-\theta)r,r]\\
			\dot h_+(\rho, w) 		& \text{if } \rho \in [r,(1+\theta) r]\\
			\nabla \fxone(c+\rho w) 	& \text{if } \rho \in [(1+\theta) r, \infty).
		\end{cases}
	\end{align*}
\end{lemma}
Given Lemma~\ref{lem:smthconstvalues}, we showed that the interpolated function
$f(x)$ has function values and gradients as specified in the lemma,
which agrees with our intended construction in Appendix~\ref{sec:proof-algmaxsmth}.
Note that positive scaling and translation does not hurt the correctness of interpolation.

Now, we prove the smoothness and strong convexity constants of $f(x)$.
\begin{lemma}
	\label{lem:smthconstparams}
	Under the same setting as Lemma~\ref{lem:smthconstvalues},
	the output $f(x)$ of $\algmaxsmth(\fxone, \fxtwo, \alpha, \eta, \delta)$ is $\left (2+\frac{1-\alpha}{\theta}\right )$-smooth and $2\alpha$-strongly convex.
\end{lemma}
The proof is in Appendix~\ref{sec:proof-smthconstparams}. 
Note that, as in the 1D case, the smoothness constant increased after interpolation while the strong convexity constant stayed the same.

We end this subsection with a lemma on the range of $f(x)$, which will prove useful for multistage construction as well as the reader's comprehension of the interpolation.
The proof is deferred to Appendix~\ref{sec:proof-smthconstrange}.
\begin{lemma}
	\label{lem:smthconstrange}
	Let 
	\begin{equation*}
		\fxone(x) \defeq s\ltwosqr{x-x_1 } + t~~\text{and}~~
		\fxtwo(x) \defeq s\alpha \ltwosqr{x-x_2} + s \beta \delta^2 + t,
	\end{equation*}
	where parameters satisfy $0<\alpha<1$, $0<\eta<1$, $\eta+\alpha+\alpha\eta<1$, $\delta>0$, $\beta = \frac{(1-\alpha)(1+\eta)^2}{4} - \frac{\alpha \eta^2}{1-\alpha}$, $s>0$ and $x_2 \in \balltwo{x_1}{\eta\delta}$.
	Then, the output $f(x)$ of $\algmaxsmth(\fxone, \fxtwo, \alpha, \eta, \delta)$ satisfies
	\begin{enumerate}
		\item \label{item:smthconstrange-7}
		$f(x) = \begin{cases}
		\fxone(x) &\forall x \in \cl(\balltwo{x_1}{\delta}^c), \\
		\fxtwo(x) &\forall x \in \balltwo{x_1}{\eta\delta},
		\end{cases}$
		\item \label{item:smthconstrange-8}
		$\nabla f(x) = \begin{cases}
		\nabla \fxone(x) &\forall x \in \cl(\balltwo{x_1}{\delta}^c), \\
		\nabla \fxtwo(x) &\forall x \in \balltwo{x_1}{\eta\delta},
		\end{cases}$
		%\item $f(x) = \fxone(x) \quad \text{for all } x\in \cl(\balltwo{x_1}{\delta}^c)$.
		%\item $f(x) = \fxtwo(x) \quad \text{for all } x\in \balltwo{x_1}{\eta\delta}$.
		\item \label{item:smthconstrange-3} 
		$s \beta \delta^2 + t\leq f(x) \leq s \delta^2 + t	\quad \forall x \in \balltwo{x_1}{\delta}$.
		\item \label{item:smthconstrange-4}
		$\ltwo{\nabla f(x)} \leq 2 s\delta \quad \forall x \in \balltwo{x_1}{\delta}$.
		%\item $\nabla f(x) = \nabla \fxone(x) \quad \text{for all } x\in \cl(\balltwo{x_1}{\delta}^c)$.
		%\item $\nabla f(x) = \nabla \fxtwo(x) \quad \text{for all } x\in \balltwo{x_1}{\eta\delta}$.
	\end{enumerate}
\end{lemma}

\subsection{Multi-stage recursive construction}
\label{sec:multiconstsscvx}
Now let us consider applying $\algmaxsmth(\cdot)$ many times in a recursive way, 
as done in the Lipschitz convex case; we will iteratively apply $\algmaxsmth$ 
while zooming into narrower regions of the domain.
The outline of the construction is the same, except a bit of difference in details.

As seen in Section~\ref{sec:proof-lbmain-const}, we construct nested maximal packings. For $\ffamily_{H,\lambda}$ this is done with $\ell_2$ norm.
Given the maximal packings, we can recursively choose chain of parameters $u_1, \dots, u_M$ and $v$. Algorithm~\ref{alg:ftnconstrSmthstrcvx}
constructs a function $\ftnuv(x)$ that corresponds to the specific choice of $u_1, \dots, u_M$ and $v$.
\begin{algorithm}[t]
	\caption{Construction of smooth strongly convex $\ftnuv$.}  \label{alg:ftnconstrSmthstrcvx}
	\begin{algorithmic}[1]  % [1] is line numbering
		\STATEx Given parameters $u_1, u_2, \dots, u_M, v$, size $\delta_1, \dots, \delta_M$, 
		\STATEx $\alpha, \eta \in (0,1)$ satisfying $\eta + \alpha + \alpha\eta < 1$, and $C>0$,
		\STATE Let $\beta \defeq \frac{(1-\alpha)(1+\eta)^2}{4} - \frac{\alpha \eta^2}{1-\alpha}$
		\STATE Start with $\ftnuone (x) = \ftninuone(x) \defeq C\ltwosqr{x-u_1}$.
		\FOR{ $t = 2$ to $M$ }
		\STATE $\ftninut (x) \defeq C\alpha^{t-1} \ltwosqr{x-u_{t}} + C \beta \sum_{m=1}^{t-1} \alpha^{m-1}\delta_m^2$
		\STATE $\gut(x) \defeq \algmaxsmth(\ftninutmone, \ftninut, \alpha, \eta, \delta_{t-1})$.
		\STATE \label{line:max1} $\ftnut (x) \defeq \max \{ \ftnutmone (x), \gut(x) \}$.
		\ENDFOR
		\STATE $\ftninuv (x) \defeq C\alpha^M \ltwosqr{x-u_M-v \eta \delta_M \unitvec} + C \beta \sum_{m=1}^{M}  \alpha^{m-1} \delta_m^2$.
		\STATE $\guv(x) \defeq \algmaxsmth(\ftninuM, \ftninuv,\alpha,\eta,\delta_M)$.
		\STATE \label{line:max2}Return $\ftnuv(x) \defeq \max \{ \ftnuM(x), \guv(x) \}$.
	\end{algorithmic}
\end{algorithm}
Algorithm~\ref{alg:ftnconstrSmthstrcvx} defines a series of quadratic functions $\ftninut(x)$ and repeatedly takes smooth maximum with previous ones to get the final function.

With Algorithm~\ref{alg:ftnconstrSmthstrcvx}, we can show that the outputs of Algorithm~\ref{alg:ftnconstrSmthstrcvx} satisfy Condition~\ref{con:ftnconstrunified}, as desired.
\begin{lemma}
	\label{lem:ftnconstrsscvx}
	The functions constructed by Algorithm~\ref{alg:ftnconstrLipsconv} satisfy Condition~\ref{con:ftnconstrunified} with 
	\begin{equation*}
	(C, \alpha, \eta, \beta, \kappa, p) = \left (C, \alpha, \eta, \frac{(1-\alpha)(1+\eta)^2}{4} - \frac{\alpha \eta^2}{1-\alpha}, 2, 2 \right ),
	\end{equation*}
	if $\alpha, \eta \in (0,1)$, $\eta + \alpha + \alpha\eta < 1$, and $C>0$.
\end{lemma}
The proof of Lemma~\ref{lem:ftnconstrsscvx} is deferred to Appendix~\ref{sec:proof-ftnconstrsscvx}.

We also want to check whether the functions $\ftnuv(x)$ we are constructing are indeed smooth and strongly convex. Especially, one might wonder if the $\max$ operations at Lines~\ref{line:max1} and \ref{line:max2} in Algorithm~\ref{alg:ftnconstrSmthstrcvx} can hurt the smoothness. The following lemma addresses this points, whose proof is deferred to Appendix~\ref{sec:proof-ftnconstrsscvx2}.
\begin{lemma}
	\label{lem:ftnconstrsscvx2}
	The functions constructed with Algorithm~\ref{alg:ftnconstrSmthstrcvx} are $\left (C \left ( 2 + \frac{1-\alpha}{\theta} \right )\right )$-smooth and $2C\alpha^M$-strongly convex, where 
	$\theta \defeq \frac{1-\eta-\alpha-\alpha\eta}{1+\eta-\alpha-\alpha\eta}$ as in Eq~\eqref{eqn:smthconstthetaval}.
	Moreover, when $H/5 \geq \lambda$, with the parameter choice 
	\begin{equation*}
	\alpha = \left (\half \right)^{\frac{1}{M}},~\eta = \frac{1-\alpha}{2},~C = \frac{H}{5}
%	\label{eqn:smthconstdesignparams}
	\end{equation*}
	the constructed functions are $H$-smooth and $\lambda$-strongly convex.
%	, and they satisfy 
%	\begin{equation}
%	\fndist(\ftnuvm,\ftnuvp) 
%	= \frac{H}{20}\left(1-\left(\half\right)^{\frac{1}{M}}\right)^2 \delta_M^2.
%	\end{equation}
	For $H/5 < \lambda < H$, there also exists choice of parameters that returns $H$-smooth and $\lambda$-strongly convex functions, although a bit more complicated.
\end{lemma}


\section{Technical Proofs for Section~\ref{sec:proof-lbmain} }
\label{sec:defer-proof-lbmain}

\subsection{Proof of Lemma~\ref{lem:opt2hyptest}}
\label{sec:proof-opt2hyptest}
Consider a hypothesis testing problem, where $v \in \{-,+\}$ is sampled uniformly at random by the nature
and $v$ is not known to us. We have to estimate the $v$ using a hypothesis test $\Psi$.
Observe that if $f_v(\what X) - f_v\opt \leq \fndist(f_-, f_+) / 2$ for some $v \in \{-,+\}$, then for $v' \neq v$,
\begin{align*}
	&\fndist(f_-, f_+) \leq f_v (\what X) + f_{v'}(\what X) - f_v\opt - f_{v'}\opt
	\leq \tfrac{\fndist(f_-, f_+)}{2} + f_{v'}(\what X) - f_{v'}\opt\\
	\implies & \tfrac{\fndist(f_-, f_+)}{2} \leq f_{v'}(\what X) - f_{v'}\opt.
\end{align*}
so only a single $v \in \{-,+\}$ may satisfy $f_v(\what{X}) - f_v\opt \leq \fndist(f_-, f_+) / 2$.
From this observation, we can define our test $\what \Psi$ using our optimization estimate $\what X$,
$\what \Psi = \argmin_{v \in \{-,+\}} f_v (\what{X}) - f_v\opt$, where ties are broken arbitrarily.
Notice that for any $v$, $\what \Psi \neq v$ implies $f_v (\what{X}) - f_v\opt \geq \tfrac{\fndist(f_-, f_+)}{2}$.
Then, using Markov's inequality,
\begin{align*}
	&\max_{v \in \{-,+\}} \E_v \left [ f_v(\what X) - f_v\opt \mid G \right ]
	\geq \half \sum_{v \in \{-,+\}} \E_v \left [ f_v(\what X) - f_v\opt \mid G \right ]\\
	\geq&\frac{\fndist(f_-, f_+)}{4} \sum_{v \in \{-,+\}} \P_v \left (\ftnuv (\what{X}) - (\ftnuv)^\star \geq \tfrac{\fndist(f_-, f_+)}{2} \mid G \right )\\
	\geq&\frac{\fndist(f_-, f_+)}{4} \sum_{v \in \{-,+\}} \P_v (\what \Psi \neq v \mid G)
	\geq \frac{\fndist(f_-, f_+)}{4} \inf_\Psi \sum_{v \in \{-,+\}} \P_v (\Psi \neq v \mid G).
\end{align*}
where last the infimum is taken over all possible tests.
By a classical inequality on hypothesis testing and total variation distance,
\begin{align*}
	\inf_\Psi  \sum_{v \in \{-,+\}} \P_v (\Psi \neq v \mid G) 
	\geq 1-\tvnorm{ \P_- (\cdot \mid G) -\P_+ (\cdot \mid G) }.
\end{align*}

\subsection{Proof of Lemma~\ref{lem:ftnconstrLipcvx}}
\label{sec:proof-ftnconstrLipcvx}
We start by showing the following technical lemma, 
which illustrates how the functions in the max operation are placed above or below one another.
Its proof is deferred to Appendix~\ref{sec:proof-ftnconstrLipcvxsublem}.
\begin{lemma}
	\label{sublem:ftnconstrLipcvx}
	For any set of parameters $u_1, u_2, \dots, u_M, v$ chosen by
	$u_1 \in \Uone$, $u_t \in \Utemp{t-1}{t}$ for $t \in 2:M$, and $v \in \V$,
	run Algorithm~\ref{alg:ftnconstrLipsconv} and get $\ftnuv(x)$.
	Then, for any $t \in 2:M$, we have: 
	\begin{enumerate}
		%\item \label{itemsub:ftnconstrLipcvx-1} $\ftnutmone(x) \geq \ftninut (x) \quad \text{for all } x\notin \Binftmone$.
		%\item \label{itemsub:ftnconstrLipcvx-2} $\ftnutmone(x) \leq \ftninut (x)  \quad \text{for all } x\in \Binftmoneh$.
		\item \label{itemsub:ftnconstrLipcvx-1} 
		$\ftnut(x) = \begin{cases}
		\ftnutmone(x) &\forall x \not\in \Binftmone, \\
		\ftninut(x)&\forall x\in \Binftmoneh,
		\end{cases}$
		\item \label{itemsub:ftnconstrLipcvx-3} $\sum_{m=1}^{t-1} \frac{L\delta_m}{2\cdot3^{m-1}}\leq\ftnut (x)\leq
		\sum_{m=1}^{t-2} \frac{L\delta_m}{2\cdot3^{m-1}} +\frac{L\delta_{t-1}}{3^{t-2}}
		\quad \text{for all } x \in \Binftmone$.
	\end{enumerate}
	Also, at the final step,
	\begin{enumerate}
		\setcounter{enumi}{2}
		%\item \label{itemsub:ftnconstrLipcvx-4} $\ftnuM(x) \geq \ftninuv (x) \quad \text{for all }x\notin \BinfM$. 
		%\item \label{itemsub:ftnconstrLipcvx-5} $\ftnuM(x) \leq \ftninuv (x) \quad \text{for all } x\in \BinfMh$.
		\item \label{itemsub:ftnconstrLipcvx-4} $\ftnuv(x) = \begin{cases}
		\ftnuM(x) & \forall x\notin \BinfM, \\
		\ftninuv(x)&\forall x\in \BinfMh,
		\end{cases}$
		\item \label{itemsub:ftnconstrLipcvx-6} $\sum_{m=1}^{M} \frac{L\delta_m}{2\cdot3^{m-1}}\leq\ftnuv (x)\leq
		\sum_{m=1}^{M-1} \frac{L\delta_m}{2\cdot3^{m-1}} +\frac{L\delta_{M}}{3^{M-1}}
		\quad \text{for all } x \in \BinfM$.
	\end{enumerate}
\end{lemma}

%From definitions $\ftnut(x) = \max\{\ftnutmone(x), \ftninut(x)\}$ and 
%$\ftnuv(x) = \max\{\ftnuM(x), \ftninuv(x)\}$, a direct consequence of 
%Lemma~\ref{sublem:ftnconstrLipcvx} is that $\ftnut(x)$ (for $t \in 2:M$) and $\ftnuv(x)$ satisfy: 
%\begin{equation}
%\label{eqn:characterization-of-Lipshcitz-function}
%\ftnut(x) = \begin{cases}
%\ftnutmone(x) &\forall x \not\in \Binftmone \\
%\ftninut(x)&\forall x\in \Binftmoneh
%\end{cases}
%~\text{and}~
%\ftnuv(x) = \begin{cases}
%\ftnuM(x) & \forall x\notin \BinfM \\
%\ftninuv(x)&\forall x\in \BinfMh
%\end{cases}.
%\end{equation}
%Using this equation, we will prove the statements of Lemma~\ref{lem:ftnconstrLipcvx}.

We prove Lemma~\ref{lem:ftnconstrLipcvx}.\ref{item:ftnconstrunified-1} and \ref{lem:ftnconstrLipcvx}.\ref{item:ftnconstrunified-4} using simple argument 
that $\max$ operations done in Algorithm~\ref{alg:ftnconstrLipsconv} only changes limited parts of the domain.
Recall the definition $\ftnut(x) \defeq \max\{\ftnutmone(x), \ftninut(x)\}$.
From Lemma~\ref{sublem:ftnconstrLipcvx}.\ref{itemsub:ftnconstrLipcvx-1},
note that 
whenever we have $\ftnutmone(x)$ and take $\max$ operation with $\ftninut(x)$ to construct $\ftnut(x)$,
any point $\forall x \not\in \Binftmone$ does not change its value.
This means that the $\max$ operation can only change function values in $\Binftmone$.
Also, later iterations of the algorithm do not change that the function values at $x \notin \Binftmone$, 
because $\Binftmone \supset \Binft \supset \cdots \supset \BinfM$.
From this argument, we can see that $\ftnuv(x) = \ftnuvtildet(x) = \ftnutmone(x)$ for all $x \notin \Binftmone$,
therefore proving Lemma~\ref{lem:ftnconstrLipcvx}.\ref{item:ftnconstrunified-1}. 
Similarly, from Lemma~\ref{sublem:ftnconstrLipcvx}.\ref{itemsub:ftnconstrLipcvx-4}, the final line $\ftnuv(x) \defeq \max \{ \ftnuM(x), \ftninuv(x) \}$ in Algorithm~\ref{alg:ftnconstrLipsconv}
can only change function values in $\BinfM$, 
so $\ftnuvm(x) = \ftnuvp(x) = \ftnuM(x)$ for all  $x \notin \BinfM$, proving Lemma~\ref{lem:ftnconstrLipcvx}.\ref{item:ftnconstrunified-4}.

Lemma~\ref{lem:ftnconstrLipcvx}.\ref{item:ftnconstrunified-5} can be implied directly by Lemma~\ref{sublem:ftnconstrLipcvx}.\ref{itemsub:ftnconstrLipcvx-6}.
In order to prove Lemma~\ref{lem:ftnconstrLipcvx}.\ref{item:ftnconstrunified-2}, note the following facts from Lemma~\ref{sublem:ftnconstrLipcvx}.\ref{itemsub:ftnconstrLipcvx-6} and \ref{sublem:ftnconstrLipcvx}.\ref{itemsub:ftnconstrLipcvx-3}:
\begin{align*}
&\sum_{m=1}^{M} \frac{L\delta_m}{2\cdot3^{m-1}}
\leq
\ftnuv (x)
\leq
\sum_{m=1}^{M-1} \frac{L\delta_m}{2\cdot3^{m-1}} +\frac{L\delta_{M}}{3^{M-1}}
\quad \text{for all } x \in \BinfM,\\
&\sum_{m=1}^{M-1} \frac{L\delta_m}{2\cdot3^{m-1}}
\leq
\ftnuM (x)
\leq
\sum_{m=1}^{M-2} \frac{L\delta_m}{2\cdot3^{m-1}} +\frac{L\delta_{M-1}}{3^{M-2}}
\quad \text{for all } x \in \BinfMmone.
\end{align*}
Note from Lemma~\ref{sublem:ftnconstrLipcvx}.\ref{itemsub:ftnconstrLipcvx-4} that $\ftnuv (x) = \ftnuM(x)$ for all $x \notin \BinfM$, 
and that, for all $x \in \BinfM$,
\begin{equation*}
\ftnuv (x)
\leq
\sum_{m=1}^{M-1} \frac{L\delta_m}{2\cdot3^{m-1}} +\frac{L\delta_{M}}{3^{M-1}}
\leq
\sum_{m=1}^{M-2} \frac{L\delta_m}{2\cdot3^{m-1}} +\frac{L\delta_{M-1}}{3^{M-2}}.
\end{equation*}
The last inequality is because $\frac{3 \delta_{M-1}}{2} \geq \delta_M$ holds for large enough $n$ by assumption
that $\delta_M = o(\delta_{M-1})$.
From these observations, we have
\begin{align*}
\sum_{m=1}^{M-1} \frac{L\delta_m}{2\cdot3^{m-1}}
\leq
\ftnuv (x)
\leq
\sum_{m=1}^{M-2} \frac{L\delta_m}{2\cdot3^{m-1}} +\frac{L\delta_{M-1}}{3^{M-2}}
\quad \text{for all } x \in \BinfMmone.
\end{align*}
Again note that, for any $x \notin \BinfMmone$ we also have $x \notin \BinfM$, 
so $\ftnuv(x) = \ftnuM(x) = \ftnuMmone(x)$. We can repeat a similar argument and obtain
\begin{align*}
\sum_{m=1}^{M-2} \frac{L\delta_m}{2\cdot3^{m-1}}
\leq
\ftnuv (x)
\leq
\sum_{m=1}^{M-3} \frac{L\delta_m}{2\cdot3^{m-1}} +\frac{L\delta_{M-2}}{3^{M-3}}
\quad \text{for all } x \in \Binftemp{M-2}.
\end{align*}
For any $t \in 2:M$, we can repeat this argument until $\Binftemp{t-1}$, so that we get
\begin{align*}
\sum_{m=1}^{t-1} \frac{L\delta_m}{2\cdot3^{m-1}}
\leq
\ftnuv (x)
\leq
\sum_{m=1}^{t-2} \frac{L\delta_m}{2\cdot3^{m-1}} +\frac{L\delta_{t-1}}{3^{t-2}}
\quad \text{for all } x \in \Binftmone,
\end{align*}
which directly implies Lemma~\ref{lem:ftnconstrLipcvx}.\ref{item:ftnconstrunified-2} that we are after.

In order to prove Lemma~\ref{lem:ftnconstrLipcvx}.\ref{item:ftnconstrunified-3} and \ref{lem:ftnconstrLipcvx}.\ref{item:ftnconstrunified-6}, 
we first need to show that the function value $\ftnuv(x)$ in $\Binftmone$ can be expressed as
\begin{equation}
\label{eq:ftninsidebox}
\ftnuv(x) = \max \left \{ \max_{k \in t-1:M}\left \{ \ftninutemp{k}(x) \right \}, \ftninuv(x) \right \},\text{ for all } x \in \Binftmone.
\end{equation}
Notice from Lemma~\ref{sublem:ftnconstrLipcvx}.\ref{itemsub:ftnconstrLipcvx-1} that 
$\ftnutmone(x) = \ftninutmone(x)$ for all $x \in \Binftmtwoh$.
Recall that $\Binftmone \subset \Binftmtwoh$, so $\ftnutmone(x) = \ftninutmone(x)$ in $\Binftmone$.
After this point, $\ftnuv(x)$ is obtained from $\max$ operations with $\ftninut, \dots, \ftninuM, \ftninuv$.
This proves Eq~\eqref{eq:ftninsidebox}. 
Now notice that the subgradient of 
\begin{equation*}
\ftninut (x) \defeq \frac{L}{3^{t-1}} \linf{x-u_{t}} + \sum_{m=1}^{t-1} \frac{L \delta_m}{2 \cdot 3^{m-1}}
\end{equation*}
always has $\ell_1$ norm exactly $\frac{L}{3^{t-1}}$.
From Eq~\eqref{eq:ftninsidebox}, we can observe that for any point $x \in \Binftmone$, 
any subgradient $\grduv \in \sgrduv(x)$ has $\lone{\grduv} \leq \frac{L}{3^{t-2}}$.
Since this holds for any set of parameters $u_{t:M}$, $v$, $\tilde u_{t:M}$, and $\tilde v$, we get Lemma~\ref{lem:ftnconstrLipcvx}.\ref{item:ftnconstrunified-3}.
From a similar argument as Eq~\eqref{eq:ftninsidebox}, we have
\begin{equation*}
\ftnuv(x) = \max \left \{ \ftninuM(x), \ftninuv(x) \right \}, \text{ for all } x \in \BinfM,
\end{equation*}
whereby we can prove Lemma~\ref{lem:ftnconstrLipcvx}.\ref{item:ftnconstrunified-6}.

Finally, we have to show Lemma~\ref{lem:ftnconstrLipcvx}.\ref{item:ftnconstrunified-7}. 
To do so, we first show that, for any choice of $u_1, u_2, \ldots, u_M$ and $v$, 
\begin{equation}
\label{eqn:min-of-ftnuv}
\inf_{x} \ftnuv(x) = \sum_{m=1}^{M} \frac{L\delta_m}{2\cdot3^{m-1}}
\end{equation}
In fact, from Lemma~\ref{sublem:ftnconstrLipcvx}.\ref{itemsub:ftnconstrLipcvx-4}, 
we have $\ftnuv(x) = \ftninuv(x)$ for all $x \in \BinfMh$.
Also, $\ftninuv(x)$ is minimized at $u_M+ \frac{v \delta_M}{2} \ones \in \BinfMh$,
whose minimum value is the RHS of Eq~\eqref{eqn:min-of-ftnuv}.
So, for any $x \in \domain$, 
\begin{equation*}
\ftnuv(x) \geq \ftninuv(x) 
\geq \ftninuv\left (u_M+ \frac{v \delta_M}{2} \ones\right ) 
= \sum_{m=1}^{M} \frac{L\delta_m}{2\cdot3^{m-1}},
\end{equation*}
proving Eq~\eqref{eqn:min-of-ftnuv}.

Next, we show that
\begin{equation}
\label{eqn:min-of-sum-ftnuv}
\inf_x (\ftnuvp(x) + \ftnuvm(x)) =  \sum_{m=1}^{M} \frac{L\delta_m}{3^{m-1}}
+ \frac{L\delta_M}{3^M}.
\end{equation}
Again note that $\ftnuv(x) = \ftninuv(x)$ for all $x \in \BinfMh$.
That is, for $x\in \BinfMh$, we have $\ftnuvp(x) = \ftninuvp(x)$ and $\ftnuvm(x) = \ftninuvm(x)$.
Therefore, for any $x\in \BinfMh$, 
\begin{align*}
&\ftnuvp(x) + \ftnuvm(x) 
= \ftninuvp(x) + \ftninuvm(x)\\
=& \frac{L}{3^M} \left(\linf{x-u_M-\tfrac{\delta_M}{2}\ones} + \linf{x-u_M+\tfrac{\delta_M}{2}\ones}\right)
+  \sum_{m=1}^{M} \frac{L\delta_m}{3^{m-1}}.
\end{align*}
By triangle inequality, we have
\begin{equation*}
\linf{x-u_M-\tfrac{\delta_M}{2}\ones} + \linf{x-u_M+\tfrac{\delta_M}{2}\ones} 
\geq
\linf{\left(u_M+\tfrac{\delta_M}{2} \ones\right)-\left( u_M-\tfrac{\delta_M}{2} \ones\right)}
=
\delta_M
\end{equation*}
Note also that $x = u_M$ in fact attains this lower bound.
So, for any $x \in \domain$, 
\begin{equation*}
\ftnuvp(x) + \ftnuvm(x) \geq \ftninuvp(x) + \ftninuvm(x) 
\geq \ftninuvp(u_M) + \ftninuvm(u_M)
=  \sum_{m=1}^{M} \frac{L\delta_m}{3^{m-1}}	+ \frac{L\delta_M}{3^M},
\end{equation*}
thus proving Eq~\eqref{eqn:min-of-sum-ftnuv}.
Now, Lemma~\ref{lem:ftnconstrLipcvx}.\ref{item:ftnconstrunified-7} follows from Eq~\eqref{eqn:min-of-ftnuv} and Eq~\eqref{eqn:min-of-sum-ftnuv}.

\subsection{Proof of Lemma~\ref{lem:ftnconstrLipcvx2}}
\label{sec:proof-ftnconstrLipcvx2}
Consider the functions constructed in Algorithm~\ref{alg:ftnconstrLipsconv}. 
For any $t \in 1:M$, the function $\ftninut(x)$ is convex and $L$-Lipschitz 
with respect to $\ell_p$ norm for any $p \geq 1$. 
Hence,  $\ftnuM (x) \defeq \max_{1\leq t\leq M}\left\{\ftninut(x)\right\}$ is also convex and $L$-Lipschitz.
With the same reasoning, $\ftninuv(x)$ is convex and $L$-Lipshcitz,
and so $\ftnuv(x) \defeq \max \{ \ftnuM(x), \ftninuv(x) \}$ is.

\subsection{Proof of Lemma~\ref{sublem:ftnconstrLipcvx}}
\label{sec:proof-ftnconstrLipcvxsublem}
We demonstrate in details the proof for Lemma~\ref{sublem:ftnconstrLipcvx} below,
which is based on an induction argument.
\paragraph{Base case $t=2$.}
In the base case, for any $u_1 \in \Uone$ and $u_2 \in \Utwo$, we want to show that 
\begin{equation*}
\ftnuone(x) \geq \ftninutwo(x)~\text{for all $x\not\in \Binfone$ and }
\ftnuone(x) \leq \ftninutwo(x)~\text{for any $x\in \Binfoneh$},
\end{equation*}
which correspond to Lemma~\ref{sublem:ftnconstrLipcvx}.\ref{itemsub:ftnconstrLipcvx-1}.
Recall the definitions that
\begin{equation*}
\ftnuone (x) = \ftninuone(x) \defeq L\linf{x-u_1}
\text{ and }
\ftninutwo (x) \defeq \frac{L}{3} \linf{x-u_{2}} + \frac{L \delta_1}{2}.
\end{equation*}
Indeed, note that, since by definition $u_2 \in \ballinf{u_1}{\tfrac{\delta_1}{2}-\delta_2}$, 
we have $\linf{u_2 - u_1} \leq \delta_1/2$. Therefore, by triangle inequality, 
we have $\linf{x-u_2} \leq \linf{x-u_1} + \delta_1/2$. 
Thus, for any $x\not\in \Binfone$, we have 
\begin{align*}
\ftnuone(x) - \ftninutwo(x) 
&= L \left(\linf{x-u_1} - \frac{\linf{x-u_2}}{3} - \frac{\delta_1}{2}\right) \\
&\geq L \left(\linf{x-u_1} - \frac{\linf{x-u_1}}{3} - \frac{2\delta_1}{3}\right) \geq 0.
\end{align*}
On the other hand, by triangle inequality $\linf{x-u_2} +  \delta_1/2 \geq \linf{x-u_1}$.
Thus, for $x\in \Binfoneh$, 
\begin{align*}
\ftnuone(x) - \ftninutwo(x) 
&= L \left(\linf{x-u_1} - \frac{\linf{x-u_2}}{3} - \frac{\delta_1}{2}\right) \\
&\leq L \left(\linf{x-u_1} - \frac{\linf{x-u_1}}{3} - \frac{\delta_1}{3}\right) \leq 0, 
\end{align*}
which proves Lemma~\ref{sublem:ftnconstrLipcvx}.\ref{itemsub:ftnconstrLipcvx-1}. 

We are left with the inequalities below:
\begin{equation}
\label{eqn:bounds1}
\frac{L\delta_1}{2} \leq \ftnutwo(x)  \leq L\delta_1~~\text{for all $x\in \Binfone$}.
\end{equation}
Recall the definition $\ftnutwo(x) = \max\{\ftnuone(x), \ftninutwo(x)\}$.
Indeed, the LHS of Eq~\eqref{eqn:bounds1} follows from the fact that $L\delta_1/2 \leq \ftninutwo(x) \leq \ftnutwo(x)$.
To show the RHS of inequality~\eqref{eqn:bounds1}, we note first that
for any $x\in \Binfone$, $\ftnuone(x) = L \linf{x-u_1} \leq L\delta_1$. 
In addition to that, since $\linf{u_1 - u_2} \leq \delta_1/2$, by 
triangle inequality, we also have that, for any $x\in \Binfone$, 
\begin{equation*}
\ftninutwo(x) \defeq \frac{L}{3} \linf{x-u_2} + \frac{L \delta_1}{2} 
\leq \frac{L}{3} \left(\linf{x-u_1} + \frac{\delta_1}{2}\right) 
+ \frac{L\delta_1}{2} \leq L\delta_1
\end{equation*}
Thus, we have, for any $x\in \Binfone$, 
$\ftnutwo(x) = \max\left\{\ftnuone(x), \ftninutwo(x)\right\} \leq L\delta_1$. 
Together, we prove the desired inequality~\eqref{eqn:bounds1},
which corresponds to Lemma~\ref{sublem:ftnconstrLipcvx}.\ref{itemsub:ftnconstrLipcvx-3} for $t=2$.

\paragraph{Inductive case $2<t \leq M$.}
In the first step, we show our first claim in Lemma~\ref{sublem:ftnconstrLipcvx}.\ref{itemsub:ftnconstrLipcvx-1}: 
\begin{equation*}
\ftnutmone(x) \geq \ftninut (x) ~\text{for any $x\notin \Binftmone$ and } 
\ftnutmone(x) \leq \ftninut (x) ~\text{for any $x\in \Binftmoneh$}.
\end{equation*}
Recall the definitions that
\begin{align*}
\ftninutmone (x) &\defeq \frac{L}{3^{t-2}} \linf{x-u_{t-1}} + \sum_{m=1}^{t-2} \frac{L \delta_m}{2 \cdot 3^{m-1}},\\
\ftninut (x) &\defeq \frac{L}{3^{t-1}} \linf{x-u_{t}} + \sum_{m=1}^{t-1} \frac{L \delta_m}{2 \cdot 3^{m-1}},\\
\ftnutmone (x) &\defeq \max \{ \ftnutmtwo (x), \ftninutmone (x) \} \geq  \ftninutmone (x),\\
\ftnut (x) &\defeq \max \{ \ftnutmone (x), \ftninut (x) \} \geq  \ftninut (x).
\end{align*}
Indeed, note that, since by definition $u_t \in \ballinf{u_{t-1}}{\tfrac{\delta_{t-1}}{2}-\delta_t}$,
we have $\linf{u_t - u_{t-1}} \leq \delta_{t-1}/2$.
Hence, by triangle inequality, we have $\linf{x-u_t} \leq \linf{x-u_{t-1}} + \delta_{t-1}/2$. 
Also, by definition $\ftnutmone(x) \geq \ftninutmone(x)$ for all $x$.
Thus, for any $x\not\in \Binftmone$, we have
\begin{align*}
\ftnutmone(x) - \ftninut(x) 
&\geq \ftninutmone(x)- \ftninut(x) 
= \frac{L}{3^{t-2}} \left(\linf{x-u_{t-1}} - \frac{\linf{x-u_t}}{3} - \frac{\delta_{t-1}}{2}\right) \\
&\geq \frac{L}{3^{t-2}} \left(\linf{x-u_{t-1}} - \frac{\linf{x-u_{t-1}}}{3} - \frac{2\delta_{t-1}}{3}\right) 
\geq 0.
\end{align*}
On the other hand, recall that $\ballinf{u_{t-1}}{\delta_{t-1}} \subset \ballinf{u_{t-2}}{\delta_{t-2}/2}$.
When $x \in \Binftmoneh \subset \ballinf{u_{t-2}}{\delta_{t-2}/2}$,
Observe that by inductive hypothesis $\ftnutmtwo(x) \leq \ftninutmone (x)$.
By definition of $\ftnutmone (x) \defeq \max \{ \ftnutmtwo (x), \ftninutmone (x) \}$,
we can see that $\ftnutmone (x) = \ftninutmone (x)$ for $x \in \Binftmoneh$.
By triangle inequality, $\linf{x-u_{t}} +  \delta_{t-1}/2 \geq \linf{x-u_{t-1}}$.
Thus, for $x\in \Binftmoneh$,
\begin{align*}
\ftnutmone(x) - \ftninut(x)  
&= \ftninutmone(x) -  \ftninut(x) 
= \frac{L}{3^{t-2}} \left(\norm{x-u_{t-1}}_\infty - \frac{\norm{x-u_t}_\infty}{3} - \frac{\delta_{t-1}}{2}\right) \\
&\leq  \frac{L}{3^{t-2}} \left(\norm{x-u_{t-1}}_\infty - \frac{\norm{x-u_{t-1}}_\infty}{3} -
\frac{\delta_{t-1}}{3}\right) \leq 0, 
\end{align*}
which proves Lemma~\ref{sublem:ftnconstrLipcvx}.\ref{itemsub:ftnconstrLipcvx-1}. 

We are left with Lemma~\ref{sublem:ftnconstrLipcvx}.\ref{itemsub:ftnconstrLipcvx-3}, which we restate below:
\begin{equation*}
\sum_{m=1}^{t-1} \frac{L\delta_m}{2\cdot3^{m-1}}
\leq
\ftnut (x)
\leq
\sum_{m=1}^{t-2} \frac{L\delta_m}{2\cdot3^{m-1}} +\frac{L\delta_{t-1}}{3^{t-2}}
\quad \text{for all } x \in \Binftmone.
\end{equation*}
Recall the definition $\ftnut (x) \defeq \max \{ \ftnutmone (x), \ftninut (x) \}$.
Indeed, the LHS of Lemma~\ref{sublem:ftnconstrLipcvx}.\ref{itemsub:ftnconstrLipcvx-3} follows from the fact that 
$\sum_{m=1}^{t-1} \frac{L\delta_m}{2\cdot3^{m-1}} \leq \ftninut(x) \leq \ftnut(x)$.
To show the RHS of Lemma~\ref{sublem:ftnconstrLipcvx}.\ref{itemsub:ftnconstrLipcvx-3}, recall first that if
$x \in \ballinf{u_{t-1}}{\delta_{t-1}} \subset \ballinf{u_{t-2}}{\delta_{t-2}/2}$,
$\ftnutmtwo(x) \leq \ftninutmone (x)$ by induction hypothesis, so $\ftnutmone (x) = \ftninutmone (x)$.
Thus, for all $x \in \ballinf{u_{t-1}}{\delta_{t-1}}$,
\begin{equation*}
\ftnutmone (x)
= 
\ftninutmone(x)
\defeq 
\frac{L}{3^{t-2}} \linf{x-u_{t-1}} + \sum_{m=1}^{t-2} \frac{L \delta_m}{2 \cdot 3^{m-1}}
\leq
\sum_{m=1}^{t-2} \frac{L\delta_m}{2\cdot3^{m-1}} +\frac{L\delta_{t-1}}{3^{t-2}}.
\end{equation*}
Also, $\linf{u_{t-1} - u_t} \leq \delta_{t-1}/2$, and by triangle inequality, $\linf{x-u_{t}} \leq \linf{x - u_{t-1}} + \delta_{t-1}/2$.
Using this, for all $x \in \ballinf{u_{t-1}}{\delta_{t-1}}$
\begin{align*}
\ftninut(x)
&\defeq 
\frac{L}{3^{t-1}} \linf{x-u_{t}} + \sum_{m=1}^{t-1} \frac{L \delta_m}{2 \cdot 3^{m-1}}
\leq
\frac{L}{3^{t-1}} \linf{x-u_{t-1}} + \frac{L \delta_{t-1}}{2 \cdot 3^{t-1}} + \sum_{m=1}^{t-1} \frac{L \delta_m}{2 \cdot 3^{m-1}}\\
&
\leq
\frac{L \delta_{t-1}}{3^{t-1}} + \frac{L \delta_{t-1}}{2 \cdot 3^{t-1}} + \sum_{m=1}^{t-1} \frac{L \delta_m}{2 \cdot 3^{m-1}}
=
\sum_{m=1}^{t-2} \frac{L\delta_m}{2\cdot3^{m-1}} +\frac{L\delta_{t-1}}{3^{t-2}}.
\end{align*}
This finishes showing the RHS of Lemma~\ref{sublem:ftnconstrLipcvx}.\ref{itemsub:ftnconstrLipcvx-3}.

\paragraph{Final Case.}
It is left to prove Lemma~\ref{sublem:ftnconstrLipcvx}.\ref{itemsub:ftnconstrLipcvx-4}--\ref{itemsub:ftnconstrLipcvx-6}. 
Their proof can be done in a similar way as Lemma~\ref{sublem:ftnconstrLipcvx}.\ref{itemsub:ftnconstrLipcvx-1}--\ref{itemsub:ftnconstrLipcvx-3} for the inductive cases, hence omitted.

%\section{Deferred proofs from Section~\ref{sec:constsscvx}}

\subsection{Proof of Lemma~\ref{lem:packingno}}
\label{sec:proof-packingno}

Since $\delta_1 = o(1)$, for large enough $n$ we have $\delta_1 \leq 1/8$, so
\begin{align*}
| \Uone | &= \text{$2\delta_1$-packing number (w.r.t.\ $\ell_p$) of $[\delta_1, 1-\delta_1]^d$} \\
&\geq \text{$2\delta_1$-packing number (w.r.t.\ $\ell_p$) of $\left [\frac{1}{4}, \frac{3}{4}\right ]^d$}\\
&\geq \text{$2\delta_1$-covering number (w.r.t.\ $\ell_p$) of $\ballp{0}{1/4}$}\\
&\geq \frac{\text{Vol}(\ballp{0}{1/4})}{\text{Vol}(\ballp{0}{2\delta_1})} = \left(\frac{1}{8\delta_1} \right)^d.
\end{align*}
Similarly, by $\delta_t = o(\delta_{t-1})$, for large enough $n$ we have $\delta_t \leq \eta \delta_{t-1}/4$.
\begin{align*}
| \Ut | &= \text{$2\delta_t$-packing number (w.r.t.\ $\ell_p$) of $\ballp{u_{t-1}}{\eta\delta_{t-1}-\delta_t}$} \\
&\geq \text{$2\delta_t$-packing number (w.r.t.\ $\ell_p$) of $\ballp{u_{t-1}}{\eta\delta_{t-1}/2}$} \\
&\geq \frac{\text{Vol}(\ballp{u_{t-1}}{\eta\delta_{t-1}/2})}{\text{Vol}(\ballp{0}{2\delta_t})} = \left(\frac{\eta \delta_{t-1}}{4\delta_t} \right)^d.
\end{align*}

\subsection{Proof of Lemma~\ref{lem:lblem1}}
\label{sec:proof-lblem1}
In this section and the following two, for simplicity in notation, let 
\begin{equation*}
	\EvSmSplFrmOneTo{k} = \bigcap\nolimits_{m=1}^k \EvSmSplm.
\end{equation*}
The proofs of Lemmas~\ref{lem:lblem1} and \ref{lem:lblem2}
rely heavily on the following lemma which uses likelihood ratio and concentration inequality arguments
to prove bounds between different probability measures defined by ``similar'' functions.
\begin{lemma}
	\label{lem:likelihood1}
	For $t \in 2:M$, 
	let two sets of parameters share the same value $u_{1:t-1}$ and 
	then they differ afterwards: $u_{t:M}$, $v$ and $\tilde u_{t:M}$, $\tilde v$.
	Consider any probabilistic event $G$ that is a function of the random variables 
	\begin{align*}
		&X_{1:n}^{(1:k)}, Y_{1:n}^{(1:k)}, X_{1:n}^{(k+1)} 
		\quad\quad\quad\quad \text{ if oracle is zeroth-order, or }\\
		&X_{1:n}^{(1:k)}, Y_{1:n}^{(1:k)}, Z_{1:n}^{(1:k)}, X_{1:n}^{(k+1)}
		\quad \text{ if oracle is first-order,}
	\end{align*}
	where $k \in 1:(t-1)$.
	Then, the following holds for any choice of $u_{1:t-1}$, $u_{t:M}$, $v$ and $\tilde u_{t:M}$, $\tilde v$:
	\begin{equation*}
		\probuv \left( G \cap \EvSmSplFrmOneTo{k} \right)\leq K_1 \probuvtildet ( G \cap \EvSmSplFrmOneTo{k} ) + K_2,
	\end{equation*}
	where the quantities $K_1$, $K_2$ are
	\begin{align*}
		K_1 &\defeq \exp \left ( \frac{\sum_{j=0}^\zeta \sum_{m=1}^{k} \xi_m^{(j)}}{\sigma^2} + 
		\frac {(\sum_{j=0}^\zeta \tilde C_j^2 \alpha^{2t-4} \delta_{t-1}^{2(\kappa-j)}) \sum_{m=1}^k h_m}{2 \sigma^2}  \right ),\\
		K_2 &\defeq 2 \sum_{m=1}^{k} \sum_{j=0}^{\zeta}
		\exp \left (  -\frac {(\xi_m^{(j)})^2}{2 \sigma^2  h_{m} \tilde C_j^2 \alpha^{2t-4} \delta_{t-1}^{2(\kappa-j)}}  \right ),
	\end{align*}
	where $\xi_m^{(j)}$ for $m \in 1:k$ and $j \in 0:\zeta$ is any positive quantity we can choose. 
\end{lemma}
The proof of Lemma~\ref{lem:likelihood1} is deferred to Appendix~\ref{sec:proof-likelihood1}.

By assumption we have $u_{1:M}$ and $v$ 
such that $\probuv (\bigcap_{m=1}^{t-1} \EvSmSplm ) \geq \frac{1}{4^{t-1}}$ for sufficiently large $n$.
Then, re-choose any $\tilde u_{t:M}$ and $\tilde v$, and apply Lemma~\ref{lem:likelihood1}, with
\begin{align*}
	k = t-2, 
	G = \EvSmSpltemp{t-1}, \text{ and }~
	\xi_m^{(j)} = h_m^{\frac{1}{4}} \delta_{t-1}^{\frac{\kappa-j}{2}} \text{ for } m \in 1:t-2, j \in 0:\zeta.
\end{align*}
Then, we get 
\begin{equation*}
	\probuv ( \EvSmSplFrmOneTo{t-1} )\leq K_1 \probuvtildet ( \EvSmSplFrmOneTo{t-1} ) + K_2,
\end{equation*}
where the quantities $K_1$ and $K_2$ are
\begin{align*}
	K_1 &\defeq \exp \left ( \frac{(\sum_{j=0}^\zeta \delta_{t-1}^{\frac{\kappa-j}{2}}) \sum_{m=1}^{t-2} h_m^{\frac{1}{4}} }{\sigma^2} + 
	\frac {(\sum_{j=0}^\zeta \tilde C_j^2 \alpha^{2t-4} \delta_{t-1}^{2(\kappa-j)}) \sum_{m=1}^{t-2} h_m}{2 \sigma^2}  \right ),\\
	K_2 &\defeq 2 \sum_{m=1}^{t-2} \sum_{j=0}^{\zeta}
	\exp \left (  -\frac {1}{2 \sigma^2  h_{m}^\half \tilde C_j^2 \alpha^{2t-4} \delta_{t-1}^{\kappa-j}}  \right ).
\end{align*}

%Recall the definitions from Eqs~\eqref{eq:defgammat}, \eqref{eq:defdeltat}, and \eqref{eq:defht}:
%\begin{equation*}
%	\begin{array}{ll}
%		\gamma_t = \frac{1}{d+2(\kappa-\zeta)} \sum_{m=0}^{t-1} \left(\frac{d}{d+2(\kappa-\zeta)}\right)^m = 
%		\frac{1}{2(\kappa-\zeta)} \left[1 - \left( \frac{d}{d+2(\kappa-\zeta)} \right)^t\right] 
%		&\text{ for } t \in 0:M,\\
%		\delta_t = 
%		D_t n^{ - \gamma_t } \exp \left( -2 \sqrt{2} \gamma_{t-1} \sqrt{\log n} \right) & \text{ for } t = 1:M-1,\\
%		\delta_M = 
%		D_M n^{ -\gamma_M } \exp \left( -2 \sqrt{2} \gamma_{M-1} \sqrt{\log n} \right) \log^{-\nu/\kappa} n,\\
%		h_t = \left ( \frac{\sigma}{\tilde C_\zeta \alpha^{t-1} D_t^{\kappa-\zeta}} \right )^2 
%		n^{2(\kappa-\zeta) \gamma_t} 
%		\exp \left( 4 \sqrt{2} (\kappa-\zeta) \gamma_{t-1} \sqrt{\log n} \right) &\text{ for } t= 1:M.
%	\end{array}
%\end{equation*}
By definitions of $\gamma_t$ \eqref{eq:defgammat}, $\delta_t$ \eqref{eq:defdeltat}, and $h_t$ \eqref{eq:defht}, whenever $m < t-1$, $h_m \delta_{t-1}^{2(\kappa-j)}$ 
polynomially decays to 0 as $n$ increases.
So, $K_1 \downarrow 1$ and $K_2 \downarrow 0$ as $n \rightarrow \infty$.
Therefore, for large enough $n$, $K_1 \leq \sqrt 2$ and $K_2 \leq \left ( 1 - \frac{1}{\sqrt 2} \right )\frac{1}{4^{t-1}}$, resulting in
\begin{equation}
\probuv \left( \EvSmSplFrmOneTo{t-1} \right)\leq \sqrt 2 \probuvtildet ( \EvSmSplFrmOneTo{t-1} ) + \left ( 1 - \frac{1}{\sqrt 2} \right )\frac{1}{4^{t-1}}.\label{eqn:lem1eq5}
\end{equation}
From the assumption, we have $\probuv (\EvSmSplFrmOneTo{t-1}) \geq \frac{1}{4^{t-1}}$.
Using this with Eq~\eqref{eqn:lem1eq5}, we finish the proof of Lemma~\ref{lem:lblem1}:
\begin{equation*}
	\probuvtildet (\EvSmSplFrmOneTo{t-1}) \geq \frac{1}{2 \cdot 4^{t-1}},
\end{equation*}
for any $\tilde u_{t:M}$ and $\tilde v$, as desired.

\subsection{Proof of Lemma~\ref{lem:lblem2}}
\label{sec:proof-lblem2}
For the proof of Lemmas~\ref{lem:lblem2} and \ref{lem:lblem3}, the following elementary lemma is useful.
\begin{lemma}
	\label{lem:elem-ineq}
	If $a_1 \leq c a_1' + b$ and $a_2' \leq ca_2 + b$, then
	\begin{equation*}
		\frac{a_1}{a_1+a_2} \leq \frac{c^2 a_1'}{a_1'+a_2'} + \frac{cb}{a_1' + a_2'}
	\end{equation*}
	for any $a_1, a_2, a_1', a_2', b \geq 0$ and $a_1+a_2 > 0$, $a_1^\prime + a_2^\prime > 0$ and $c \geq 1$.
\end{lemma} 
\begin{proof}
	The proof of the above lemma is straightforward calculation. Note that, the function 
	$f(x) \defeq \frac{x}{x+y}$ is an increasing function of $x > 0$ for any $y > 0$, so we have 
	\begin{equation*}
		\frac{a_1}{a_1 + a_2} \leq \frac{c a_1'+b}{c a_1^\prime + a_2 + b} 
		\leq \frac{c a_1^\prime+b}{a_1' + a_2 + b}, 
	\end{equation*}
	where in the last inequality, we use the assumption that $c \geq 1$.
	Now, notice that, the function $f(x) = \frac{y}{x + z}$ is a decreasing function of 
	$x > 0$ for any positive number $y, z$, so we have,  
	\begin{equation*}
		\frac{c^2 a_1' + cb}{a_1'+a_2'} \geq \frac{c^2 a_1'+cb}{a_1' + c a_2 + b} 
		= \frac{c a_1'+b}{ (a_1' + b)/ c + a_2} \geq \frac{c a_1'+b}{ a_1' + a_2 + b} 
	\end{equation*}
	where in the last inequality, we use the assumption that $c \geq 1$. Combining the two 
	elementary inequalities above, we have shown the claim in the lemma. 
\end{proof}

By assumption we have $u_{1:M}$ and $v$ 
such that $\probuv (\bigcap_{m=1}^{t-1} \EvSmSplm ) \geq \frac{1}{4^{t-1}}$ for sufficiently large $n$.
For any re-chosen $\tilde u_{t:M}$ and $\tilde v$,
let $\EvSpliInBallt$ be an event on which the $i$-th sample of $t$-th round is in $\ballp{\tilde u_t}{\delta_t}$:
\begin{equation*}
	\EvSpliInBallt \defeq \{ X_i^{(t)} \in \ballp{\tilde u_t}{\delta_t} \}, \text{ for } i \in 1:n.
\end{equation*}
Then, apply Lemma~\ref{lem:likelihood1}, with
\begin{align*}
	&k = t-1, ~
	G = (\EvSpliInBallt)^c,\\
	&\xi_m^{(j)} = h_m^{\frac{1}{4}} \delta_{t-1}^{\frac{\kappa-j}{2}} \text{ for } m \in 1:t-2, j \in 0:\zeta,\\
	&\xi_{t-1}^{(\zeta)} = \sqrt{2}\sigma^2 \sqrt{\log n}, ~
	\xi_{t-1}^{(0)} = h_{t-1}^{\frac{1}{4}} \delta_{t-1}^{\frac{\kappa}{2}} \text { if } \zeta = 1.
\end{align*}
Then, we get 
\begin{equation*}
	\probuv ( (\EvSpliInBallt)^c \cap \EvSmSplFrmOneTo{t-1} )
	\leq K_1 \probuvtildet ( (\EvSpliInBallt)^c \cap \EvSmSplFrmOneTo{t-1} ) + K_2,
\end{equation*}
where the quantities $K_1$ and $K_2$ are
\begin{align*}
	K_1 \defeq& \exp \left ( \frac{(\sum_{j=0}^\zeta \delta_{t-1}^{\frac{\kappa-j}{2}}) \sum_{m=1}^{t-2} h_m^{\frac{1}{4}} }{\sigma^2} + 
	\frac {(\sum_{j=0}^\zeta \tilde C_j^2 \alpha^{2t-4} \delta_{t-1}^{2(\kappa-j)}) \sum_{m=1}^{t-2} h_m}{2 \sigma^2}  \right )\\
	&\times \exp \left ( \frac{\sqrt 2 \sigma^2 \sqrt{\log n} + \IND{\zeta = 1} \delta_{t-1}^{\frac{\kappa}{2}} h_{t-1}^{\frac{1}{4}} }{\sigma^2} + 
	\frac {(\sum_{j=0}^\zeta \tilde C_j^2 \alpha^{2t-4} \delta_{t-1}^{2(\kappa-j)}) h_{t-1}}{2 \sigma^2}  \right )\\
	K_2 \defeq& 2 \sum_{m=1}^{t-2} \sum_{j=0}^{\zeta}
	\exp \left (  -\frac {1}{2 \sigma^2  h_{m}^\half \tilde C_j^2 \alpha^{2t-4} \delta_{t-1}^{\kappa-j}}  \right )\\
	&+2 \exp \left (  -\frac {\sigma^2 \log n}{h_{t-1} \tilde C_\zeta^2 \alpha^{2t-4} \delta_{t-1}^{2(\kappa-\zeta)}}  \right )
	+ \IND{\zeta = 1}
	\exp \left (  -\frac {1}{2 \sigma^2  h_{t-1}^\half \tilde C_0^2 \alpha^{2t-4} \delta_{t-1}^{\kappa}} \right ).
\end{align*}
As seen the proof of Lemma~\ref{lem:lblem1}, the first multiplicative term in $K_1$ and the first additive term in $K_2$ go down to 1 and down to 0, respectively.
Moreover, if $\zeta = 1$, then $h_t \delta_t^{2\kappa}$ also polynomially decays to zero.
So, if $\zeta = 1$, the terms
\begin{equation*}
	\exp \left ( \frac{\IND{\zeta = 1} \delta_{t-1}^{\frac{\kappa}{2}} h_{t-1}^{\frac{1}{4}} }{\sigma^2} + 
	\frac {( \tilde C_0^2 \alpha^{2t-4} \delta_{t-1}^{2\kappa}) h_{t-1}}{2 \sigma^2}  \right )
	\downarrow 1, \text{ and }
	\exp \left (  -\frac {1}{2 \sigma^2  h_{m}^\half \tilde C_0^2 \alpha^{2t-4} \delta_{t-1}^{\kappa}}\right ) \downarrow 0.
\end{equation*}
Also, by noting the identity $\tilde C_\zeta^2 \alpha^{2t-4} h_t \delta_{t-1}^{2(\kappa-\zeta)} = \sigma^2$ from Eq~\eqref{eq:defht2},
\begin{equation*}
	\exp \left ( \frac { \tilde C_\zeta^2 \alpha^{2t-4} \delta_{t-1}^{2(\kappa-\zeta)} h_{t-1}}{2 \sigma^2}  \right ) = \sqrt e,~
	2 \exp \left (  -\frac {\sigma^2 \log n}{h_{t-1} \tilde C_\zeta^2 \alpha^{2t-4} \delta_{t-1}^{2(\kappa-\zeta)}}  \right ) = \frac{2}{n}.
\end{equation*}
Summarizing all these observations, for large enough $n$, we have
\begin{equation}
\label{eqn:lem2eq2}
\probuv ( (\EvSpliInBallt)^c \cap \EvSmSplFrmOneTo{t-1} )
\leq K_1 \probuvtildet ( (\EvSpliInBallt)^c \cap \EvSmSplFrmOneTo{t-1} ) + K_2,
\end{equation}
where 
\begin{equation}
\label{eq:ConeCtwo}
1 \leq K_1 \leq \sqrt {3e} \exp(\sqrt 2 \sqrt {\log n}), ~
K_2 = \frac{2}{n} + O(\exp(-n^\tau)).
\end{equation}
for some $\tau > 0$.

Note that, we can switch between $u_{t:M}$, $v$ and $\tilde u_{t:M}$, $\tilde v$,
and apply Lemma~\ref{lem:likelihood1} again, this time for $G = \EvSpliInBallt$.
This time, the equation we get is
\begin{equation}
\label{eqn:lem2eq3}
\probuvtildet ( \EvSpliInBallt \cap \EvSmSplFrmOneTo{t-1} )
\leq K_1 \probuv ( \EvSpliInBallt \cap \EvSmSplFrmOneTo{t-1} ) + K_2,
\end{equation}
with the same $K_1$ and $K_2$.
Now, apply lemma Lemma~\ref{lem:elem-ineq} for Eqs~\eqref{eqn:lem2eq3} and \eqref{eqn:lem2eq2},
then
\begin{align*}
	\probuvtildet ( \EvSpliInBallt \mid \EvSmSplFrmOneTo{t-1} )
	&\leq K_1^2 
	\probuv ( \EvSpliInBallt \mid \EvSmSplFrmOneTo{t-1} )
	+ 
	\frac{K_1 K_2}
	{\probuv ( \EvSmSplFrmOneTo{t-1} )}\\
	&\leq K_1^2 \probuv (\EvSpliInBallt  \mid\EvSmSplFrmOneTo{t-1})
	+ 4^{t-1}K_1 K_2. \numberthis \label{eqn:lem2eq4}
\end{align*}

Now we are ready to use the pigeonhold principle argument for this lemma.
We then sum up both sides of Eq~\eqref{eqn:lem2eq4}
for all possible values of $\tilde u_{t:M}$, $\tilde v$, and $i = 1:n$.
For simplicity in notation, let us denote the summation 
\begin{equation*}
	\sum_{\tilde u_{t+1} \in \mathcal{U}_{\tilde u_t}^{(t+1)}}
	\cdots
	\sum_{\tilde u_M \in \mathcal{U}_{\tilde u_{M-1}}^{(M)}}
	\sum_{\tilde v \in \V}
	\equiv
	\sum_{\tilde u_{t+1:M}, \tilde v}.
\end{equation*}
Also, recall that there are $2 | \Utemp{t-1}{t}  |  \prod_{m=t+1}^M  | \mc{U}_{\tilde u_{m-1}}^{(m)}  |$ possible values of $\tilde u_{t:M}$ and $\tilde v$.
After summing up, we get
\begin{align*}
	&\sum_{\tilde u_t} \sum_{\tilde u_{t+1:M}, \tilde v}
	\sum_{i=1}^n
	\probuvtildet (\EvSpliInBallt  \mid \EvSmSplFrmOneTo{t-1}) \\
	= &\sum_{\tilde u_t} \sum_{\tilde u_{t+1:M}, \tilde v}
	\exptuvtildet \left (\sum_{i=1}^n \IND{ X_{i}^{(t)}\in \ballp{\tilde u_t}{\delta_t} } 
	\mid \EvSmSplFrmOneTo{t-1} \right )\\
	\leq &K_1^2
	\sum_{i=1}^n
	\sum_{\tilde u_t } \sum_{\tilde u_{t+1:M}, \tilde v}
	\probuv (\EvSpliInBallt  \mid \EvSmSplFrmOneTo{t-1})
	+ 4^{t-1} K_1 K_2
	\cdot 2n  | \Utemp{t-1}{t}  |  \prod_{m=t+1}^M  | \mc{U}_{\tilde u_{m-1}}^{(m)}  | \\
%	= &  K_1^2
%	\sum_{i=1}^n
%	\sum_{\tilde u_t } \left ( 2\prod_{m=t+1}^M  | \mc{U}_{\tilde u_{m-1}}^{(m)}  | \right )
%	\probuv (\EvSpliInBallt  \mid \EvSmSplFrmOneTo{t-1})
%	+ 4^{t-1} K_1 K_2
%	\cdot 2n  | \Utemp{t-1}{t}  |  \prod_{m=t+1}^M  | \mc{U}_{\tilde u_{m-1}}^{(m)}  |\\
	\leq& K_1^2 \cdot
	2n\prod_{m=t+1}^M  | \mc{U}_{\tilde u_{m-1}}^{(m)}  |
	+ 4^{t-1} K_1 K_2
	\cdot 2n  | \Utemp{t-1}{t}  |  \prod_{m=t+1}^M  | \mc{U}_{\tilde u_{m-1}}^{(m)}  |,
	\numberthis \label{eqn:lem2eq5}
\end{align*}
where the last inequality used that $\sum_{\tilde u_t } \probuv (\EvSpliInBallt  \mid \EvSmSplFrmOneTo{t-1}) \leq 1$.

Since there are $2 | \Utemp{t-1}{t}  |  \prod_{m=t+1}^M  | \mc{U}_{\tilde u_{m-1}}^{(m)}  |$ possible values of $\tilde u_{t:M}$ and $\tilde v$, Equation \ref{eqn:lem2eq5} implies that by the pigeonhole principle, there exists 
at least one set of parameters $\tilde u_{t:M}$ and $\tilde v$ that satisfies 
\begin{align*}
	&\exptuvtildet \left (\sum_{i=1}^n \IND{ X_{i}^{(t)}\in \ballp{\tilde u_t}{\delta_t} } 
	\mid \EvSmSplFrmOneTo{t-1} \right )\\
	\leq& \frac{n K_1^2}{| \Utemp{t-1}{t}  |} +4^{t-1} n K_1 K_2
	\leq n K_1^2 \left ( \frac{4\delta_t} {\eta \delta_{t-1}} \right )^d + 4^{t-1} n K_1 K_2,
\end{align*}
where Lemma~\ref{lem:packingno} is used in the last inequality.
This implies, by Markov's inequality,
\begin{align*}
	& \probuvtildet (\EvSmSpltildet \mid \EvSmSplFrmOneTo{t-1}) \geq 1 - \frac{1}{h_t} \exptuvtildet \left (\sum_{i=1}^n \IND{ X_{i}^{(t)}\in \ballp{\tilde u_t}{\delta_t} } \mid \EvSmSplFrmOneTo{t-1} \right ) \\
	\geq & 1 - \frac{n K_1^2}{h_t} \left ( \frac{4\delta_t} {\eta \delta_{t-1}} \right )^d - \frac{4^{t-1} n K_1 K_2}{h_t}.
\end{align*}

To finish the proof of Lemma~\ref{lem:lblem2}, it suffices to show that the RHS of the last inequality is greater than or equal to $\half$,
which is equivalent to
\begin{equation}
\label{eqn:lem2eq6}
n K_1^2 \left ( \frac{4\delta_t} {\eta \delta_{t-1}} \right )^d + 4^{t-1} n K_1 K_2 \leq \frac{h_t}{2}.
\end{equation}
Recall from Eq~\eqref{eq:ConeCtwo} that $1 \leq K_1 \leq \sqrt {3e} \exp(\sqrt 2 \sqrt {\log n})$
and $K_2 = \frac{2}{n} + O(\exp(-n^\tau))$. Substituting these to the first term in the LHS of Eq~\eqref{eqn:lem2eq6} yields
\begin{align*}
	n K_1^2 \left ( \frac{4\delta_t} {\eta \delta_{t-1}} \right )^d 
	&\leq 3 e n \exp \left (2\sqrt 2 \sqrt{\log n} \right ) \left ( \frac{4\delta_t} {\eta \delta_{t-1}} \right )^d.
\end{align*}
From the definition of $\gamma_t$ \eqref{eq:defgammat}, $\delta_t$ \eqref{eq:defdeltat}, and $D_t$ \eqref{eq:defDt},
we can get useful identities 
\begin{align*}
	&\left ( \frac{4D_t} {\eta D_{t-1}} \right )^d = \frac{\sigma^2}{8 e \tilde C_\zeta^2 \alpha^{2t-2} D_t^{2(\kappa-\zeta)}}, \text{ and }\\
	&1-d(\gamma_t - \gamma_{t-1}) = 1-\left( \frac{d}{d+2(\kappa-\zeta)} \right)^t = 2(\kappa-\zeta) \gamma_t,
\end{align*}
thereby one can get
\begin{align*}
	&3 e n \exp \left (2\sqrt 2 \sqrt{\log n} \right ) \left ( \frac{4\delta_t} {\eta \delta_{t-1}} \right )^d\\
	=& \frac{3 \sigma^2}{8 C_\zeta^2 \alpha^{2t-2} D_t^{2(\kappa-\zeta)}} 
	n^{2(\kappa-\zeta) \gamma_t} 
	\exp \left (4\sqrt{2} (\kappa-\zeta) \gamma_{t-1} \sqrt{\log n} \right)
	(\log n)^{-\IND{t=M}d\nu/\kappa}.
\end{align*}
comparing with $h_t$ \eqref{eq:defht}, we check that 
\begin{equation}
\label{eq:1sttermht}
n K_1^2 \left ( \frac{4\delta_t} {\eta \delta_{t-1}} \right )^d \leq \frac{3}{8} h_t.
\end{equation}

Now, substitute $K_1 \leq \sqrt {3e} \exp(\sqrt 2 \sqrt {\log n})$
and $K_2 = \frac{2}{n} + O(\exp(-n^\tau))$ to the second term in the LHS of Eq~\eqref{eqn:lem2eq6},
the we get:
\begin{align*}
	4^{t-1} n K_1 K_2 
	& \leq 2 \cdot 4^{t-1} \sqrt{3e} \exp \left (\sqrt 2 \sqrt{\log n} \right ) 
	\left ( 1 + O(n\exp(-n^\tau)) \right ).
\end{align*}
Since $O(n\exp(-n^\tau)) \downarrow 0$ as $n \rightarrow \infty$,
we can see that the first term in LHS of Eq~\eqref{eqn:lem2eq6} dominates 
the second one for sufficiently large $n$.
With the observation in Eq~\eqref{eq:1sttermht}, we can see 
Eq~\eqref{eqn:lem2eq6} holds for large enough $n$,
so this finishes the proof of Lemma~\ref{lem:lblem2}.


\subsection{Proof of Lemma~\ref{lem:lblem3}}
\label{sec:proof-lblem3}
For this lemma, we present a variant of Lemma~\ref{lem:likelihood1}, for fixed $u_{1:M}$ and different $v$ and $\tilde v$. Lemma~\ref{lem:likelihood2} is just a simple variant of Lemma~\ref{lem:likelihood1} for the case $t = M+1$ (final case), so its proof is omitted.
\begin{lemma}
	\label{lem:likelihood2}
	Pick any set of parameters $u_{1:M}$, and $v, \tilde v \in \V$ where $v \neq \tilde v$.
	Consider any probabilistic event $G$ that is a function of the random variables 
	\begin{align*}
		&X_{1:n}^{(1:k)}, Y_{1:n}^{(1:k)}, X_{1:n}^{(k+1)} 
		\quad\quad\quad\quad \text{ if oracle is zeroth-order, or }\\
		&X_{1:n}^{(1:k)}, Y_{1:n}^{(1:k)}, Z_{1:n}^{(1:k)}, X_{1:n}^{(k+1)}
		\quad \text{ if oracle is first-order,}
	\end{align*}
	where $k \in 1:M-1$. 
	Otherwise, the probabilistic event $G$ could also be a function of $X_{1:n}^{(1:M)}, Y_{1:n}^{(1:M)},$ and/or $Z_{1:n}^{(1:M)}$, in which case we let $k = M$.
	Then, the following holds for any $u_{1:M}$ and $v \neq \tilde v$.
	\begin{equation*}
		\probuv \left( G \cap \EvSmSplFrmOneTo{k} \right)\leq K_1 \probuvtildetfin ( G \cap \EvSmSplFrmOneTo{k} ) + K_2,
	\end{equation*}
	where the quantities $K_1$, $K_2$ are
	\begin{align*}
		K_1 &\defeq \exp \left ( \frac{\sum_{j=0}^\zeta \sum_{m=1}^{k} \xi_m^{(j)}}{\sigma^2} + 
		\frac {(\sum_{j=0}^\zeta \tilde C_j^2 \alpha^{2M-2} \delta_{M}^{2(\kappa-j)}) \sum_{m=1}^k h_m}{2 \sigma^2}  \right ),\\
		K_2 &\defeq 2 \sum_{m=1}^{k} \sum_{j=0}^{\zeta}
		\exp \left (  -\frac {(\xi_m^{(j)})^2}{2 \sigma^2  h_{m} \tilde C_j^2 \alpha^{2M-2} \delta_{M}^{2(\kappa-j)}}  \right ),
	\end{align*}
	where $\xi_m^{(j)}$ for $m \in 1:k$ and $j \in 0:\zeta$ is any positive quantity we can choose.
\end{lemma}

To prove the first statement of Lemma~\ref{lem:lblem3}, we can repeat the same process as Lemma~\ref{lem:lblem1}, this time with Lemma~\ref{lem:likelihood2}, and
\begin{align*}
	k = M-1, 
	G = \EvSmSpltemp{M}, \text{ and }~
	\xi_m^{(j)} = h_m^{\frac{1}{4}} \delta_{t-1}^{\frac{\kappa-j}{2}} \text{ for } m \in 1:M-1, j \in 0:\zeta.
\end{align*}
Then, we get 
\begin{equation*}
	\probuv ( \EvSmSplFrmOneTo{M} )\leq K_1 \probuvtildetfin ( \EvSmSplFrmOneTo{M} ) + K_2,
\end{equation*}
where the quantities $K_1 \downarrow 1$ and $K_2 \downarrow 0$ as $n \rightarrow \infty$.

Therefore, for large enough $n$, $K_1 \leq \sqrt 2$ and $K_2 \leq \left ( 1 - \frac{1}{\sqrt 2} \right )\frac{1}{4^{t-1}}$, resulting in
\begin{equation}
\probuv \left( \EvSmSplFrmOneTo{M} \right)\leq \sqrt 2 \probuvtildetfin ( \EvSmSplFrmOneTo{M} ) + \left ( 1 - \frac{1}{\sqrt 2} \right )\frac{1}{4^{M}}.
\end{equation}
From the assumption, we have $\probuv (\EvSmSplFrmOneTo{M}) \geq \frac{1}{4^{M}}$.
With these observations, we finish the proof of the first part of Lemma~\ref{lem:lblem3}:
\begin{equation*}
	\probuvtildetfin (\EvSmSplFrmOneTo{M}) \geq \frac{1}{2 \cdot 4^{M}},
\end{equation*}
for $\tilde v \neq v$, as desired.

Now, the last goal is to show that the total variation between 
$\probuvm$ and $\probuvp$ conditional on the event 
$\EvSmSplFrmOneTo{M}$ is small when the number of sample size $n$ 
is sufficiently large. The technique is essentially the 
same as that appeared in the proof of lemma~\ref{lem:lblem2}.
We apply Lemma~\ref{lem:likelihood2}, with
\begin{align*}
	k = M, ~
	G = \text{any } G, ~
	\xi_m^{(j)} = h_m^{\frac{1}{4}} \delta_{M}^{\frac{\kappa-j}{2}} \text{ for } m \in 1:M, j \in 0:\zeta.
\end{align*}
Then, we get 
\begin{equation*}
	\probuv ( G \cap \EvSmSplFrmOneTo{M} )
	\leq K_1 \probuvtildetfin ( G \cap \EvSmSplFrmOneTo{M} ) + K_2,
\end{equation*}
where the quantities $K_1$ and $K_2$ are
\begin{align*}
	K_1 \defeq& \exp \left ( \frac{(\sum_{j=0}^\zeta \delta_{M}^{\frac{\kappa-j}{2}}) \sum_{m=1}^{M} h_m^{\frac{1}{4}} }{\sigma^2} + 
	\frac {(\sum_{j=0}^\zeta \tilde C_j^2 \alpha^{2M-2} \delta_{M}^{2(\kappa-j)}) \sum_{m=1}^{M} h_m}{2 \sigma^2}  \right )\\
	K_2 \defeq& 2 \sum_{m=1}^{M} \sum_{j=0}^{\zeta}
	\exp \left (  -\frac {1}{2 \sigma^2  h_{m}^\half \tilde C_j^2 \alpha^{2M-2} \delta_{M}^{\kappa-j}}  \right ).
\end{align*}
In this case, we can note that since $\tilde C_\zeta^2 \alpha^{2M-2} h_M \delta_M^{2(\kappa-\zeta)} = \sigma^2 \log^{-\frac{2\nu(\kappa-\zeta)}{\kappa}} n$ \eqref{eq:defht2},
any $h_m \delta_M^{2(\kappa-j)}$ decreases to zero with $n \rightarrow \infty$.
So, we can see $K_1 \downarrow 1$ and $K_2 \downarrow 0$ as $n \rightarrow \infty$,
which allows us to write
\begin{equation}
\label{eqn:lem3eq3}
\probuv ( G \cap \EvSmSplFrmOneTo{M} )
\leq \sqrt{\frac{5}{4}} \probuvtildetfin ( G \cap \EvSmSplFrmOneTo{M} ) + \frac{1}{\sqrt{5} \cdot 4^{M+1}},
\end{equation}
for $n$ sufficiently large. Using exactly the same techniques, we can also get similar inequalities as follows:
\begin{align}
	\label{eqn:lem3eq4}
	\probuvtildetfin (G^c \cap \EvSmSplFrmOneTo{M}) &\leq \sqrt{\frac{5}{4}} \probuv ( G^c \cap \EvSmSplFrmOneTo{M} ) + \frac{1}{\sqrt 5 \cdot 4^{M+1}}\\
	\label{eqn:lem3eq5}
	\probuvtildetfin (G \cap \EvSmSplFrmOneTo{M}) &\leq \sqrt{\frac{5}{4}} \probuv ( G \cap \EvSmSplFrmOneTo{M} ) + \frac{1}{\sqrt 5 \cdot 4^{M+1}}\\
	\label{eqn:lem3eq6}
	\probuv (G^c \cap \EvSmSplFrmOneTo{M}) &\leq \sqrt{\frac{5}{4}} \probuvtildetfin ( G^c \cap \EvSmSplFrmOneTo{M} ) + \frac{1}{\sqrt 5 \cdot 4^{M+1}}
\end{align}


Now apply Lemma~\ref{lem:elem-ineq} to Eqs~\eqref{eqn:lem3eq3} and \eqref{eqn:lem3eq4} to get
\begin{equation*}
	\probuv (G \mid \EvSmSplFrmOneTo{M}) 
	\leq \frac{5}{4} \probuvtilde(G \mid \EvSmSplFrmOneTo{M}) + \frac{1}{2\cdot 4^{M+1} \probuvtilde(\EvSmSplFrmOneTo{M})} 
	\leq \frac{5}{4} \probuvtilde(G \mid \EvSmSplFrmOneTo{M}) + \frac{1}{4}.
\end{equation*}
Similarly, from Eqs~\eqref{eqn:lem3eq5} and \eqref{eqn:lem3eq6},
\begin{equation*}
	\probuvtilde (G \mid \EvSmSplFrmOneTo{M}) 
	\leq \frac{5}{4} \probuv(G \mid \EvSmSplFrmOneTo{M}) + \frac{1}{4}.
\end{equation*}
From these two equations, we have
\begin{equation*}
	\left|\probuv (G \mid \EvSmSplFrmOneTo{M})- \probuvtilde (G \mid \EvSmSplFrmOneTo{M})\right| \leq \half,
\end{equation*}
for any $G$ and $v \neq \tilde v$. Thus, by definition of total variation distance, we have 
\begin{equation*}
	\tvnorm{ \probuvm \left (\cdot \mid \EvSmSplFrmOneTo{M} \right ) -
		\probuvp \left (\cdot \mid \EvSmSplFrmOneTo{M} \right ) } \leq \frac{1}{2},
\end{equation*}
as desired.

\subsection{Proof of Lemma~\ref{lem:likelihood1}}
\label{sec:proof-likelihood1}

For $t \in 2:M$ and $k \in 1:t-1$, consider any event $G$ that is a function of 
\begin{align*}
	&X_{1:n}^{(1:k)}, Y_{1:n}^{(1:k)}, X_{1:n}^{(k+1)} 
	\quad\quad\quad\quad \text{ if oracle is zeroth-order, or }\\
	&X_{1:n}^{(1:k)}, Y_{1:n}^{(1:k)}, Z_{1:n}^{(1:k)}, X_{1:n}^{(k+1)} 
	\quad \text{ if oracle is first-order.}
\end{align*}
In this proof, we develop a generic technique that provides an upper bound of the probability 
$G \cap \bigcap_{m=1}^{k} \EvSmSplm$ under measure $\probuv$ in terms of 
the probability of the same event under another measure $\probuvtildet$ based on a different function.
Recall that the two probability 
measure are defined by their corresponding functions $\ftnuv$ and $\ftnuvtildet$ that only differ
in the interior of $\Bptmone$.
Note that, the conclusion stated in the lemma provides relationship between the probabilities of 
an event under different measures and therefore the technique used in
this proof can be of independent interest to the readers. 

\paragraph{Notation.}
To give a clear illustration of how we make that bound happen, we need to introduce some notation, mainly 
to ``partition'' the events $\EvSmSplm$. 
For $m \in 1:k$, we enumerate and index all the possible $2^n$ subsets of $\left\{1, 2, \ldots, n\right\}$
by $S_{l_m}$, where $l_m \in 1:2^n$. 
Also, for all $m \in 1:k$ and $i \in 1:n$, define 
\begin{align*}
	\epsilon_i^{(m,0)} &= Y_i^{(m)} - \ftnuv(X_i^{(m)}), \\
	\epsilon_i^{(m,1)} &= Z_i^{(m)} - \nabla \ftnuv(X_i^{(m)}),\\
	\Delta_i^{(m,0)} &= \ftnuv(X_i^{(m)}) - \ftnuvtildet(X_i^{(m)}),\\
	\Delta_i^{(m,1)} &= \nabla \ftnuv(X_i^{(m)}) - \nabla \ftnuvtildet(X_i^{(m)}).
\end{align*}
Here, whenever the function is not differentiable at $x$, 
we replace $\nabla \ftnuv(x)$ with any subgradient of $\ftnuv$ at $x$.
To interpret these quantities, based on the assumption that $\ftnuv$ is the true function,
$\epsilon_i^{(m,0)}$ is the (i.i.d.\ Gaussian) error of the zeroth order oracle at $X_i^{(m)}$, and $\Delta_i^{(m,0)}$ is the difference between two functions $\ftnuv$ and $\ftnuvtildet$ at $X_i^{(m)}$. Similarly, $\epsilon_i^{(m,1)}$ and $\Delta_i^{(m,1)}$ are the error and difference in the first order information (gradient values) at $X_i^{(m)}$. Note that $\epsilon_i^{(m,0)}$ and $\Delta_i^{(m,0)}$ are scalars while the other two are vectors.
Note that, by Condition~\ref{con:ftnconstrunified}.\ref{item:ftnconstrunified-1}, $\Delta_i^{(m,0)}$ and $\Delta_i^{(m,1)}$ are zero when $X_i^{(m)} \notin \Bptmone$. Also note that, based on the assumption that $\ftnuvtildet$ is the true function, then $\epsilon_i^{(m,0)} + \Delta_i^{(m,0)}$ and $\epsilon_i^{(m,1)} + \Delta_i^{(m,1)}$ are the errors of the oracle.

Next, for each set $S_{l_m}$, we introduce the following groups of events,
\begin{align*}
	\EvSlmInBallm \defeq &\left\{ X_i^{(m)} \in \Bpm, ~\forall i \in S_{l_m} \right\} \bigcap \left\{ X_i^{(m)} 
	\not\in \Bpm, ~\forall i \not\in S_{l_m} \right\}, \quad \textup{ for } m \in 1:k,\\
	\EvSlmNoisem \defeq & \bigcap\nolimits_{j=0}^{\zeta} \EvSlmNoisemzettemp{j}, \quad \textup{ for } m \in 1:k,\\
	\EvSlmNoisemzettemp{j} \defeq & \left\{ \left| \sum_{i\in S_{l_m}} \< \epsilon^{(m,j)}_i, \Delta^{(m,j)}_i\> \right| \leq \xi_m^{(j)} \right\},
	\quad \textup { for } m \in 1:k, j \in 0:1.
\end{align*}
The event $\EvSlmInBallm$ occurs when $X_i^{(m)}$ is in the ball $\Bpm$ if and only if the index $i \in S_{l_m}$. 
Recall that $\zeta$ represents the order of the oracle. So if we are using zeroth-order oracle,
$\EvSlmNoisem  = \EvSlmNoiseZom$, while $\EvSlmNoisem = \EvSlmNoiseZom \cap \EvSlmNoiseFom$ for first-order oracles.
The event $\EvSlmNoisemzettemp{j}$ is that the sum of noise introduced by the oracle is smaller than some positive quantity $\xi_m^{(j)}$ which we can choose. Recall that $\xi_m^{(j)}$ appeared in the statement of the lemma. 

\paragraph{Partitioning  $\EvSlmInBallm$ into disjoint events.}
Notice the events $\EvSlmInBallm$ are disjoint from one another for different $l_m \in 1:2^n$ because it can never happen at the same time with different values of $l_m$.
From this observation, we can get a partition of the event $\bigcap_{m=1}^{k} \EvSmSplm$, 
as the following equation suggests, 
\begin{equation*}
	\bigcap_{m=1}^{k} \EvSmSplm 
	= \bigcap_{m=1}^{k} \left( \bigcup_{l_m} \left(\EvSlmInBallm \cap \EvSmSplm \right)\right) 
	= \bigcup_{l_1, l_2, \ldots, l_{k}} \left( \bigcap_{m=1}^{k} \left( \EvSlmInBallm \cap \EvSmSplm \right) \right)
\end{equation*}
From the above equation, we note that in order to establish an upper bound of 
$\probuv \left( G \cap \bigcap_{m=1}^{k} \EvSmSplm \right)$, it suffices to get an upper bound of 
$\probuv \left( G \cap \bigcap_{m=1}^{k} \left( \EvSlmInBallm \cap \EvSmSplm \right) \right)$ 
for any fixed sequence $\{l_m\}_{m=1}^{k} \in \{1:2^{n}\}^{k}$ and then do the summation over all possible $\{l_m\}_{m=1}^{k}$.
As we will see later, when restricted on the set 
$ \bigcap_{m=1}^{k} \EvSlmInBallm$, we can give an explicit form of the likelihood ratio between 
the two probability measures $\probuv$ and $\probuvtildet$, which greatly helps us 
analyze the relationship between the two probabilities that are computed under different measures. 

Now \emph{fix any} $\{l_m\}_{m=1}^{k}$. For the sake of simplicity in notation, let us denote 
\begin{equation*}
	\EvSmSplFrmOneTo{k} = \bigcap_{m=1}^k \EvSmSplm, ~~
	\EvSlmInBallFrmOneTo{k} = \bigcap_{m=1}^k \EvSlmInBallm, ~~
	\EvSlmNoiseFrmOneTo{k} = \bigcap_{m=1}^k \EvSlmNoisem.
\end{equation*}
Expressed in this compact form, we want to get an upper bound for
$\probuv \left( G \cap \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{k} \right)$.
Starting from this point, we have the following inequality: 
\begin{align*}
	&\probuv \left( G \cap \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{k} \right)\\
	=&\probuv \left( G \cap \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{k} \cap \EvSlmNoiseFrmOneTo{k} \right)
	+ \probuv \left( G \cap \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{k} \cap (\EvSlmNoiseFrmOneTo{k})^c \right)\\
%	\leq&  \probuv \left( G \cap \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{k} \cap \EvSlmNoiseFrmOneTo{k} \right)
%	+ \sum_{j=0}^{\zeta} \sum_{m=1}^{k} 
%	\probuv \left( \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{k} \cap (\EvSlmNoisemzettemp{j})^c \right )\\
	\leq&  \probuv \left( G \cap \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{k} \cap \EvSlmNoiseFrmOneTo{k} \right)
	+ \sum_{j=0}^{\zeta} \sum_{m=1}^{k} 
	\probuv \left( \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{m} \cap (\EvSlmNoisemzettemp{j})^c \right ).
	\numberthis \label{eqn:lem1eq1}
\end{align*}
Note that the RHS of the Eq~\eqref{eqn:lem1eq1} consists of two parts. 
The rest of the proof consists of two parts: we first bound the first term of RHS of the Eq~\eqref{eqn:lem1eq1}, and the bound the second term while summing up disjoint events together.

\paragraph{Bounding the first term of Eq~\eqref{eqn:lem1eq1}.}
We first give an upper bound of the first term using the likelihood ratio between two different measures.
Recall from Condition~\ref{con:ftnconstrunified}.\ref{item:ftnconstrunified-1} that 
the functions $\ftnuv$ and $\ftnuvtildet$ have the same values at all $x \notin \Bptmone$.
Given the event $\EvSlmInBallm$, for $m\in1:k$ and any $i \notin S_{l_m}$, we have
\begin{equation*}
	\begin{array}{ll}
		p_{u_{1:M}}^{v}(y_i^{(m)} \mid x_i^{(m)}) = p_{u_{1:t-1},\tilde u_{t:M}}^{\tilde v}(y_i^{(m)} \mid x_i^{(m)}) 
		&\text{ if } \zeta = 0,\\
		p_{u_{1:M}}^{v}(y_i^{(m)},z_i^{(m)} \mid x_i^{(m)}) = p_{u_{1:t-1},\tilde u_{t:M}}^{\tilde v}(y_i^{(m)},z_i^{(m)} \mid x_i^{(m)})
		&\text{ if } \zeta = 1.
	\end{array}
\end{equation*}

Also recall the definition of $\tilde C_\zeta=C(1-\beta)^{1-\zeta}(2\kappa)^\zeta$ from Eq~\eqref{eq:deftildeC} and Condition~\ref{con:ftnconstrunified}.\ref{item:ftnconstrunified-2}--\ref{item:ftnconstrunified-3} that 
\begin{align*}
	| \ftnuv(x) - \ftnuvtildet(x) | &\leq \tilde C_0 \alpha^{t-2} \delta_{t-1}^\kappa,~\forall x \in \Bptmone \subset \Bpm\\
	\ltwo{\nabla \ftnuv(x) - \nabla \ftnuvtildet(x)} &\leq \tilde C_1 \alpha^{t-2} \delta_{t-1}^{\kappa-1},~\forall x \in \Bptmone \subset \Bpm.
\end{align*}
For $i \in S_{l_m}$, the ratio between $p_{u_{1:M}}^{v}$ and $p_{u_{1:t-1},\tilde u_{t:M}}^{\tilde v}$
is the ratio between two Gaussian distributions.
Thus, if $\zeta = 0$ (zeroth-order oracle), we have
\begin{align*}
	&\IND{\EvSlmInBallm \cap \EvSmSplm \cap \EvSlmNoisem}
	\frac {p_{u_{1:M}}^{v} (y_{1:n}^{(m)} \mid x_{1:n}^{(m)})}
	{p_{u_{1:t-1},\tilde u_{t:M}}^{\tilde v} (y_{1:n}^{(m)} \mid x_{1:n}^{(m)})} \\
%	= &\IND{\EvSlmInBallm \cap \EvSmSplm \cap \EvSlmNoisem}
%	\frac {\exp (-\frac{1}{2\sigma^2} \sum_{i \in S_{l_m}}(y_i^{(m)}-\ftnuv(x_i^{(m)}))^2)}
%	{\exp (-\frac{1}{2\sigma^2} \sum_{i \in S_{l_m}}(y_i^{(m)}-\ftnuvtildet(x_i^{(m)}))^2)} \\
%	= &\IND{\EvSlmInBallm \cap \EvSmSplm \cap \EvSlmNoisem}
%	\frac {\exp (-\frac{1}{2\sigma^2} \sum_{i \in S_{l_m}}(\epsilon_i^{(m,0)})^2)}
%	{\exp (-\frac{1}{2\sigma^2} \sum_{i \in S_{l_m}}(\epsilon_i^{(m,0)}+\Delta_i^{(m,0)})^2)} \\
	= &\IND{\EvSlmInBallm \cap \EvSmSplm \cap \EvSlmNoisem}
	\exp \left (\frac{1}{\sigma^2} \sum_{i \in S_{l_m}} \epsilon_i^{(m,0)} \Delta_i^{(m,0)} + 
	\frac{1}{2\sigma^2} \sum_{i \in S_{l_m}} (\Delta_i^{(m,0)})^2 \right ) \\
	\leq &\IND{\EvSlmInBallm \cap \EvSmSplm}
	\exp \left ( \frac{\xi_m^{(0)}}{\sigma^2} 
	+ \frac {h_m \tilde C_0^2 \alpha^{2t-4} \delta_{t-1}^{2\kappa}}{2 \sigma^2} \right ),
	\numberthis \label{eq:likelihood2}
\end{align*}
where the last inequality used the definitions of $\EvSlmNoisem$ and $\EvSmSplm$, 
and Condition~\ref{con:ftnconstrunified}.\ref{item:ftnconstrunified-2}.
For $\zeta = 1$ (first-order oracle), note that
\begin{align*}
	p_{u_{1:M}}^{v} (y_{1:n}^{(m)}, z_{1:n}^{(m)} \mid x_{1:n}^{(m)}) 
	&\propto
%	\exp \Bigg( -\frac{1}{2\sigma^2} \sum_{i \in S_{l_m}}(y_i^{(m)}-\ftnuv(x_i^{(m)}))^2 \\
%	&	\quad\quad\quad\quad-\frac{1}{2\sigma^2} \sum_{i \in S_{l_m}}\ltwosqr{z_i^{(m)}-\nabla \ftnuv(x_i^{(m)})} \Bigg)\\
	\exp \left (-\frac{1}{2\sigma^2} \sum_{i \in S_{l_m}}(\epsilon_i^{(m,0)})^2 
	-\frac{1}{2\sigma^2} \sum_{i \in S_{l_m}}\ltwosqr{\epsilon_i^{(m,1)}} \right )				
\end{align*}
By a similar argument as the zeroth-order case, we can get
\begin{align*}
	&\IND{\EvSlmInBallm \cap \EvSmSplm \cap \EvSlmNoisem}
	\frac {p_{u_{1:M}}^{v} (y_{1:n}^{(m)}, z_{1:n}^{(m)} \mid x_{1:n}^{(m)})}
	{p_{u_{1:t-1},\tilde u_{t:M}}^{\tilde v} (y_{1:n}^{(m)}, z_{1:n}^{(m)} \mid x_{1:n}^{(m)})} \\
	= &\IND{\EvSlmInBallm \cap \EvSmSplm \cap \EvSlmNoisem}
	\exp \left (\frac{1}{\sigma^2} \sum_{i \in S_{l_m}} \sum_{j=0}^1 \< \epsilon_i^{(m,j)}, \Delta_i^{(m,j)}\> + 
	\frac{1}{2\sigma^2} \sum_{i \in S_{l_m}} \sum_{j=0}^1 \ltwosqr{\Delta_i^{(m,j)}} \right ) \\
	\leq &\IND{\EvSlmInBallm \cap \EvSmSplm}
	\exp \left ( \frac{\xi_m^{(0)}+\xi_m^{(1)}}{\sigma^2} 
	+ \frac {h_m \tilde C_0^2 \alpha^{2t-4} \delta_{t-1}^{2\kappa} + h_m \tilde C_1^2 \alpha^{2t-4} \delta_{t-1}^{2\kappa-2}}{2 \sigma^2} \right ),
	\numberthis \label{eq:likelihood3}
\end{align*}
where the last inequality used the definitions of $\EvSlmNoisem$ and $\EvSmSplm$, 
and Condition~\ref{con:ftnconstrunified}.\ref{item:ftnconstrunified-2}--\ref{item:ftnconstrunified-3}.

Now consider again the zeroth-order case.
For any event $E$ that is a function of random variables
\begin{equation*}
	X_{1:n}^{(1:k)}, Y_{1:n}^{(1:k)}, X_{1:n}^{(k+1)},
\end{equation*}
the probability $\probuv (E)$ can be expressed as 
\begin{align*}
	&\probuv (E) = \exptuv[ \IND{E} ]\\
	=&\int
	\IND{E}
	dQ^{(k+1)}(x_{1:n}^{(k+1)} \mid x_{1:n}^{(1:k)}, y_{1:n}^{(1:k)}) 
	dP_{u_{1:M}}^{v} (y_{1:n}^{(k)} \mid x_{1:n}^{(k)})
	dQ^{(k)}(x_{1:n}^{(k)} \mid x_{1:n}^{(1:k-1)}, y_{1:n}^{(1:k-1)})
	\\
	&\quad\times \cdots \times
	dP_{u_{1:M}}^{v} (y_{1:n}^{(1)} \mid x_{1:n}^{(1)})
	dQ^{(1)}(x_{1:n}^{(1)}).\\
	=&\int
	\IND{E} \prod_{m=1}^k  \frac{dP_{u_{1:M}}^{v} (y_{1:n}^{(m)} \mid x_{1:n}^{(m)})}
	{dP_{u_{1:t-1},\tilde u_{t:M}}^{\tilde v}(y_{1:n}^{(m)} \mid x_{1:n}^{(m)})} 
	dQ^{(k+1)}(x_{1:n}^{(k+1)} \mid x_{1:n}^{(1:k)}, y_{1:n}^{(1:k)}) dP_{u_{1:t-1},\tilde u_{t:M}}^{\tilde v} (y_{1:n}^{(k)} \mid x_{1:n}^{(k)})\\
	&\quad dQ^{(k)}(x_{1:n}^{(k)} \mid x_{1:n}^{(1:k-1)}, y_{1:n}^{(1:k-1)}) \times \cdots \times
	dP_{u_{1:t-1},\tilde u_{t:M}}^{\tilde v} (y_{1:n}^{(1)} \mid x_{1:n}^{(1)})
	dQ^{(1)}(x_{1:n}^{(1)})\\
	=& \exptuvtildet\left[ \IND{E} \prod_{m=1}^k \frac{p_{u_{1:M}}^{v} (y_{1:n}^{(m)} \mid x_{1:n}^{(m)})}
	{p_{u_{1:t-1},\tilde u_{t:M}}^{\tilde v}(y_{1:n}^{(m)} \mid x_{1:n}^{(m)})} \right ].
	\numberthis \label{eq:likelihood1}
\end{align*}
Substituting $E = G \cap \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{k} \cap \EvSlmNoiseFrmOneTo{k}$ 
to Eq~\ref{eq:likelihood1} gives
\begin{align*}
	&\probuv(G \cap \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{k} \cap \EvSlmNoiseFrmOneTo{k}) \\
	= &\exptuvtildet \left [ \IND{G}
	\prod_{m=1}^{k} 
	\left ( \IND{\EvSlmInBallm \cap \EvSmSplm \cap \EvSlmNoisem}
	\frac {p_{u_{1:M}}^{v} (y_{1:n}^{(m)} \mid x_{1:n}^{(m)})}
	{p_{u_{1:t-1},\tilde u_{t:M}}^{\tilde v} (y_{1:n}^{(m)} \mid x_{1:n}^{(m)})}
	\right )
	\right ]\\
	\leq &\exptuvtildet \left [ \IND{G}
	\prod_{m=1}^{k} 
	\left ( \IND{\EvSlmInBallm \cap \EvSmSplm}
	\exp \left ( \frac{\xi_m^{(0)}}{\sigma^2} 
	+ \frac {h_m \tilde C_0^2 \alpha^{2t-4} \delta_{t-1}^{2\kappa}}{2 \sigma^2} \right )
	\right )
	\right ]\\
	= &\probuvtildet (G \cap \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{k})
	\exp \left ( \frac{\sum_{m=1}^{k} \xi_m^{(0)}}{\sigma^2} + 
	\frac {\tilde C_0^2 \alpha^{2t-4} \delta_{t-1}^{2\kappa} \sum_{m=1}^k h_m}{2 \sigma^2}  \right ),
\end{align*}
where the inequality used Eq~\eqref{eq:likelihood2}.
We can get a similar upper bound for the first-order case using Eq~\eqref{eq:likelihood3}, and in fact,
we can express both cases into a unified form using $\zeta$:
\begin{align*}
	\probuv(G \cap \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{k} \cap \EvSlmNoiseFrmOneTo{k})
	\leq K_1 \probuvtildet (G \cap \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{k}),
	\numberthis \label{eqn:lem1eq1term1}
\end{align*}
where we define
\begin{equation*}
	K_1 \defeq \exp \left ( \frac{\sum_{j=0}^\zeta \sum_{m=1}^{k} \xi_m^{(j)}}{\sigma^2} + 
	\frac {(\sum_{j=0}^\zeta \tilde C_j^2 \alpha^{2t-4} \delta_{t-1}^{2(\kappa-j)}) \sum_{m=1}^k h_m}{2 \sigma^2}  \right ),
\end{equation*}
so simplify the notation.
By this, we finished bounding the first term of RHS in Eq~\eqref{eqn:lem1eq1}.

\paragraph{Bounding the second term of Eq~\eqref{eqn:lem1eq1} and summing disjoint events.}
Now, we need to deal with the second term. Note that
\begin{align*}
	&\sum_{j=0}^{\zeta} \sum_{m=1}^{k} 
	\probuv \left( \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{m} \cap (\EvSlmNoisemzettemp{j})^c \right )\\
	=&\sum_{j=0}^{\zeta} \sum_{m=1}^{k-1} 
	\probuv \left( \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{m} \cap (\EvSlmNoisemzettemp{j})^c \right )
	+\sum_{j=0}^{\zeta} 
	\probuv \left( \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{k} \cap (\EvSlmNoisekzettemp{j})^c \right )
	\\
	=&\sum_{j=0}^{\zeta} \sum_{m=1}^{k-1} 
	\probuv \left( \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{m} \cap (\EvSlmNoisemzettemp{j})^c \right )\\
	&+
	\probuv \left(  \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{k} \right )
	\sum_{j=0}^{\zeta} 
	\probuv \left( (\EvSlmNoisekzettemp{j})^c \mid \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{k} \right )
	\numberthis \label{eqn:lem1eq1term2}
\end{align*}
We can use a concentration inequality of Gaussian random variables to bound above the conditional probability term.
We should first note that once $\EvSlmInBallk$ and $\EvSmSplk$ are given,
this means that at most $h_k$ among $X_i^{(k)}$ are in $\Bptmone$, so
\begin{align*}
	\sum_{i \in S_{l_{k}}} (\Delta_i^{(k,0)})^2 \leq h_{k} \tilde C_0^2 \alpha^{2t-4} \delta_{t-1}^{2\kappa}
	\text{ and }
	\sum_{i \in S_{l_{k}}} \ltwosqr {\Delta_i^{(k,1)}} \leq h_{k} \tilde C_1^2 \alpha^{2t-4} \delta_{t-1}^{2(\kappa-1)}.
\end{align*}
Since the observation noise is independent zero-mean Gaussian with variance $\sigma^2$,
given that the true function is $\ftnuv$ we have $\epsilon_i^{(m,0)} \sim \mc N(0, \sigma^2)$ and 
$\epsilon_i^{(m,1)} \sim \mc N(0, \sigma^2 I_d)$.
Now, we can apply the concentration inequalities to get
\begin{equation}
\label{eqn:concentration}
\probuv\left( (\EvSlmNoisekzettemp{j})^c \mid \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{k} \right)
\leq 2\exp \left (  -\frac {(\xi_k^{(j)})^2}{2 \sigma^2  h_{k} \tilde C_j^2 \alpha^{2t-4} \delta_{t-1}^{2(\kappa-j)}}  \right ),
\end{equation}

Substituting Eqs~\eqref{eqn:lem1eq1term1}, \eqref{eqn:lem1eq1term2}, and \eqref{eqn:concentration} into the RHS of Eq~\eqref{eqn:lem1eq1}, we have
\begin{align*}
	&\probuv \left( G \cap \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{k} \right)\\
	\leq &
	K_1 \probuvtildet ( G \cap \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{k} )
	+\sum_{j=0}^{\zeta} \sum_{m=1}^{k-1} 
	\probuv \left( \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{m} \cap (\EvSlmNoisemzettemp{j})^c \right )\\
	&+2 \probuv \left( \EvSlmInBallFrmOneTo{k} \cap \EvSmSplFrmOneTo{k} \right ) 
	\sum_{j=0}^{\zeta}
	\exp \left (  -\frac {(\xi_k^{(j)})^2}{2 \sigma^2  h_{k} \tilde C_j^2 \alpha^{2t-4} \delta_{t-1}^{2(\kappa-j)}}  \right ).
	\numberthis \label{eqn:lem1eq2}
\end{align*}

Recall that the events $\EvSlmInBallk$ are mutually disjoint for all possible values of $l_{k} \in 1:2^n$, and their union is the whole probability space. Using this, we can sum up both sides of Eq~\eqref{eqn:lem1eq2} over all possible values of $l_{k}$ to eliminate $\EvSlmInBallk$ and obtain
\begin{align*}
	&\probuv \left( G \cap \EvSlmInBallFrmOneTo{k-1} \cap \EvSmSplFrmOneTo{k} \right)\\
	\leq &
	K_1 \probuvtildet ( G \cap \EvSlmInBallFrmOneTo{k-1} \cap \EvSmSplFrmOneTo{k} )
	+\sum_{j=0}^{\zeta} \sum_{m=1}^{k-1} 
	\probuv \left( \EvSlmInBallFrmOneTo{k-1} \cap \EvSmSplFrmOneTo{m} \cap (\EvSlmNoisemzettemp{j})^c \right )\\
	&+2 \probuv \left( \EvSlmInBallFrmOneTo{k-1} \cap \EvSmSplFrmOneTo{k} \right ) 
	\sum_{j=0}^{\zeta}
	\exp \left (  -\frac {(\xi_k^{(j)})^2}{2 \sigma^2  h_{k} \tilde C_j^2 \alpha^{2t-4} \delta_{t-1}^{2(\kappa-j)}}  \right ).
	\numberthis \label{eqn:lem1eq3}
\end{align*}
In the second term of RHS in Eq~\eqref{eqn:lem1eq3}, we can apply a similar concentration inequality to get
\begin{align*}
	&\sum_{j=0}^{\zeta} \probuv \left( \EvSlmInBallFrmOneTo{k-1} \cap \EvSmSplFrmOneTo{k-1} \cap (\EvSlmNoisezettemp{k-1}{j})^c \right )\\
	\leq & 2\probuv \left( \EvSlmInBallFrmOneTo{k-1} \cap \EvSmSplFrmOneTo{k-1} \right ) 
	\sum_{j=0}^{\zeta} \exp \left (  -\frac {(\xi_{k-1}^{(j)})^2}{2 \sigma^2  h_{k-1} \tilde C_j^2 \alpha^{2t-4} \delta_{t-1}^{2(\kappa-j)}}  \right ),
\end{align*}
and substituting this bound to Eq~\eqref{eqn:lem1eq3} and summing over all possible $\EvSlmInBalltemp{k-1}$ gives
\begin{align*}
	&\probuv \left( G \cap \EvSlmInBallFrmOneTo{k-2} \cap \EvSmSplFrmOneTo{k} \right)\\
	\leq &
	K_1 \probuvtildet ( G \cap \EvSlmInBallFrmOneTo{k-2} \cap \EvSmSplFrmOneTo{k} )
	+ \sum_{j=0}^{\zeta} \sum_{m=1}^{k-2} 
	\probuv \left( \EvSlmInBallFrmOneTo{k-2} \cap \EvSmSplFrmOneTo{m} \cap (\EvSlmNoisemzettemp{j})^c \right )\\
	& +2 \sum_{m=k-1}^{k} \probuv \left( \EvSlmInBallFrmOneTo{k-2} \cap \EvSmSplFrmOneTo{m} \right )
	\sum_{j=0}^{\zeta}
	\exp \left (  -\frac {(\xi_m^{(j)})^2}{2 \sigma^2  h_{m} \tilde C_j^2 \alpha^{2t-4} \delta_{t-1}^{2(\kappa-j)}}  \right ).
\end{align*}
After repeating this process until we eliminate $\EvSlmInBalltemp{1}$, we get
\begin{align*}
	&\probuv \left( G \cap \EvSmSplFrmOneTo{k} \right)\\
	\leq &
	K_1 \probuvtildet ( G \cap \EvSmSplFrmOneTo{k} )
	+2 \sum_{m=1}^{k} \probuv \left( \EvSmSplFrmOneTo{m} \right )
	\sum_{j=0}^{\zeta}
	\exp \left (  -\frac {(\xi_m^{(j)})^2}{2 \sigma^2  h_{m} \tilde C_j^2 \alpha^{2t-4} \delta_{t-1}^{2(\kappa-j)}}  \right )\\
	\leq &
	K_1 \probuvtildet ( G \cap \EvSmSplFrmOneTo{k} )
	+2 \sum_{m=1}^{k} \sum_{j=0}^{\zeta}
	\exp \left (  -\frac {(\xi_m^{(j)})^2}{2 \sigma^2  h_{m} \tilde C_j^2 \alpha^{2t-4} \delta_{t-1}^{2(\kappa-j)}}  \right )
	\numberthis \label{eqn:lem1eq4}
\end{align*}
Now, we define
\begin{equation*}
	K_2 \defeq 2 \sum_{m=1}^{k} \sum_{j=0}^{\zeta}
	\exp \left (  -\frac {(\xi_m^{(j)})^2}{2 \sigma^2  h_{m} \tilde C_j^2 \alpha^{2t-4} \delta_{t-1}^{2(\kappa-j)}}  \right ).
\end{equation*}
Then we get the claim of the lemma.


\section{Technical Proofs for Section~\ref{sec:constsscvx}}
\label{sec:defer-proof-const}
%This section provides proofs for lemmas that were presented in Section~\ref{sec:funcConst}.
%Sections~\ref{sec:proof-ftnconstrLipcvx}--\ref{sec:proof-ftnconstrLipcvxsublem} correspond to the Lipschitz case (Section~\ref{sec:constLipcvx}), while the rest of the sections correspond to the smooth strongly convex case (Section~\ref{sec:constsscvx}).
%\subsection{Proof of Lemma~\ref{lem:ssftn1d}}
%\label{sec:proof-ssftn1d}
%We will first prove a smaller sublemma and then use it to prove Lemma~\ref{lem:ssftn1d}.
%\begin{lemma}
%	\label{sublem:ssftn1d}
%	\begin{enumerate}
%		\item \label{itemsub:ssftn1d-1} Fix any $2 \leq \rho_1 < \rho_2 \leq 3$. For any $x \leq \rho_1$ and $y \geq \rho_2$, $x, y \in [0, \infty)$,
%		\begin{equation*}
%			f_-^{\rho_1} (x) > f_-^{\rho_2} (x) \text{ and } f_-^{\rho_1} (y) < f_-^{\rho_2} (y).
%		\end{equation*}
%		\item \label{itemsub:ssftn1d-2} Fix any $3 \leq \rho_1 < \rho_2 \leq 4$. For any $x \leq \rho_1$ and $y \geq \rho_2$, $x, y \in [0, \infty)$,
%		\begin{equation*}
%			f_+^{\rho_1} (x) > f_+^{\rho_2} (x) \text{ and } f_+^{\rho_1} (y) < f_+^{\rho_2} (y).
%		\end{equation*}
%	\end{enumerate}
%\end{lemma}
%\begin{proof}
%	Recall that 
%	\begin{equation*}
%		f_-^{\rho}(x) \defeq \dot h_-(\rho)(x-\rho)+h_-(\rho) 
%		= (6\rho-10)(x-\rho)+3\rho^2-10\rho+23 
%		= 6 \rho x - 10 x - 3\rho^2 + 23.
%	\end{equation*}
%	Thus, 
%	\begin{align*}
%		&f_-^{\rho_1} (x) > f_-^{\rho_2} (x) 
%		\iff 6 \rho_1 x -  3\rho_1^2 > 6 \rho_2 x -  3\rho_2^2
%		\iff \rho_1+\rho_2 > 2x,\\
%		&f_-^{\rho_1} (y) < f_-^{\rho_2} (y) 
%		\iff 6 \rho_1 y -  3\rho_1^2 < 6 \rho_2 y -  3\rho_2^2
%		\iff \rho_1+\rho_2 < 2y.
%	\end{align*}
%	This proves Part~\ref{itemsub:ssftn1d-1} of the lemma. Part~\ref{itemsub:ssftn1d-2} can be proved in a very similar way.
%\end{proof}
%
%Given this helper lemma, we prove Lemma~\ref{lem:ssftn1d} by partitioning the domain $[0, \infty)$ into 7 intervals and prove each case separately.
%Specifically, for each case we show that the supremum is achieved by exactly one or two functions.
%
%\paragraph{Case 1: $x \in [0, 2)$.}
%In this region, $f_2(x)$ is strictly bigger than any other functions, 
%so $f(x) = f_2(x)$ and $\dot f(x) = \dot f_2(x)$.
%\begin{enumerate}
%	\item $f_2(x) > f_-^\rho(x)$ for $\rho \in [2,3]$. 
%	This is because $f_2(x) = x^2-2x+15 > f_-^2(x) = 2x+11$ for $x \in [0,2)$,
%	and $f_-^2(x) > f_-^\rho(x)$ for $\rho \in (2,3]$ by Lemma~\ref{sublem:ssftn1d}.\ref{itemsub:ssftn1d-1}.
%	\item $f_2(x) > f_+^\rho(x)$ for $\rho \in [3,4]$.
%	This is because $f_2(x) = x^2-2x+15 > f_+^3(x) = 8x-4$ for $x \in [0,2)$,
%	and $f_+^3(x) > f_+^\rho(x)$ for $\rho \in (3,4]$ by Lemma~\ref{sublem:ssftn1d}.\ref{itemsub:ssftn1d-2}.
%	\item $f_2(x) > f_1(x)$. This follows from the definitions of $f_1$ and $f_2$.
%\end{enumerate}
%
%\paragraph{Case 2: $x = 2$.}
%At $x=2$, we prove that $f_2(x)$ and $f_-^2(x)$ are the ones achieving the supremum,
%but they have the same function value 15 and gradient 2, so the gradient still exists.
%\begin{enumerate}
%	\item $f_2(2) = f_-^2(2) > f_-^\rho(2)$ for $\rho \in (2,3]$. 
%	This is because of Lemma~\ref{sublem:ssftn1d}.\ref{itemsub:ssftn1d-1}.
%	\item $f_2(2) = f_-^2(2) > f_+^\rho(2)$ for $\rho \in [3,4]$.
%	This is because $f_2(2) = 15 > f_+^3(2) = 12$,
%	and $f_+^3(2) > f_+^\rho(2)$ for $\rho \in (3,4]$ by Lemma~\ref{sublem:ssftn1d}.\ref{itemsub:ssftn1d-2}.
%	\item $f_2(2) = f_-^2(2) = 15 > f_1(2) = 8$.
%\end{enumerate}
%
%\paragraph{Case 3: $x \in (2,3)$.}
%In this region, for any $x$, the affine function $f_-^\rho$ with $\rho = x$ dominates other functions.
%So, the function value at this point is $h_-(x)$, and the gradient is $\dot h_-(x)$.
%Note by definition of $f_-^\rho(x) \defeq \dot h_-(\rho)(x-\rho)+h_-(\rho)$ that $f_-^x(x) = h_-(x) = 3x^2-10x+23$.
%\begin{enumerate}
%	\item $f_-^x(x) = 3x^2-10x+23 > f_2(x) = (x-1)^2+14$ for $x \in (2,3)$.
%	\item $f_-^x(x) > f_-^\rho(x)$ for $\rho \in [2,x)$.
%	To see why, try substituting $(\rho, x, x)$ in place of $(\rho_1, \rho_2, y)$ of Lemma~\ref{sublem:ssftn1d}.\ref{itemsub:ssftn1d-1}.
%	\item $f_-^x(x) > f_-^\rho(x)$ for $\rho \in (x,3]$.
%	This is also by Lemma~\ref{sublem:ssftn1d}.\ref{itemsub:ssftn1d-1}. This time substitute $(x, x, \rho)$ in place of $(x, \rho_1, \rho_2)$.
%	\item $f_-^x(x) > f_+^\rho(x)$ for $\rho \in [3,4]$.
%	This is because $f_-^x(x) = 3x^2-10x+23 > f_+^3(x) = 8x-4$ for $x \in (2,3)$,
%	and $f_+^3(x) > f_+^\rho(x)$ for $\rho \in (3,4]$ by Lemma~\ref{sublem:ssftn1d}.\ref{itemsub:ssftn1d-2}.
%	\item $f_-^x(x) = 3x^2-10x+23 > f_1(x) = 2x^2$ for $x \in (2,3)$.
%\end{enumerate}
%
%\paragraph{Case 4: $x = 3$.}
%At $x = 3$, we prove that $f_-^3(x)$ and $f_+^3(x)$ are the ones achieving the supremum.
%However, they have the same function value 20 and gradient 8.
%\begin{enumerate}
%	\item $f_-^3(3) = 20 > f_2(3) = 18$.
%	\item $f_-^3(3) > f_-^\rho(3)$ for $\rho \in [2,3)$.
%	This is because of Lemma~\ref{sublem:ssftn1d}.\ref{itemsub:ssftn1d-1}.
%	\item $f_-^3(3) = f_+^3(3) > f_+^\rho(3)$ for $\rho \in (3,4]$.
%	This is by Lemma~\ref{sublem:ssftn1d}.\ref{itemsub:ssftn1d-2}.
%	\item $f_-^3(3) = 20 > f_1(3) = 12$.
%\end{enumerate}
%
%\paragraph{Case 5: $x \in (3,4)$.}
%In this region, for any $x$, the affine function $f_+^\rho$ with $\rho = x$ dominates other functions.
%So, the function value at this point is $h_+(x)$, and the gradient $\dot h_+(x)$.
%Note that $f_+^x(x) = h_+(x) = 4x^2-16x+32$.
%\begin{enumerate}
%	\item $f_+^x(x) = 4x^2-16x+32 > f_2(x) = (x-1)^2+14$ for $x \in (3,4)$.
%	\item $f_+^x(x) > f_-^\rho(x)$ for $\rho \in [2,3]$.
%	This is because $f_+^x(x) = 4x^2-16x+32 > f_-^3(x) = 8x-4$ for $x \in (3,4)$,
%	and $f_-^3(x) > f_-^\rho(x)$ for $\rho \in [2,3)$ by Lemma~\ref{sublem:ssftn1d}.\ref{itemsub:ssftn1d-1}.
%	\item $f_+^x(x) > f_+^\rho(x)$ for $\rho \in [3,x)$.
%	To see why, substitute $(\rho, x, x)$ in place of $(\rho_1, \rho_2, y)$ of Lemma~\ref{sublem:ssftn1d}.\ref{itemsub:ssftn1d-2}.
%	\item $f_+^x(x) > f_+^\rho(x)$ for $\rho \in (x,4]$.
%	This is also by Lemma~\ref{sublem:ssftn1d}.\ref{itemsub:ssftn1d-2}. This time substitute $(x, x, \rho)$ in place of $(x, \rho_1, \rho_2)$.
%	\item $f_+^x(x) = 4x^2-16x+32 > f_1(x) = 2x^2$ for $x \in (3,4)$.
%\end{enumerate}
%
%\paragraph{Case 6: $x = 4$.}
%At $x=4$, we prove that $f_+^4(x)$ and $f_1(x)$ are the ones achieving the supremum,
%but they have the same function value 32 and gradient 16, so the gradient still exists.
%\begin{enumerate}
%	\item $f_1(4) = f_+^4(4) = 32 > f_2(4) = 23$.
%	\item $f_1(4) = f_+^4(4) > f_-^\rho(4)$ for $\rho \in [2,3]$.
%	This is because $f_1(4) = 32 > f_-^3(4) = 28$,
%	and $f_-^3(4) > f_-^\rho(4)$ for $\rho \in [2,3)$ by Lemma~\ref{sublem:ssftn1d}.\ref{itemsub:ssftn1d-1}.
%	\item $f_1(4) = f_+^4(4) > f_+^\rho(4)$ for $\rho \in [3,\rho)$. 
%	This is because of Lemma~\ref{sublem:ssftn1d}.\ref{itemsub:ssftn1d-2}.
%\end{enumerate}
%
%\paragraph{Case 7: $x \in (4, \infty)$.}
%In this region, $f_1(x)$ is strictly bigger than any other functions, 
%so $f(x) = f_1(x)$ and $\dot f(x) = \dot f_1(x)$.
%\begin{enumerate}
%	\item $f_1(x) > f_2(x)$. This follows from the definitions of $f_1$ and $f_2$.
%	\item $f_1(x) > f_-^\rho(x)$ for $\rho \in [2,3]$. 
%	This is because $f_1(x) = 2x^2 > f_-^3(x) = 8x-4$ for $x \in (4,\infty)$,
%	and $f_-^3(x) > f_-^\rho(x)$ for $\rho \in [2,3)$ by Lemma~\ref{sublem:ssftn1d}.\ref{itemsub:ssftn1d-1}.
%	\item $f_1(x) > f_+^\rho(x)$ for $\rho \in [3,4]$.
%	This is by $f_1(x) = 2x^2 > f_+^4(x) = 16x-32$ for $x \in (4,\infty)$,
%	and $f_+^4(x) > f_+^\rho(x)$ for $\rho \in [3,4)$ by Lemma~\ref{sublem:ssftn1d}.\ref{itemsub:ssftn1d-2}.
%\end{enumerate}
%
%After this case analysis, we can see that,
%\begin{equation*}
%	f(x) = 
%	\begin{cases}
%		(x-1)^2 + 14 & \text{if } 0 \leq x \leq 2\\
%		3x^2-10x+23 & \text{if } 2 \leq x \leq 3\\
%		4x^2-16x+32 & \text{if } 3 \leq x \leq 4\\
%		2x^2 & \text{if } x \geq 4,
%	\end{cases}
%	\text{ and }
%	\dot f(x) = 
%	\begin{cases}
%		2x - 2 & \text{if } 0 \leq x \leq 2\\
%		6x-10  & \text{if } 2 \leq x \leq 3\\
%		8x-16 & \text{if } 3 \leq x \leq 4\\
%		4x & \text{if } x \geq 4.
%	\end{cases}
%\end{equation*}
%It is straightforward to observe that 
%this resulting function is $8$-smooth and $2$-strongly convex.
%

\subsection{Proof of Lemma~\ref{lem:ssftnmdparam}}
\label{sec:proof-ssftnmdparam}
Originally we had requirements $0<\eta<1$, $\delta > 0$, $0<\alpha<1$, $0<\beta<1$, and $0<\theta<1$.
Let the parameter values satisfy
\begin{equation*}
	\eta+\alpha+\alpha\eta < 1 ,~~
	\beta = \frac{(1-\alpha)(1+\eta)^2}{4} - \frac{\alpha \eta^2}{1-\alpha},~~
	\theta = \frac{1-\eta-\alpha-\alpha\eta}{1+\eta-\alpha-\alpha\eta}.
\end{equation*}
First, we check that all the parameters are in the desired range. We start by noting that the inequalities $0<\alpha<1, 0<\eta<1, \eta+\alpha+\alpha\eta < 1$ are feasible.
With these assumptions on $\alpha$ and $\eta$, it is easy to check $0< \theta<1$.
Also, 
\begin{equation*}
	\eta+\alpha+\alpha\eta < 1 
	\iff
	\eta < \frac{(1-\alpha)(1+\eta)}{2} 
	\iff 
	\eta^2 < \frac{(1-\alpha)(1+\eta)^2}{4} - \frac{\alpha \eta^2}{1-\alpha} = \beta,
\end{equation*}
so we can ensure $\beta > 0$.
Also,
\begin{equation*}
	\beta = \frac{(1-\alpha)(1+\eta)^2}{4} - \frac{\alpha \eta^2}{1-\alpha} < \frac{(1-\alpha)(1+\eta)^2}{4} < 1.
\end{equation*}

Now consider the interpolation set,
\begin{equation*}
	\itpset \defeq \left \{ x \mid (1-\theta)r \leq \ltwo{x-c} \leq (1+\theta)r \right \},
\end{equation*}
where $r$ and $c$ are defined in Eqs~\eqref{eq:defcent} and \eqref{eq:defradi},
which we repeat below, and further evaluate with the assumption \eqref{eqn:smthconstbetaval} on $\beta$:
\begin{align*}
	c &= \frac{1}{1-\alpha} x_1 - \frac{\alpha}{1-\alpha} x_2 = x_1 - \frac{\alpha}{1-\alpha} (x_2-x_1),\\
	r &= \sqrt{\frac{\alpha}{(1-\alpha)^2} \ltwosqr{x_1-x_2} + \frac{\beta\delta^2}{1-\alpha}}
	=\sqrt{\frac{(1+\eta)^2 \delta^2}{4}-\frac{\alpha}{(1-\alpha)^2} (\eta^2 \delta^2-\ltwosqr{x_1-x_2})}.
\end{align*}

We now show that, with the choice of $\theta$ in Eq~\eqref{eqn:smthconstthetaval},
\begin{equation*}
	\itpset \subset \cl(\balltwo{x_1}{\eta\delta}^c \cap \balltwo{x_1}{\delta}) ~\text{ for any } x_2 \in \balltwo{x_1}{\eta\delta},
\end{equation*}
which is the goal of this lemma, Eq~\eqref{eqn:itpsubset}.
Consider $x_2 = x_1 + \rho\delta \unitvec$, where $0 \leq \rho \leq \eta$.
This choice of $x_2$ is a representative of all other $x_2$ that are $\rho\delta$ away from $x_1$.
The center point $c$ and radius $r$ of intersection set, 
and the interpolation set $\itpset$ can be represented as functions of $\rho$:
\begin{align*}
	c(\rho) &= x_1 - \frac{\alpha\rho\delta }{1-\alpha} \unitvec, \\
	r(\rho) &= \sqrt{\frac{(1+\eta)^2\delta^2}{4}-\frac{\alpha}{(1-\alpha)^2}(\eta^2-\rho^2)\delta^2},\\
	\itpset(\rho) &= \left \{ x \mid (1-\theta)r(\rho) \leq \ltwo{x-c(\rho)} \leq (1+\theta)r(\rho) \right \}.
\end{align*}
Define $\Lmax(\rho)$ and $\Lmin(\rho)$ that are farthest and closest 
distance of points in $\itpset(\rho)$ to $x_1$, defined as follows:
\begin{equation*}
	\Lmax(\rho) 
	\defeq \sup_{x \in \itpset(\rho)} \ltwo{x-x_1},~~
	\Lmin(\rho) 
	\defeq \inf_{x \in \itpset(\rho)} \ltwo{x-x_1}.
\end{equation*}
The desired condition $\itpset \in \balltwo{x_1}{\eta\delta}^c \cap \balltwo{x_1}{\delta}$
can now be written as $\Lmax(\rho) \leq \delta$ and $\Lmin(\rho) \geq \eta\delta$
for all $\rho \in [0, \eta]$.

Since $x_1$ and $c$ are away by $\frac{\alpha \rho \delta}{1-\alpha}$, the farthest distance $\Lmax(\rho)$ can be calculated as
\begin{equation*}
	\Lmax(\rho) 
	= (1+\theta)r(\rho) + \frac{\alpha \rho \delta}{1-\alpha}
	= (1+\theta)\sqrt{\frac{(1+\eta)^2\delta^2}{4}-\frac{\alpha}{(1-\alpha)^2}(\eta^2-\rho^2)\delta^2} 
	+ \frac{\alpha \rho \delta}{1-\alpha} 
\end{equation*}
and it is clearly an increasing function of $\rho$, so we only need to check that $\Lmax(\eta) \leq \delta$.
\begin{align*}
	\Lmax(\eta) 
	&= (1+\theta)\sqrt{\frac{(1+\eta)^2\delta^2}{4}} + \frac{\alpha \eta \delta}{1-\alpha} 
	= \left( \frac{2(1-\alpha-\alpha\eta)}{1+\eta-\alpha-\alpha\eta} \right)\frac{(1+\eta)\delta}{2} + \frac{\alpha \eta \delta}{1-\alpha}\\
	&= \frac{(1-\alpha-\alpha\eta)\delta}{1-\alpha}+\frac{\alpha \eta \delta}{1-\alpha} = \delta.
\end{align*}
For $\Lmin(\rho)$, it requires a bit more thought. As long as
$(1-\theta) r(\rho) > \frac{\alpha \rho \delta}{1-\alpha}$, we have $x_1 \notin \itpset(\rho)$ and
\begin{equation*}
	\Lmin(\rho)
	= (1-\theta) r(\rho)-\frac{\alpha \rho \delta}{1-\alpha}.
\end{equation*}
Thus, by showing
\begin{equation}
\label{eq:lminineq}
(1-\theta) r(\rho)-\frac{\alpha \rho \delta}{1-\alpha}\geq \eta\delta > 0, 
\end{equation}
we can prove $(1-\theta) r(\rho) > \frac{\alpha \eta \delta}{1-\alpha}$,
and $\Lmin(\rho)\geq \eta\delta$.
To do this, we show that the LHS of Eq~\eqref{eq:lminineq} is a decreasing function of $\rho$,
and then show Eq~\eqref{eq:lminineq} for $\rho = \eta$.
We can easily see that $-\theta r(\rho)$ is a decreasing function of $\rho$.
For the rest of the LHS,
\begin{align*}
	&\frac{d}{d\rho} \left (r(\rho) - \frac{\alpha \rho \delta}{1-\alpha} \right )
	= \frac{\frac{2\alpha\delta^2\rho}{(1-\alpha)^2}}
	{2\sqrt{\frac{(1+\eta)^2\delta^2}{4}-\frac{\alpha}{(1-\alpha)^2}(\eta^2-\rho^2)\delta^2}}
	- \frac{\alpha\delta}{1-\alpha} \leq 0, 
	\forall \rho \in [0, \eta]\\
	\iff&
	\rho^2 \leq \frac{(1+\eta)^2(1-\alpha)^2}{4} - \alpha (\eta^2 - \rho^2), \forall \rho \in [0, \eta]\\
	\iff&
	(1-\alpha) \rho^2 \leq \frac{(1+\eta)^2(1-\alpha)^2}{4} - \alpha \eta^2, \forall \rho \in [0, \eta]\\
	\iff&
	(1-\alpha) \eta^2 \leq \frac{(1+\eta)^2(1-\alpha)^2}{4} - \alpha \eta^2\\
	\iff&
	2\eta \leq (1+\eta)(1-\alpha) = 1+\eta-\alpha-\alpha\eta\\
	\iff&
	\eta+\alpha+\alpha\eta \leq 1.
\end{align*}
Since we know from Eq~\eqref{eq:constraints} that the last statement is true,
we proved that the LHS of Eq~\eqref{eq:lminineq} is decreasing.
Finally, we examine Eq~\eqref{eq:lminineq} for $\rho = \eta$.
\begin{align*}
	(1-\theta)r(\eta) - \frac{\alpha \eta \delta}{1-\alpha} 
	= \frac{2\eta}{(1+\eta)(1-\alpha)} \frac{(1+\eta) \delta}{2} - \frac{\alpha \eta \delta}{1-\alpha}
	= \eta\delta.
\end{align*}
Thus far, we showed that with our choice of $\theta$ in Eq~\eqref{eqn:smthconstthetaval}, 
the interpolation set $\itpset$ satisfies
$\itpset \in \balltwo{x_1}{\eta\delta}^c \cap \balltwo{x_1}{\delta}$ for any $x_2 \in \balltwo{x_1}{\eta\delta}$.

\subsection{Development of Algorithm~\ref{alg:algmaxsmth}}
\label{sec:proof-algmaxsmth}
We define
\begin{align*}
	\dot g_-(w) &\defeq \nabla \fxtwo (c+(1-\theta)rw) = \frac{2\alpha}{1-\alpha}(x_1-x_2) + 2\alpha(1-\theta)rw,\\
	\dot g_0(w) &\defeq \frac{\nabla \fxtwo (c+rw)+\nabla \fxone (c+rw)}{2} = \frac{2\alpha}{1-\alpha}(x_1-x_2) + (1+\alpha)rw,\\
	\dot g_+(w) &\defeq \nabla \fxone (c+(1+\theta)rw) = \frac{2\alpha}{1-\alpha}(x_1-x_2) + 2(1+\theta)rw
\end{align*}
and then interpolate the gradients along each direction $w$:
\begin{align*}
	\dot h_-(\rho,w) 
	&\defeq \dot g_-(w) + \frac{\dot g_0(w) - \dot g_-(w)}{\theta r} (\rho-(1-\theta) r) \\
	&= 	\frac{2\alpha}{1-\alpha}(x_1-x_2) -
	\frac{(1-\alpha)(1-\theta)r}{\theta} w +
	\left ( \frac{1-\alpha}{\theta}+2\alpha \right ) \rho w
	& \text{ for } \rho \in [(1-\theta)r,r], 
	\numberthis \label{eqn:smthconstgradm}\\
	\dot h_+(\rho,w) 
	&\defeq \dot g_+(w) - \frac{\dot g_0(w) - \dot g_+(w)}{\theta r} (\rho-(1+\theta) r)\\
	&= 	\frac{2\alpha}{1-\alpha}(x_1-x_2) -
	\frac{(1-\alpha)(1+\theta)r}{\theta} w +
	\left ( \frac{1-\alpha}{\theta}+2 \right ) \rho w
	& \text{ for } \rho \in [r,(1+\theta)r]
	\numberthis \label{eqn:smthconstgradp}.
\end{align*}
We can see that $\dot h_-(r, w) = \dot h_+(r, w) = \dot g_0 (w)$.

Interpolation of gradients also changes the function values on $\itpset$. The original function values at $x = c + (1-\theta) r w$ and $x = c + (1+\theta) r w$, which are the points we start the interpolation from, are
\begin{align*}
	\fxtwo(c+(1-\theta)rw)
	&= 	\frac{\alpha}{(1-\alpha)^2} \ltwosqr{x_1-x_2} + 
	\frac{2\alpha(1-\theta)r}{1-\alpha} \< x_1-x_2, w \> +
	\alpha (1-\theta)^2r^2+
	\beta \delta^2,\\
	\fxone(c+(1+\theta)rw)
	&= 	\frac{\alpha^2}{(1-\alpha)^2} \ltwosqr{x_1-x_2} + 
	\frac{2\alpha(1+\theta)r}{1-\alpha} \< x_1-x_2, w \> +
	(1+\theta)^2r^2.
\end{align*}
The function values after interpolation is calculated by integrating the directional derivatives,
\begin{align*}
	\numberthis \label{eq:hminusdef}
	h_-(\rho,w)& \defeq 	\fxtwo(c+(1-\theta)rw) + 
	\int_{(1-\theta)r}^\rho 
	\<
	\dot h_-(t,w), w
	\>
	dt
	& \text{ for } \rho \in [(1-\theta)r,r],\\
	h_+(\rho,w)& \defeq 	\fxone(c+(1+\theta)rw) -
	\int_\rho^{(1+\theta)r}
	\<
	\dot h_+(t,w), w
	\>
	dt
	& \text{ for } \rho \in [r,(1+\theta)r],
\end{align*}
and the integrals are evaluated as
\begin{align*}
	\int_{(1-\theta)r}^\rho 
	\<
	\dot h_-(t,w), w
	\>
	dt
	= 	&\left ( 
	\frac{2\alpha}{1-\alpha} \< x_1-x_2, w \> -
	\frac{(1-\alpha)(1-\theta)r}{\theta}
	\right )
	(\rho - (1-\theta) r) \\
	&+ \frac{1}{2} 
	\left ( \frac{1-\alpha}{\theta} + 2\alpha \right )
	(\rho^2 - (1-\theta)^2 r^2),\\
	\int_\rho^{(1+\theta)r}
	\<
	\dot h_+(t,w), w
	\>
	dt
	=	&\left ( 
	\frac{2\alpha}{1-\alpha} \< x_1-x_2, w \> -
	\frac{(1-\alpha)(1+\theta)r}{\theta}
	\right )
	((1+\theta)r - \rho)\\
	&+\frac{1}{2}
	\left ( \frac{1-\alpha}{\theta} + 2 \right )
	((1+\theta)^2 r^2 - \rho^2).
\end{align*}
Substituting the integrals and arranging the terms, we get the following:
\begin{align*}
	h_-(\rho,w) 
	\defeq &\frac{\alpha}{(1-\alpha)^2} \ltwosqr{x_1-x_2}
	+ \beta \delta^2 + \frac{(1-\alpha)(1-\theta)^2 r^2}{2\theta}\\
	&+ \left ( \frac{2\alpha}{1-\alpha} \< x_1-x_2, w \> - \frac{(1-\alpha)(1-\theta)r}{\theta} \right ) \rho
	+ \left ( \frac{1-\alpha}{2\theta} + \alpha \right ) \rho^2
	& \text{ for } \rho \in [(1-\theta)r,r],\\
	h_+(\rho,w) 
	\defeq &\frac{\alpha^2}{(1-\alpha)^2} \ltwosqr{x_1-x_2}
	+ \frac{(1-\alpha)(1+\theta)^2 r^2}{2\theta}\\
	&+ \left ( \frac{2\alpha}{1-\alpha} \< x_1-x_2, w \> - \frac{(1-\alpha)(1+\theta)r}{\theta} \right ) \rho
	+ \left ( \frac{1-\alpha}{2\theta} + 1 \right ) \rho^2
	& \text{ for } \rho \in [r,(1+\theta)r].
\end{align*}
We can double-check that $h_-(r, w) = h_+(r,w)$ by substituting $\rho = r$ to both functions,
arranging terms, and noting that
\begin{equation}
\frac{\alpha}{(1-\alpha)^2} \ltwosqr{x_1-x_2} + \alpha r^2 + \beta \delta^2 = 
\frac{\alpha^2}{(1-\alpha)^2} \ltwosqr{x_1-x_2} + r^2,
\label{eqn:smthconstidentity}
\end{equation}
by definition of $\beta$ in Eq~\eqref{eqn:smthconstbetaval}.

Using $\dot h_-$, $\dot h_+$, $h_-$, and $h_+$ defined as above, we can define 
infinite number of hyperplanes corresponding to each point $c+\rho w$ in $\itpset$,
\begin{align*}
	\fhypm (x) &\defeq \< \dot h_-(\rho,w), x-(c+\rho w) \> +h_-(\rho,w) 
	& \text{ for } \rho \in [(1-\theta)r,r], \ltwo{w} = 1,\\
	\fhypp (x) &\defeq \< \dot h_+(\rho,w), x-(c+\rho w) \> +h_+(\rho,w) 
	& \text{ for } \rho \in [r,(1+\theta)r], \ltwo{w} = 1.
\end{align*}
After substituting $\dot h_-(\rho,w)$, $\dot h_+(\rho,w)$, $h_-(\rho,w)$, and $h_+(\rho,w)$, 
and then arranging terms, we get
\begin{align*}
	\fhypm (x) = & \frac{\alpha}{(1-\alpha)^2} \ltwosqr{x_1-x_2}
	+ \beta \delta^2 + \frac{(1-\alpha)(1-\theta)^2 r^2}{2\theta} 
	- \left ( \frac{1-\alpha}{2\theta} + \alpha \right ) \rho^2\\
	& + \left \< \frac{2\alpha}{1-\alpha} (x_1- x_2)
	-\frac{(1-\alpha)(1-\theta)r}{\theta} w
	+\left ( \frac{1-\alpha}{\theta}+2\alpha \right ) \rho w, x-c
	\right \> \numberthis \label{eq:hypminus}\\
	\fhypp (x) = & \frac{\alpha^2}{(1-\alpha)^2} \ltwosqr{x_1-x_2}
	+ \frac{(1-\alpha)(1+\theta)^2 r^2}{2\theta} 
	- \left ( \frac{1-\alpha}{2\theta} + 1 \right ) \rho^2\\
	& + \left \< \frac{2\alpha}{1-\alpha} (x_1- x_2)
	-\frac{(1-\alpha)(1+\theta)r}{\theta} w
	+\left ( \frac{1-\alpha}{\theta}+2 \right ) \rho w, x-c
	\right \>.
\end{align*}
We can finally state the definition of the interpolated function, which is
\begin{equation}
\label{eqn:smthconstsupdef}
f(x) = \max \left \{ 
\fxone(x), \fxtwo(x), 
\sup_{\rho \in [(1-t)r,r], \ltwo w = 1} \fhypm(x),
\sup_{\rho \in [r,(1+t)r], \ltwo w = 1} \fhypp(x)
\right \}.
\end{equation}

\subsection{Proof of Lemma~\ref{lem:smthconstvalues}}
\label{sec:proof-smthconstvalues}
Before the proof of Lemma~\ref{lem:smthconstvalues}, we state and prove a ``helper'' lemma.
\begin{lemma}
	\label{sublem:smthconstvalues}
	The following holds:
	\begin{enumerate}
		\item \label{itemsub:smthconstvalues-1} For any $0 < \rho \leq \rho_1 \leq \rho_2$ where $\rho_1, \rho_2 \in [(1-\theta)r, r]$, 
		and any unit vectors $w, w'$, we have 
		%\begin{equation*}
			$\fhypmtemp{\rho_1}{w}(c+\rho w) \geq \fhypmtemp{\rho_2}{w'}(c+\rho w)$, 
		%\end{equation*}
		where equality holds if and only if $\rho_1 = \rho_2$ and $w = w'$.
		\item \label{itemsub:smthconstvalues-2} For any $\rho \geq \rho_1 \geq \rho_2$ where $\rho_1, \rho_2 \in [(1-\theta)r, r]$, 
		and any unit vectors $w, w'$, we have 
		%\begin{equation*}
			$\fhypmtemp{\rho_1}{w}(c+\rho w) \geq \fhypmtemp{\rho_2}{w'}(c+\rho w)$, 
		%\end{equation*}
		where equality holds if and only if $\rho_1 = \rho_2$ and $w = w'$.
		\item \label{itemsub:smthconstvalues-3} For any $0 < \rho \leq \rho_1 \leq \rho_2$ where $\rho_1, \rho_2 \in [r, (1+\theta)r]$, 
		and any unit vectors $w, w'$, we have
		%\begin{equation*}
			$\fhypptemp{\rho_1}{w}(c+\rho w) \geq \fhypptemp{\rho_2}{w'}(c+\rho w)$, 
		%\end{equation*}
		where equality holds if and only if $\rho_1 = \rho_2$ and $w = w'$.
		\item \label{itemsub:smthconstvalues-4} For any $\rho \geq \rho_1 \geq \rho_2$ where $\rho_1, \rho_2 \in [r, (1+\theta)r]$, 
		and any unit vectors $w, w'$, we have
		%\begin{equation*}
			$\fhypptemp{\rho_1}{w}(c+\rho w) \geq \fhypptemp{\rho_2}{w'}(c+\rho w)$, 
		%\end{equation*}
		where equality holds if and only if $\rho_1 = \rho_2$ and $w = w'$.
		\item \label{itemsub:smthconstvalues-5} For any $\rho \in [(1-\theta)r,r]$,
		%\begin{equation*}
			$h_-(\rho,w) \geq \fxtwo(c+\rho w)$,
		%\end{equation*}
		where equality holds if and only if $\rho = (1-\theta)r$.
		\item \label{itemsub:smthconstvalues-6} For any $\rho \in [r,(1+\theta)r]$,
		%\begin{equation*}
			$h_+(\rho,w) \geq \fxone(c+\rho w)$,
		%\end{equation*}
		where equality holds if and only if $\rho = (1+\theta)r$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	For Part~\ref{itemsub:smthconstvalues-1}, we first show that 
	$\fhypmtemp{\rho_1}{w}(c+\rho w) \geq \fhypmtemp{\rho_2}{w}(c+\rho w)$, 
	and then show $\fhypmtemp{\rho_2}{w}(c+\rho w) \geq \fhypmtemp{\rho_2}{w'}(c+\rho w)$.
	From Eq~\eqref{eq:hypminus}, we can see that
	\begin{align*}
		&\fhypmtemp{\rho_1}{w}(c+\rho w) \geq \fhypmtemp{\rho_2}{w}(c+\rho w)\\
		\iff&- \left ( \frac{1-\alpha}{2\theta} + \alpha \right ) \rho_1^2
		+\left ( \frac{1-\alpha}{\theta}+2\alpha \right ) \rho_1 \rho 
		\geq 
		- \left ( \frac{1-\alpha}{2\theta} + \alpha \right ) \rho_2^2
		+\left ( \frac{1-\alpha}{\theta}+2\alpha \right ) \rho_2 \rho\\
		\iff& (\rho_2-\rho_1)(\rho_2+\rho_1) \geq 2(\rho_2-\rho_1)\rho,
	\end{align*}
	which is true because $\rho \leq \rho_1 \leq \rho_2$. Also, we can check that
	equality holds if and only if $\rho_1 = \rho_2$.
	For the next step,
	\begin{align*}
		&\fhypmtemp{\rho_2}{w}(c+\rho w) \geq \fhypmtemp{\rho_2}{w'}(c+\rho w)\\
		\iff&\< -\frac{(1-\alpha)(1-\theta)r}{\theta} w
		+\left ( \frac{1-\alpha}{\theta}+2\alpha \right ) \rho_2 w, \rho w
		\>
		\geq
		\< -\frac{(1-\alpha)(1-\theta)r}{\theta} w'
		+\left ( \frac{1-\alpha}{\theta}+2\alpha \right ) \rho_2 w', \rho w
		\>\\
		\iff& \left ( \frac{1-\alpha}{\theta} (\rho_2 - (1-\theta)r) + 2\alpha \rho_2 \right )
		\rho (1-\<w',w\>) \geq 0,
	\end{align*}
	which is true and equality holds if and only if $w = w'$.
	Parts~\ref{itemsub:smthconstvalues-2}--\ref{itemsub:smthconstvalues-4} can be proved in a very similar way.
	
	Part~\ref{itemsub:smthconstvalues-5} holds because of the definition of $h_-(\rho,w)$ in Eq~\eqref{eq:hminusdef} and we can check that
	\begin{equation*}
		\< \dot h_-(\rho,w), w \> \geq \< \nabla \fxtwo(c+\rho w), w \> \text{ for } \rho \in [(1-\theta)r,r],
	\end{equation*}
	where equality holds if and only if $\rho = (1-\theta)r$.  Part~\ref{itemsub:smthconstvalues-6} can be proved similarly.
\end{proof}

Given the helper lemma, we prove Lemma~\ref{lem:smthconstvalues} by partitioning $\rho$ into 8 intervals and prove each case separately. 
Specifically, for each case we show that for any given $\rho$ and $w$,
the supremum $f(c+\rho w)$ is achieved by exactly one or two functions among all the functions.
If there are two functions that achieve the supremum, we show that they have the same gradients.
If this is true, the statement about $\nabla f(c+\rho w)$ will naturally follow. 

%\label{eqn:smthconstidentity}
\paragraph{Case 1: $\rho = 0$.}
When $\rho = 0$, $\fxtwo$ is strictly bigger than all other functions.
We can show this by directly comparing function values at $x = c$.
For this case, recall from Eq~\eqref{eqn:smthconstidentity} and
definition of $\fxone$ and $\fxtwo$ that
\begin{equation}
\label{eqn:smthconstidentity2}
\fxone(c) 
=
\frac{\alpha^2}{(1-\alpha)^2} \ltwosqr{x_1-x_2} 
= 
\frac{\alpha}{(1-\alpha)^2} \ltwosqr{x_1-x_2} + \beta \delta^2 - (1-\alpha) r^2
= 
\fxtwo(c) - (1-\alpha) r^2.
\end{equation}
There are three things that we need to check:
\begin{enumerate}
	\item $\fxtwo(c) > \fhypmtemp{\rho'}{w'}(c), \forall \rho' \in [(1-\theta)r,r], \ltwo{w'} = 1$\\
	$\iff 0 > \frac{(1-\alpha)(1-\theta)^2 r^2}{2\theta} 
	- \left ( \frac{1-\alpha}{2\theta} + \alpha \right ) \rho'^2, 
	\forall \rho' \in [(1-\theta)r,r]$\\
	$\iff 0 > \frac{(1-\alpha)(1-\theta)^2 r^2}{2\theta} 
	- \left ( \frac{1-\alpha}{2\theta} + \alpha \right ) (1-\theta)^2 r^2 
	= -\alpha (1-\theta)^2 r^2$.
	\item $\fxtwo(c) > \fhypptemp{\rho'}{w'}(c), \forall \rho' \in [r,(1+\theta)r], \ltwo{w'} = 1$.
	Note that 
	\begin{align*}
		\fhypptemp{\rho'}{w'}(c) &= \frac{\alpha^2}{(1-\alpha)^2} \ltwosqr{x_1-x_2} 
		+ \frac{(1-\alpha)(1+\theta)^2 r^2}{2\theta} 
		- \left ( \frac{1-\alpha}{2\theta} + 1 \right ) \rho'^2\\
		&= \fxtwo(c) - (1-\alpha) r^2 + \frac{(1-\alpha)(1+\theta)^2 r^2}{2\theta} 
		- \left ( \frac{1-\alpha}{2\theta} + 1 \right ) \rho'^2,
	\end{align*}
	so we need to show
	\begin{align*}
		&- (1-\alpha) r^2 + \frac{(1-\alpha)(1+\theta)^2 r^2}{2\theta} 
		- \left ( \frac{1-\alpha}{2\theta} + 1 \right ) \rho'^2<0, \forall \rho' \in [r,(1+\theta)r],\\
		\iff &- (1-\alpha) r^2 + \frac{(1-\alpha)(1+\theta)^2 r^2}{2\theta} 
		- \left ( \frac{1-\alpha}{2\theta} + 1 \right ) r^2<0
		\iff (1-\alpha) \theta < 2,
	\end{align*}
	which is true because $\alpha, \theta \in (0,1)$.
	\item $\fxtwo(c) > \fxone(c)$. This is already shown by Eq~\eqref{eqn:smthconstidentity2}.
\end{enumerate}

\paragraph{Case 2: $\rho \in (0, (1-\theta)r)$.}
In this region, 
\begin{equation*}
	\fxtwo(c+\rho w) 
	= \frac{\alpha}{(1-\alpha)^2} \ltwosqr{x_1-x_2} 
	+ \beta \delta^2
	+ \frac{2\alpha \rho}{1-\alpha} \<x_1-x_2, w \> 
	+ \alpha \rho^2
\end{equation*}
dominates all other functions.
\begin{enumerate}
	\item $\fxtwo(c+\rho w) > \fhypmtemp{\rho'}{w'}(c+\rho w), \forall \rho' \in [(1-\theta)r,r], \ltwo{w'} = 1$.
	To show this, it suffices to show $\fxtwo(c+\rho w) > \fhypmtemp{(1-\theta)r}{w}(c+\rho w)$,
	and the rest follows because of Lemma~\ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-1}.
	We can calculate and arrange $\fhypmtemp{(1-\theta)r}{w}(c+\rho w)$ to get
	\begin{equation*}
		\fhypmtemp{(1-\theta)r}{w}(c+\rho w) 
		= \frac{\alpha}{(1-\alpha)^2} \ltwosqr{x_1-x_2} 
		+ \beta \delta^2
		+ \frac{2\alpha \rho}{1-\alpha} \<x_1-x_2, w \> 
		- \alpha(1-\theta)^2 r^2 + 2\alpha(1-\theta)r \rho,
	\end{equation*}
	so 
	\begin{equation*}
		\fxtwo(c+\rho w) > \fhypmtemp{(1-\theta)r}{w}(c+\rho w)
		\iff \alpha (1-\theta)^2 r^2 - 2\alpha (1-\theta) \rho + \alpha \rho^2 > 0
		\iff \alpha((1-\theta)r-\rho)^2 > 0,
	\end{equation*}
	which is true.
	\item $\fxtwo(c+\rho w) > \fhypptemp{\rho'}{w'}(c+\rho w), \forall \rho' \in [r,(1+\theta)r], \ltwo{w'} = 1$.
	Notice that we just showed that 
	$\fxtwo(c+\rho w) > \fhypmtemp{r}{w}(c+\rho w) = \fhypptemp{r}{w}(c+\rho w)$.
	The rest follows by Lemma~\ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-3}.
	\item $\fxtwo(c+\rho w) > \fxone(c+\rho w)$. This can be shown by direct comparison.
\end{enumerate}

\paragraph{Case 3: $\rho = (1-\theta)r$.}
In this case, 
$\fxtwo(c+(1-\theta)r w) = \fhypmtemp{(1-\theta)r}{w}(c+(1-\theta)r w) = h_-((1-\theta)r,w)$
is strictly greater than all other functions.
The gradient still exists because the gradients of these two functions are the same:
$\nabla \fxtwo(c+(1-\theta)r w) = \nabla\fhypmtemp{(1-\theta)r}{w}(c+(1-\theta)r w) = \dot h_-((1-\theta)r, w)$,
as we can see from Eq~\eqref{eqn:smthconstgradm}. We need to show:
\begin{enumerate}
	\item $\fxtwo(c+(1-\theta)r w)  > \fhypmtemp{\rho'}{w'}(c+(1-\theta)r w), 
	\forall \rho' \in [(1-\theta)r,r], \ltwo{w'} = 1$, as long as $(\rho', w') \neq ((1-\theta)r, w)$.
	This is true because of Lemma~\ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-1}.
	\item $\fxtwo(c+(1-\theta)r w) > \fhypptemp{\rho'}{w'}(c+(1-\theta)r w), 
	\forall \rho' \in [r,(1+\theta)r], \ltwo{w'} = 1$.
	We just showed that 
	$\fxtwo(c+(1-\theta)r w) > \fhypmtemp{r}{w}(c+(1-\theta)r w) = \fhypptemp{r}{w}(c+(1-\theta)r w)$,
	so the rest follows by Lemma~\ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-3}.
	\item $\fxtwo(c+(1-\theta)r w) > \fxone(c+(1-\theta)r w)$. This can be shown by direct comparison.
\end{enumerate}

\paragraph{Case 4: $\rho \in ((1-\theta)r, r)$.} 
In this region, the function $\fhypmtemp{\rho}{w}$ dominates all other functions.
Note that $\fhypmtemp{\rho}{w}(c+\rho w) = h_-(\rho, w)$.
\begin{enumerate}
	\item $\fhypmtemp{\rho}{w}(c+\rho w) > \fxtwo(c+\rho w)$.
	This is true by Lemma~\ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-5}.
	\item $\fhypmtemp{\rho}{w}(c+\rho w) > \fhypmtemp{\rho'}{w'}(c+\rho w),
	\forall \rho' \in [(1-\theta)r,r], \ltwo{w'} = 1$, as long as $(\rho', w') \neq (\rho, w)$.
	This is true because of Lemma~\ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-1} and \ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-2}. 
	\item $\fhypmtemp{\rho}{w}(c+\rho w) > \fhypptemp{\rho'}{w'}(c+\rho w), \forall \rho' \in [r,(1+\theta)r], \ltwo{w'} = 1$.
	We just showed that 
	$\fhypmtemp{\rho}{w}(c+\rho w) > \fhypmtemp{r}{w}(c+\rho w) = \fhypptemp{r}{w}(c+\rho w)$,
	so the rest follows by Lemma~\ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-3}.
	\item $\fhypmtemp{\rho}{w}(c+\rho w) > \fxone(c+(1-\theta)r w)$. This can be shown by noting that 
	$\fxtwo(c+\rho w) > \fxone(c+\rho w)$ by direct comparison, and then Lemma~\ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-5}.
\end{enumerate}

\paragraph{Case 5: $\rho = r$.}
In this case, $\fhypmtemp{r}{w}(c+r w) = h_-(r, w) = \fhypptemp{r}{w}(c+r w) = h_+(r, w)$ is greater than all other functions, but their gradients are also the same.
\begin{enumerate}
	\item $\fhypmtemp{r}{w}(c+r w) > \fxtwo(c+r w)$.
	This is true by Lemma~\ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-5}.
	\item $\fhypmtemp{r}{w}(c+r w) > \fhypmtemp{\rho'}{w'}(c+r w),
	\forall \rho' \in [(1-\theta)r,r], \ltwo{w'} = 1$, as long as $(\rho', w') \neq (r, w)$.
	This is because of Lemma~\ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-2}. 
	\item $\fhypptemp{r}{w}(c+r w) > \fhypptemp{\rho'}{w'}(c+r w), 
	\forall \rho' \in [r,(1+\theta)r], \ltwo{w'} = 1$, as long as $(\rho', w') \neq (r, w)$,
	which is implied by Lemma~\ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-3}.
	\item $\fhypptemp{r}{w}(c+r w) > \fxone(c+r w)$, by Lemma~\ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-6}.
\end{enumerate}

\paragraph{Case 6: $\rho \in (r, (1+\theta)r)$.}
We have to show that $\fhypptemp{\rho}{w}(c+\rho w) = h_+(\rho, w)$ is greater than values from all other functions.
\begin{enumerate}
	\item $\fhypptemp{\rho}{w}(c+\rho w) > \fxone(c+\rho w)$.
	This is true by Lemma~\ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-6}.
	\item $\fhypptemp{\rho}{w}(c+\rho w) > \fhypptemp{\rho'}{w'}(c+\rho w),
	\forall \rho' \in [r,(1+\theta)r], \ltwo{w'} = 1$, as long as $(\rho', w') \neq (\rho, w)$.
	This is true because of Lemma~\ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-3} and \ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-4}. 
	\item $\fhypptemp{\rho}{w}(c+\rho w) > \fhypmtemp{\rho'}{w'}(c+\rho w), \forall \rho' \in [(1-\theta)r,r], \ltwo{w'} = 1$.
	We just showed that 
	$\fhypptemp{\rho}{w}(c+\rho w) > \fhypptemp{r}{w}(c+\rho w) = \fhypmtemp{r}{w}(c+\rho w)$,
	so the rest follows by Lemma~\ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-2}.
	\item $\fhypptemp{\rho}{w}(c+\rho w) > \fxtwo(c+(1-\theta)r w)$. This can be shown by noting that 
	$\fxone(c+\rho w) > \fxtwo(c+\rho w)$ by direct comparison, and then Lemma~\ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-6}.
\end{enumerate}

\paragraph{Case 7: $\rho = (1+\theta)r$.}
Here, $\fxone(c+(1+\theta)r w) = \fhypptemp{(1+\theta)r}{w}(c+(1+\theta)r w) = h_+((1+\theta)r, w)$ dominate,
but the gradients at the point are the same. 
\begin{enumerate}
	\item $\fxone(c+(1+\theta)r w)  > \fhypptemp{\rho'}{w'}(c+(1+\theta)r w), 
	\forall \rho' \in [r,(1+\theta)r], \ltwo{w'} = 1$, as long as $(\rho', w') \neq ((1+\theta)r, w)$.
	This is true because of Lemma~\ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-4}.
	\item $\fxone(c+(1+\theta)r w) > \fhypmtemp{\rho'}{w'}(c+(1+\theta)r w), 
	\forall \rho' \in [(1-\theta)r,r], \ltwo{w'} = 1$.
	We just showed that
	$\fxone(c+(1+\theta)r w) > \fhypptemp{r}{w}(c+(1+\theta)r w) = \fhypmtemp{r}{w}(c+(1+\theta)r w)$,
	so the rest follows by Lemma~\ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-2}.
	\item $\fxone(c+(1+\theta)r w) > \fxtwo(c+(1+\theta)r w)$. This can be shown by direct comparison.
\end{enumerate}

\paragraph{Case 8: $\rho \in ((1+\theta)r, \infty)$.}
In the final case, 
\begin{equation*}
	\fxone(c+\rho w)
	= \frac{\alpha^2}{(1-\alpha)^2} \ltwosqr{x_1-x_2}
	+ \frac{2\alpha\rho}{1-\alpha} \< x_1-x_2, w \>
	+ \rho^2.
\end{equation*}
dominates all other functions.
\begin{enumerate}
	\item $\fxone(c+\rho w) > \fhypptemp{\rho'}{w'}(c+\rho w), \forall \rho' \in [r,(1+\theta)r], \ltwo{w'} = 1$.
	To show this, it suffices to show $\fxone(c+\rho w) > \fhypptemp{(1+\theta)r}{w}(c+\rho w)$,
	and the rest follows because of Lemma~\ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-4}.
	We can calculate and arrange $\fhypptemp{(1+\theta)r}{w}(c+\rho w)$ to get
	\begin{equation*}
		\fhypptemp{(1+\theta)r}{w}(c+\rho w) 
		= \frac{\alpha^2}{(1-\alpha)^2} \ltwosqr{x_1-x_2}
		+ \frac{2\alpha\rho}{1-\alpha} \< x_1-x_2, w \>
		- (1+\theta)^2 r^2 + 2(1+\theta)r \rho,
	\end{equation*}
	so 
	\begin{equation*}
		\fxone(c+\rho w) > \fhypptemp{(1+\theta)r}{w}(c+\rho w)
		\iff (1+\theta)^2 r^2 - 2(1+\theta) \rho + \rho^2 > 0
		\iff ((1+\theta)r-\rho)^2 > 0.
	\end{equation*}
	\item $\fxone(c+\rho w) > \fhypmtemp{\rho'}{w'}(c+\rho w), \forall \rho' \in [(1-\theta)r,r], \ltwo{w'} = 1$.
	Notice that we just showed that 
	$\fxone(c+\rho w) > \fhypptemp{r}{w}(c+\rho w) = \fhypmtemp{r}{w}(c+\rho w)$.
	The rest follows by Lemma~\ref{sublem:smthconstvalues}.\ref{itemsub:smthconstvalues-2}.
	\item $\fxone(c+\rho w) > \fxtwo(c+\rho w)$. This can be shown by direct comparison.
\end{enumerate}

\subsection{Proof of Lemma~\ref{lem:smthconstparams}}
\label{sec:proof-smthconstparams}
From Lemma~\ref{lem:smthconstvalues}, it became clear that 
\begin{equation*}
	\nabla f(c+\rho w) =
	\begin{cases}
		\nabla \fxtwo(c+\rho w) 	& \text{if } \rho \in [0,(1-\theta)r]\\
		\dot h_-(\rho, w) 		& \text{if } \rho \in [(1-\theta)r,r]\\
		\dot h_+(\rho, w) 		& \text{if } \rho \in [r, (1+\theta) r]\\
		\nabla \fxone(c+\rho w) 	& \text{if } \rho \in [(1+\theta) r, \infty).
	\end{cases}
\end{equation*}
In order to show that the function is $\left (2+\frac{1-\alpha}{\theta}\right )$-smooth, it suffices to show that each piece of the function has $\left (2+\frac{1-\alpha}{\theta}\right )$-Lipschitz gradient. Since $\fxone$ and $\fxtwo$ are quadratic functions, it is easy to see that they are $2$-smooth and $2\alpha$-smooth, respectively. 
In case of $\dot h_-(\rho, w)$, for $\rho_1, \rho_2 \in [(1-\theta)r,r]$ and
any arbitrary unit vectors $w_1$ and $w_2$,
\begin{align*}
	&\ltwo{\dot h_-(\rho_1, w_1) - \dot h_-(\rho_2, w_2)} \\
	=& 	\ltwo{
		- \frac{(1-\alpha)(1-\theta)r}{\theta} (w_1-w_2)
		+ \left ( \frac{1-\alpha}{\theta}+2\alpha \right ) (\rho_1 w_1 - \rho_2 w_2)
	}\\
	=& 	\ltwo{
		\frac{1-\alpha}{\theta} [(\rho_1-(1-\theta)r) w_1-(\rho_2-(1-\theta)r)w_2]
		+ 2\alpha (\rho_1 w_1 - \rho_2 w_2)
	}\\
	\leq& \frac{(1-\alpha)}{\theta} \ltwo{(\rho_1-(1-\theta)r) w_1-(\rho_2-(1-\theta)r)w_2} + 2\alpha  \ltwo{\rho_1 w_1 - \rho_2 w_2}\\
	\leq& \left ( 2\alpha + \frac{1-\alpha}{\theta} \right ) \ltwo{\rho_1 w_1 - \rho_2 w_2}.
\end{align*}
The last inequality sign used that $\ltwo{(\rho_1-(1-\theta)r) w_1-(\rho_2-(1-\theta)r)w_2} \leq \ltwo{\rho_1 w_1 - \rho_2 w_2}$.
To see this, whenever $0\leq z_3 \leq z_1$ and $0 \leq z_3\leq z_2$,
\begin{align*}
	&\ltwo{(z_1-z_3) w_1-(z_2-z_3)w_2} \leq \ltwo{z_1 w_1 - z_2 w_2}\\
	\iff & (z_1-z_3)^2 + (z_2-z_3)^2 - 2(z_1-z_3)(z_2-z_3)\<w_1,w_2\> \leq z_1^2 + z_2^2 - 2 z_1 z_2 \<w_1,w_2\>\\
	\iff & ((z_1-z_3)-(z_2-z_3))^2 + 2(z_1-z_3)(z_2-z_3)(1-\<w_1,w_2\>) \leq (z_1-z_2)^2 + 2 z_1 z_2 (1-\<w_1,w_2\>).
\end{align*}

Similarly, for $\rho_1, \rho_2 \in [r,(1+\theta)r]$ and any arbitrary unit vectors $w_1$ and $w_2$,
\begin{align*}
	&\ltwo{h_+(\rho_1, w_1) - h_+(\rho_2, w_2)} \\
	=& 	\ltwo{
		- \frac{(1-\alpha)(1+\theta)r}{\theta} (w_1-w_2)
		+ \left ( \frac{1-\alpha}{\theta}+2 \right ) (\rho_1 w_1 - \rho_2 w_2)
	}\\
	=& 	\ltwo{
		\frac{1-\alpha}{\theta} [((1+\theta)r-\rho_2)w_2-((1+\theta)r-\rho_1)w_1]
		+ 2 (\rho_1 w_1 - \rho_2 w_2)
	}\\
	\leq& \frac{1-\alpha}{\theta} \ltwo{((1+\theta)r-\rho_2) w_2-((1+\theta)r-\rho_1)w_1} + 2 \ltwo{\rho_1 w_1 - \rho_2 w_2}\\
	\leq& \left ( 2+\frac{1-\alpha}{\theta} \right ) \ltwo{\rho_1 w_1 - \rho_2 w_2}.
\end{align*}
The last inequality sign used that $\ltwo{((1+\theta)r-\rho_2) w_2-((1+\theta)r-\rho_1)w_1} \leq \ltwo{\rho_1 w_1 - \rho_2 w_2}$.
To see this, note that whenever $\frac{z_3}{2} \leq z_1 \leq z_3$ and $\frac{z_3}{2} \leq z_2 \leq z_3$,
\begin{align*}
	&\ltwo{(z_3-z_2) w_2-(z_3-z_1)w_1} \leq \ltwo{z_1 w_1 - z_2 w_2}\\
	\iff & ((z_3-z_2)-(z_3-z_1))^2 + 2(z_3-z_2)(z_3-z_1)(1-\<w_1,w_2\>) \leq (z_1-z_2)^2 + 2 z_1 z_2 (1-\<w_1,w_2\>)\\
	\iff & (z_3-z_2)(z_3-z_1) \leq z_2 z_1.
\end{align*}
The last statement holds because $0 \leq z_3-z_1 \leq z_1$ and $0 \leq z_3-z_2 \leq z_2$.
Recalling $\theta \in (0,1)$, $\rho_1$, $\rho_2$, and $(1+\theta)r$ corresponds to $z_1$, $z_2$, and $z_3$, respectively.
This concludes that the whole function has $\left ( 2+\frac{1-\alpha}{\theta} \right )$-Lipschitz gradient.

Now, since we know that $\fxtwo(x)$ is $2\alpha$-strongly convex, 
the proof of $2\alpha$-strong convexity can be done by showing that
$\tilde f(x) \defeq f(x) - \fxtwo(x)$ is convex, or equivalently, that $\nabla \tilde f(x)$ is monotone.
The value of $\nabla \tilde f(c + \rho w)$ depending on different values of $\rho$ is as the following:
\begin{equation*}
	\nabla \tilde f(c+\rho w) =
	\begin{cases}
		0 						
		& \text{if } 0 \leq \rho \leq (1-\theta)r\\
		\frac{(1-\alpha)(\rho - (1-\theta) r)}{\theta} w
		& \text{if } (1-\theta)r \leq \rho \leq r\\
		\frac{(1-\alpha)((1+2\theta) \rho - (1+\theta) r)}{\theta} w
		& \text{if } r \leq \rho \leq (1+\theta) r\\
		2(1-\alpha) \rho w
		& \text{if } \rho \geq (1+\theta) r.
	\end{cases}
\end{equation*}
Now, we need to show that $\nabla \tilde f(c+\rho w)$ is monotone, meaning that
\begin{equation*}
	\<
	\nabla \tilde f(c+\rho' w') - \nabla \tilde f(c+\rho w),
	\rho' w' - \rho w
	\> \geq 0
	,~ \forall \rho \geq 0, \rho' \geq 0, \ltwo {w} = \ltwo {w'} = 1.
\end{equation*}
For notational simplicity in the proof, define 
$(\star) = \<
\nabla \tilde f(c+\rho' w') - \nabla \tilde f(c+\rho w),
\rho' w' - \rho w
\>$.
\begin{enumerate}
	\item If $\rho, \rho' \in [0,(1-\theta)r]$, $(\star) = 0$.
	\item If $\rho \in [0,(1-\theta)r], \rho' \in ((1-\theta)r, r]$, 
	\begin{equation*}
		(\star) = \frac{(1-\alpha)(\rho'-(1-\theta)r)}{\theta}(\rho' - \rho \<w,w'\>) \geq 0.
	\end{equation*}
	\item If $\rho \in [0,(1-\theta)r], \rho' \in (r, (1+\theta)r]$, the proof is similar to Case 2.
	\item If $\rho \in [0,(1-\theta)r], \rho' \in ((1+\theta)r, \infty)$, the proof is similar to Case 2.
	\item If $\rho, \rho' \in ((1-\theta)r,r]$,
	\begin{align*}
		(\star) &= \frac{(1-\alpha)}{\theta} 
		\left (    (\rho'-(1-\theta)r)(\rho' - \rho \<w,w'\>) 
		+ (\rho-(1-\theta)r)(\rho - \rho' \<w,w'\>) \right )\\
		&\geq \frac{(1-\alpha)}{\theta} 
		\left (    (\rho'-(1-\theta)r)(\rho' - \rho) 
		+ (\rho-(1-\theta)r)(\rho - \rho') \right )\\
		&= \frac{(1-\alpha)(\rho'-\rho)^2}{\theta} \geq 0.
	\end{align*}
	\item if $\rho \in ((1-\theta)r,r], \rho' \in (r,(1+\theta)r]$,
	\begin{align*}
		(\star) &\geq \frac{(1-\alpha)(\rho'-\rho)}{\theta} 
		\left (    ((1+2\theta)\rho' - (1+\theta) r) 
		- (\rho - (1-\theta)r ) \right )\\
		&= \frac{(1-\alpha)(\rho'-\rho)}{\theta}
		\left (    2\theta(\rho'-r)+(\rho'-\rho) \right ) \geq 0.
	\end{align*}
	\item if $\rho \in ((1-\theta)r,r], \rho' \in ((1+\theta)r,\infty)$,
	\begin{align*}
		(\star) &\geq \frac{(1-\alpha)(\rho'-\rho)}{\theta} 
		\left (    2\theta\rho' - (\rho - (1-\theta)r) ) \right )\\
		&= \frac{(1-\alpha)(\rho'-\rho)}{\theta}
		\left (    \theta(2\rho'-r)+r-\rho \right ) \geq 0.
	\end{align*}
	\item If $\rho, \rho' \in (r,(1+\theta)r]$, the proof is similar to Case 5.
	\item If $\rho \in (r,(1+\theta)r], \rho' \in ((1+\theta)r,\infty)$,
	\begin{align*}
		(\star) &\geq \frac{(1-\alpha)(\rho'-\rho)}{\theta} 
		\left (    2\theta\rho' - ((1+2\theta)\rho - (1+\theta)r) ) \right )\\
		&= \frac{(1-\alpha)(\rho'-\rho)}{\theta}
		\left (    2\theta(\rho'-\rho) + (1+\theta)r - \rho \right ) \geq 0.
	\end{align*}
	\item If $\rho, \rho' \in ((1+\theta)r,\infty)$, the proof is similar to Case 5.
\end{enumerate}

\subsection{Proof of Lemma~\ref{lem:smthconstrange}}
\label{sec:proof-smthconstrange}
By Lemma~\ref{lem:ssftnmdparam}, 
\begin{equation*}
	\itpset
	=
	\left \{ 
	c + \rho w 
	\mid
	(1-\theta) r \leq \rho \leq (1+\theta) r, 
	\ltwo{w} = 1
	\right \}
	\subset \cl (\balltwo{x_1}{\eta\delta}^c \cap \balltwo{x_1}{\delta}).
\end{equation*}
So, Lemma~\ref{lem:smthconstrange}.\ref{item:smthconstrange-7} and \ref{lem:smthconstrange}.\ref{item:smthconstrange-8} are implied by Lemma~\ref{lem:smthconstvalues}.

For Lemma~\ref{lem:smthconstrange}.\ref{item:smthconstrange-3},
By observing $\fxtwo(x_2) = s \beta \delta^2+t \leq \fxtwo(x) \leq f(x)$ for all $x$, 
it is easy to see that the global minimum value of $f(x)$ is $s \beta \delta^2+t$ and is attained at $x_2 \in \balltwo{x_1}{\eta\delta}$.
Also, at any $x$ such that $\ltwo{x-x_1} = \delta$, we can check $f(x) = \fxone(x) =s \delta^2+t$.
By convexity, any point in between $x$ and $x_2$ cannot be larger than $s \delta^2+t$, which means that
$s \beta \delta^2 + t \leq f(x) \leq s \delta^2 + t$ for all $x \in \balltwo{x_1}{\delta}$.

For the last statement Lemma~\ref{lem:smthconstrange}.\ref{item:smthconstrange-4}, we will assume that the scaling factor $s = 1$ and prove that
$\ltwo{\nabla f(x)} \leq 2\delta$. The norm of gradient $\ltwo{\nabla f(x)}$ naturally scales with $s$, so Lemma~\ref{lem:smthconstrange}.\ref{item:smthconstrange-4} follows for any $s > 0$.
Note that $\ltwo{\nabla \fxone(x)} = \ltwo{2(x-x_1)} \leq 2\delta$ for any $x \in \balltwo{x_1}{\delta}$.
Also, for $x \in \balltwo{x_1}{\delta}$,
\begin{align*}
	\ltwo{\nabla \fxtwo(x)} = 2\alpha \ltwo{x-x_2} = 2\alpha \ltwo{x-x_1} + 2\alpha\ltwo{x_1-x_2} \leq 2\alpha \delta + 2\alpha \eta\delta \leq 2\delta,
\end{align*}
where the last inequality is by Eq~\eqref{eq:constraints}: $\eta+\alpha+\alpha\eta < 1$, which implies $\alpha+\alpha\eta < 1$.
For $x = c + \rho w$ where $\rho \in [(1-\theta)r, r]$ and $w$ is any unit vector, $\nabla f(x) = \dot h_-(x)$.
\begin{align*}
	&\ltwo{\dot h_-(c + \rho w)}
	= \ltwo{ \frac{2\alpha}{1-\alpha}(x_1-x_2) + \left [ \frac{(1-\alpha)(\rho-(1-\theta)r)}{\theta} + 2\alpha\rho \right] w}\\
	\leq& \frac{2\alpha}{1-\alpha}\ltwo{x_1-x_2} + \left [ \frac{(1-\alpha)(\rho-(1-\theta)r)}{\theta} + 2\alpha\rho \right]
	\leq 
	\frac{2\alpha \eta \delta}{1-\alpha} + (1+\alpha) r,
\end{align*}
where the last inequality is obtained by substituting $\rho = r$, the maximum possible $\rho$ in the range.
\begin{align*}
	&\ltwo{\dot h_+(c + \rho w)} 
	= \ltwo{ \frac{2\alpha}{1-\alpha}(x_1-x_2) + \left [ \frac{(1-\alpha)(\rho-(1+\theta)r)}{\theta} + 2\rho \right] w}\\
	\leq& \frac{2\alpha}{1-\alpha}\ltwo{x_1-x_2} + \left [ \frac{(1-\alpha)(\rho-(1+\theta)r)}{\theta} + 2\rho \right]
	\leq \frac{2\alpha \eta \delta}{1-\alpha} + 2(1+\theta) r,
\end{align*}
also where the last inequality is obtained by substituting $\rho = (1+\theta)r$.
We can check that $1+\alpha < 2 < 2(1+\theta)$, so it suffices to show
\begin{equation}
\label{eq:smthconstrange-4goal}
\frac{2\alpha \eta \delta}{1-\alpha} + 2(1+\theta) r \leq 2\delta.
\end{equation}
First, recall from Eqs~\eqref{eq:defradi} and \eqref{eqn:smthconstbetaval} that
\begin{equation*}
	r = \sqrt{\frac{\alpha}{(1-\alpha)^2} \ltwosqr{x_1-x_2} + \frac{\beta\delta^2}{1-\alpha}},~\text{ and }~
	\beta = \frac{(1-\alpha)(1+\eta)^2}{4} - \frac{\alpha \eta^2}{1-\alpha}.
\end{equation*}
Substituting $\beta$, 
\begin{equation*}
	r = \sqrt{\frac{(1+\eta)^2 \delta^2}{4}-\frac{\alpha}{(1-\alpha)^2} (\eta^2 \delta^2-\ltwosqr{x_1-x_2})} \leq \frac{(1+\eta)\delta}{2}.
\end{equation*}
substituting this to LHS of Eq~\eqref{eq:smthconstrange-4goal} and also $\theta = \frac{1-\eta-\alpha-\alpha\eta}{1+\eta-\alpha-\alpha\eta}$ as defined in Eq~\eqref{eqn:smthconstthetaval},
\begin{align*}
	\frac{2\alpha \eta \delta}{1-\alpha} + 2(1+\theta) r 
	&\leq 
	\frac{2\alpha \eta \delta}{1-\alpha} + 2 \left(1+ \frac{1-\eta-\alpha-\alpha\eta}{1+\eta-\alpha-\alpha\eta} \right) \frac{(1+\eta)\delta}{2}\\
	&=
	\frac{2\alpha \eta \delta}{1-\alpha} + 2 \left(\frac{1-\alpha-\alpha\eta}{(1+\eta)(1-\alpha)} \right) (1+\eta) \delta = 2\delta.
\end{align*}
Thus, we have shown that for any $x \in \balltwo{x_1}{\delta}$, $\ltwo{\nabla f(x)} \leq 2\delta$, which is our desired Lemma~\ref{lem:smthconstrange}.\ref{item:smthconstrange-4} with $s = 1$.

\subsection{Proof of Lemma~\ref{lem:ftnconstrsscvx}} 
\label{sec:proof-ftnconstrsscvx}
We start by showing the following technical lemma, 
which illustrates how the functions after the smooth max operations look like.
Its proof is deferred to Appendix~\ref{sec:proof-ftnconstrsscvxsublem}.
\begin{lemma}
	\label{sublem:ftnconstrsscvx}
	For any set of parameters $u_1, u_2, \dots, u_M, v$ chosen by
	$u_1 \in \Uone$, $u_t \in \Utemp{t-1}{t}$ for $t \in 2:M$, and $v \in \V$,
	run Algorithm~\ref{alg:ftnconstrSmthstrcvx} and get $\ftnuv(x)$.
	Then, for any $t \in 2:M$, we have: 
	\begin{enumerate}
		%\item $\ftnutmone(x) \geq \gut(x) \text{ for all } x\notin \Btwotmone$.
		%\item $\ftnutmone(x) \leq \gut(x) = \ftninut(x) \text{ for all } x\in \Btwotmoneh$.
		\item \label{itemsub:ftnconstrsscvx-1} 
		$\ftnut(x) = \begin{cases}
		\ftnutmone(x) &\forall x \not\in \Btwotmone, \\
		\ftninut(x) = \gut(x) &\forall x\in \Btwotmoneh,
		\end{cases}$
		\item \label{itemsub:ftnconstrsscvx-3} 
		$C \beta \sum_{m=1}^{t-1} \alpha^{m-1}\delta_m^2 
		\leq
		\ftnut (x)
		\leq
		C \beta \sum_{m=1}^{t-2} \alpha^{m-1}\delta_m^2 + C \alpha^{t-2} \delta_{t-1}^2
		\text{ for all } x \in \Btwotmone$.
		\item \label{itemsub:ftnconstrsscvx-4}
		$\ftnut(x)\defeq \max \{ \ftnutmone (x), \gut(x) \}$ is smooth.
	\end{enumerate}
	Also, at the final step,
	\begin{enumerate}
		\setcounter{enumi}{3}
		%\item $\ftnuM(x) \geq \guv (x) \text{ for all }x\notin \BtwoM$.
		%\item $\ftnuM(x) \leq \guv (x) = \ftninuv(x) \text{ for all } x\in \BtwoMh$.
		\item \label{itemsub:ftnconstrsscvx-5} 
		$\ftnuv(x) = \begin{cases}
		\ftnuM(x) &\forall x \not\in \BtwoM, \\
		\ftninuv(x) = \guv (x) &\forall x\in \BtwoMh,
		\end{cases}$
		\item \label{itemsub:ftnconstrsscvx-7}
		$C \beta \sum_{m=1}^{M} \alpha^{m-1}\delta_m^2
		\leq
		\ftnuv (x)
		\leq
		C \beta \sum_{m=1}^{M-1} \alpha^{m-1}\delta_m^2 + C \alpha^{M-1} \delta_{M}^2
		\text{ for all } x \in \BtwoM$.
		\item \label{itemsub:ftnconstrsscvx-8}
		$\ftnuv(x) \defeq \max \{ \ftnuM(x), \guv(x) \}$ is smooth.
	\end{enumerate}
\end{lemma}

As done for Lemma~\ref{lem:ftnconstrLipcvx} in Section~\ref{sec:proof-ftnconstrLipcvx}, 
we prove Lemma~\ref{lem:ftnconstrsscvx}.\ref{item:ftnconstrunified-1} and \ref{lem:ftnconstrsscvx}.\ref{item:ftnconstrunified-4} using simple and intuitive argument 
that $\max$ operations done in Algorithm~\ref{alg:ftnconstrSmthstrcvx} only changes limited parts of the domain.
From Lemma~\ref{sublem:ftnconstrsscvx}.\ref{itemsub:ftnconstrsscvx-1},
note that whenever we have $\ftnutmone(x)$ and take $\max$ operation with $\gut(x)$ to construct $\ftnut(x)$,
any point $\forall x \not\in \Btwotmone$ does not change its value; i.e.\ $\ftnut(x)=\ftnutmone(x)$.
This means that the Line~\ref{line:max1}: $\ftnut (x) \defeq \max \{ \ftnutmone (x), \gut(x) \}$ in Algorithm~\ref{alg:ftnconstrSmthstrcvx}
can only possibly change function values in $\Btwotmone$.
Also, later iterations of the algorithm do not change that the function values at $x \notin \Btwotmone$, 
because $\Btwotmone \supset \Btwot \supset \cdots \supset \BtwoM$.
From this argument, we can see that $\ftnuv(x) = \ftnuvtildet(x) = \ftnutmone(x)$ for all $x \notin \Btwotmone$, 
therefore proving Lemma~\ref{lem:ftnconstrsscvx}.\ref{item:ftnconstrunified-1}. 
Similarly, Line~\ref{line:max2}: $\ftnuv(x) \defeq \max \{ \ftnuM(x), \guv(x) \}$ in Algorithm~\ref{alg:ftnconstrSmthstrcvx}
can only change function values in $\BtwoM$, 
so $\ftnuvm(x) = \ftnuvp(x) = \ftnuM(x)$ for all $x \notin \BtwoM$, proving Lemma~\ref{lem:ftnconstrsscvx}.\ref{item:ftnconstrunified-4}.

Lemma~\ref{lem:ftnconstrsscvx}.\ref{item:ftnconstrunified-5} can be implied directly by Lemma~\ref{sublem:ftnconstrsscvx}.\ref{itemsub:ftnconstrsscvx-7}.
In order to prove Lemma~\ref{lem:ftnconstrsscvx}.\ref{item:ftnconstrunified-2}, note the following facts from Lemma~\ref{sublem:ftnconstrsscvx}.\ref{itemsub:ftnconstrsscvx-7} and \ref{sublem:ftnconstrsscvx}.\ref{itemsub:ftnconstrsscvx-3}:
\begin{align*}
	&C \beta \sum_{m=1}^{M} \alpha^{m-1}\delta_m^2
	\leq
	\ftnuv (x)
	\leq
	C \beta \sum_{m=1}^{M-1} \alpha^{m-1}\delta_m^2 + C \alpha^{M-1} \delta_{M}^2
	\text{ for all } x \in \BtwoM,\\
	&C \beta \sum_{m=1}^{M-1} \alpha^{m-1}\delta_m^2 
	\leq
	\ftnuM (x)
	\leq
	C \beta \sum_{m=1}^{M-2} \alpha^{m-1}\delta_m^2 + C \alpha^{M-2} \delta_{M-1}^2
	\text{ for all } x \in \BtwoMmone.
\end{align*}
Note from Lemma~\ref{sublem:ftnconstrsscvx}.\ref{itemsub:ftnconstrsscvx-5} that $\ftnuv (x) = \ftnuM(x)$ for all $x \notin \BtwoM$, 
and that, for all $x \in \BtwoM$,
\begin{equation*}
	\ftnuv (x)
	\leq
	C \beta \sum_{m=1}^{M-1} \alpha^{m-1}\delta_m^2 + C \alpha^{M-1} \delta_{M}^2
	\leq
	C \beta \sum_{m=1}^{M-2} \alpha^{m-1}\delta_m^2 + C \alpha^{M-2} \delta_{M-1}^2.
\end{equation*}
The last inequality is because $\frac{(1-\beta)\delta_{M-1}^2}{\alpha} \geq \delta_M^2$ holds for large enough $n$ by assumption
that $\delta_M = o(\delta_{M-1})$.
From these observations, we have
\begin{align*}
	C \beta \sum_{m=1}^{M-1} \alpha^{m-1}\delta_m^2
	\leq
	\ftnuv (x)
	\leq
	C \beta \sum_{m=1}^{M-2} \alpha^{m-1}\delta_m^2 + C \alpha^{M-2} \delta_{M-1}^2
	\text{ for all } x \in \BtwoMmone.
\end{align*}
Again note that, for any $x \notin \BtwoMmone$ we also have $x \notin \BtwoM$, 
so $\ftnuv(x) = \ftnuM(x) = \ftnuMmone(x)$. We can repeat a similar argument and obtain
\begin{align*}
	C \beta \sum_{m=1}^{M-2} \alpha^{m-1}\delta_m^2
	\leq
	\ftnuv (x)
	\leq
	C \beta \sum_{m=1}^{M-3} \alpha^{m-1}\delta_m^2 + C \alpha^{M-3} \delta_{M-2}^2
	\text{ for all } x \in \Btwotemp{M-2}.
\end{align*}
For any $t \in 2:M$, we can repeat this argument until $\Btwotemp{t-1}$, so that we get
\begin{align*}
	C \beta \sum_{m=1}^{t-1} \alpha^{m-1}\delta_m^2
	\leq
	\ftnuv (x)
	\leq
	C \beta \sum_{m=1}^{t-2} \alpha^{m-1}\delta_m^2 + C \alpha^{t-2} \delta_{t-1}^2
	\text{ for all } x \in \Btwotmone,
\end{align*}
which directly implies Lemma~\ref{lem:ftnconstrsscvx}.\ref{item:ftnconstrunified-2} that we are after.

For Lemma~\ref{lem:ftnconstrsscvx}.\ref{item:ftnconstrunified-3} and \ref{lem:ftnconstrsscvx}.\ref{item:ftnconstrunified-6},
we will show that the function value $\ftnuv(x)$ in $\Btwotmone$ can be expressed as
\begin{equation}
\label{eq:ftninsideboxsscvx}
\ftnuv(x) = \max \left \{ \ftninutmone(x), \max_{k \in t:M}\left \{ \gutemp{k}(x) \right \}, \guv(x) \right \},\text{ for all } x \in \Btwotmone.
\end{equation}
Notice from Lemma~\ref{sublem:ftnconstrsscvx}.\ref{itemsub:ftnconstrsscvx-1} that 
$\ftnutmone(x) = \ftninutmone(x)$ for all $x \in \Btwotmtwoh$.
Recall that $\Btwotmone \subset \Btwotmtwoh$, so $\ftnutmone(x) = \ftninutmone(x)$ in $\Btwotmone$.
After this point, $\ftnuv(x)$ is obtained from $\max$ operations with $\gut, \dots, \guM, \guv$.
This proves Eq~\eqref{eq:ftninsideboxsscvx}. 
Given Eq~\eqref{eq:ftninsideboxsscvx}, we prove Lemma~\ref{lem:ftnconstrsscvx}.\ref{item:ftnconstrunified-3} by showing that for any $x \in \Btwotmone$,
all the operands of the $\max$ operation in Eq~\eqref{eq:ftninsideboxsscvx} satisfy that $\ell_2$ norm of the gradient is 
bounded above by $2C\alpha^{t-2}\delta_{t-1}$.
First, the gradient of $\ftninutmone(x) \defeq C\alpha^{t-2} \ltwosqr{x-u_{t-1}} + C\beta \sum_{m=1}^{t-2} \alpha^{m-1}\delta_m^2$
is $\nabla \ftninutmone(x) = 2 C\alpha^{t-2}(x-u_{t-1})$, so 
\begin{equation*}
	\ltwo{\nabla \ftninutmone(x)} \leq 2 C \alpha^{t-2}\delta_{t-1}, \text{ for any } x \in \Btwotmone.
\end{equation*}
Now, for $k \in t:M$, apply Lemma~\ref{lem:smthconstrange} for $\gutemp{k}(x) = \algmaxsmth(\ftninutemp{k-1}, \ftninutemp{k}, \alpha, \eta, \delta_{k-1})$.
Recall the definition
\begin{align*}
	\ftninutemp{k-1}(x) &\defeq C\alpha^{k-2} \ltwosqr{x-u_{k-1}} + C\beta \sum\nolimits_{m=1}^{k-2} \alpha^{m-1}\delta_m^2\\
	\ftninutemp{k}(x) &\defeq C\alpha^{k-1} \ltwosqr{x-u_{k}} + C\beta \sum\nolimits_{m=1}^{k-1} \alpha^{m-1}\delta_m^2,
\end{align*}
then we can note that in terms of the formulation in Lemma~\ref{lem:smthconstrange},
$s = C\alpha^{k-2}$, $t = C\beta \sum_{m=1}^{k-2} \alpha^{m-1}\delta_m^2$, and $\delta = \delta_{k-1}$.
From Lemma~\ref{lem:smthconstrange}.\ref{item:smthconstrange-7} and \ref{lem:smthconstrange}.\ref{item:smthconstrange-4}, we have
\begin{align*}
	&\gutemp{k}(x) = \ftninutemp{k-1}(x) \quad \text{for all } x\notin \balltwo{u_{k-1}}{\delta_{k-1}}.\\
	&\ltwo{\nabla \gutemp{k}(x)} \leq 2 C\alpha^{k-2} \delta_{k-1} \leq 2 C \alpha^{t-2}\delta_{t-1} \quad \text{for all } x \in \balltwo{u_{k-1}}{\delta_{k-1}}.
\end{align*}
For $x \in \balltwo{u_{k-1}}{\delta_{k-1}}$ we already proved that $\ltwo{\nabla \gutemp{k}(x)} \leq 2 C \alpha^{t-2}\delta_{t-1}$.
We now have to consider points in $\balltwo{u_{k-1}}{\delta_{k-1}}^c \cap \Btwotmone$. Note that for $k = t$, this is $\emptyset$.
For $k \in (t+1):M$, 
\begin{align*}
	&\ltwo{\nabla \gutemp{k}(x)} 
	= \ltwo{\nabla \ftninutemp{k-1}(x)} 
	= \ltwo{2C\alpha^{k-2} (x-u_{k-1})}\\
	\leq& 2C\alpha^{k-2} \ltwo{x-u_{t-1}} + 2C\alpha^{k-2} \ltwo{u_{t-1}-u_{k-1}}
	\leq 2C\alpha^{k-2} \delta_{t-1} + 2C\alpha^{k-2} \eta \delta_{t-1}\\
	\leq& 2C\alpha^{t-1} \delta_{t-1} + 2C\alpha^{t-1} \eta \delta_{t-1}
	= 2C\alpha^{t-2} \delta_{t-1} (\alpha + \eta \alpha)
	\leq 2C\alpha^{t-2} \delta_{t-1}.
\end{align*}
Thus, for any $x \in \Btwotmone$, $\ltwo{\nabla \gutemp{k}(x)} \leq 2C\alpha^{t-2} \delta_{t-1}$ for $k \in t:M$.
We can prove this inequality for $\guv(x)$ in the same way.
Since all the function in the $\max$ operation satisfies upper bound on $\ell_2$ norm of gradient, we have
\begin{equation*}
	\ltwo{\nabla \ftnuv(x)} \leq 2C\alpha^{t-2} \delta_{t-1} \text{ for all } x \in \Btwotmone,
\end{equation*}
which implies Lemma~\ref{lem:ftnconstrsscvx}.\ref{item:ftnconstrunified-3}.
From a similar argument as Eq~\eqref{eq:ftninsideboxsscvx}, we have
\begin{equation*}
	\ftnuv(x) = \max \left \{ \ftninuM(x), \guv(x) \right \}, \text{ for all } x \in \BtwoM,
\end{equation*}
whereby we can prove Lemma~\ref{lem:ftnconstrsscvx}.\ref{item:ftnconstrunified-6}.

Finally, we have to show Lemma~\ref{lem:ftnconstrsscvx}.\ref{item:ftnconstrunified-7}. 
To do so, we first show that, for any choice of $u_1, u_2, \ldots, u_M$ and $v$, 
\begin{equation}
\label{eqn:min-of-ftnuvsscvx}
\inf_{x} \ftnuv(x) = C\beta \sum_{m=1}^{M} \alpha^{m-1}\delta_m^2.
\end{equation}
In fact, from Lemma~\ref{sublem:ftnconstrsscvx}.\ref{itemsub:ftnconstrsscvx-5}, 
we have $\ftnuv(x) = \ftninuv(x)$ for all $x \in \BtwoMh$.
Also, $\ftninuv(x)$ is minimized at $u_M+ v \eta \delta_M \unitvec \in \BtwoMh$,
whose minimum value is the RHS of Eq~\eqref{eqn:min-of-ftnuvsscvx}.
So, for any $x \in \domain$, 
\begin{equation*}
	\ftnuv(x) \geq \ftninuv(x) 
	\geq \ftninuv (u_M+ v \eta \delta_M \unitvec ) 
	= C\beta \sum_{m=1}^{M} \alpha^{m-1}\delta_m^2,
\end{equation*}
proving Eq~\eqref{eqn:min-of-ftnuvsscvx}.

Next, we show that
\begin{equation}
\label{eqn:min-of-sum-ftnuvsscvx}
\inf_x (\ftnuvp(x) + \ftnuvm(x)) =  2C \beta \sum_{m=1}^{M}  \alpha^{m-1} \delta_m^2 + 2C \alpha^M \eta^2 \delta_M^2.
\end{equation}
Again note that $\ftnuv(x) = \ftninuv(x)$ for all $x \in \BtwoMh$.
That is, for $x\in \BtwoMh$, we have $\ftnuvp(x) = \ftninuvp(x)$ and $\ftnuvm(x) = \ftninuvm(x)$.
Therefore, for any $x\in \BtwoMh$, 
\begin{align*}
	&\ftnuvp(x) + \ftnuvm(x) 
	= \ftninuvp(x) + \ftninuvm(x)\\
	=& C\alpha^M \left(\ltwosqr{x-u_M- \eta \delta_M\unitvec} + \ltwosqr{x-u_M+ \eta \delta_M\unitvec}\right)
	+ 2C \beta \sum_{m=1}^{M}  \alpha^{m-1} \delta_m^2.
\end{align*}
Note also that $x = u_M$ attains minimum, which evaluates to the RHS of Eq~\eqref{eqn:min-of-sum-ftnuvsscvx}.
So, for any $x \in \domain$, 
\begin{align*}
	\ftnuvp(x) + \ftnuvm(x) 
	&\geq \ftninuvp(x) + \ftninuvm(x) 
	\geq \ftninuvp(u_M) + \ftninuvm(u_M)\\
	&=  2C \beta \sum_{m=1}^{M}  \alpha^{m-1} \delta_m^2 + 2C \alpha^M \eta^2 \delta_M^2,
\end{align*}
thus proving Eq~\eqref{eqn:min-of-sum-ftnuvsscvx}.
Now, Lemma~\ref{lem:ftnconstrsscvx}.\ref{item:ftnconstrunified-7} follows from Eq~\eqref{eqn:min-of-ftnuvsscvx} and Eq~\eqref{eqn:min-of-sum-ftnuvsscvx}.

\subsection{Proof of Lemma~\ref{lem:ftnconstrsscvx2}} 
\label{sec:proof-ftnconstrsscvx2}
First note that 
\begin{equation}
\label{eq:ftnuvmaxdef}
\ftnuv(x) = \max \left \{ \ftninuone(x), \max_{k \in 2:M}\left \{ \gutemp{k}(x) \right \}, \guv(x) \right \}.
\end{equation}
As seen in Lemma~\ref{sublem:ftnconstrsscvx}.\ref{itemsub:ftnconstrsscvx-8}, $\ftnuv$ is smooth. 
Thus, the smoothness constant is determined by the ``piece'' of the function with the largest smoothness constant.
This appears at $\gutwo(x)$, whose smoothness constant is $C\left ( 2 + \frac{1-\alpha}{\theta} \right)$, as seen from Lemma~\ref{lem:smthconstparams}.
In order to prove that $\ftnuv(x)$ is $2C\alpha^M$-strongly convex, 
it suffices to show that every operand in the $\max$ operation in Eq~\eqref{eq:ftnuvmaxdef}
is at least $2C\alpha^M$-strongly convex.
This can be readily checked using Lemma~\ref{lem:smthconstparams}.

Now, we are ready to pick parameters $\alpha$, $\eta$, and $C$ of Algorithm~\ref{alg:ftnconstrSmthstrcvx}
so as to make sure output $\ftnuv(x)$ is $H$-smooth and $\lambda$-strongly convex, for the case $H/5\geq\lambda$. The other case ($H/5 < \lambda$) will be handled later.
That is, we have to choose the right parameters to make
\begin{equation}
\label{eq:ftnconstrsscvx2goal}
H \geq C\left ( 2 + \frac{1-\alpha}{\theta} \right) ~\text{ and }~
\lambda \leq 2C\alpha^M.
\end{equation}
We first choose 
\begin{equation*}
	\eta = \frac{1-\alpha}{2},
\end{equation*}
so that $0<\eta<\half$ for $\alpha \in (0,1)$.
Which this choice, we have
\begin{equation*}
	\eta+\alpha+\alpha\eta = \half+\alpha-\frac{\alpha^2}{2},
\end{equation*}
which satisfies $\half < \eta+\alpha+\alpha\eta < 1$ for $\alpha \in (0,1)$.
Also, from Eq~\eqref{eqn:smthconstbetaval},
\begin{equation*}
	\beta \defeq \frac{(1-\alpha)(1+\eta)^2}{4} - \frac{\alpha \eta^2}{1-\alpha} = \frac{(9-\alpha)(1-\alpha)^2}{16},
\end{equation*}
which satisfies $0<\beta<\frac{9}{16}$ for $\alpha \in (0,1)$.
From Eq~\eqref{eqn:smthconstthetaval},
\begin{equation*}
	\theta \defeq \frac{1-\eta-\alpha-\alpha\eta}{1+\eta-\alpha-\alpha\eta} = \frac{1-\alpha}{3-\alpha},
\end{equation*}
which satisfies $0<\theta<\frac{1}{3}$ for $\alpha \in (0,1)$.
We finished checking that constraints on $\eta$, $\beta$, and $\theta$ are met, under this particular choice of $\eta$.

With this choice of $\eta$,
\begin{equation*}
	\frac{1-\alpha}{\theta} = 3-\alpha,
\end{equation*}
so $2<\frac{1-\alpha}{\theta}<3$ for $\alpha \in (0,1)$.
This also means that
\begin{equation*}
	C\left ( 2 + \frac{1-\alpha}{\theta} \right) \leq 5C.
\end{equation*}
Now choose
\begin{equation*}
	%\alpha = \left (\frac{5 \max\{\lambda, H/2\}}{2H} \right )^{1/M}, \text{ and } 
	\alpha = \left (\half \right )^{1/M}, \text{ and } 
	C = \frac{H}{5}.
\end{equation*}
With $\eta = (1-\alpha)/2$, we can check that
\begin{equation*}
	C\left ( 2 + \frac{1-\alpha}{\theta} \right) \leq 5C \leq H, \text{ and }
	2C\alpha^M = \frac{2H}{5} \cdot \half  \geq \lambda,
\end{equation*}
thus proving Eq~\eqref{eq:ftnconstrsscvx2goal}.
%Also, recall from Lemma~\ref{lem:ftnconstrsscvx}.\ref{item:ftnconstrunified-7} that 
%\begin{equation*}
%	\inf_{x\in \domain} [\ftnuvm(x) + \ftnuvp(x)] - \inf_{x\in \domain} \ftnuvm(x) - \inf_{x\in \domain} \ftnuvp(x)
%	= 2C\alpha^M \eta^2\delta_M^2.
%\end{equation*}
%With these parameters,
%\begin{equation*}
%	\inf_{x\in \domain} [\ftnuvm(x) + \ftnuvp(x)] - \inf_{x\in \domain} \ftnuvm(x) - \inf_{x\in \domain} \ftnuvp(x)
%	= \frac{H}{20}\left(1-\left(\half\right)^{\frac{1}{M}}\right)^2 \delta_M^2.
%\end{equation*}

For $H/5 < \lambda < H$, the choice of parameters is a bit more complicated;
we choose $\eta = \frac{(1-\alpha)^2}{2}$, which satisfies $0< \eta < \half$ for $\alpha \in (0,1)$.
With this choice, $\half < \eta + \alpha + \eta\alpha < 1$. Also, $0<\beta<\frac{9}{16}$ and $\frac{1}{3} < \theta < 1$ is satisfied, so all the constraints are met.
With this choice of $\eta$,
\begin{equation*}
	\frac{H}{\lambda}
	= \frac{1}{2\alpha^M} \left ( 2 + \frac{1-\alpha}{\theta} \right) 
	= \frac{1}{2\alpha^M} \left ( 2 + \frac{(1-\alpha)(\alpha^2-2\alpha+3)}{1+\alpha^2} \right).
\end{equation*}
When expressed as a function of $\alpha$, the RHS of the last equation has limit $\infty$ when $\alpha \rightarrow 0^+$, and limit 1 when $\alpha \rightarrow 1^-$. Therefore, for any ratio of $H/\lambda>1$, there exists a $\alpha_0$ that satisfies the above equation.
Choose $\alpha = \alpha_0$, $\eta = \frac{(1-\alpha_0)^2}{2}$, and $C = \frac{\lambda}{2\alpha_0^M}$, then the output of Algorithm~\ref{alg:ftnconstrSmthstrcvx}
is $H$-smooth and $\lambda$-strongly convex.

\subsection{Proof of Lemma~\ref{sublem:ftnconstrsscvx}}
\label{sec:proof-ftnconstrsscvxsublem}
We demonstrate in details the proof for Lemma~\ref{sublem:ftnconstrsscvx} below,
which is based on an induction argument.
\paragraph{Base case $t=2$.}
In the base case, recall the definitions that
\begin{align*}
	&\ftnuone (x) = \ftninuone(x) \defeq C\ltwosqr{x-u_1},~
	\ftninutwo (x) \defeq C \alpha \ltwosqr{x-u_{2}} + C\beta \delta_1^2,\\
	&\gutwo(x) \defeq \algmaxsmth(\ftninuone, \ftninutwo, \alpha, \eta, \delta_1)
\end{align*}
Apply Lemma~\ref{lem:smthconstrange} to $\ftninuone$ and $\ftninutwo$.
Note that in this case $s = C$, $t = 0$, and $\delta = \delta_1$.
Then, Lemma~\ref{sublem:ftnconstrsscvx}.\ref{itemsub:ftnconstrsscvx-1}--\ref{itemsub:ftnconstrsscvx-3} is immediately proved
by Lemma~\ref{lem:smthconstrange}.\ref{item:smthconstrange-7} and \ref{lem:smthconstrange}.\ref{item:smthconstrange-3}.

For Lemma~\ref{sublem:ftnconstrsscvx}.\ref{itemsub:ftnconstrsscvx-4}, we want to prove that
$\ftnutwo(x) = \max \{\ftnuone(x), \gutwo (x)\}$ is smooth.
By $\ftnuone (x) = \ftninuone(x)$, already $\gutwo (x) \geq \ftninuone(x) = \ftnuone (x)$.
Thus, $\ftnutwo(x) = \gutwo (x)$, and it is proven to be smooth by Lemma~\ref{lem:smthconstparams}.

\paragraph{Inductive case $2<t \leq M$.}
Recall the definitions that
\begin{align*}
	\ftninutmone(x) &\defeq C\alpha^{t-2} \ltwosqr{x-u_{t-1}} + C \beta \sum\nolimits_{m=1}^{t-2} \alpha^{m-1}\delta_m^2,\\
	\ftninut(x) &\defeq C\alpha^{t-1} \ltwosqr{x-u_{t}} + C \beta \sum\nolimits_{m=1}^{t-1} \alpha^{m-1}\delta_m^2,\\
	\gutmone(x) &\defeq \algmaxsmth(\ftninutmtwo, \ftninutmone, \alpha, \eta, \delta_{t-2}),\\
	\gut(x) &\defeq \algmaxsmth(\ftninutmone, \ftninut, \alpha, \eta, \delta_{t-1}),\\
	\ftnutmone(x) &\defeq \max \{ \ftnutmtwo (x), \gutmone(x) \},\\
	\ftnut(x) &\defeq \max \{ \ftnutmone (x), \gut(x) \}.
\end{align*}

Apply Lemma~\ref{lem:smthconstrange} to $\ftninutmone$ and $\ftninut$.
Note that in this case $s = C\alpha^{t-2}$, $t = C \beta \sum_{m=1}^{t-2} \alpha^{m-1}\delta_m^2$, and $\delta = \delta_{t-1}$.
By Lemma~\ref{lem:smthconstrange}.\ref{item:smthconstrange-7}--\ref{item:smthconstrange-3},
\begin{align}
	&\gut(x) = \ftninutmone(x) \quad \text{ for any } x \in \cl(\Btwotmone^c), \label{eq:smoothpf1}\\
	&\gut(x) = \ftninut(x) \quad \text{ for any } x \in \Btwotmoneh, \label{eq:smoothpf2}\\
	&C \beta \sum_{m=1}^{t-1} \alpha^{m-1}\delta_m^2
	\leq
	\gut(x)
	\leq
	C \beta \sum_{m=1}^{t-2} \alpha^{m-1}\delta_m^2 + C\alpha^{t-2} \delta_{t-1}^2
	\quad \text{ for any } x \in \Btwotmone, \label{eq:smoothpf3}\\
	&\nabla \gut(x) = \nabla \ftninutmone(x) \quad \text{ for any } x \in \cl(\Btwotmone^c).\label{eq:smoothpf4}
\end{align}

We will prove Lemma~\ref{sublem:ftnconstrsscvx}.\ref{itemsub:ftnconstrsscvx-1}--\ref{itemsub:ftnconstrsscvx-4} using this list of facts.
Note that $\ftnutmone(x) \geq \gutmone(x) \geq \ftninutmone(x)$ for all $x \in \domain$.
Together with Eq~\eqref{eq:smoothpf1}, this yields
\begin{equation}
\label{eq:sublemftnconstrsscvx1}
\ftnutmone(x) \geq \gut(x) \text{ for all } x \in \cl(\Btwotmone^c).
\end{equation}
For the other case, by using Lemma~\ref{sublem:ftnconstrsscvx}.\ref{itemsub:ftnconstrsscvx-1} for case $t-1$ (induction hypothesis), 
we have $\ftnutmtwo(x) \leq \gutmone(x) = \ftninutmone(x)$ for all $x \in \Btwotmtwoh$.
From the definition $\ftnutmone(x)\defeq \max \{ \ftnutmtwo (x), \gutmone(x) \}$, we have
$\ftnutmone(x) = \gutmone(x) = \ftninutmone(x)$ for $x \in \Btwotmtwoh$.
Note that $\Btwotmtwoh \supset \Btwotmone$, so 
\begin{equation}
\label{eq:sublemftnconstrsscvx2}
\ftnutmone(x) = \gutmone(x) = \ftninutmone(x) \text{ for all } x \in \Btwotmone.
\end{equation}
By Eq~\eqref{eq:smoothpf2}, $\gut(x) = \ftninut(x) \geq \ftninutmone(x)$ for any $x \in \Btwotmoneh$.
With Eq~\eqref{eq:sublemftnconstrsscvx2}, this proves 
\begin{equation*}
\ftnutmone(x) \leq \gut(x) = \ftninut(x) \text{ for } x \in \Btwotmoneh,
\end{equation*}
hence finishing the proof of Lemma~\ref{sublem:ftnconstrsscvx}.\ref{itemsub:ftnconstrsscvx-1}.

By Eq~\eqref{eq:sublemftnconstrsscvx2}, and that $\ftninutmone(x) \leq \gut(x)$,
we have $\ftnutmone(x) \leq \gut(x)$ for all $x \in \Btwotmone$.
Together with Eq~\eqref{eq:sublemftnconstrsscvx1}, this means 
\begin{equation}
\label{eq:sublemftnconstrsscvx3}
	\ftnut(x) \defeq \max \{ \ftnutmone (x), \gut(x) \}
	= \begin{cases}
		\gut(x) & \text{ if } \ltwo{x - u_{t-1}} \leq \delta_{t-1},\\
		\ftnutmone(x) & \text{ if } \ltwo{x - u_{t-1}} \geq \delta_{t-1}.
	\end{cases}
\end{equation}
Combining Eqs~\eqref{eq:smoothpf3} and \eqref{eq:sublemftnconstrsscvx3}, this proves Lemma~\ref{sublem:ftnconstrsscvx}.\ref{itemsub:ftnconstrsscvx-3}.

We now have Lemma~\ref{sublem:ftnconstrsscvx}.\ref{itemsub:ftnconstrsscvx-4} to prove.
If we look into Eq~\eqref{eq:sublemftnconstrsscvx3} more closely, we can see that 
$\ftnutmone(x) = \gut(x)$ if $\ltwo{x - u_{t-1}} = \delta_{t-1}$.
We know by induction hypothesis that $\ftnutmone(x)$ is smooth, and by Lemma~\ref{lem:smthconstparams} that $\gut(x)$ is also smooth.
So, the proof of smoothness of $\ftnut(x)$ suffices to check 
if $\nabla \ftnutmone(x) = \nabla \gut(x)$ for all $x$ such that $\ltwo{x - u_{t-1}} = \delta_{t-1}$.
From Eq~\eqref{eq:smoothpf4},
\begin{equation*}
\nabla \gut(x) = \nabla \ftninutmone(x) \text{ if } \ltwo{x - u_{t-1}} = \delta_{t-1}.
\end{equation*}
Also, $\ftnutmone(x)$ is a smooth function, meaning that 
$\nabla \ftnutmone(x) = \nabla \gutmone(x)$ whenever $\ftnutmone(x) = \gutmone(x)$.
Together with Eq~\eqref{eq:sublemftnconstrsscvx2}, we have 
\begin{equation*}
\nabla \ftnutmone(x) = \nabla \gutmone(x) = \nabla \ftninutmone(x) \text{ if } \ltwo{x - u_{t-1}} = \delta_{t-1}.
\end{equation*}
This shows $\nabla \ftnutmone(x) = \nabla \gut(x)$ whenever $x$ satisfies $\ltwo{x - u_{t-1}} = \delta_{t-1}$. 
So $\ftnut$ is smooth, hence Lemma~\ref{sublem:ftnconstrsscvx}.\ref{itemsub:ftnconstrsscvx-4} is shown.

\paragraph{Final Case.}
It is left to prove Lemma~\ref{sublem:ftnconstrsscvx}.\ref{itemsub:ftnconstrsscvx-5}--\ref{itemsub:ftnconstrsscvx-8}. 
Their proof can be done in a similar way as Lemma~\ref{sublem:ftnconstrsscvx}.\ref{itemsub:ftnconstrsscvx-1}--\ref{itemsub:ftnconstrsscvx-4} for the inductive cases, hence omitted.

