\documentclass[final,12pt]{colt2018} % Anonymized submission
% \documentclass{colt2017} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e
\usepackage{macros}

\usepackage{macros-lb}
\usepackage{csquotes}
\usepackage{psfrag,amsfonts,verbatim}
\usepackage[small,bf]{caption}
\usepackage{hyperref}
\usepackage{enumerate}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{algcompatible}
\usepackage{overpic}

\title[Minimax Bounds on Stochastic Batched Convex Optimization]{Minimax Bounds on Stochastic Batched Convex Optimization}
\usepackage{times}
 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % Two authors with the same address
  % \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
  %  \Name{Author Name2} \Email{xyz@sample.com}\\
  %  \addr Address}

 % Three or more authors with the same address:
 % \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
 %  \Name{Author Name2} \Email{an2@sample.com}\\
 %  \Name{Author Name3} \Email{an3@sample.com}\\
 %  \addr Address}


 % Authors with different addresses:
 \coltauthor{\Name{John Duchi} \Email{jduchi@stanford.edu}\\
 \Name{Feng Ruan} \Email{fengruan@stanford.edu}\\
 \addr Stanford University
 \AND
 \Name{Chulhee Yun} \Email{chulheey@mit.edu}\\
 \addr Massachusetts Institute of Technology
 }

\begin{document}

\maketitle 

\begin{abstract}
  We study the stochastic batched convex optimization problem, in which we
  use many \emph{parallel} observations to optimize a convex function given
  limited rounds of interaction.  In each of $M$ rounds, an algorithm may
  query for information at $n$ points, and after issuing all $n$ queries, it
  receives unbiased noisy function and/or (sub)gradient evaluations at the
  $n$ points.  After $M$ such rounds, the algorithm must output an
  estimator.  We provide lower and upper bounds on the performance of such
  batched convex optimization algorithms in zeroth and first-order settings
  for Lipschitz convex and smooth strongly convex functions.  Our rates of
  convergence (nearly) achieve the fully sequential rate once $M = O(d \log
  \log n)$, where $d$ is the problem dimension, but the rates may
  exponentially degrade as the dimension $d$ increases, in distinction from
  fully sequential settings.
\end{abstract}

\begin{keywords}
Stochastic convex optimization, batched optimization, parallel computing
\end{keywords}

\input{introduction}
\input{colt2018-prob-setting}
\input{big-ideas}
%\input{colt2018_const}
\input{colt2018_lb}
\input{colt2018_up_main}


\bibliography{bib}

\newpage
\appendix

%\input{colt2018_const_apdx}

\input{colt2018_lb_apdx}

\input{colt2018_up_apdx}

\end{document}
