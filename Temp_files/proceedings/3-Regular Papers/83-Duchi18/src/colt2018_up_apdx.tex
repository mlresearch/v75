\section{Proof of Theorem~\ref{thm:ubmain}: Roadmap}
\label{sec:ubmain-roadmap}
The proof of theorem~\ref{thm:ubmain} is fairly involved. Indeed, 
we divide the proof of Thoerem~\ref{thm:ubmain} into four parts: 
in Section~\ref{sec:proof-Smooth-fo}, we present the proof for the case 
where the function class $\ffamily = \ffamily_{H, \lambda}$ and the oracle is 
first order oracle, in Section~\ref{sec:proof-Smooth-zo}, we present the proof 
for the case where the function class $\ffamily = \ffamily_{H, \lambda}$ and the 
oracle is zeroth-order oracle, in Section~\ref{sec:proof-Lip-fo}, we present the 
proof for the case where the function class $\ffamily = \ffamily_L$ and the oracle is 
the first order oracle, and lastly, in Section~\ref{sec:proof-Lip-zo}, we present the 
proof for the case where the function class $\ffamily = \ffamily_L$ and the oracle 
is zeroth order oracle. Broadly speaking, for each of those four cases, we prove 
the corresponding upper bound via first introducing a concrete algorithm, and then 
showing that the algorithm achieves the upper bound in Theorem 
\ref{thm:ubmain} through careful theoretical justifications. 

\paragraph{Common Notations from Section~\ref{sec:proof-Smooth-fo} to
Section~\ref{sec:proof-Lip-zo}.} The following notations are useful throughout 
the proofs of the upper bounds from Section~\ref{sec:proof-Smooth-fo} to
Section~\ref{sec:proof-Lip-zo}. For any $c\in \R^d$, $r \in \R_+$ and $k \in \N$, 
we use $G(c, r, k)$ to denote the following grids in $\R^d$:
\begin{equation*}
G(c, r, k) \defeq \left\{c + \left(\frac{r}{k}i_1, \frac{r}{k}i_2, \ldots, \frac{r}{k}i_d\right)^\prime 
	\mid i_j \in \{-k, -(k-1),\ldots, k-1, k\}~\text{for all}~1\leq j\leq d\right\}.
\end{equation*}
Denote $\gamma: \R_+ \to \R$ to be the function $\gamma(x) \defeq x/\log(x)$, 
which we heavily use in the proof. 


\section{Smooth Function with First Order Oracle}
\label{sec:proof-Smooth-fo}
\subsection{Description of Algorithms}
In this section, we propose two generic algorithms: algorithm~\ref{alg:smoothcvxfo} 
parameterized by parameters $(c,r,k,T)\in \xdomain \times \R_+ \times \N \times \N$ 
and algorithm~\ref{alg:smoothcvxfoRrounds} that builds from algorithm~\ref{alg:smoothcvxfo}.  
We note here that, algorithm~\ref{alg:smoothcvxfoRrounds} in essence, builds 
from $M$ times of repeated calls of algorithm~\ref{alg:smoothcvxfo}.
As will be shown immediately in the later subsections, it turns out that the two algorithms 
with careful choice of parameters return the minimax estimator in single and multi rounds 
respectively.
\begin{algorithm}[htp]
\caption{Generic Routine for One Stage Smooth Functions $\ffamily_{H, \lambda}$ (First-order Oracle)} 
\begin{algorithmic}[1]  
\Statex Input: Prior knowledge on $\lambda, H \in \R_+$ satisfying $\lambda \leq H$ and
	the noise level $\sigma \in \R_+$. User specify the sampling center $c\in \xdomain$, 
	radius $r\in \R_+$, grid size parameter $k\in \N$, the sampling times $T \in \N$ 
	and the confidence level $\delta \in (0, 1)$.
\State Compute the grid points $G = G(c, r, k)$.
\State At each point $x\in G$, query the first oracle $T$ times and denote each 
	sample gradient value via $\{\what{\grad} f(x)^{(1)}, \what{\grad} f(x)^{(2)}, \ldots, 
		\what{\grad} f(x)^{(T)}\}$.
\State Compute the gradient estimate at each point $x \in G$ via 
	$\what{\grad } f(x) \defeq \frac{1}{T} \sum_{i=1}^{T} \what{\grad} f(x)^{(i)}$.
\State Define $r^S$ as $r^S \defeq \frac{4r}{k \lambda} \cdot 
	\left( 2\sigma \left(\log \frac{1}{\delta} + 3d\right)^\half +  H d^\half\right)$.
Compute the `candidate' set $S \in G$:
\begin{equation*}
S = \left\{x \in G: \left<\what{\grad} f(x), y - x\right>  + 
	\frac{\lambda}{2}\ltwo{y-x}^2  > 0 ~~\text{for all $y \in G$ 
	satisfying $\ltwo{y-x} \geq r^S$}\right\}.
\end{equation*}  
\State Find the center $x^{S} \in S$, defined by $
		x^{S} \defeq \argmin_{x\in S} \max_{y \in S} \norm{x-y}_2$.
\State Define $\hat{r}$ as $\hat{r} \defeq \frac{10r}{\lambda k}
		\left(\sigma \left(\log \frac{1}{\delta} + 3d\right)^\half + Hd^\half \right)$. 
	Define the following rectangular $W$ by: 
	\begin{equation*}
		W \defeq \left\{x\in \xdomain: \norm{x-x^{S}}_\infty 
			\leq \hat{r}\right\}
	\end{equation*}
\State Return the center $\hat{c} = x^S$, the radius $\hat{r}$ 
	and the estimate $\hat{x} \in W$, defined by 
	\begin{equation*}
		\hat{x} \defeq \argmax_{x\in W} |\{i \in \{1, 2, \ldots, d\}:|x_i - c_i| = r\}|, 
	\end{equation*}
	where for each $1\leq i\leq d$, $x_i, c_i$ denotes the $i$th coordinate of $x, c\in \R^d$.
\end{algorithmic}
\label{alg:smoothcvxfo}
\end{algorithm}

\begin{algorithm}[htp]
\caption{Generic Routine for Multi-stage Smooth Functions $\ffamily_{H, \lambda}$ (First-order Oracle)} 
\begin{algorithmic}[1]  
\Statex Input: Prior knowledge on $\lambda, H \in \R_+$ satisfying $\lambda \leq H$, 
	the noise level $\sigma \in \R_+$ and number of rounds $R \in \N_+$. 
	Initialization of parameters $(c_1, r_1, k_1, T_1) \in \R^d \times \R_+ \times \N \times \N$. 
	User specifies the confidence level $\delta \in (0, 1]$ and the updating rule used in line 
	(3) of the algorithm.
\FOR{$i = 1$ to $M$}
	\State Run algorithm~\ref{alg:smoothcvxfo} with input parameters $(c_i, r_i, k_i, T_i)$.
		Denote the output to be $\hat{c}_i$, radius $\hat{r}_i$ and estimate $\hat{x}_i$.
	\State Update $(c_{i+1}, r_{i+1}, k_{i+1}, T_{i+1})$. The updating 
		rule may take $\hat{c}_i$, $\hat{r}_i$ and $\hat{x}_i$ as input. 
\ENDFOR
\State \Return $\hat{x}_M$ as estimate of $x_f\opt$ and the radius $r_{M+1}$.
\end{algorithmic}
\label{alg:smoothcvxfoRrounds}
\end{algorithm} 
%
%\FOR{$i = 1$ to $M$}
%	\State Run algorithm~\ref{alg:smoothcvxfo} with input parameters $(c_i, r_i, k_i, T_i)$.
%		Denote the output to be $\hat{c}_i$, radius $\hat{r}_i$ and estimate $\hat{x}_i$.
%	\State Update $(c_{i+1}, r_{i+1}, k_{i+1}, T_{i+1})$. The updating 
%		rule may take $\hat{c}_i$, $\hat{r}_i$ and $\hat{x}_i$ as input. 
%\ENDFOR
%\State \Return $\hat{x}_M$ as estimate of $x_f\opt$ and the radius $r_{M+1}$.
%\end{algorithmic}
%\label{alg:smoothcvxfoRrounds}
%\end{algorithm}
\subsection{Analysis of Algorithm~\ref{alg:smoothcvxfo}: Single-Stage Analysis}
In this section, we analyze the single-stage algorithm~\ref{alg:smoothcvxfo}. 
For purpose of convenience in later discussion for multi-stage algorithm
\ref{alg:smoothcvxfoRrounds}, we slightly generalize the domain of interest. 
In fact, we consider the domain to be $\xdomain_{c, r} = \{x: \norm{x-c}_\infty \leq r\}$ 
parameterized by $c \in \R^d$ and $r\in \R_+$, and the goal of the algorithm 
is to find $x_{f, c, r}\opt$, the minimum of $f$ in $\xdomain_{c, r}$ so that the 
performance of the algorithm is measured by $\E[f(\hat{x}) - f(x_{f, c, r}\opt))]$.
Note that, if we substitute $c= \half \cdot \ones$ and $r = \half$ into the results 
below, it leads to corresponding results to the original domain $\xdomain = [0, 1]^d$.
%In this section, we show that, with careful choice of input parameters 
%$(c, r, k, T)$, algorithm~\ref{alg:smoothcvxfo} returns some 
%estimator $\hat{x}$ so that its associated risk $\risk$ is upper bounded 
%by $\tilde{O}(n^{-\frac{2}{d+2}})$. For purpose of convenience in later 
%discussion on upper bounds for multi-stage algorithm, in this section, 
%we slightly generalize the domain of interest by considering 
%$\xdomain_{c, r} = \{x: \norm{x-c}_\infty \leq r\}$ parameterized by $c \in \R^d$
%and $r\in \R$ (while noting that $c = \half \cdot \ones$ and $r = \half$ 
%correspond to the original domain $\xdomain = [0,1]^d$). In this sense, 
%the algorithm discussed in this section seeks to estimate $x_{f, c, r}\opt$, 
%the unique minimum of $f$ in the domain $\xdomain_{c, r}$, while the risk 
%of interest would be $\E[f(\hat{x}) - f(x_{f, c, r}\opt))]$. Finally, we note that, 
%substituting $c= \half \cdot \ones$ and $r = \half$ in the results gives the 
%corresponding result for the original domain 
%$\xdomain = [0, 1]^d$.
\begin{proposition}
\label{proposition:first-order-smooth-one-round}
Given any fix $c\in \R^d$ and $r \in (0, 1]$, suppose there exists $k \in \N$ 
satisfying 
\begin{align}
\label{eqn:existence-of-k-one-round-fo}
%k = \max \left\{k \in \N: 
(2k + 1)^d \ceil{2k^2 \log (2k+1)} \leq nr^2, ~
%\right\}
\text{and}~
k > \frac{10}{\lambda} \left(\sigma \left(\log \frac{1}{\delta} + 3d\right)^\half+Hd^\half\right).
\end{align}
Then, pick any $k$ satisfying Eq~\eqref{eqn:existence-of-k-one-round-fo} and 
set $T= \floor{\frac{n}{(2k+1)^d}}$. Denote $\hat{c}$, $\hat{r}$ and 
$\hat{x}$ to be the output from algorithm~\ref{alg:smoothcvxfo} when we input
$(c, k, T, r)$ as the input parameters. Then, we have, 
\begin{equation*}
\P\left(f(\hat{x}) - f(x_{f, c, r}\opt) \leq 2Hd\hat{r}^2 ~\text{and}~ 
	\norm{x_{f, c, r}\opt - \hat{c}}_\infty \leq \hat{r} \right)\geq 1-\delta.
\end{equation*}
\end{proposition}
\begin{remark}
Before we give the proof of proposition~\ref{proposition:first-order-smooth-one-round}, 
we give some high-level intuitions why the algorithm~\ref{alg:smoothcvxfo} \emph{should}
work. First of all, since the noise are all light-tailed random variables, it is expected 
that all gradient estimate $\hat{\grad} f(x)$ concentrates at the true gradient $\grad f(x)$,
so that they give useful information of the gradient value at all grid points in $G$. So 
the question becomes, how can we utiliize the noisy gradient information in $G$ to 
find the minimum $x_{f, c, r}\opt$, which can be characterized as the following: 
\begin{equation}
\label{eqn:optimality-condition}
\langle \grad f(x_{f, c, r}\opt), x - x_{f, c, r}\opt \rangle \geq 0~~\text{for all $x\in \xdomain_{c, r}$}.
\end{equation}
%First of all, by optimality condition, we know the following characterization 
%of the minimum $x_{f, c, r}\opt$: 
%\begin{equation}
%\label{eqn:optimality-condition}
%\langle \grad f({x_{f, c, r}\opt}), y - x_{f, c, r}\opt \rangle \geq 0~~\text{for all 
%	$\norm{y - c}_\infty \leq r$}.
%\end{equation}
Due to the construction of the grids, it is immediate that there always exists some 
point $\bar{x}$ belonging to the grid set $G$ such that $\bar{x}$ is close to 
$x_{f, c, r}\opt$ up to $\norm{\bar{x} - x_{f, c, r}\opt}_\infty \leq \frac{r}{k}$. 
A more careful analysis from lemma~\ref{lemma:simple-but-important} gives 
us a stronger but useful result: we can always find some $\bar{x} \in G$ such that
it satisfies below two equations simultaneously, 
\begin{equation}
\label{eqn:close-but-useful}
\norm{\bar{x} - x_{f, c, r}\opt}_\infty \leq \frac{r}{k}~~\text{and}~~
	\langle \grad f(x_{f, c, r}\opt), x - \bar{x}\rangle \geq 0~~\text{for all $x\in \xdomain_{c,r}$}.
\end{equation}
Basically, Eq~\eqref{eqn:close-but-useful} and Eq~\eqref{eqn:optimality-condition} tell
us that, there always exists some grid point $\bar{x}\in G$ close enough to $x_{f, c, r}\opt$
so that, it has similar property as $x_{f, c, r}\opt$. This motivates us to search for points
as $\bar{x} \in G$. One difficulty in searching for $\bar{x}$ is that its characterization 
from the second inequality of Eq~\eqref{eqn:close-but-useful}, as it requires knowledge 
of the value $\grad f(x_{f, c, r}\opt)$. Our strategy is to approximate the unknown gradient 
value $\grad f(x_{f, c, r}\opt)$ by $\hat{f}(\bar{x})$. However, due to such approximation,  
we need additional regularization to make the term to be non-negative. This motivates 
us to define the set $S$ in line 4 of the algorithm~\ref{alg:smoothcvxfo}. As will be made 
more precise in lemma~\ref{lemma:crucial-property}, we know that, with high probability, 
$\bar{x} \in S$ and all points in $S$ is close to $\bar{x}$ up to $O(k^{-1})$. This crucial 
observation also motivates the construction of our algorithm from line 5 to line 7. In line 
$5$, we first find the center of the set $S$, i.e, $x^S$ and then in line 6 construct the 
set $W$ centered at $x^S$ with an appropriate radius so that we can make sure 
both $\bar{x}$ and thus $x_{f, c, r}\opt$ are within the box $W$ with high probability. 
Finally, we carefully select $\hat{x}$ from the box $W$ in line $7$ to make sure the 
function value $f(\hat{x})$ is also close to the minimum $f(x_{f, c, r}\opt)$.


%\rfcomment{Should change things down there} 
%
%The key of the analysis of the algorithm is the following result, proven in 
%lemma~\ref{lemma:crucial-property}: with probability at 
%least $1- \delta$, $\bar{x}$ lies in $S$, and all points in $S$ is close to 
%$\bar{x}$ up to $O\left(\lambda^{-1} H \ltwo{\bar{x} - x_{f,c,r}\opt}\right)$. 
%The proof of this result relies on the second equation in Eq 
%\eqref{eqn:close-but-useful}. 
%%The intuition for the above fact is that, we can show that, for all $y \in G_1$, 
%%and $\ltwo{y - \bar{x}} \succeq \lambda^{-1} H \ltwo{\bar{x} - x_{f, c, r}\opt}$, 
%%we have, 
%%\begin{equation*}
%%\langle \grad f(\bar{x}), y - \bar{x}\rangle + \frac{\lambda}{2} \ltwo{y - \bar{x}}^2 > 0
%%	~~\text{and}~~
%%\langle \grad f(y), \bar{x} - y\rangle + \frac{\lambda}{2} \ltwo{y - \bar{x}}^2 < 0
%%\end{equation*}
%Now, the crucial observation above motivates the construction of our algorithm 
%in step 5-7. In line $5$, we first find the center of the set $S$, i.e, $x^S$
%and then in line 6 construct the set $W$ centered at $x^S$ with an appropriate 
%radius so that both $\bar{x}$ and $x_{f, c, r}\opt$ are within the box $W$ 
%with probability at least $1-\delta$. Finally, we carefully select $\hat{x}$ from the
%box $W$ in line $7$ in order to make sure the function value $f(\hat{x})$ is also 
%close to the minimum $f(x_{f, c, r}\opt)$.
\end{remark}

\begin{proof}
%As suggested in the previous remark, we need to show the following two key steps:
%\begin{enumerate}
%\item show the existence of $\bar{x}$ satisfying Eq~\eqref{eqn:close-but-useful} 
%\item with probability $1-\delta$, $S$ contains $\bar{x}$ and all points in $S$ is 
%close to $\bar{x}$. 
%\end{enumerate}
%We start by showing the first bullet point of above. 
%
%\rfcomment{Might change things up there}
We start proving the proposition by considering the following high probability event. 
Denote $\event$ to be the following event: 
\begin{equation*}
\event = \left\{\norm{\hat{\grad} f(x) - \grad f(x)}_2 \leq r^a \defeq 
	2\sigma\sqrt{\frac{2}{T}\left(\log \frac{1}{\delta} + 2d + d\log(2k+1) \right)}
		~\text{for all $x \in G$}\right\}.
\end{equation*} 
The lemma below shows that $\event$ happens with probability at least $1-\delta$.
\begin{lemma}
\label{lemma:high-prob-zo-one-round}
We have $\P(\Gamma) \geq 1-\delta$. 
\end{lemma}
\begin{proof}
First, let us for each $x\in G$, denote $\noise(x) \defeq \hat{\grad} f(x) - \grad f(x)$. 
Then, since by our assumption the noise vectors $\{\hat{\grad} f(x)^{(i)} - \grad f(x)\}_{i=1}^{T_1}$ 
are mean $\zeros$, independent and is subgaussian with parameter $\sigma^2$, 
we have that $\noise(x)$ is mean $\zeros$ and is subgaussian with parameter $\sigma^2/T$. 
Therefore, we have, for any fix $x\in G$, 
\begin{align*}
\P\left(\norm{\noise(x)}_2 \geq r^a \right) 
	\leq \exp(2d)\exp\left(-\frac{(r^a)^2 T}{8\sigma^2}\right) \leq \delta (2k+1)^{-d}, 
\end{align*}
where the last inequality above uses the definition of $r^a$. 
Now, the desired claim of the lemma follows from the fact that 
$|G| = (2k+1)^d$ and the union bound of the above events. 
\end{proof}
Note that, since our condition on $k$ in Eq~\eqref{eqn:existence-of-k-one-round-fo}, 
we get, $T \geq 2k^2 \log(2k+1) r^{-2}$, and hence, 
\begin{equation}
\label{eqn:r-a-upper-bound-fo-one-round}
r^a = 2\sigma\sqrt{\frac{2}{T}\left(\log \frac{1}{\delta} + 2d + d\log(2k+1) \right)} 
	\leq \frac{2\sigma r}{k} \left( \log \frac{1}{\delta} + 3d\right)^\half. 
\end{equation}


\begin{lemma}
\label{lemma:simple-but-important}
There exists some $\bar{x} \in G$ satisfying the below conditions: 
\begin{equation}
\label{eqn:condition-on-bar-x-fo}
\norm{\bar{x} - x_{f, c, r}\opt}_\infty \leq \frac{r}{k}~~\text{and}~~
	\left<\grad f(x_{f, c, r}\opt), \bar{x} - x_{f, c, r}\opt \right> = 0.
\end{equation}
Any $\bar{x}$ satisfying Eq~\eqref{eqn:condition-on-bar-x-fo} 
satisfies the crucial property below: 
\begin{equation}
\label{eqn:condition-on-bar-x-fo-ext}
\left<\grad f(x_{f, c, r}\opt), x - \bar{x}\right> \geq 0~\text{for all $x\in \xdomain_{c, r}$}.
\end{equation}
\end{lemma}
\begin{proof}
To start with, let us define the set $K\opt$ to be: 
\begin{equation*}
K\opt = \{i\in \{1, 2, \ldots, d\}: |(x_{f, c, r}\opt)_i - c_i| = r\},
\end{equation*}
where in above, $(x_{f, c, r}\opt)_i$ denotes the $i$th coordinate of $x_{f, c, r}\opt$. 
Now, using optimality condition of $x_{f, c, r}\opt$ (actually complementary slackness 
condition from the KKT characterization of $x_{f, c, r}\opt$), we know that, 
$(\grad f(x_{f, c, r}\opt))_i = 0$ for all $i\not\in K\opt$. Now, denote the following sets:
\begin{equation*}
R^{K\opt} = \{x\in \R^d: x_i = (x_{f, c, r}\opt)_i~\text{for all $i \in K\opt$}\},~
	G^{K\opt}  = G \cap R^{K\opt}~
		\text{and}~\xdomain^{K\opt} = \xdomain \cap R^{K\opt}.
\end{equation*}
Then $x_{f, c, r}\opt \in \xdomain^{K\opt}$. Now, denote 
$\bar{x} = \argmin_{x\in G^{K\opt}} \norm{x- x_{f, c, r}\opt}_\infty$.
Then, it is easy to see that, $\norm{\bar{x} - x_{f, c, r}\opt}_\infty \leq \frac{r}{k}$ since 
$G^{K\opt}$ forms a set of grid points of $\xdomain^{K\opt}$. Thus, we have,  
\begin{equation*}
\left<\grad f(x_{f, c, r}\opt), \bar{x} - x_{f, c, r}\opt\right> = 
	\sum_{i\in K\opt}(\grad f(x_{f, c, r}\opt))_i \underbrace{(\bar{x}_{i} - x\opt_{f, i})}_0
	+ \sum_{i\not\in K\opt}\underbrace{(\grad f(x_{f, c, r}\opt))_i}_0 (\bar{x}_{i} - x\opt_{f, i}) = 0.
\end{equation*}
Now, the above identity and the optimality condition of $x_{f, c, r}\opt$ in 
Eq~\eqref{eqn:optimality-condition} together imply that
\begin{equation*}
\left<\grad f(x_{f, c, r}\opt), x - \bar{x}\right> = 	
	\left<\grad f(x_{f, c, r}\opt), x - x_{f, c, r}\opt\right> \geq 0
	~~\text{for all $x\in \xdomain_{c, r}$}.
\end{equation*}
\end{proof}
% Next, we show the second bullet point of above. 



%
%We construct some event 
%$\Gamma$ such that not only $\P(\Gamma) \geq 1-\delta$, but also on 
%$\Gamma$, we get $S$ contains $\bar{x}$ and all points in $S$ is close 
%to $\bar{x}$. 
%
%The proof of proposition~\ref{proposition:first-order-smooth-one-round} consists 
%of two parts. In the first part, we define some `good' event $\event$ and show 
%that the event happens with probability at least $1-\delta$. In the second part, we 
%show by some deterministic arguments that on the `good' event $\event$, 
%the desired statements $f(\hat{x}) - f(x_{f, c, r}\opt) \leq 2Hd\hat{r_1}^2$ and 
%$x_{f, c, r}\opt \in W_1$ always hold. 
%These two pieces together give us the desired claim of the proposition. Now, we start 
%with the first part. Denote $G_1 = G(c_1, r_1, k_1)$, the grid points that we construct in 
%the first line of algorithm~\ref{alg:smoothcvxfo} for the input parameter $(c_1, r_1, k_1)$.
%Denote $a_1 = \frac{HR\sqrt{d}}{5k_1}$.  Consider the `good' event
%$\event$ defined by, 
%\begin{equation*}
%\event = \left\{\norm{\hat{\grad} f(x) - \grad f(x)}_2 \leq r\new ~\text{for all $x \in G_1$}\right\}, 
%\end{equation*}
%where $\hat{\grad} f(x)$ is defined in the third line of the algorithm~\ref{alg:smoothcvxfo}. 
%The lemma below shows that, the event $\event$ happens with probability at least $1-\delta$.  
%\begin{lemma}
%We have $\P(\Gamma) \geq 1-\delta$. 
%\end{lemma}
%\begin{proof}
%First, let us for each $x\in G_1$, denote $\noise(x) \defeq \hat{\grad} f(x) - \grad f(x)$. 
%Then, since by our assumption the noise vectors $\{\hat{\grad} f(x)^{(i)} - \grad f(x)\}_{i=1}^{T_1}$ 
%are mean $\zeros$, independent and has subgaussian coordinates with parameter $\sigma^2$, 
%we have that $\noise(x)$ is mean $\zeros$ and has subgaussian coordinate 
%with parameter $\sigma^2/T_1$. Therefore, we have, for any fix $x\in G_1$, 
%\begin{align*}
%\P\left(\norm{\noise(x)}_2 \geq a_1\right) 
%	\leq \P\left(\sqrt{d}\norm{\noise(x)}_\infty \geq a_1\right) 
%		\leq \sum_{i=1}^d \P\left(|\noise(x)_i| \geq \frac{a_1}{\sqrt{d}}\right)
%			\leq 2d\exp\left(-\frac{a_1^2T_1}{2d\sigma^2}\right), 
%\end{align*}
%where in the second inequality above, we use the subgaussianity of $(\noise(x))_i$, 
%the $i$th coordinate of $\noise(x)$. Now, by definition of $k_1$ and $T_1$, we have that, 
%$2d\exp\left(-\frac{a_1^2T_1}{2d\sigma^2}\right) \leq \frac{\delta}{(2k_1+1)^d}$. Now, 
%the desired claim of the lemma follows from the fact that $|G_1| = (2k_1+1)^d$ and 
%a probability union bound of the events $\{\norm{\noise(x)}_2 \geq a_1\}_{x\in G_1}$.
%\end{proof}
%
%Now, given $\event$ and the first part of the analysis, we are ready for the second 
%deterministic part of our analysis. We first point out the below simple but important 
%property of the grid points $G_1$. 
%\begin{lemma}
%\label{lemma:simple-but-important}
%There exists some $\bar{x} \in G_1$ satisfying the below conditions: 
%\begin{equation}
%\label{eqn:condition-on-bar-x-fo}
%\norm{\bar{x} - x_{f, c, r}\opt}_2 \leq \frac{R\sqrt{d}}{4k_1}
%	~~\text{and}~~
%		\left<\grad f(x_{f, c, r}\opt), \bar{x} - x_{f, c, r}\opt\right> = 0.
%\end{equation}
%\end{lemma}
%\begin{proof}
%To do so, let us define $K\opt = \{i\in \{1, 2, \ldots, d\}: (x_{f, c, r}\opt))_i \in \{ 0, 1\}\}$, 
%where we use $(x_{f, c, r}\opt))_i$ to denote the $i$th coordinate of $x_{f, c, r}\opt)$ for $1\leq i\leq d$. 
%Then, by optimality condition of $x_{f, c, r}\opt$, we have that, for all $i\not\in K\opt$, 
%$(\grad f(x_{f, c, r}\opt))_i = 0$, where we use $(\grad f(x_{f, c, r}\opt))_i$ to denote the $i$th 
%coordinate of $\grad f(x_{f, c, r}\opt)$. Now, consider the following set $G_1^{K\opt} 
%= G_1 \cap \{x\in \R^d: x_i = (x_{f, c, r}\opt)_i~\text{for all $i \in K\opt$}\}$ and 
%$\xdomain^{K\opt} = \xdomain \cap \{x\in \R^d: x_i = (x_{f, c, r}\opt)_i~\text{for all $i \in K\opt$}\}$. 
%Then $G_1^{K\opt}$ is the grid point of $\xdomain^{K\opt}$ and $x_{f, c, r}\opt \in \xdomain^{K\opt}$. 
%Find the point $x\in G_1^{K\opt}$ that is closest to $x_{f, c, r}\opt$ in $\ell_2$ distance. Then, 
%we have $\norm{\bar{x} - x_{f, c, r}\opt}_2 \leq \frac{R\sqrt{d}}{4k_1}$ by construction of $G_1$. 
%In addition, if we denote $\bar{x}_i$ the $i$th coordinate of $\bar{x}$ for $1\leq i\leq d$, 
%then we have, 
%\begin{equation*}
%\left<\grad f(x_{f, c, r}\opt), \bar{x} - x_{f, c, r}\opt\right> = 
%	\sum_{i\in K\opt}(\grad f(x_{f, c, r}\opt))_i \underbrace{(\bar{x}_{i} - x\opt_{f, i})}_0
%	+ \sum_{i\not\in K\opt}\underbrace{(\grad f(x_{f, c, r}\opt))_i}_0 (\bar{x}_{i} - x\opt_{f, i}) = 0.
%\end{equation*}
%\end{proof}
%Now, we are ready to show the second bullet. The lemma below provides the 
%critical property of the points $\bar{x}$.  

\newcommand{\exc}{^\text{ex}}
\begin{lemma}
\label{lemma:crucial-property}
Let $\bar{x}$ be any point in $\xdomain_{c, r}$ satisfying Eq~\eqref{eqn:condition-on-bar-x-fo}.
Denote $S$ to be the `candidate' set $S \subseteq G$ in the line 4 of algorithm 
\ref{alg:smoothcvxfo}. Then, on event $\event$, we have,
\begin{equation}
\label{eqn:crucial-property}
\bar{x} \in S~~\text{and}~~S \subseteq \left\{x\in \xdomain_{c, r}: 
	\ltwo{x - \bar{x}} \leq \frac{4r}{\lambda k} 
		\left(2\sigma \left(\log \frac{1}{\delta} + 3d\right)^\half + Hd^\half\right)\right\}. 
\end{equation}
% for any point $x \in G_1$ satisfying 
%$\ltwo{x-\bar{x}} \geq \frac{4}{\lambda}\left(r^a + 
%	\frac{Hr\sqrt{d}}{k}\right)$, the below two inequalities hold: 
%\begin{equation*}
%\left<\what{\grad} f(\bar{x}), x- \bar{x}\right>  + \frac{\lambda}{2}\ltwo{x-\bar{x}}^2 > 0 
%~~\text{and}~~
%\left<\what{\grad} f(x), \bar{x}- x\right>  + \frac{\lambda}{2}\ltwo{x-\bar{x}}^2 < 0.
%\end{equation*}
\end{lemma}
\begin{proof}
Throughout the proof, we assume event $\event$ happens. Note the upper bound 
on $r^a$ in Eq~\eqref{eqn:r-a-upper-bound-fo-one-round}, we know that, on event 
$\event$,  
\begin{equation}
\label{eqn:event-happens}
\sup_{x\in G} \norm{\what{\grad} f(x) - \grad f(x)}_2  \leq 
	\frac{2\sigma r}{k} \left(\log \frac{1}{\delta} + 3d\right)^\half.
\end{equation} 
To show the desired result, let us define $\xdomain_{c, r}\exc$ to be the following subset of $\xdomain$, 
\begin{equation*}
\xdomain_{c, r}\exc = \left\{x: \ltwo{x-\bar{x}} > \frac{4r}{\lambda k} 
		\left(2\sigma \left(\log \frac{1}{\delta} + 3d\right)^\half + Hd^\half\right)\right\}.
\end{equation*}
By definition of $S$, it suffices to show that, for all $x \in \xdomain_{c, r}\exc$, 
\begin{equation}
\label{eqn:goal-crucial-property}
\left<\what{\grad} f(\bar{x}), x- \bar{x}\right>  + \frac{\lambda}{2}\ltwo{x-\bar{x}}^2 > 0 
~~\text{and}~~
\left<\what{\grad} f(x), \bar{x}- x\right>  + \frac{\lambda}{2}\ltwo{x-\bar{x}}^2 < 0.
\end{equation}
To do so, let $\bar{x} \in \xdomain_{c, r}$ satisfying Eq~\eqref{eqn:condition-on-bar-x-fo}.
%For any point $x\in \xdomain_{c, r}$, we have, 
%\begin{equation*}
%\left<\grad f(x_{f, c, r}\opt), x - \bar{x}\right> = \left<\grad f(x_{f, c, r}\opt), x - x_{f, c, r}\opt\right> \geq 0, 
%\end{equation*}
%where in the first identity, we use the fact that $\bar{x}$ satisfies
%$\left<\grad f(x_{f, c, r}\opt), \bar{x} -x_{f, c, r}\opt\right> = 0$ and in the 
%second inequality, we use the optimality condition of $x_{f, c, r}\opt$. 
Then for any $x\in \xdomain_{c, r}$, using Eq~\eqref{eqn:condition-on-bar-x-fo-ext}, 
we get, 
\begin{equation}
\label{eqn:inequality-one}
\left<\grad f(\bar{x}), x- \bar{x}\right> %= 
	%\left<\grad f(\bar{x}) - \grad f(x_{f, c, r}\opt), x - \bar{x}\right> + 
	%	\left<\grad f(x_{f, c, r}\opt), x - \bar{x}\right> 
	\geq \left<\grad f(\bar{x}) - \grad f(x_{f, c, r}\opt), x - \bar{x}\right>.
\end{equation}
Now, note that $\bar{x}$ satisfies $\norm{\bar{x} - x_{f, c, r}\opt}_\infty 
\leq \frac{r}{k}$. Since the function $f$ is smooth, we get that, 
\begin{equation*}
\norm{\grad f(\bar{x}) - \grad f(x_{f, c, r}\opt)}_2 \norm{x-\bar{x}}_2 
\leq H \ltwo{\bar{x} - x_{f, c, r}\opt} \leq H\sqrt{d}\norm{\bar{x} - x_{f, c, r}\opt}_\infty
	\leq \frac{Hr\sqrt{d}}{k}.
\end{equation*} 
Hence, by Cauchy Schwartz inequality, we further get, 
\begin{equation}
\label{eqn:inequality-two}  
\left<\grad f(\bar{x}) - \grad f(x_{f, c, r}\opt), x - \bar{x}\right> \geq - 
	\norm{\grad f(\bar{x}) - \grad f(x_{f, c, r}\opt)}_2 \norm{x-\bar{x}}_2
	\geq -\frac{Hr\sqrt{d}}{k}\norm{x- \bar{x}}_2.
\end{equation}
Noticing the definition of $\xdomain_{c, r}\exc$, Eq~\eqref{eqn:inequality-one} and 
Eq~\eqref{eqn:inequality-two} together imply that, for all $x\in \xdomain_{c, r}\exc$, 
\begin{equation}
\label{eqn:inequality-three}
\left<\grad f(\bar{x}), x- \bar{x}\right>  + \frac{\lambda}{4}\ltwo{x-\bar{x}}^2 
	\geq -\frac{Hr\sqrt{d}}{k}\norm{x- \bar{x}}_2 + \frac{\lambda}{4}\norm{x-\bar{x}}_2^2 \geq 0.
\end{equation}
Now, using Cauchy Schwartz inequality, we get that, for all $x\in \xdomain_{c, r}\exc$, 
\begin{equation}
\label{eqn:inequality-five}
\left<\what{\grad f}(\bar{x}) - \grad f(\bar{x}), x- \bar{x}\right> + \frac{\lambda}{4}\norm{x-\bar{x}}_2^2 
	\geq -\norm{\what{\grad f}(\bar{x}) - \grad f(\bar{x})}_2\norm{ x- \bar{x}}_2	
		+ \frac{\lambda}{4}\norm{x-\bar{x}}_2^2 > 0
\end{equation}
Now, inequality~\eqref{eqn:inequality-three} and~\eqref{eqn:inequality-five} gives that
for all $x\in \xdomain_{c, r}\exc$, 
\begin{align}
&\left<\what{\grad }f(\bar{x}), x- \bar{x}\right>  + \frac{\lambda}{2}\ltwo{x-\bar{x}}^2 \nonumber \\
&= \left(\left< \grad f(\bar{x}), x- \bar{x}\right>  + \frac{\lambda}{4}\ltwo{x-\bar{x}}^2 \right)
	+ \left( \left<\what{\grad}f(\bar{x}) - \grad f(\bar{x}), x- \bar{x}\right> 
		+ \frac{\lambda}{4}\ltwo{x-\bar{x}}^2\right) > 0 
	\label{eqn:first-part-of-S}
\end{align}
This gives the first part of Eq~\eqref{eqn:goal-crucial-property}. Now, since the function 
$f(\cdot)$ is $\lambda$ strongly convex, for all $x\in \xdomain_{c, r}$, we have, 
\begin{equation*}
\left<\grad f(\bar{x}) - \grad f(x), \bar{x}-x\right> \geq \lambda \norm{x-\bar{x}}_2^2.
\end{equation*}
Together with Eq~\eqref{eqn:inequality-three}, it shows that, for all $x\in \xdomain_{c, r}\exc$, 
\begin{equation}
\label{eqn:inequality-four}
\left<\grad f(x), \bar{x} - x\right> + \frac{3\lambda}{4}\ltwo{x-\bar{x}}^2 \leq 
	\lambda \norm{x-\bar{x}}_2^2 - \left<\grad f(\bar{x}) - \grad f(x), \bar{x}-x\right> \leq 0.
\end{equation}
Now, using again by Cauchy Schwartz inequality,  we get for all $x\in\xdomain_{c,r }\exc$,
\begin{equation}
\left<\what{\grad }f(\bar{x}) - \grad f(\bar{x}), x- \bar{x}\right> 
		- \frac{\lambda}{4}\ltwo{x-\bar{x}}^2 
	\leq \ltwo{\what{\grad }f(\bar{x}) - \grad f(\bar{x})}\ltwo{ x- \bar{x}} 
		- \frac{\lambda}{4}\ltwo{x-\bar{x}}^2  < 0.
\end{equation}
Thus, for all  $x\in \xdomain_{c, r}\exc$,  the inequality below holds
\begin{align*}
&\left<\what{\grad} f(x), \bar{x}- x\right>  + \frac{\lambda}{2}\ltwo{x-\bar{x}}^2 \\
&= \left(\left< \grad f(\bar{x}), x- \bar{x}\right>  + \frac{3\lambda}{4}\ltwo{x-\bar{x}}^2 \right)
	+ \left( \left<\what{\grad} f(\bar{x}) - \grad f(\bar{x}), x- \bar{x}\right> 
		- \frac{\lambda}{4}\ltwo{x-\bar{x}}^2\right) < 0. 
\end{align*}
This gives the second part of Eq~\eqref{eqn:goal-crucial-property}. As discussed 
previously, this shows that $\bar{x} \in S$. 
\end{proof}
The lemma above has a lot of nice implications. Indeed, denote $W$ to be the 
set that we construct in the $6$th line of the algorithm. In fact, using Eq
\eqref{eqn:condition-on-bar-x-fo} and Eq~\eqref{eqn:crucial-property}, on 
event $\event$, the distance between $x^S$ and $x_{f, c, r}\opt$ can be upper 
bounded by, 
%Now, let us denote $S$ to be the `candidate' set $S \in G$ we construct 
%in the fourth line of algorithm~\ref{alg:smoothcvxfo}. The above lemma 
%suggests that, on $\event$, 
%\begin{equation*}
%\bar{x} \in S~~\text{and}~~S \subseteq \left\{x \in \xdomain_{c, r}: \ltwo{x-\bar{x}} 
%	\leq \frac{4}{\lambda}\left(r^a + \frac{Hr\sqrt{d}}{k}\right)\right\}.
%\end{equation*}
%Hence, in particular, we know that, the distance 
\begin{equation*}
\ltwo{x^S - x_{f, c, r}\opt} \leq \ltwo{x^S - \bar{x}} + \ltwo{\bar{x} - x_{f, c, r}\opt}
	\leq  \frac{10r}{\lambda k}\left(\sigma \left(\log \frac{1}{\delta} + 3d\right)^\half 
		+ Hd^\half \right) = \hat{r}.
\end{equation*}
%Hence, this shows that, on event $\event$, 
%\begin{equation*}
%\ltwo{x^S - x_{f, c, r}\opt} \leq \frac{5}{\lambda}\left(r^a + \frac{Hrd^\half}{k}\right)
%	\leq \frac{10r}{\lambda k}\left(\sigma \left(\log \frac{1}{\delta} + 3d\right)^\half 
%		+ Hd^\half \right) = \hat{r}, 
%\end{equation*}
By definition, this means that $x_{f, c, r}\opt \in W$ on event $\event$. 
% meaning that $x_{f, c, r}\opt \in W$, where 
% $W$ is the set we construct in the $6$th line of the algorithm.  
Finally, we show that, our careful choice of $\hat{x} \in W$ makes $f(\hat{x})$ 
is close to $f(x_{f, r, c}\opt)$ close up to $O((\hat{r})^2)$. 
\begin{lemma}
\label{lemma:careful-choice-of-estimator}
Assume that $k$ satisfies Eq~\eqref{eqn:existence-of-k-one-round-fo}. 
Then, $\hat{r} < r$, and on event $\event$, 
$\hat{x}$ satisfies, 
\begin{equation*}
f(\hat{x}) - f(x_{f, c, r}\opt) \leq 2 Hd \hat{r}^2.
\end{equation*}
\end{lemma}
\begin{proof}
We prove the desired inequality by showing the following crucial property of 
$\hat{x}$: 
\begin{equation}
\label{eqn:crucial-property-hat-x}
\left<\grad f(x_{f, c, r}\opt), \hat{x} - x_{f, c, r}\opt\right> = 0
	~~\text{and}~~\norm{\hat{x} - x_{f, c, r}\opt}_2\leq 2\sqrt{d} \hat{r}.
\end{equation}
Given above equation, the desired inequality follows, since by smoothness of $f$,  
\begin{equation*}
f(\hat{x}) - f(x_{f, c, r}\opt) \leq 
	\left<\grad f(x_{f, c, r}\opt), \hat{x} - x_{f, c, r}\opt\right> 
		+ \frac{H}{2} \ltwo{\hat{x} - x_{f, c, r}\opt}^2 
	\leq  2 Hd \hat{r}^2, 
\end{equation*}
The rest of the proof is thus devoted to proving Eq~\eqref{eqn:crucial-property-hat-x}.
Note that, the second inequality of Eq~\eqref{eqn:crucial-property-hat-x} follows 
easily since on $\event$, $x_{f, c, r}\opt \in W$ and the diameter of $W$ is exactly 
$2\sqrt{d} \hat{r}$. Now, we prove the first equality in Eq 
\eqref{eqn:crucial-property-hat-x}. To do so, denote $K\opt$ and $\hat{K}$
respectively as follows: 
\begin{equation*}
K\opt = \{i\in \{1, 2, \ldots, d\}: |(x_{f, c, r}\opt)_i - c_i| = r\}~~\text{and}~~
\hat{K} = \{i\in \{1, 2, \ldots, d\}: |\hat{x}_i - c_i| = r\}.
\end{equation*}
Now, we show $K\opt \subseteq \hat{K}$. Suppose on the contrary, then, 
consider some $x$ such that $x_i = \hat{x}_{i}$ for $i\in K_1 \cup (K\opt)^c$ 
and $x_i = (x_{f, c, r}\opt)_i$ for $i\in K\opt \setminus K_1$. Then $x\in W$ 
and the set $K_x$ defined below, 
\begin{equation*}
K_x \defeq \{i\in \{1, 2, \ldots, d\} : |x_i - c_i| = r \} \supseteq K\opt \cup \hat{K},
\end{equation*}
and hence $K_x$ contains $K$. This contradicts the definition of $\hat{x}$. Thus, we have, 
$K\opt \subseteq \hat{K}$. Now, the optimality condition of $x\opt$ gives
that $(\grad f(x_{f, c, r}\opt))_i = 0$ for all $i\not\in K\opt$. In addition, if 
$k$ satisfies Eq~\eqref{eqn:existence-of-k-one-round-fo}, then $\hat{r} < r$. 
Since both $x_{f, c, r}\opt$ and $\hat{x}$ belong to $W$, we know that, 
$(x_{f, c, r}\opt)_i = \hat{x}_{i}$ for all $i\in K\opt$. Together, it gives the first
part of Eq~\eqref{eqn:crucial-property-hat-x}, as
\begin{equation*}
\left<\grad f(x_{f, c, r}\opt), \hat{x} - x_{f, c, r}\opt\right> = 
	\sum_{i\in K\opt}(\grad f(x_{f, c, r}\opt))_i \underbrace{(\hat{x}_{i} - (x_{f, c, r}\opt)_i)}_0
	+ \sum_{i\not\in K\opt}\underbrace{(\grad f(x_{f, c, r}\opt))_i}_0 (\hat{x}_{i} - (x_{f, c, r}\opt)_i) = 0.
\end{equation*}
\end{proof}
%Then, the above lemma shows that, 
%on good event $\event$, for any $\bar{x}\in G_1$ satisfying Eq~\eqref{eqn:condition-on-bar-x-fo}, 
%we have $\bar{x} \in S_1$. In addition, for any fix $\bar{x}\in G_1$ satisfying 
%Eq~\eqref{eqn:condition-on-bar-x-fo}, the lemma also shows that, any other point in 
%$S_1$ can be at most $\frac{HR\sqrt{d}}{k_1\lambda}$ away from $\bar{x}$ in $\ell_2$ 
%distance, and hence can be at most $\frac{5HR\sqrt{d}}{4k_1\lambda}$ 
%away from $x_{f, c, r}\opt$ in $\ell_2$ distance. This shows that, if we denote $x^{S_1}$ to be the 
%center that we construct in the fifth line of algorithm~\ref{alg:smoothcvxfo} and $W_1$ to be 
%the set as we construct in the six line of algorithm~\ref{alg:smoothcvxfo}, then we have, 
%on good event $\event$, $\norm{x^{S_1} - x_{f, c, r}\opt}_2 \leq \frac{5HR\sqrt{d}}{4k_1\lambda}$ 
%and thus $x_{f, c, r}\opt \in W_1$. Note here now, any point in $W_1$ is now at most 
%$\frac{4HRd}{k_1\lambda}$ away from $x_{f, c, r}\opt$ in $\ell_2$ distance, as the diameter of 
%the rectangular $W_1$ is at most $\frac{4HRd}{k_1\lambda}$. Now, we show that, 
%our choice of $\hat{x}_1\in W_1$ satisfies an additional property below so that it can 
%also guarantee a nice upper bound on the difference between $f(\hat{x}_1) - f(x_{f, c, r}\opt))$: 
%\begin{lemma}
%\label{lemma:careful-choice-of-estimator}
%Suppose $\frac{4Hd}{k_1 \lambda} < 1$. Then, we have, on event $\event$, 
%$\hat{x}_1$ satisfies, 
%\begin{equation*}
%\left<\grad f(x_{f, c, r}\opt), \hat{x}_1 - x_{f, c, r}\opt\right> = 0
%	~~\text{and}~~\norm{\hat{x}_1 - x_{f, c, r}\opt}_2\leq \frac{4HRd}{k_1\lambda} = 
%		2\hat{r}_1\sqrt{d}.
%\end{equation*}
%\end{lemma}
%\begin{proof}
%On event $\event$, we have $x_{f, c, r}\opt \in W_1$ and hence since $\hat{x}_1\in W_1$, 
%the second claim of the lemma follows by the fact that the diameter of $W_1$ is 
%at most $\frac{4HRd}{k_1\lambda}$. To show the first claim, let us denote 
%$K_1 = \{i\in \{1, 2, \ldots, d\}: \hat{x}_{1,i} \in \{ 0, R\}\}$ where we use 
%$\hat{x}_{1,i}$ to denote the $i$th coordinate of $\hat{x}_1$. Now, we show that, 
%if we correspondingly define $K\opt = \{i\in \{1, 2, \ldots, d\}: x_{f, i}\opt \in \{0, R\}\}$ 
%where $x_{f, i}\opt$ denotes the $i$th coordinate of $x_{f, c, r}\opt$, then we have 
%$K\opt \subset K_1$. Suppose on the contrary that this is not true, then, consider 
%some $x$ such that $x_i = \hat{x}_{1, i}$ for $i\in K_1 \cup (K\opt)^c$ and 
%$x_i = (x_{f, c, r}\opt))_i$ for $i\in K\opt \setminus K_1$. Then $x\in W_1$ and the set 
%$\{i\in \{1, 2, \ldots, d\}, x_1 \in \{0, R\}\} = K\opt \cup K_1$, which strictly contains 
%$K_1$. This contradicts the definition of $\hat{x}_1$. Thus, we have, $K\opt \subset K_1$. 
%Now, use optimality condition of $x\opt$, we know that, $(\grad f(x_{f, c, r}\opt))_i = 0$ 
%for all $i\not\in K\opt$. In addition, if $\frac{4Hd}{k_1\lambda} < 1$, then it implies 
%that, $x\opt_{f, i} = \hat{x}_{1, i}$ for all $i\in K\opt$. Together, it shows that, 
%\begin{equation*}
%\left<\grad f(x_{f, c, r}\opt), \hat{x}_1 - x_{f, c, r}\opt\right> = 
%	\sum_{i\in K\opt}(\grad f(x_{f, c, r}\opt))_i \underbrace{(\hat{x}_{1,i} - x\opt_{f, i})}_0
%	+ \sum_{i\not\in K\opt}\underbrace{(\grad f(x_{f, c, r}\opt))_i}_0 (\hat{x}_{1,i} - x\opt_{f, i}) = 0.
%\end{equation*}
%\end{proof}
%Note that, the above lemma also implies that $\hat{x}_1$ satisfies the below 
%inequality,  
%\begin{equation*}
%f(\hat{x}_1) - f(x_{f, c, r}\opt) \leq 
%	\left<\grad f(x_{f, c, r}\opt), \hat{x}_1 - x_{f, c, r}\opt\right> + \frac{H}{2} \norm{\hat{x}_1 - x_{f, c, r}\opt}^2 
%	\leq 2Hd \hat{r}_1^2, 
%\end{equation*}
%where in the first inequality of above, we use the fact that $\grad f$ is $H$-Lipschitz 
%in $\ell_2$ norm. The desired claim of the proposition is thus proved. 
The desired claim of the proposition now follows easily from Lemma
\ref{lemma:high-prob-zo-one-round} and Lemma~\ref{lemma:careful-choice-of-estimator}.
\end{proof}

Motivated by Proposition~\ref{proposition:first-order-smooth-one-round}, 
it becomes important to understand when such $k$ exists in Eq 
\eqref{eqn:existence-of-k-one-round-fo} and how large it is. 

\begin{lemma}
\label{lemma:ef-first-order-smooth-one-round}
Assume $n$ is large enough satisfying 
\begin{equation}
\label{eqn:ef-n-condition-fo-smooth-one-round}
nr^2 \geq (6B)^{2(d+2)},~~\text{where}~
	B =  \frac{10}{\lambda} \left(\sigma \left(\log \frac{1}{\delta} + 3d\right)^\half+Hd^\half\right).
\end{equation}
Denote $k(r)= \left(\gamma\left(nr^2\right)\right)^\frac{1}{d+2}$
and $k\opt = \floor{\frac{1}{3} k(r)}$. Then $k\opt\in \N$,
and $k\opt$ satisfies Eq~\eqref{eqn:existence-of-k-one-round-fo}.
\end{lemma}

\begin{proof}
Note that, $\gamma(x) \geq \sqrt{x}$ whenever $x \geq 3$. 
Thus, by our assumption on $n$, we get that, 
\begin{equation*}
\left(\gamma(nr^2)\right)^\frac{1}{d+2} \geq 6B \geq 6. 
\end{equation*}
This immediately gives us that $k\opt \geq 1$ and $k\opt$ satisfies the second 
inequality of Eq~\eqref{eqn:existence-of-k-one-round-fo}. Now, we 
show that $k\opt$ satisfies the first inequality of Eq~\eqref{eqn:existence-of-k-one-round-fo}.
In fact, when $k=k\opt$, we have, 
\begin{equation*}
(2k+1)^d \ceil{2k^2 \log(2k+1)} \leq (3k)^{d+2} \log (3k)^{d+2} \leq
	 (k(r))^{d+2} \log (k(r))^{d+2} \leq nr^2,
\end{equation*}
where the last inequality follows from the fact that, for any $x > 0$, 
$\gamma(x) \log \gamma(x) \leq x$.
\end{proof}
Proposition~\ref{proposition:first-order-smooth-one-round} and 
Lemma~\ref{lemma:ef-first-order-smooth-one-round} together 
immediately give the corollary below. 

\begin{corollary}
\label{corollary:first-order-smooth-one-round}
Given any fix $c\in \R^d$ and $r \in [0, 1]$, set 
$k = \floor{\frac{1}{3} \left(\gamma\left(nr^2\right)\right)^\frac{1}{d+2}}$,
and $T = \floor{\frac{n}{(2k+1)^d}}$. Assume $n$ is large enough 
satisfying Eq~\eqref{eqn:ef-n-condition-fo-smooth-one-round}. Then 
if we denote $\hat{c}, \hat{r}$ and $\hat{x}$ to be the output of Algorithm
\ref{alg:smoothcvxfo} when we input $(c, k, T, r)$ as the input parameters, 
we have, 
\begin{equation*}
\hat{r} \leq \min\{r, 6B r^\frac{d}{d+2} n^{-\frac{1}{d+2}} \log (nr^2)^\frac{1}{d+2}\},
~~\text{where}~
	B =  \frac{10}{\lambda} \left(\sigma \left(\log \frac{1}{\delta} + 3d\right)^\half+Hd^\half\right).
\end{equation*}
In addition, we get that, 
\begin{equation*}
\P\left(f(\hat{x}) - f(x-{f, c, r}\opt) \leq \gap\opt \right) \geq 1-\delta, 
\end{equation*}
where 
\begin{equation*}
\gap\opt = 2 Hd \hat{r}^2 \leq 2 Hd  (6B)^2 
	r^\frac{2d}{d+2} n^{-\frac{2}{d+2}} \log (nr^2)^\frac{2}{d+2}.
\end{equation*}
\end{corollary}

%\begin{proof}
%The bound on $\hat{r}$ follows easily from the fact that, 
%\begin{equation*}
%\hat{r} = \frac{Br}{k} \leq \frac{12B r}{\gamma(nr^2)^\frac{1}{d+2}}
%\end{equation*}
%\end{proof}

%
%Now, given Proposition~\ref{proposition:first-order-smooth-one-round}, 
%it is important to know whether there exists some $k\in \N$ satisfying 
%Eq~\eqref{eqn:existence-of-k-one-round-fo}. For this purpose, we introduce 
%the following auxiliary function $r \to k(r)$: 
%\begin{equation}
%\label{eqn:definition-of-k-r-smooth-fo}
%k(r) = \frac{1}{6} \left(\frac{nH^2r^2}{\max\left\{\sigma^2 
%	\log \frac{2d}{\delta}, \sigma^2 \log \frac{nH^2r^2}{\sigma^2}, H^2 r^2\right\}} \right)^\frac{1}{d+2}.
%\end{equation}
%
%\begin{corollary}
%\label{corollary:first-order-smooth-one-round}
%Suppose $n$ is large enough so that 
%\begin{equation*}
%nH^2 r^2 \geq 
%	\left(\sigma^2\left(\log \frac{2d}{\delta} + 1\right) + H^2 r^2\right)
%		\left(\frac{30Hd}{\lambda}\right)^{2(d+2)}
%%		~\text{and}~
%%			n \geq 2^\frac{d+2}{2}
%%				\left(\frac{H^2R^2}{25\sigma^2 \log \frac{2d}{\delta}}\right)^\frac{d}{2}
%\end{equation*} 
%then $k\opt = \ceil{k(r)}$ satisfies Eq~\eqref{eqn:existence-of-k-one-round-fo}.
%Moreover, we have,  
%\begin{equation*}
%\P\left(f(\hat{x}) - f(x_{f, c, r}\opt) \leq \gap\opt \right)\geq 1-\delta, 
%\end{equation*}
%where, $\gap\opt$ is defined by, 
%\begin{equation*}
%\gap\opt = \frac{25 H^3 r^2 d^2}{2(k\opt)^2 \lambda^2} \leq
%	\frac{450 H^3 r^2  d^2}{\lambda^2}
% \left(\frac{\max\left\{\sigma^2 \log \frac{2d}{\delta}, 
% 	\sigma^2 \log \frac{nH^2r^2}{\sigma^2}, H^2 r^2\right\}}{nH^2r^2} 
%		\right)^\frac{2}{d+2} 
%\end{equation*}
%\end{corollary}
%\begin{proof}
%Note that, the last claim of the lemma follows immediately from 
%proposition~\ref{proposition:first-order-smooth-one-round}. Now, we show 
%that $k\opt \geq \frac{4Hd}{\lambda} \geq 1$ under our 
%assumptions. Note that, by elementary inequality that $x/\log(x) \geq \sqrt{x}$ 
%for all $x\geq 3$,  our first assumption on $n$ gives us that, 
%\begin{equation*}
%\frac{nH^2r^2}{\sigma^2 \log \frac{nH^2r^2}{\sigma^2}} 
%	\geq \left(\frac{nH^2r^2}{\sigma^2}\right)^\half
%		\geq \left(\frac{30Hd}{\lambda}\right)^{d+2}, ~
%			\frac{nH^2r^2}{\sigma^2 \log \frac{2d}{\delta}} \geq 
%				\left(\frac{30Hd}{\lambda}\right)^{d+2}
%	~~\text{and}~~ \frac{nH^2r^2}{H^2 r^2} \geq  
%		\left(\frac{30Hd}{\lambda}\right)^{d+2}
%\end{equation*}
%which immediately leads to the elementary fact, 
%\begin{equation*}
%\left(\frac{nH^2r^2}{\max\left\{\sigma^2 
%	\log \frac{2d}{\delta}, \sigma^2 \log \frac{nH^2r^2}{\sigma^2}, 
%		H^2 r^2\right\}}\right)^{\frac{1}{d+2}}
%\geq \frac{30Hd}{\lambda}.
%\end{equation*}
%Hence, we get that, $k\opt \geq k(r) \geq \frac{5Hd}{\lambda} \geq 5$ by definition of 
%$k\opt$. Now, we show that $k \geq k\opt$. Indeed, it suffices to show that, 
%$k = k\opt$ satisfies below three inequalities, 
%\begin{equation}
%\label{eqn:inequality-k-one}
%2(2k+1)^d \leq n,~
%	(2k+1)^d \log \frac{2d}{\delta}\leq \frac{nH^2r^2}{8\sigma^2k^2} 
%		~\text{and}~
%			(2k+1)^d \log (2k+1)^d \leq \frac{nH^2r^2}{8\sigma^2k^2}. 
%\end{equation}
%In fact, if the above three inequalities hold for $k = k\opt$, then we have, when 
%$k = k\opt$, 
%\begin{align*}
%&(2k+1)^d \ceil{\frac{2\sigma^2k^2}{H^2r^2} \log \frac{2d(2k+1)^d}{\delta}} 
%\leq (2k+1)^d \left(1+\frac{2\sigma^2k^2}{H^2r^2} \log \frac{2d(2k+1)^d}{\delta}\right) \\
%&=  \underbrace{(2k+1)^d}_{\leq \frac{n}{2}}
% + \underbrace{(2k+1)^d \frac{2\sigma^2k^2}{H^2r^2} \log \frac{2d}{\delta}}_{\leq \frac{n}{4}}
% + \underbrace{(2k+1)^d \frac{2\sigma^2k^2}{H^2r^2}\log (2k+1)^d}_{\leq \frac{n}{4}}\leq n, 
%\end{align*}
%and hence $k$ by its definition must satisfy $k \geq k\opt$. 
%We first show $k\opt$ satisfies the first inequality in 
%Eq~\eqref{eqn:inequality-k-one}. Indeed, by definition of $k(r)$,
%one can easily see that, $k(r) \leq \frac{1}{6} n^{\frac{1}{d+2}}$. Hence, 
%\begin{equation*}
%2(2k\opt+1)^d \leq (6k(r))^{d+2} \leq n.
%\end{equation*}
%%In fact, notice that,
%%our second assumption on $n$ gives us that, 
%%\begin{equation*}
%%n \geq 2^\frac{d+2}{2}\left(\frac{H^2R^2}{25\sigma^2 \log \frac{2d}{\delta}}\right)^\frac{d}{2} 
%%~\Leftrightarrow \left(\frac{nH^2R^2}{25\sigma^2 
%%	\log \frac{2d}{\delta}}\right)^{\frac{d}{d+2}} \leq \frac{n}{2}.
%%\end{equation*}
%%Therefore, we get that, for $k = k_1\opt$, we have, 
%%\begin{equation*}
%%2(2k+1)^d \leq 2(3k)^d \leq \left(\frac{nH^2R^2}{25\sigma^2 
%%	\log \frac{2d}{\delta}}\right)^{\frac{d}{d+2}} \leq \frac{n}{2}, 
%%\end{equation*}
%%which proves the desired first inequality in Eq~\eqref{eqn:inequality-k-one}.
%Next, we show that $k = k\opt$ satisfies the second inequality in 
%Eq~\eqref{eqn:inequality-k-one}. Again, by definition of $k(r)$, we have, 
%$k(r) \leq \frac{1}{6}\left(\frac{nH^2r^2}{\sigma^2 \log \frac{2d}{\delta}}\right)^{\frac{1}{d+2}}$.
%Hence, we have, 
%\begin{equation*}
%(2k\opt+1)^d (k\opt)^2 \log \frac{2d}{\delta}\leq \frac{1}{9} 
%	(6k(r))^{d+2} \log \frac{2d}{\delta} 
%	\leq \frac{nH^2r^2}{8\sigma^2}, 
%\end{equation*}
%which gives the second inequality in  Eq~\eqref{eqn:inequality-k-one}. 
%Finally, we show that $k = k\opt$ satisfies the third (and also the last) 
%inequality in Eq~\eqref{eqn:inequality-k-one}. To do so, denote
%$m = nH^2 r^2/\sigma^2$, then $m \geq 3$ by our 
%first assumption on $n$. Note that, our definition of $k(r)$ satisfies 
%$k(r) \leq \frac{1}{6}\left(\frac{m}{\log m}\right)^{\frac{1}{d+2}}$, 
%or equivalently, $(6k(r))^{d+2} \leq \frac{m}{\log m}$. Hence, 
%\begin{equation*}
%(2k\opt+1)^d (k\opt)^2 \log (2k+1)^d \leq \frac{1}{9}(6k(r))^{d+2} 
%	\log (6k(r))^{d+2} \leq \frac{m}{9\log m} \log \frac{m}{\log m} 
%		\leq \frac{m}{9} \leq \frac{nH^2r^2}{8\sigma^2}, 
%\end{equation*}
%which gives the last inequality of Eq~\eqref{eqn:inequality-k-one}. The
%desired claim of the corollary now follows. 
%\end{proof}


\subsection{Analysis of Algorithm~\ref{alg:smoothcvxfoRrounds}: Multi-Stage Analysis}
In this section, we show that, with careful choice of input parameters 
$(c_1, r_1, k_1, T_1)$ and updating rule, algorithm~\ref{alg:smoothcvxfoRrounds} 
returns some minimax estimator $\hat{x}$. In essence, algorithm 
\ref{alg:smoothcvxfoRrounds} recursively uses algorithm~\ref{alg:smoothcvxfo}
to build smaller confidence region of the optimum $x_f\opt$ through iterations.
Indeed, an important message from proposition~\ref{proposition:first-order-smooth-one-round} 
shows that, given any $\delta > 0$, with appropriate choice of parameters, one 
can find some rectangular $W$ such that $x_f\opt$ lies inside the rectangular 
with probability at least $1-\delta$. This means that, after one round, one can 
`localize' the search of the optimum $x_f\opt$ by searching the optimum of 
$f$ inside $W$. Now, treating $W$ as the original $\xdomain$, one can 
thus get an improved rate of convergence in the second round. Finally, we note that 
such `localized' search can be recursively applied in all rounds from the first to 
the last round. 

To be clear about how we specify the updating rule in line 3 of the algorithm 
\ref{alg:smoothcvxfoRrounds}, we summarize it as follows: given the output 
parameters $(\hat{c}_i, \hat{r}_i)$, we update $(c_{i+1}, r_{i+1}, k_{i+1}, T_{i+1})$ via
algorithm~\ref{alg:update-rule-smooth-fo-Rrounds}. The next proposition shows 
that with appropriate choice of the initial parameter, we have nice convergence 
guarantees for the output of algorithm~\ref{alg:smoothcvxfoRrounds}.

\begin{algorithm}[htp]
\caption{Updating Rule in Algorithm~\ref{alg:smoothcvxfoRrounds}}
\begin{algorithmic}
%\Procedure{updating rule}{}
  \Statex Input: $\hat{c}_i \in \R^d$ and $\hat{r}_i \in \R_+$.
  \State (i) Update $c_{i+1}$ coordinate-wisely via: 
  	\begin{equation*}
		c_{i+1, j} = \min\{1-\hat{r}_i, \max\{r_i, \hat{c}_{i, j}\}\},
	\end{equation*}
	where $c_{i, j}$ and $c_{i+1, j}$ denote the $j$th coordinate of 
		$c_i$ and $c_{i+1}$. 
  \State (ii) Update $r_{i+1}$ as $r_{i+1} = \hat{r}_i$.
  \State (iii) Update $k_{i+1}$ to be the largest $k \in \N$ such that 
  	\begin{equation} 
		\label{eqn:existence-of-k-i+1-r-rounds-fo}
		(2k + 1)^d \ceil{2k^2 \log (2k+1)} \leq nr_i^2.
	\end{equation}
  	 If no such $k$ exists, return FAIL. 
  \State (iv )Update $T_{i+1}$ to be $T_{i+1} = \floor{\frac{n}{(2k_i+1)^d}}$.
%\EndProcedure
\end{algorithmic}
\label{alg:update-rule-smooth-fo-Rrounds}
\end{algorithm}

%
%Below proposition shows that such `localization' idea indeed 
%gives us the minimax optimal rate with an appropriate choice of the initial parameter 
%$(c_1, r_1, k_1, T_1)$ and appropriate choice of the updating rule. 
%
\begin{proposition}
\label{proposition:first-order-smooth-r-rounds}
Let $\xdomain = [0, 1]^d$, and we are given the confidence level $\delta > 0$. 
We initialize the initial parameters as follows: set $c_1 = \half \cdot \ones$, 
$r_1 = \half$, and set $k_1$ to be the largest $k \in \N$ (if exists) such that, 
$(2k + 1)^d \ceil{2k^2 \log (2k+1)} \leq nr_1^2$ and $T_1 = \floor{\frac{n}{(2k_1 +1)^d}}$.
Consider algorithm~\ref{alg:smoothcvxfoRrounds} that uses algorithm
\ref{alg:update-rule-smooth-fo-Rrounds} to be the updating rule. Denote 
$\hat{x}_M$ and $r_{M+1} = \hat{r}_M$ to be the output of estimate and radius
from algorithm~\ref{alg:smoothcvxfoRrounds}. Now, assuming that the 
$\{k_i\}_{i=1}^M$ exist for Eq~\eqref{eqn:existence-of-k-i+1-r-rounds-fo} 
and satisfy the following lower bounds: 
\begin{equation}
\label{eqn:lower-bound-on-k-i-fo}
k_i > \frac{10}{\lambda} \left(\sigma \left(\log \frac{M}{\delta} + 3d\right)^\half+Hd^\half\right)
	~\text{for all $1\leq i\leq M$}. 
\end{equation}
%\begin{equation}
%\label{eqn:existence-of-k1-r-rounds-fo}
%(2k + 1)^d \ceil{2k^2 \log (2k+1)} \leq nr_1^2.
% ~\text{and}~
%	k > \frac{10}{\lambda} \left(\sigma \left(\log \frac{M}{\delta} + 3d\right)^\half+Hd^\half\right).
%\end{equation}
%Set $T_1 = \floor{\frac{n}{(2k_1 +1)^d}}$. Now, suppose we use algorithm 
%\ref{alg:update-rule-smooth-fo-Rrounds} as the updating rule for algorithm
%\ref{alg:smoothcvxfoRrounds}. Now, suppose that, 
%Consider the following updating rule for algorithm~\ref{alg:smoothcvxfoRrounds} (used 
%in the third line of algorithm~\ref{alg:smoothcvxfoRrounds}). In the $i$th iteration, 
%define $\hat{c}^{\prime}_i$ coordinate-wisely via 
%\begin{equation*}
%\hat{c}^{\prime}_{i, j} = \min\{1-\hat{r}_i, \max\{\hat{r}_i, \hat{c}_{i, j}^{S}\}\}
%%\hat{x}^{S^\prime}_{i, j} = \begin{cases}
%%	\hat{r}_i  & \text{if}~\hat{x}_{i, j}^S \leq \hat{r}_i \\
%%	1-\hat{r}_i  & \text{if}~\hat{x}_{i, j}^S \geq 1- \hat{r}_i \\
%%	\hat{x}_{i, j}^S  & \text{otherwise}, \end{cases}
%\end{equation*}
%where for each $1\leq j\leq d$, $\hat{x}^{S^\prime}_{i, j}$ and $\hat{x}_{i, j}^S$ denote
%the $j$th coordinate of $\hat{x}^{S^\prime}$ and $\hat{x}_i$ respectively. 
%Update $c_{i+1} = \hat{x}^{S^\prime}_i$ and $r_{i+1} = \hat{r}_i$. Suppose there 
%exists some $k \in \N$ satisfying, 
%\begin{align}
%\label{eqn:existence-of-k-i+1-r-rounds-fo}
%(2k + 1)^d \ceil{2k^2 \log (2k+1)} \leq nr_i^2, ~\text{and}~
%	k > \frac{10}{\lambda} \left(\sigma \left(\log \frac{M}{\delta} + 3d\right)^\half+Hd^\half\right).
%\end{align}
%Set $k_{i+1} = k$ satisfying Eq~\eqref{eqn:existence-of-k-i+1-r-rounds-fo} and 
%$T_{i+1} = \floor{\frac{n}{(2k_{i+1} +1)^d}}$. 
%%\begin{align*}
%%k_{i+1} = \max_{k\in \N} \left\{(2k + 1)^d \ceil{\frac{2\sigma^2 k^2}{H^2 r_{i+1}^2} 
%%	\log \frac{2d(2k + 1)^d }{\delta/M}} \leq n\right\} 
%%~\text{and}~
%%	T_{i+1}=\floor{\frac{n}{(2k_{i+1} +1)^d}}.
%%\end{align*}
%Let $\hat{x}_M$ and $r_{M+1} = \hat{r}_M$ denote the output of estimate and 
%radius respectively from algorithm~\ref{alg:smoothcvxfoRrounds}. 
Then, we have, 
\begin{equation*}
\P\left(f(\hat{x}_M) - f(x_f\opt) \leq 2 Hd r_{M+1}^2\right)\geq 1-\delta. 
\end{equation*}
\end{proposition}
\begin{proof}
Note that, when $M = 1$, proposition~\ref{proposition:first-order-smooth-r-rounds} reduces 
to proposition~\ref{proposition:first-order-smooth-one-round}. When $M > 1$, note that, 
the definition of $k_1$ and $T_1$ takes the same form as that in Eq~\eqref{eqn:existence-of-k-one-round-fo}. 
In addition, the condition of $k_1$ in Eq~\eqref{eqn:lower-bound-on-k-i-fo} changes by 
substituting $\delta$ by $\delta/M$.
Hence, denoting $W_1^\prime = \{x\in \xdomain: \norm{x-\hat{c}_1}_\infty \leq \hat{r}_1\}$, 
Proposition~\ref{proposition:first-order-smooth-one-round} gives 
$\P(x_f\opt \in W_1^\prime) \geq 1-\delta/M$. Now, denote similarly 
$W_1 = \{x: \norm{x-c_2}_\infty \leq r_2\}$. Then, by definition of $c_2$ and $r_2$, 
$W_1 \subseteq W_1^\prime \subseteq \xdomain$, 
and hence $\P(x_f\opt \in W_1) \geq \P(x_f\opt \in W_1^\prime) \geq 1-\delta/M$. 

Now, in the second round of sampling and estimation, the algorithm essentially view 
$W_1$ as the entire domain $\xdomain$ and sample the points using the same 
strategy as that in the first round. As the noise vectors in the second round is 
independent of the first one, Proposition~\ref{proposition:first-order-smooth-one-round} 
gives that $\P\left(x_f\opt \in W_2 \mid x_f\opt \in W_1\right) \geq 1-\delta/M$, 
where $W_2 = \{x: \norm{x-c_3}_\infty \leq r_3\}$. Indeed, the same conclusion holds 
for the $i$th round: denoting $W_i$ analogously to be $W_i= \{x: \norm{x-c_{i+1}} \leq r_{i+1}\}$, we get, 
 $\P\left(x_f\opt \in W_{i+1} \mid x_f\opt \in W_i\right) \geq 1-\delta/M$ for 
all $1\leq i\leq M-1$. Thus, denoting via convection that $W_0 = \xdomain$, we get,
\begin{equation*}
\P (x_f\opt \in W_M) \geq \Pi_{i=1}^M \left[	
	\P\left(x_f\opt \in W_{i} \mid x_f\opt \in W_{i-1}\right)\right] 
		\geq \left(1-\delta/M\right)^M \geq 1-\delta. 
\end{equation*} 
Now, we introduce the notation $\event = \{x_f\opt \in W_M\}$. Then, 
$\P(\event) \geq 1-\delta$. Now the requirements on $k$ in Eq~\eqref{eqn:lower-bound-on-k-i-fo}
guarantees that the size of $W_i$ is strictly decreasing when $1\leq i\leq M$. 
Hence, we can prove similarly to lemma~\ref{lemma:careful-choice-of-estimator} 
to get that, on event $\event$, $f(\hat{x}_M) - f(x_f\opt) \leq 2 Hd \hat{r}_m^2$ for 
our careful choice of the estimator $\hat{x}_M \in W_M^\prime$. 
%On $\event$, 
%$\hat{x}_M \in W_M^\prime$, whose diameter is $2\sqrt{d}\hat{r}_M$. Since 
%$\hat{x}_M \in W_M^\prime$, it gives that, $\norm{\hat{x}_M - x_f\opt}_2 \leq 2\sqrt{d}\hat{r}_M$.
%
%
% then we have, $\P(\event) \geq 1-\delta$.  
%Since $\hat{x}_M \in W_M \subset W_M^\prime$ whose diameter is $2\sqrt{d}\hat{r}_M$, 
%this shows that in particular that, on $\event$,  we have $\norm{\hat{x}_M - x_f\opt}_2 \leq 
%2\sqrt{d}\hat{r}_M$. Finally, our assumption that $4Hd < \min_{i=1}^M \{k_i \lambda\}$, 
%guarantees that the side length of $W_i^\prime$ is strictly decreasing when $1\leq i\leq M$. 
%As a consequence, the side length of $W_M^\prime$ is strictly less than $1$ on event $\event$. 
%Therefore, lemma~\ref{lemma:careful-choice-of-estimator} implies that on event $\event$, 
%our careful choice of $\hat{x}_M$ enables us to guarantee that $\left<\grad f(x_f\opt), 
%\hat{x}_M - x_f\opt\right> = 0$. The desired claim of the proposition thus follows by 
%\begin{equation*}
%f(\hat{x}_M) - f(x_f\opt) \leq 
%	\left<\grad f(x_f\opt), \hat{x}_M - x_f\opt\right> + 
%		\frac{H}{2} \norm{\hat{x}_M - x_f\opt}^2 
%			\leq 2Hd \hat{r}_m^2, 
%\end{equation*}
%where in the first inequality above, we use the fact that $\grad f$ is $H$-Lipschitz in 
%$\ell_2$ norm.
\end{proof}

\newcommand{\residual}{\text{residual}}

Motivated by Proposition~\ref{proposition:first-order-smooth-r-rounds}, it becomes 
important to understand when $k_i \in \N$ exists to satisfy both 
Eq~\eqref{eqn:existence-of-k-i+1-r-rounds-fo} and Eq~\eqref{eqn:lower-bound-on-k-i-fo}.

\begin{lemma}
\label{lemma:ef-first-order-smooth-r-rounds}
Assume $n$ is large enough satisfying the bounds below: 
\begin{equation}
\label{eqn:ef-n-condition-fo-smooth-r-rounds}
\log \log n > M \log \left(1+ \frac{2}{d}\right)
	+ \log \left(2M \log(6B \log n) + (2d+5)\log (6B)\right), 
\end{equation}
where we denote $B$ to be $
B = \frac{10}{\lambda} \left(\sigma \left(\log \frac{M}{\delta} + 3d\right)^\half + H d^\half \right).
$
Then, the sequence $\{r_i\}_{i=1}^M$, $\{k_i\}_{i=1}^M$ and $\{T_i\}_{i=1}^M$ are 
well defined via algorithm~\ref{alg:update-rule-smooth-fo-Rrounds}. In addition, 
the sequence $\{k_i\}_{i=1}^M$ satisfy Eq~\eqref{eqn:lower-bound-on-k-i-fo}.
Finally, the output $\{r_i\}_{i=1}^M$ satisfy the bound below: 
\begin{equation*}
r_i \leq (6B)^{\frac{d+2}{2} \left(1-\left(\frac{d}{d+2}\right)^M\right)}
		D_n^{\half \left(1 - \left(\frac{d}{d+2}\right)^M\right)}
		n^{-\half \left(1 - \left(\frac{d}{d+2}\right)^M\right)}~~\text{for all $1\leq i\leq M$}.
\end{equation*}
\end{lemma}
\begin{proof}
%It is helpful to introduce some new notations. Denote $B, C, D_n$ as the following: 
%\begin{equation*}
%B = \frac{30 H\sqrt{d}}{\lambda},  
%	C = \max\left\{\frac{H^2}{\sigma^2 \log \frac{2d}{\delta/M}}, 1\right\}
%~~\text{and}~~D_n = 
%	\max \left\{\frac{\sigma^2}{H^2} \log \frac{2d}{\delta/M}, 
%		\frac{\sigma^2}{H^2} \log \frac{nH^2}{\sigma^2}, 1\right\}.
%\end{equation*}
We prove the desired claim of the lemma via induction. Our strategy is 
to show via induction that the below hypothesis hold for all $1\leq i\leq M$:
\begin{align}
%\text{Induction Hypothesis for $1\leq i\leq M$: }
&(i) n r_i^2 > (6B)^{2(d+2)}~
(ii) k_i ~\text{is well defined}~\text{and}~k_i > B
\label{eqn:induction-hypothesis-one-fo} \\
&(iii)~r_i \leq \min
	\left\{1, (6B)^{\frac{d+2}{2}\left(1-\left(\frac{d}{d+2}\right)^{i-1}\right)}
		(\log n)^{\half \left(1-\left(\frac{d}{d+2}\right)^{i-1}\right)}
		n^{-\half \left(1-\left(\frac{d}{d+2}\right)^{i-1}\right)}\right\}
\label{eqn:induction-hypothesis-two-fo}.
\end{align} 
We first show the base case $i=1$. Note that, the first part of Eq
\eqref{eqn:induction-hypothesis-one-fo} is implied by the assumption 
on $n$, the second part of Eq~\eqref{eqn:induction-hypothesis-one-fo}
follows from the first part and corollary~\ref{corollary:first-order-smooth-one-round},
and Eq~\eqref{eqn:induction-hypothesis-two-fo} is trivial when $i=1$. 
Now, for some $i < M$, assuming that the induction hypothesis holds for 
all $j \in \{1, 2, \ldots, i\}$, we show that the hypothesis holds for $i+1$.  
We first show the second part of Eq~\eqref{eqn:induction-hypothesis-one-fo}
for $i+1$. Indeed, by induction hypothesis, we know that, the first 
inequality of Eq~\eqref{eqn:induction-hypothesis-one-fo} is true for $i$,
and thus the second part of Eq~\eqref{eqn:induction-hypothesis-one-fo}
follows from a direct application of corollary~\ref{corollary:first-order-smooth-one-round}
(by substituting $\delta$ by $\delta/M$ and $r$ by $r_i$ there).
%Note that corollary~\ref{corollary:first-order-smooth-one-round} also provides 
%the following lower bound on $k_{i+1}$: 
%\begin{equation}
%\label{eqn:lower-bound-induction-on-k-fo}
%k_{i+1} \geq  \frac{1}{3} \left(\frac{nH^2r_i^2}
%	{\max \left\{ \sigma^2 \log \frac{2d}{\delta/M}, 
%		\sigma^2 \log \frac{nH^2r_i^2}{25\sigma^2}, 
%			H^2 r_i^2\right\}} \right)^\frac{1}{d+2}.
%\end{equation}
Next, we show Eq~\eqref{eqn:induction-hypothesis-two-fo} holds for $i+1$. 
Again, we know from corollary~\ref{corollary:first-order-smooth-one-round}
that $r_{i+1} \leq r_i \leq 1$ and the bound that 
$r_{i+1} \leq (6B) r_i^\frac{d}{d+2}n^{-\frac{1}{d+2}} (\log n)^\frac{1}{d+2}$. 
Now, using the induction hypothesis of the upper bound on $r_i$, we get, 
 \begin{align*}
 r_{i+1}  &\leq 6B \left((6B)^{\frac{d+2}{2}\left(1-\left(\frac{d}{d+2}\right)^{i-1}\right)}
		\left(\log n\right)^{\frac{1}{2}\left(1-\left(\frac{d}{d+2}\right)^{i-1}\right)}
		n^{-\frac{1}{2}\left(1-\left(\frac{d}{d+2}\right)^{i-1}\right)}\right)
			^\frac{d}{d+2} (\log n)^\frac{1}{d+2} n^{-\frac{1}{d+2}}  \\
	&= (6B)^{\frac{d+2}{2}\left(1-\left(\frac{d}{d+2}\right)^{i}\right)}
		\left(\log n\right)^{\frac{1}{2}\left(1-\left(\frac{d}{d+2}\right)^{i}\right)}
		n^{-\frac{1}{2}\left(1-\left(\frac{d}{d+2}\right)^{i}\right)},
 \end{align*}
so we have shown Eq~\eqref{eqn:induction-hypothesis-two-fo} for $i+1$. 
Finally, we show the first part of Eq~\eqref{eqn:induction-hypothesis-one-fo} 
for $i+1$. Note that, by definition of $\{r_i\}_{i=1}^M$, we know that, 
\begin{equation*}
r_{i+1} = r_1 \cdot \Pi_{j=1}^i \frac{r_{j+1}}{r_j} = r_1 \cdot B^i \cdot \frac{1}{\Pi_{j=1}^i k_j}
	= \frac{B^i}{2 \Pi_{j=1}^i k_j}. 
\end{equation*}
Hence, the first part of Eq~\eqref{eqn:induction-hypothesis-one-zo} 
for $i+1$ is equivalent to
\begin{equation}
\label{eqn:induction-condition-inequality-two-fo}
nB^{2i} > 4 (6B)^{2(d+2)} \cdot \Pi_{j=1}^{i} k_j^2. 
\end{equation}
To establish Eq~\eqref{eqn:induction-condition-inequality-two-zo}, note 
first that, by definition of $k_j$, we have, for all $j \leq i$, 
\begin{equation*}
k_j \leq \frac{1}{3} (\gamma(n r_j^2))^\frac{1}{d+2} \leq 
	 \frac{1}{3} (nr_j^2)^{\frac{1}{d+2}}. 
\end{equation*}
Now, using Eq~\eqref{eqn:induction-condition-inequality-two-zo} 
from the induction hypothesis for $j \leq i$, we get that, 
\begin{equation*}
k_j \leq (n r_j^2)^\frac{1}{d+2}  \leq 
	(6B)^{\left(1-\left(\frac{d}{d+2}\right)^i\right)}
	n^{\frac{1}{d+2}(\frac{d}{d+2})^{j-1}} (\log n)^{\frac{1}{d+2}\left(1-\left(\frac{d}{d+2}\right)^i\right)}
	\leq 6 B n^{\frac{1}{d+2}(\frac{d}{d+2})^{j-1}} (\log n).
\end{equation*}
Hence, to prove Eq~\eqref{eqn:induction-condition-inequality-two-zo}, it 
suffices to show that, for all $1\leq i\leq M$, 
\begin{equation*}
nB^{2i} > 4 (6B)^{2(d+2)}(6B \log n)^{2i}
	n^{\frac{2}{d+2}\sum_{j=0}^{i-1} \left(\frac{d}{d+2}\right)^j}
	= 4 (6B)^{2(d+2)} (6B \log n)^{2i} n^{1- \left(\frac{d}{d+2}\right)^i}.
\end{equation*}
Note that, it suffices if $n$ is large enough satisfying the bound below, 
\begin{equation*}
n^{\left(\frac{d}{d+2}\right)^M} > 4 (6B)^{2(d+2)} (6B\log n)^{2M}, 
\end{equation*}
which would suffice is $n$ satisfies
\begin{equation*}
\log \log n > M \log \left(1+ \frac{2}{d}\right)
	+ \log \left(2M \log(6B \log n) + (2d+5)\log (6B)\right).
\end{equation*}
\end{proof}
Now, Proposition~\ref{proposition:first-order-smooth-r-rounds} and Lemma 
\ref{lemma:ef-first-order-smooth-r-rounds} together immediately give the 
corollary below. 

\begin{corollary}
\label{corollary:first-order-smooth-r-rounds}
Let $\xdomain = [0, 1]^d$. Consider algorithm~\ref{alg:smoothcvxfoRrounds}. 
Suppose we use the same initialization rule as that in Proposition
\ref{proposition:first-order-smooth-r-rounds} and use Algorithm
\ref{alg:update-rule-smooth-fo-Rrounds} to be the updating rules 
for algorithm~\ref{alg:smoothcvxfoRrounds}, then, when $n$ is large enough 
so that it satisfies Eq~\eqref{eqn:ef-n-condition-fo-smooth-r-rounds}, then
the output $\hat{x}_M$ from algorithm~\ref{alg:smoothcvxfoRrounds} satisfies 
\begin{equation*}
\P\left(f(\hat{x}_M) - f(x_f\opt) \leq \gap_M\opt \right)\geq 1-\delta
\end{equation*}
with 
\begin{equation*}
\gap_M\opt \defeq 2 Hd (6B)^{(d+2) \left(1-\left(\frac{d}{d+2}\right)^M\right)}
		(\log n)^{\left(1 - \left(\frac{d}{d+2}\right)^M\right)}
		n^{-\left(1 - \left(\frac{d}{d+2}\right)^M\right)}.
\end{equation*}
\end{corollary}


%
%Recall the definition of $r_{i+1}$:
%\begin{equation}
%\label{eqn:recursion-of-r-i}
%r_{i+1} = \frac{5 H\sqrt{d}}{\lambda} \cdot \frac{r_i}{k_{i}}.
%\end{equation}
%Since, $k_i \lambda > 5Hd$ (based on the induction hypothesis of the second 
%part of Eq~\eqref{eqn:induction-hypothesis-one-fo} for $i$), we get easily 
%$r_{i+1} \leq r_i \leq 1$. In addition, since $k_{i} = \ceil{k(r_i)} \geq k(r_{i})$, 
%we get that, 
%\begin{equation}
%\label{eqn:direct-upper-bound-on-r-i-plus-one}
%r_{i+1} = \frac{5H\sqrt{d}}{\lambda} \cdot \frac{r_i}{k_{i}}  
%	\leq \frac{30H\sqrt{d}}{\lambda} r_i^{\frac{d}{d+2}}
%		\left(\frac{\max\left\{\sigma^2 \log \frac{2d}{\delta/M}, 
%			\sigma^2 \log \frac{nH^2r_i^2}{\sigma^2}, H^2 r_i^2\right\}}
%				{nH^2}\right)^{\frac{1}{d+2}}
%	\leq B r_i^{\frac{d}{d+2}}  (D_n/n)^{\frac{1}{d+2}}, 
%\end{equation}
%where in the last inequality, we use $r_i \leq 1$ from our induction hypothesis. 
%Now, plugging the upper bound of $r_i$ in
%Eq~\eqref{eqn:induction-hypothesis-two-fo} into 
%Eq~\eqref{eqn:direct-upper-bound-on-r-i-plus-one}, we get, 
%\begin{align*}
%r_{i+1} \leq B r_i^{\frac{d}{d+2}} (D_n/n)^{\frac{1}{d+2}}
%	\leq B^{\frac{d+2}{2} \left(1-\left(\frac{d}{d+2}^i\right)\right)}
%		D_n^{\half \left(1 - \left(\frac{d}{d+2}\right)^i\right)}
%		n^{-\half \left(1 - \left(\frac{d}{d+2}\right)^i\right)}.
%\end{align*}
%Together with the proved bound $r_{i+1} \leq 1$, we have shown the 
%inequality in Eq~\eqref{eqn:induction-hypothesis-two-fo} for $i+1$. 
%Finally, we show the first part of Eq~\eqref{eqn:induction-hypothesis-one-fo} 
%for $i+1$. Note that, by definition of $\{r_i\}_{i=1}^M$, 
%\begin{equation*}
%r_{i+1} = r_1 \cdot \Pi_{j=1}^{i} \frac{r_{j+1}}{r_{j}} = r_1 \cdot 
%	\left(\frac{5H\sqrt{d}}{\lambda}\right)^{i} \cdot 
%	\frac{1 }{\Pi_{j=1}^{i} k_j} \geq \frac{1}{\Pi_{j=1}^{i} k_j}. 
%\end{equation*}
%Since $r_{i+1} \leq 1$ and $n \geq 2 B^{2(d+2)}$, to prove the
%first part of Eq~\eqref{eqn:induction-hypothesis-one-fo}, it suffices to show, 
%\begin{equation}
%\label{eqn:induction-condition-inequality-two-fo}
%nH^2 \geq  2\sigma^2 \left(\log \frac{2d}{\delta/M} + 1\right)
%	\Pi_{j=1}^{i} k_j^2 
%\end{equation}
%To establish Eq~\eqref{eqn:induction-condition-inequality-two-fo}, note 
%first that, by definition of $k_j$, we have, for all $j \leq i$, 
%\begin{equation*}
%k_j \leq 2 k(r_j) \leq \frac{1}{3} \left(\frac{nH^2r_j^2}{\max\left\{\sigma^2 
%	\log \frac{2d}{\delta}, \sigma^2 \log \frac{nH^2r_j^2}{\sigma^2}, 
%		H^2 r_j^2\right\}} \right)^\frac{1}{d+2} 
%\leq  (nr_j^2)^{\frac{1}{d+2}} C^{\frac{1}{d+2}}, 
%\end{equation*}
%According to Eq~\eqref{eqn:induction-condition-inequality-two-fo} 
%from the induction hypothesis, we get that, 
%\begin{equation*}
%k_j \leq (n r_j^2)^\frac{1}{d+2} C^\frac{1}{d+2} \leq B^{\left(1-\left(\frac{d}{d+2}\right)^i\right)}
%	C^\frac{1}{d+2}
%	n^{\frac{1}{d+2}(\frac{d}{d+2})^{j-1}} D_n^{\frac{1}{d+2}\left(1-\left(\frac{d}{d+2}\right)^i\right)}
%	\leq BCD_n n^{\frac{1}{d+2}(\frac{d}{d+2})^{j-1}}.
%\end{equation*}
%Hence, to prove Eq~\eqref{eqn:induction-condition-inequality-two-fo}, it 
%suffices to show that, for all $1\leq i\leq M$, 
%\begin{equation*}
%nC \geq 4(BCD_n)^{2i}
%	n^{\frac{2}{d+2}\sum_{j=0}^{i-1} \left(\frac{d}{d+2}\right)^j}
%	= 4(BCD_n)^{2i}n^{1- \left(\frac{d}{d+2}\right)^i}.
%\end{equation*}
%Note that, it suffices if $n$ is large enough satisfying the bound below, 
%\begin{equation*}
%n^{\left(\frac{d}{d+2}\right)^M} \geq 4(BCD_n)^{2M}
%~~\Leftrightarrow~~\log \log n \geq M \log \left(1+ \frac{2}{d}\right)
%	+ \log \left(2M \log(BCD_n) + 2\log 2\right).
%\end{equation*}


%\newpage
\section{Smooth Function with Zeroth Order Oracle}
\label{sec:proof-Smooth-zo}
\subsection{Description of Algorithms}
In this section, we propose two generic algorithms: algorithm~\ref{alg:smoothcvxzo} 
parameterized by parameters $(c,r,k,T)\in \xdomain \times \R_+ \times \N \times \N$ 
and algorithm~\ref{alg:smoothcvxzoRrounds} that builds from algorithm~\ref{alg:smoothcvxzo}.  
We note here that, algorithm~\ref{alg:smoothcvxzoRrounds} in essence, builds 
from $M$ times of repeated calls of algorithm~\ref{alg:smoothcvxzo}.
As will be shown immediately in the later subsections, it turns out that the two algorithms 
with careful choice of parameters return the minimax estimator in single and multi rounds 
respectively.
\begin{algorithm}[htp]
\caption{Generic Routine for One Stage Smooth Functions $\ffamily_{H, \lambda}$ (Zeroth-order Oracle)} 
\begin{algorithmic}[1]  
\Statex Input: Prior knowledge on $\lambda, H \in \R_+$ satisfying $\lambda \leq H$ and 
	the noise level $\sigma \in \R_+$. User specifies the sampling center $c\in \xdomain$, 
	radius $r\in \R_+$, grid size parameter $k\in \N$, the sampling times $T \in \N$ 
	and the confidence level $\delta \in (0, 1]$.
\State Compute the grid points $G = G(c, r, k)$.
\State At each point $x\in G$, query the first oracle $T$ times and denote each 
	sample function value via $\{\what{f}(x)^{(1)}, \what{f}(x)^{(2)}, \ldots, 
		\what{f}(x)^{(T)}\}$.
\State Compute the function value estimate at each point $x \in G$ via 
	$\what{f}(x) \defeq \frac{1}{T} \sum_{i=1}^{T} \what{f}(x)^{(i)}$.
\State Compute the estimate $\hat{x} \in G$, defined as, 
	$\hat{x} \defeq \argmin_{x\in G} \what{f}(x)$. 
\State Return the estimator $\hat{x}$ and the confidence radius 
	$\hat{r} = \frac{r}{k} \cdot 
		\left(\frac{2\sigma}{\lambda} \left(\log \frac{2}{\delta} + d\right)^\half + \frac{H}{\lambda}d\right)^\half$.
\end{algorithmic}
\label{alg:smoothcvxzo}
\end{algorithm}

\begin{algorithm}[htp]
\caption{Generic Routine for Multi-stage Smooth Functions $\ffamily_{H, \lambda}$ (Zeroth-order Oracle)} 
\begin{algorithmic}[1]  
\Statex Input: Prior knowledge on $\lambda, H \in \R_+$ satisfying $\lambda \leq H$ and 
	the noise level $\sigma \in \R_+$ and number of rounds $R \in \N_+$. 
	Initialization of parameters  $(c_1, r_1, k_1, T_1) \in \R^d \times \R_+ \times \N \times \N$. 
	User specifies the confidence level $\delta \in (0, 1]$ and the updating rule that used 
	in line (3) of the algorithm. 
\FOR{$i = 1$ to $M$}
	\State Run algorithm~\ref{alg:smoothcvxfo} with input parameters $(c_i, r_i, k_i, T_i)$.
	Denote the output to be $\hat{x}_i$, confidence radius $\hat{r}_i$. 
	\State Update $(c_{i+1}, r_{i+1}, k_{i+1}, T_{i+1})$. The updating rule may take 
		$\hat{x}_i$ and $\hat{r}_i$ as input.   
\ENDFOR
\State \Return $\hat{x}_M$ as estimate of $x_f\opt$ and the radius $r_{M+1}$.
\end{algorithmic}
\label{alg:smoothcvxzoRrounds}
\end{algorithm}

\subsection{Analysis of Algorithm~\ref{alg:smoothcvxzo}: Single-Stage Analysis}
In this section, we show that a single call of algorithm~\ref{alg:smoothcvxzo} 
with careful choice of input parameters $(c, r, k, T)$ returns some 
estimator $\hat{x}$ that is minimax optimal. To serve for the purpose for latter 
discussion on multi-stage algorithm, in this section, we slightly generalize 
the domain of interest $\xdomain_{c, r} = \{x\in \R^d: \norm{x-c}_\infty \leq r\}$ 
and denote $x_{f, c, r}\opt$ the unique minimum of $f$ on domain $\xdomain_{c, r}$.
We consider finding the minimax estimator $\hat{x}\in \xdomain_{c, r}$ for 
$x_{f, c, r}\opt$ evaluated by $f(\hat{x}) - f(x_{f, c, r}\opt)$. Substituting 
$c = \half \cdot 1$ and $r=\half$ gives the result for the single-stage algorithm for 
the original domain $\xdomain = [0, 1]^d$.
\begin{proposition}
\label{proposition:zeroth-order-smooth-one-round}
Given any fix $c\in \R^d$ and $r \in (0, 1]$, suppose there exists some $k \in \N$ satisfying 
\begin{equation}
\label{eqn:existence-of-k-one-round-zo}
(2k+1)^d \floor{2k^4 \log(2k+1)} \leq nr^4~~\text{and}~~
	k \geq \left(\frac{2\sigma}{\lambda} \left(\log \frac{2}{\delta} + d\right)^\half + \frac{H}{\lambda}d\right)^\half.
\end{equation}
Set $T= \floor{\frac{n}{(2k+1)^d}}$. Then, if we denote $\hat{r}$ and 
$\hat{x}$ to be the output from algorithm~\ref{alg:smoothcvxzo} when inputting 
$(c, k, T, r)$ as above, then, we have $\hat{r} < r$ and 
\begin{equation*}
\P\left(f(\hat{x}) - f(x_{f, c, r}\opt) \leq  \half \lambda \hat{r}^2 ~\text{and}~ 
	\norm{x_{f, c, r}\opt - \hat{x}}_\infty \leq \hat{r} \right)\geq 1-\delta.
\end{equation*}
\end{proposition}

\begin{remark}
Before we give the proof of proposition~\ref{proposition:zeroth-order-smooth-one-round}, 
we give some high-level intuitions why algorithm~\ref{alg:smoothcvxzo} \emph{should}
work. As shown previously in lemma~\ref{lemma:simple-but-important}, we know 
that there always exists some point $\bar{x} \in G$ satisfying 
\begin{equation*}
%\label{eqn:condition-on-bar-x-zo}
\norm{\bar{x} - x_{f, c, r}\opt}_\infty \leq \frac{r}{k}
	~~\text{and}~~
		\left<\grad f(x_{f, c, r}\opt), \bar{x} - x_{f, c, r}\opt\right> = 0.
\end{equation*}
It turns out that such $\bar{x}$ also has function value $f(\bar{x})$ close to 
$f(x_{f, c, r}\opt)$ up to $O(\hat{r}^2)$. Since $\hat{x}$ is defined to be the 
smallest point in grid that minimizes $\hat{f}$, intuitively it makes sense that 
$f(\hat{x})$ should also be as good as $f(x_{f, c, r}\opt)$ when the sample 
size $n$ is large enough. 
\end{remark}


\begin{proof}
We first recall lemma~\ref{lemma:simple-but-important} in the proof of 
proposition~\ref{proposition:first-order-smooth-one-round}.
\begin{lemma}
There exists some $\bar{x} \in G$ satisfying the below conditions: 
\begin{equation}
\label{eqn:condition-on-bar-x-zo}
\norm{\bar{x} - x_{f, c, r}\opt}_\infty \leq \frac{r}{k}
	~~\text{and}~~
		\left<\grad f(x_{f, c, r}\opt), \bar{x} - x_{f, c, r}\opt\right> = 0.
\end{equation}
\end{lemma}\noindent
Now, take any point $\bar{x}\in G$ satisfying Eq~\eqref{eqn:condition-on-bar-x-zo}.
Note first that, by smoothness assumption of the objective function $f$, we have, 
\begin{equation*}
f(\bar{x}) - f(x_{f, c, r}\opt) \leq \left<\grad f(x_{f, c, r}\opt)), \bar{x} - x_{f, c, r}\opt\right>
	+ \frac{H}{2} \norm{\bar{x} - x_{f, c, r}\opt}_2^2 \leq \frac{Hr^2 d}{2k^2}.
\end{equation*}
%Second, since $f$ is strongly convex, we know for all $x\in \xdomain$ satisfying
%$\norm{x-x_{f, c, r}\opt}_2 > \frac{r\sqrt{Hd}}{k\sqrt{\lambda}}$, 
%\begin{equation*}
%f(x) - f(x_{f, c, r}\opt) \geq \left<\grad f(x_{f, c, r}\opt)), x- x_{f, c, r}\opt\right>
%	+ \frac{\lambda}{2} \norm{x-x_{f, c, r}\opt}_2^2 \geq 
%	\frac{\lambda}{2} \norm{x-x_{f, c, r}\opt}_2^2 > \frac{Hr^2 d}{k^2}.
%\end{equation*}
Now, let us consider the following event: 
\begin{equation*}
\event = \left\{\left|\what{f}(x) - f(x)\right| \leq r^a \defeq 
	\sigma \sqrt{\frac{2}{T}\log \frac{2(2k+1)^d}{\delta}}
	~\text{for all}~x \in G\right\}, 
\end{equation*}
The next lemma shows that $\event$ happens with probability at least $1-\delta$. 
\begin{lemma}
We have $\P(\Gamma) \geq 1-\delta$. 
\end{lemma}
\begin{proof}
First, for each $x \in G$, denote $\noise(x) \defeq \what{f}(x) - f(x)$.  Then, since 
by our assumption, the noise $\{\what{f}(x) - f(x)\}_{x=1}^{T}$ is mean $0$, independent 
and subgaussian with parameter $\sigma^2$, we have that $\noise(x)$ is mean $0$ 
and subgaussian with parameter $\sigma^2/T$. Therefore, for any fix $x\in G$, 
\begin{align*}
\P\left(|\noise(x)| \geq r^a \right) 
		\leq 2\exp\left(-\frac{(r^a)^2T}{2\sigma^2}\right) 
			\leq \delta (2k+1)^{-d}., 
\end{align*}
where the first inequality above uses the subgaussianity of $\noise(x)$, and 
the second inequality uses the definition of $k$ and $T$. 
Now, the desired claim of the lemma follows from the fact that 
$|G| = (2k+1)^d$ and the union bound of the above events. 
\end{proof}
Since the assumption on $k, T$ shows that $T \geq 2k^4\log(2k+1)r^{-4}$, 
and therefore, we have, 
\begin{equation*}
r^a = \sigma \left(\frac{2\log \frac{2}{\delta} + 2d \log(2k+1)}{T}\right)^\half
	\leq \frac{\sigma r^2}{k^2} \left(\log \frac{2}{\delta}+ d\right)^\half, 
\end{equation*}
this shows that, on $\event$, for all $x\in\xdomain_{c, r}$ such that 
$\ltwo{x-x_{f, c, r}\opt} \geq \hat{r}$, we have, 
\begin{equation*}
f(x) - f(x_{f, c, r}\opt) \geq \left<\grad f(x_{f, c, r}\opt)), x- x_{f, c, r}\opt\right>
	+ \frac{\lambda}{2} \norm{x-x_{f, c, r}\opt}_2^2 \geq 
	\frac{\lambda}{2} \hat{r}^2 \geq \frac{Hr^2d}{2 k^2} + 2 r^a.
\end{equation*}
Hence, on event $\event$, we have, for all $x\in \xdomain$ satisfying 
$\norm{x-x_{f, c, r}\opt}_2 \geq \hat{r}$, we have, 
\begin{equation*}
\hat{f}(x) - \hat{f}(\bar{x}) = \underbrace{\left(\hat{f}(x) - f(x)\right)}_{\geq -r^a} 
	+ \underbrace{\left(f(x) - f(x_{f, c, r}\opt)\right)}_{\geq \frac{Hr^2d}{2k^2} + 2r^a}
	+ \underbrace{\left(f(x_{f, c, r}\opt) - f(\bar{x}) \right)}_{\geq -\frac{Hr^2d}{2k^2}}
	+ \underbrace{\left(f(\bar{x}) - \hat{f}(\bar{x})\right)}_{\geq -r^a} > 0, 
\end{equation*}
which gives us that on event $\event$, we must have 
\begin{equation*}
\norm{\hat{x} - x_{f, c, r}\opt}_2 \leq \hat{r}
	~~\Leftrightarrow~~ x_{f, c, r}\opt \in W.
\end{equation*}
Finally, since always $\hat{f}(\hat{x}) \leq \hat{f}(\bar{x})$, on $\event$, we have 
below upper bound on $f(\hat{x})$ on event $\event$:
\begin{equation*}
f(\hat{x}) - f(x_{f, c, r}\opt) = 
	\underbrace{f(\hat{x}) - \hat{f}(\hat{x})}_{\leq r^a}
	+ \underbrace{\hat{f}(\hat{x}) - \hat{f}(\bar{x})}_{\leq 0}
	+ \underbrace{\hat{f}(\bar{x}) - f(\bar{x})}_{\leq r^a}
 	+ \underbrace{f(\bar{x}) -  f(x_{f, c, r}\opt))}_{\leq \frac{Hr^2d}{2k^2}} 
	\leq \frac{Hr^2d}{2k^2} + 2r^a.
\end{equation*}
The desired claim of the proposition follows from $ \frac{Hr^2d}{2k^2} + 2r^a
	\leq \frac{\lambda}{2} \hat{r}^2$.
\end{proof}

Motivated by Proposition~\ref{proposition:zeroth-order-smooth-one-round}, 
it becomes important to understand when such $k$ exists in Eq 
\eqref{eqn:existence-of-k-one-round-zo} and how large it is. 

\begin{lemma}
\label{lemma:ef-zeroth-order-smooth-one-round}
Assume $n$ is large enough satisfying 
\begin{equation}
\label{eqn:ef-zeroth-order-smooth-one-round}
nr^4 \geq (6B)^{2(d+4)},~~\text{where}~
	B = 12\left(\frac{2\sigma}{\lambda} 
	\left(\log \frac{2}{\delta} + d\right)^\half + \frac{H}{\lambda}d\right)^\half.
\end{equation}
Denote $k(r)= \left(\gamma\left(nr^4\right)\right)^\frac{1}{d+4}$
and $k\opt = \floor{\frac{1}{3} k(r)}$. Then $k\opt\in \N$,
and $k\opt$ satisfies Eq~\eqref{eqn:existence-of-k-one-round-zo}.
\end{lemma}

\begin{proof}
%Note that, the second claim of the corollary follows immediately from its 
%first part and Proposition~\ref{proposition:zeroth-order-smooth-one-round}.
%To prove the first part, note first that, 
Note that, $\gamma(x) \geq \sqrt{x}$ whenever $x \geq 3$. 
Thus, by our assumption on $n$, we get that, 
\begin{equation*}
\left(\gamma(nr^4)\right)^\frac{1}{d+4} \geq 6B \geq 6.
\end{equation*}
This immediately gives us that $k\opt \geq 1$ and $k\opt$ satisfies the second 
inequality of Eq~\eqref{eqn:existence-of-k-one-round-zo}. Now, we 
show that $k\opt$ satisfies the first inequality of Eq~\eqref{eqn:existence-of-k-one-round-zo}.
In fact, when $k=k\opt$, we have, 
\begin{equation*}
(2k+1)^d \ceil{2k^4 \log(2k+1)} \leq (3k)^{d+4} \log (3k)^{d+4} \leq
	 (k(r))^{d+4} \log (k(r))^{d+4} \leq nr^4,
\end{equation*}
where the last inequality follows from the fact that, for any $x > 0$, 
$\gamma(x) \log \gamma(x) \leq x$.
\end{proof}
Proposition~\ref{proposition:zeroth-order-smooth-one-round} and 
Lemma~\ref{lemma:ef-zeroth-order-smooth-one-round} together 
immediately give us the corollary below. 


\begin{corollary}
\label{corollary:zeroth-order-smooth-one-round}
Given any fix $c \in \R^d$ and $r\in [0, 1]$, set $k = \floor{\frac{1}{3} 
	\gamma(nr^4)^\frac{1}{d+4}}$ and $T = \floor{\frac{n}{{2k+1}^d}}$.
Assume $n$ is large enough satisfying Eq~\eqref{eqn:ef-zeroth-order-smooth-one-round}.
Then, if we denote $\hat{r}$ and $\hat{x}$ to be the output of of Algorithm
\ref{alg:smoothcvxzo} when we input $(c, k, T, r)$ as the input parameters,
 we have, 
\begin{equation*}
\hat{r} \leq \min\{r, 6B r^\frac{d}{d+4} n^{-\frac{1}{d+4}} \log (nr^4)^\frac{1}{d+4}\}, 
~\text{where}~B = \left(\frac{2\sigma}{\lambda} 
	\left(\log \frac{2}{\delta} + d\right)^\half + \frac{H}{\lambda}d\right)^\half.
\end{equation*}
%\begin{equation*}
%nr^4 \geq (6B)^{2(d+4)},~~\text{where}~
%	B = 12\left(\frac{2\sigma}{\lambda} 
%	\left(\log \frac{2}{\delta} + d\right)^\half + \frac{H}{\lambda}d\right)^\half.
%\end{equation*}
%Denote $k(r)= \left(\gamma\left(nr^4\right)\right)^\frac{1}{d+4}$
%and $k\opt = \floor{\frac{1}{6} k(r)}$. Then $k\opt\in \N$,
%and $k\opt$ satisfies Eq~\eqref{eqn:existence-of-k-one-round-zo}.
%Moreover, we have, 
%\begin{equation*}
%\hat{r} \leq \min\{r, 12B r^\frac{d}{d+4} n^{-\frac{1}{d+4}} \log (nr^4)^\frac{1}{d+4}\}.
%\end{equation*}
%Finally, 
In addition, we get that, 
\begin{equation*}
\P\left(f(\hat{x}) - f(x_{f, c, r}\opt) \leq \gap\opt \right) \geq 1-\delta, 
\end{equation*}
where 
\begin{equation*}
\gap\opt = \half \lambda\hat{r}^2 \leq (6B)^2 
	r^\frac{2d}{d+4} n^{-\frac{2}{d+4}} \log (nr^4)^\frac{2}{d+4}.
\end{equation*}
\end{corollary}
%
%\begin{proof}
%Note that, the second claim of the corollary follows immediately from its 
%first part and Proposition~\ref{proposition:zeroth-order-smooth-one-round}.
%To prove the first part, note first that, $\gamma(x) \geq \sqrt{x}$ whenever $x \geq 3$. 
%Thus, by our assumption on $n$, we get that, 
%\begin{equation*}
%\left(\gamma(nr^4)\right)^\frac{1}{d+4} \geq \max \left\{6, B\right\}
%\end{equation*}
%This immediately gives us that $k\opt \geq 1$ and $k\opt$ satisfies the second 
%inequality of Eq~\eqref{eqn:existence-of-k-one-round-zo}. Now, we 
%show that $k\opt$ satisfies the first inequality of Eq~\eqref{eqn:existence-of-k-one-round-zo}.
%In fact, when $k=k\opt$, we have, 
%\begin{equation*}
%(2k+1)^d \ceil{2k^4 \log(2k+1)} \leq (3k)^{d+4} \log (3k)^{d+4} \leq
%	 (k(r))^{d+4} \log (k(r))^{d+4} \leq nr^4,
%\end{equation*}
%where the last inequality follows from the fact that, for any $x > 0$, 
%$\gamma(x) \log \gamma(x) \leq x$.
%\end{proof}
%

%
%Now, given Proposition~\ref{proposition:zeroth-order-smooth-one-round}, 
%it is important to know whether there exists some $k\in \N$ satisfying 
%Eq~\eqref{eqn:existence-of-k-one-round-zo}. For this purpose, we introduce 
%the following auxiliary function $r \to k(r)$: 
%\begin{equation}
%\label{eqn:definition-of-k-r-smooth-zo}
%k(r) = \frac{1}{6} \left(\frac{nH^2r^4d^2}{\max\left\{\sigma^2 
%	\log \frac{2d}{\delta}, \sigma^2 \log \frac{nH^2r^4d^2}{\sigma^2}, 
%		H^2 r^4d^2\right\}} \right)^\frac{1}{d+4}.
%\end{equation}
%\begin{corollary}
%\label{corollary:zeroth-order-smooth-one-round}
%Suppose $n$ is large enough so that 
%\begin{equation*}
%nH^2 r^4 d^2 \geq 
%	\left(\sigma^2\left(\log \frac{2d}{\delta} + 1\right) + H^2 r^4d^2\right)
%		\left(\frac{Hd}{\lambda}\right)^{(d+4)}
%%		~\text{and}~
%%			n \geq 2^\frac{d+2}{2}
%%				\left(\frac{H^2R^2}{25\sigma^2 \log \frac{2d}{\delta}}\right)^\frac{d}{2}
%\end{equation*} 
%then $k\opt = \ceil{k(r)}$ satisfies Eq~\eqref{eqn:existence-of-k-one-round-fo}.
%Moreover, we have,  
%\begin{equation*}
%\P\left(f(\hat{x}) - f(x_{f, c, r}\opt) \leq \gap\opt \right)\geq 1-\delta, 
%\end{equation*}
%where, $\gap\opt$ is defined by, 
%\begin{equation*}
%\gap\opt = \frac{r^2 Hd}{(k\opt)^2} \leq
%	36 r^2 Hd 
% \left(\frac{\max\left\{\sigma^2 \log \frac{2d}{\delta}, 
% 	\sigma^2 \log \frac{nH^2r^4d^2}{\sigma^2}, H^2 r^4d^2\right\}}{nH^2r^4 d^2} 
%		\right)^\frac{2}{d+4} 
%\end{equation*}
%\end{corollary}
%\begin{proof}
%Note that, the last claim of the lemma follows immediately from 
%proposition~\ref{proposition:zeroth-order-smooth-one-round}. Now, we show 
%that $k\opt \geq \frac{4Hd}{\lambda} \geq 1$ under our 
%assumptions. Note that, by elementary inequality that $x/\log(x) \geq \sqrt{x}$ 
%for all $x\geq 3$,  our first assumption on $n$ gives us that, 
%\begin{equation*}
%\frac{nH^2r^4d^2}{\sigma^2 \log \frac{nH^2r^4d^2}{\sigma^2}} 
%	\geq \left(\frac{nH^2r^4d^2}{\sigma^2}\right)^\half
%		\geq \left(\frac{Hd}{\lambda}\right)^\frac{d+4}{2}, ~
%			\frac{nH^2r^4d^2}{\sigma^2 \log \frac{2d}{\delta}} \geq 
%				\left(\frac{Hd}{\lambda}\right)^\frac{d+4}{2}
%	~~\text{and}~~ \frac{nH^2r^4d^2}{H^2 r^4d^2} \geq  
%		\left(\frac{Hd}{\lambda}\right)^\frac{d+4}{2}
%\end{equation*}
%which immediately leads to the elementary fact, 
%\begin{equation*}
%\left(\frac{nH^2r^4d^2}{\max\left\{\sigma^2 
%	\log \frac{2d}{\delta}, \sigma^2 \log \frac{nH^2r^4d^2}{\sigma^2}, 
%		H^2 r^4\right\}}\right)^{\frac{1}{d+4}}
%\geq \left(\frac{Hd}{\lambda}\right)^\half.
%\end{equation*}
%Hence, we get that, $k\opt \geq k(r) \geq \left(\frac{Hd}{\lambda}\right)^\half \geq 1$ by definition of 
%$k\opt$. Now, we show that $k \geq k\opt$. Indeed, it suffices to show that, 
%$k = k\opt$ satisfies below three inequalities, 
%\begin{equation}
%\label{eqn:inequality-k-one-zo}
%2(2k+1)^d \leq n,~
%	(2k+1)^d \log \frac{2d}{\delta}\leq \frac{nH^2r^4d^2}{8\sigma^2k^4} 
%		~\text{and}~
%			(2k+1)^d \log (2k+1)^d \leq \frac{nH^2r^4d^2}{8\sigma^2k^4}. 
%\end{equation}
%In fact, if the above three inequalities hold for $k = k\opt$, then we have, 
%when $k = k\opt$, 
%\begin{align*}
%&(2k+1)^d \ceil{\frac{2\sigma^2k^4}{H^2r^4d^2} \log \frac{2d(2k+1)^d}{\delta}} 
%\leq (2k+1)^d \left(1+\frac{2\sigma^2k^4}{H^2r^4d^2} \log \frac{2d(2k+1)^d}{\delta}\right) \\
%&=  \underbrace{(2k+1)^d}_{\leq \frac{n}{2}}
% + \underbrace{(2k+1)^d \frac{2\sigma^2k^4}{H^2r^4d^2} \log \frac{2d}{\delta}}_{\leq \frac{n}{4}}
% + \underbrace{(2k+1)^d \frac{2\sigma^2k^4}{H^2r^4d^2}\log (2k+1)^d}_{\leq \frac{n}{4}}\leq n, 
%\end{align*}
%and hence $k$ by its definition must satisfy $k \geq k\opt$. 
%We first show $k\opt$ satisfies the first inequality in 
%Eq~\eqref{eqn:inequality-k-one-zo}. Indeed, by definition of $k(r)$,
%one can easily see that, $k(r) \leq \frac{1}{6} n^{\frac{1}{d+4}}$. Hence, 
%\begin{equation*}
%2(2k\opt+1)^d \leq (6k(r))^{d+4} \leq n.
%\end{equation*}
%%In fact, notice that,
%%our second assumption on $n$ gives us that, 
%%\begin{equation*}
%%n \geq 2^\frac{d+2}{2}\left(\frac{H^2R^2}{25\sigma^2 \log \frac{2d}{\delta}}\right)^\frac{d}{2} 
%%~\Leftrightarrow \left(\frac{nH^2R^2}{25\sigma^2 
%%	\log \frac{2d}{\delta}}\right)^{\frac{d}{d+2}} \leq \frac{n}{2}.
%%\end{equation*}
%%Therefore, we get that, for $k = k_1\opt$, we have, 
%%\begin{equation*}
%%2(2k+1)^d \leq 2(3k)^d \leq \left(\frac{nH^2R^2}{25\sigma^2 
%%	\log \frac{2d}{\delta}}\right)^{\frac{d}{d+2}} \leq \frac{n}{2}, 
%%\end{equation*}
%%which proves the desired first inequality in Eq~\eqref{eqn:inequality-k-one}.
%Next, we show that $k = k\opt$ satisfies the second inequality in 
%Eq~\eqref{eqn:inequality-k-one-zo}. Again, by definition of $k(r)$, we have, 
%$k(r) \leq \frac{1}{6}\left(\frac{nH^2r^4d^2}{\sigma^2 \log \frac{2d}{\delta}}\right)^{\frac{1}{d+4}}$.
%Hence, we have, 
%\begin{equation*}
%(2k\opt+1)^d (k\opt)^4 \log \frac{2d}{\delta}\leq \frac{1}{81} 
%	(6k(r))^{d+4} \log \frac{2d}{\delta} 
%	\leq \frac{nH^2r^4d^2}{8\sigma^2}, 
%\end{equation*}
%which gives the second inequality in  Eq~\eqref{eqn:inequality-k-one-zo}. 
%Finally, we show that $k = k\opt$ satisfies the third (and also the last) 
%inequality in Eq~\eqref{eqn:inequality-k-one-zo}. To do so, denote
%$m = nH^2 r^4d^2/\sigma^2$, then $m \geq 3$ by our 
%first assumption on $n$. Note that, our definition of $k(r)$ satisfies 
%$k(r) \leq \frac{1}{6}\left(\frac{m}{\log m}\right)^{\frac{1}{d+4}}$, 
%or equivalently, $(6k(r))^{d+4} \leq \frac{m}{\log m}$. Hence, 
%\begin{equation*}
%(2k\opt+1)^d (k\opt)^4 \log (2k+1)^d \leq \frac{1}{81}(6k(r))^{d+4} 
%	\log (6k(r))^{d+4} \leq \frac{m}{81\log m} \log \frac{m}{\log m} 
%		\leq \frac{m}{81} \leq \frac{nH^2r^4 d^2}{8\sigma^2}, 
%\end{equation*}
%which gives the last inequality of Eq~\eqref{eqn:inequality-k-one-zo}. The
%desired claim of the corollary now follows. 
%\end{proof}


\subsection{Analysis of Algorithm~\ref{alg:smoothcvxzoRrounds}: Multi-Stage Analysis}
In this section, we show that with careful choice of input parameters 
$(c_1, r_1, k_1, T_1)$ and the updating rule, algorithm~\ref{alg:smoothcvxzoRrounds} 
returns some estimator $\hat{x}$ that is minimax optimal. To do so, our 
strategy is similar to the strategy used in algorithm~\ref{alg:smoothcvxfoRrounds}.
In fact, an important message from proposition~\ref{proposition:zeroth-order-smooth-one-round} 
shows that, given any $\delta > 0$, with appropriate choice of parameters, one 
can find some rectangular $W$ such that $x_f\opt$ lies inside the rectangular 
with probability at least $1-\delta$. This means that, after one round, one can 
`localize' the search of the optimum $x_f\opt$ by searching the optimum of 
$f$ inside $W$. Now, treating $W$ as the original $\xdomain$, one can 
thus get an improved rate of convergence in the second round.  


To be clear about how we specify the updating rule in line 3 of the algorithm 
\ref{alg:smoothcvxfoRrounds}, we summarize it as follows: given the output 
parameters $(\hat{c}_i, \hat{r}_i)$, we update $(c_{i+1}, r_{i+1}, k_{i+1}, T_{i+1})$ via
algorithm~\ref{alg:update-rule-smooth-fo-Rrounds}. The next proposition shows 
that with appropriate choice of the initial parameter, we have nice convergence 
guarantees for the output of algorithm~\ref{alg:smoothcvxfoRrounds}.

\begin{algorithm}[htp]
\caption{Updating Rule in Algorithm~\ref{alg:smoothcvxzoRrounds}}
\begin{algorithmic}
%\Procedure{updating rule}{}
  \Statex Input: $\hat{x}_i \in \R^d$ and $\hat{r}_i \in \R_+$.
  \State (i) Update $c_{i+1}$ coordinate-wisely via: 
  	\begin{equation*}
		c_{i+1, j} = \min\{1-\hat{r}_i, \max\{r_i, \hat{x}_{i, j}\}\},
	\end{equation*}
	where $\hat{x}_{i, j}$ and $c_{i+1, j}$ denote the $j$th coordinate of 
		$\hat{x}_i$ and $c_{i+1}$. 
  \State (ii) Update $r_{i+1}$ as $r_{i+1} = \hat{r}_i$.
  \State (iii) Update $k_{i+1}$ to be the largest $k \in \N$ such that 
  	\begin{equation} 
		\label{eqn:existence-of-k-i+1-r-rounds-zo}
		(2k + 1)^d \ceil{2k^4 \log (2k+1)} \leq nr_i^4.
	\end{equation}
  	 If no such $k$ exists, return FAIL. 
  \State (iv )Update $T_{i+1}$ to be $T_{i+1} = \floor{\frac{n}{(2k_i+1)^d}}$.
%\EndProcedure
\end{algorithmic}
\label{alg:update-rule-smooth-zo-Rrounds}
\end{algorithm}


Below proposition quantifies such `localization' idea and shows us that 
an appropriate choice of the initial parameter $(c_1, r_1, k_1, T_1)$ and 
appropriate choice of the updating rule gives back some estimator that 
optimal minimax rate. 

\begin{proposition}
\label{proposition:zeroth-order-smooth-r-rounds}
Let $\xdomain = [0, 1]^d$, and we are given the confidence level $\delta > 0$. 
We initialize the initial parameters as follows: set $c_1 = \half \cdot \ones$, 
$r_1 = \half$, and set $k_1$ to be the largest $k \in \N$ (if exists) such that, 
$(2k + 1)^d \ceil{2k^4 \log (2k+1)} \leq nr_1^4$ and $T_1 = \floor{\frac{n}{(2k_1 +1)^d}}$.
Consider algorithm~\ref{alg:smoothcvxzoRrounds} that uses algorithm
\ref{alg:update-rule-smooth-zo-Rrounds} to be the updating rule. Denote 
$\hat{x}_M$ and $r_{M+1} = \hat{r}_M$ to be the output of estimate and radius
from algorithm~\ref{alg:smoothcvxfoRrounds}. Now, assuming that the 
$\{k_i\}_{i=1}^M$ exist for Eq~\eqref{eqn:existence-of-k-i+1-r-rounds-fo} 
and satisfy the following lower bounds: 
\begin{equation}
\label{eqn:lower-bound-on-k-i-zo}
k_i \geq \left(\frac{2\sigma}{\lambda} 
		\left(\log \frac{2M}{\delta} + d\right)^\half + \frac{H}{\lambda}d\right)^\half.
	~\text{for all $1\leq i\leq M$}. 
\end{equation}
Then, we have, 
\begin{equation*}
\P\left(f(\hat{x}_M) - f(x_f\opt) \leq \half \lambda r_{M+1}^2\right)\geq 1-\delta. 
\end{equation*}
\end{proposition}

%
%\begin{proposition}
%\label{proposition:zeroth-order-smooth-r-rounds}
%Let $c_1 = \half \cdot \ones$ and $r_1 = \half$. For $\delta > 0$, suppose
%there exists $k \in \N$ satisfying,  
%\begin{equation}
%\label{eqn:existence-of-k1-r-rounds-zo}
%(2k + 1)^d \ceil{2k^4 \log(2k+1)} \leq n r_1^4 ~\text{and}~
%	k \geq \left(\frac{2\sigma}{\lambda} 
%		\left(\log \frac{2M}{\delta} + d\right)^\half + \frac{H}{\lambda}d\right)^\half.
%\end{equation}
%Set $k_1 = k$ satisfying Eq~\eqref{eqn:existence-of-k1-r-rounds-zo} and 
%$T_1 = \floor{\frac{n}{(2k_1 +1)^d}}$. 
%Consider the following updating rule for algorithm~\ref{alg:smoothcvxzoRrounds} (used 
%in the third line of algorithm~\ref{alg:smoothcvxzoRrounds}). In the $i$th iteration, 
%define $\hat{c}^\prime_i$ coordinate-wisely via 
%\begin{equation*}
%\hat{c}^\prime_{i, j} = \min\{1-\hat{r}_i, \max\{\hat{r}_i, \hat{c}_{i, j}\}\}
%\end{equation*}
%where for each $1\leq j\leq d$, $\hat{c}^\prime_{i, j}$ and $\hat{c}_{i, j}$ denote
%the $j$th coordinate of $\hat{c}^\prime$ and $\hat{c}_i$ respectively. 
%Update $c_{i+1} = \hat{c}^\prime_i$ and $r_{i+1} = \hat{r}_i$. Suppose there 
%exists some $k \in \N$ satisfying, 
%\begin{align}
%\label{eqn:existence-of-k-i+1-r-rounds-zo}
%(2k + 1)^d \ceil{2k^4 \log(2k+1)} \leq n r_{i+1}^4  ~\text{and}~
%	k \geq \left(\frac{2\sigma}{\lambda} 
%		\left(\log \frac{2M}{\delta} + d\right)^\half + \frac{H}{\lambda}d\right)^\half.
%\end{align}
%Set $k_{i+1} = k$ satisfying Eq~\eqref{eqn:existence-of-k-i+1-r-rounds-zo} and 
%$T_{i+1} = \floor{\frac{n}{(2k_{i+1} +1)^d}}$. 
%Let $\hat{x}_M$ and $r_{M+1} = \hat{r}_M$ denote the output of estimate and 
%radius respectively from algorithm~\ref{alg:smoothcvxzoRrounds}. Then, we have, 
%\begin{equation*}
%\P\left(f(\hat{x}_M) - f(x_f\opt) \leq \half \lambda r_{M+1}^2\right)\geq 1-\delta. 
%\end{equation*}
%\end{proposition}

\begin{proof}
Note that, when $M = 1$, proposition~\ref{proposition:zeroth-order-smooth-r-rounds} reduces 
to proposition~\ref{proposition:zeroth-order-smooth-one-round}. When $M > 1$, note that, 
the definition of $k_1$ and $T_1$ take the same form as that in Eq~\eqref{eqn:existence-of-k-one-round-zo} 
However, the condition of $k_1$ in Eq~\eqref{eqn:lower-bound-on-k-i-zo} changes 
by substituting $\delta$ by $\delta/M$. Denote $W_1^\prime = \{x\in \xdomain: 
\norm{x-\hat{x}_1}_\infty \leq \hat{r}_1\}$, then proposition 
\ref{proposition:zeroth-order-smooth-one-round} asserts that 
$\P(x_f\opt \in W_1) \geq 1-\delta/M$.
Now, denote $W_1 = \{x: \norm{x-x_2}_\infty \leq r_2\}$. 
Then $W_1 \subseteq W_1^\prime \subseteq \xdomain$ by definition of 
$\hat{x}_1^\prime$ and the fact that $\hat{r}_1 \leq r$. Thus, we have 
$\P(x_f\opt \in W_1) \geq \P(x_f\opt \in W_1^\prime) \geq 1-\delta/M$. Now, in the second 
round of sampling and estimation, the algorithm essentially view $W_1$ as the 
entire domain $\xdomain$ and sample the points using the same strategy as that in the 
first round.  Since the noise in the second round is independent of the noise in the 
first round,  by proposition~\ref{proposition:zeroth-order-smooth-one-round} that 
$\P\left(x_f\opt \in W_2 \mid x_f\opt \in W_1\right) \geq 1-\delta/M$, where we 
define analogously $W_2 = \{x: \norm{x-x_3}_\infty \leq r_3\}$. The same 
reasoning applies to any round, which shows that, we have 
$\P\left(x_f\opt \in W_{i+1} \mid x_f\opt \in W_i\right) \geq 1-\delta/M$, 
where we define $W_i$ analogously by $W_i= \{x: \norm{x-x_{i+1}}_\infty \leq r_{i+1}\}$. 
In addition to that, in the $M$th round, with the same reasoning as proposition
\ref{proposition:zeroth-order-smooth-one-round}, one can easily show that 
$\P\left(f(\hat{x}_M) - f(x_f\opt) \leq \frac{\lambda}{2} r_{M+1}^2
		 \mid x_f\opt \in W_{M-1}\right) \geq 1-\delta/M$.
Thus, if we denote via convection that $W_0 = \xdomain$, then we have,  
\begin{align*}
&\P \left(f(\hat{x}_M) - f(x_f\opt) \leq \frac{\lambda}{2} r_{M+1}^2\right) \\
	&= \P\left(f(\hat{x}_M) - f(x_f\opt) \leq \frac{\lambda}{2} r_{M+1}^2
		 \mid x_f\opt \in W_{M-1}  \right) \cdot 
	\Pi_{i=1}^{M-1} \left[\P\left(x_f\opt \in W_{i} \mid x_f\opt \in W_{i-1}\right)\right]  \\
		&\geq \left(1-\delta/M\right)^M \geq 1-\delta, 
\end{align*} 
which gives the desired claim of the proposition.
\end{proof}

Motivated by Proposition~\ref{proposition:zeroth-order-smooth-r-rounds}, it 
becomes important to understand when $k_i \in \N$ exists to satisfy both 
Eq~\eqref{eqn:existence-of-k-i+1-r-rounds-zo} and Eq~\eqref{eqn:lower-bound-on-k-i-zo}.


\begin{lemma}
\label{lemma:ef-zeroth-order-smooth-r-rounds}
Assume $n$ is large enough satisfying the bounds below: 
\begin{equation}
\label{eqn:ef-n-condition-zo-smooth-r-rounds}
\log \log n \geq M \log \left(1+ \frac{4}{d}\right)
	+ \log \left(2M \log(12B \log n) + (2d+6)\log (6B)\right).
\end{equation}
where we denote $B$ to be $
B = \left(\frac{2\sigma}{\lambda} 
	\left(\log \frac{2}{\delta} + d\right)^\half + \frac{H}{\lambda}d\right)^\half.$
Then, the sequence $\{r_i\}_{i=1}^M$, $\{k_i\}_{i=1}^M$ and $\{T_i\}_{i=1}^M$ are 
well defined via algorithm~\ref{alg:update-rule-smooth-zo-Rrounds}. In addition, 
the sequence $\{k_i\}_{i=1}^M$ satisfy Eq~\eqref{eqn:lower-bound-on-k-i-zo}.
Finally, the output $\{r_i\}_{i=1}^M$ satisfy the bound below: 
\begin{equation*}
r_i \leq (6B)^{\frac{d+4}{4} \left(1-\left(\frac{d}{d+4}\right)^M\right)}
		D_n^{\frac{1}{4} \left(1 - \left(\frac{d}{d+4}\right)^M\right)}
		n^{-\frac{1}{4} \left(1 - \left(\frac{d}{d+4}\right)^M\right)}~~\text{for all $1\leq i\leq M$}.
\end{equation*}
\end{lemma}

%
%\begin{corollary}
%\label{corollary:zeroth-order-smooth-r-rounds}
%Suppose $n$ is large enough so that 
%\begin{equation*}
%\log \log n \geq M \log \left(1+ \frac{4}{d}\right)
%	+ \log \left(2M \log(12B \log n) + 4\log 2\right),
%\end{equation*}
%where we denote $B$ to be the constant: 
%\begin{equation*}
%B = 12\left(\frac{2\sigma}{\lambda} 
%	\left(\log \frac{2M}{\delta} + d\right)^\half + \frac{H}{\lambda}d\right)^\half.
%\end{equation*}
%Then, if we define recursively the sequences $\{k_i\}_{i=1}^M$ and 
%$\{r_i\}_{i=1}^{M+1}$ as follows: 
%\begin{equation*}
%r_1 = \half, ~k_i = \floor{\frac{1}{6} (\gamma(nr_i^4))^\frac{1}{d+4}}
%	~~\text{and}~~
%	r_{i+1} =  \frac{B r_i}{k_i}
%\end{equation*}
%Then, the sequences $\{k_i\}_{i=1}^M$ and $\{r_i\}_{i=1}^{M+1}$ satisfy 
%Eq~\eqref{eqn:existence-of-k-i+1-r-rounds-zo}, and in addition, 
%\begin{equation*}
%r_{M+1} \leq (12B)^{\frac{d+4}{4} \left(1-\left(\frac{d}{d+4}\right)^M\right)}
%		(\log n)^{\frac{1}{4} \left(1 - \left(\frac{d}{d+4}\right)^M\right)}
%		n^{-\frac{1}{4} \left(1 - \left(\frac{d}{d+4}\right)^M\right)}.
%\end{equation*}
%Finally, if we denote $\gap_M\opt$ to be the following quantity: 
%\begin{equation*}
%\gap_M\opt \defeq \half \lambda r_{M+1}^2
%	\leq  \half \lambda B^{\frac{d+4}{2} \left(1-\left(\frac{d}{d+4}\right)^M\right)}
%		\log n^{\half \left(1 - \left(\frac{d}{d+4}\right)^M\right)}
%		n^{-\half \left(1 - \left(\frac{d}{d+4}\right)^M\right)}
%\end{equation*}
%then we have, 
%\begin{equation*}
%\P\left(f(\hat{x}_M) - f(x_f\opt) \leq \gap_M\opt \right)\geq 1-\delta.
%\end{equation*}
%\end{corollary}
\begin{proof}
%It is helpful to introduce some new notations. Denote $B, C, D_n$ as the following: 
%\begin{equation*}
%B = \frac{Hd}{\lambda},  
%	C = \max\left\{\frac{H^2 d^2}{\sigma^2 \log \frac{2d}{\delta/M}}, 1\right\}
%~~\text{and}~~D_n = 
%	\max \left\{\frac{\sigma^2}{H^2d^2} \log \frac{2d}{\delta/M}, 
%		\frac{\sigma^2}{H^2d^2} \log \frac{nH^4d^2}{\sigma^2}, 1\right\}.
%\end{equation*}
We prove the desired claim of the corollary via induction. Our strategy is to 
show via induction that the below hypothesis hold for all 
$1\leq i \leq M$.
\begin{align}
%\text{Induction Hypothesis for $1\leq i\leq M$: }
&(i) nr_i^4 \geq(6B)^{2(d+4)}~
(ii)k_i \in \N~\text{and}~k_i \geq B	\label{eqn:induction-hypothesis-one-zo} \\
&(iii)~r_i \leq \min
	\left\{1, (6B)^{\frac{d+4}{4}\left(1-\left(\frac{d}{d+4}\right)^{i-1}\right)}
		(\log n)^{\frac{1}{4}\left(1-\left(\frac{d}{d+4}\right)^{i-1}\right)}
		n^{-\frac{1}{4}\left(1-\left(\frac{d}{d+4}\right)^{i-1}\right)}\right\}
\label{eqn:induction-hypothesis-two-zo}.
\end{align} 
We first show the base case $i=1$. Note that, the first part of Eq
\eqref{eqn:induction-hypothesis-one-zo} is implied by the assumption 
on $n$, the second part of Eq~\eqref{eqn:induction-hypothesis-one-zo}
follows from the first part and corollary~\ref{corollary:zeroth-order-smooth-one-round},
and Eq~\eqref{eqn:induction-hypothesis-two-zo} is trivial when $i=1$. 
Now, for some $i < M$, assuming that the induction hypothesis holds for 
all $j \in \{1, 2, \ldots, i\}$, we show that the hypothesis holds for $i+1$.  
We first show the second part of Eq~\eqref{eqn:induction-hypothesis-one-zo}
for $i+1$. Indeed, by induction hypothesis, we know that, the first 
inequality of Eq~\eqref{eqn:induction-hypothesis-one-zo} is true for $i$,
and thus the second part of Eq~\eqref{eqn:induction-hypothesis-one-zo}
follows from a direct application of corollary~\ref{corollary:zeroth-order-smooth-one-round}
(by substituting $\delta$ by $\delta/M$ and $r$ by $r_i$ there).  Next, 
we show Eq~\eqref{eqn:induction-hypothesis-two-zo} holds for $i+1$. 
Again, we know from corollary~\ref{corollary:zeroth-order-smooth-one-round} that
 $r_{i+1} \leq r_i \leq 1$, and the bound that, $
 r_{i+1} \leq 6B r_i^\frac{d}{d+4} n^{-\frac{1}{d+4}}\left(\log n\right)^\frac{1}{d+4}$.
 Now, using the induction hypothesis of the upper bound on $r_i$, we get, 
 \begin{align*}
r_{i+1}  &\leq 6B \left((6B)^{\frac{d+4}{4}\left(1-\left(\frac{d}{d+4}\right)^{i-1}\right)}
		\left(\log n\right)^{\frac{1}{4}\left(1-\left(\frac{d}{d+4}\right)^{i-1}\right)}
		n^{-\frac{1}{4}\left(1-\left(\frac{d}{d+4}\right)^{i-1}\right)}\right)
			^\frac{d}{d+4} (\log n)^\frac{1}{d+4} n^{-\frac{1}{d+4}}  \\
	&= (6B)^{\frac{d+4}{4}\left(1-\left(\frac{d}{d+4}\right)^{i}\right)}
		\left(\log n\right)^{\frac{1}{4}\left(1-\left(\frac{d}{d+4}\right)^{i}\right)}
		n^{-\frac{1}{4}\left(1-\left(\frac{d}{d+4}\right)^{i}\right)},
 \end{align*}
%Note that corollary~\ref{corollary:first-order-smooth-one-round} also provides 
%the following lower bound on $k_{i+1}$: 
%\begin{equation}
%\label{eqn:lower-bound-induction-on-k-fo}
%k_{i+1} \geq  \frac{1}{3} \left(\frac{nH^2r_i^2}
%	{\max \left\{ \sigma^2 \log \frac{2d}{\delta/M}, 
%		\sigma^2 \log \frac{nH^2r_i^2}{25\sigma^2}, 
%			H^2 r_i^2\right\}} \right)^\frac{1}{d+2}.
%\end{equation}
so we have shown the Eq~\eqref{eqn:induction-hypothesis-two-zo} for $i+1$. 
%
%Recall the definition of $r_{i+1}$:
%\begin{equation}
%\label{eqn:recursion-of-r-i}
%r_{i+1} = \left(\frac{Hd}{\lambda}\right)^\half \cdot \frac{r_i}{k_{i}} = B^\half \cdot \frac{r_i}{k_i}.
%\end{equation}
%Since, $k_i > B^\half$ (based on the induction hypothesis of the second 
%part of Eq~\eqref{eqn:induction-hypothesis-one-zo} for $i$), we get easily 
%$r_{i+1} \leq r_i \leq 1$. In addition, since $k_{i} = \ceil{k(r_i)} \geq k(r_{i})$, 
%we get that, 
%\begin{equation}
%\label{eqn:direct-upper-bound-on-r-i-plus-one-zo}
%r_{i+1} = B^\half \cdot \frac{r_i}{k_{i}}  
%	\leq 6B^\half r_i^{\frac{d}{d+4}}
%		\left(\frac{\max\left\{\sigma^2 \log \frac{2d}{\delta/M}, 
%			\sigma^2 \log \frac{nH^2r_i^4 d^2}{\sigma^2}, H^2 r_i^4 d^2\right\}}
%				{nH^2d^2}\right)^{\frac{1}{d+4}}
%	\leq B r_i^{\frac{d}{d+4}}  (D_n/n)^{\frac{1}{d+4}}, 
%\end{equation}
%where in the last inequality, we use $r_i \leq 1$ from our induction hypothesis. 
%Now, plugging the upper bound of $r_i$ in
%Eq~\eqref{eqn:induction-hypothesis-two-zo} into 
%Eq~\eqref{eqn:direct-upper-bound-on-r-i-plus-one-zo}, we get, 
%\begin{align*}
%r_{i+1} \leq B r_i^{\frac{d}{d+4}} (D_n/n)^{\frac{1}{d+4}}
%	\leq B^{\frac{d+4}{4} \left(1-\left(\frac{d}{d+4}\right)^i\right)}
%		D_n^{\frac{1}{4} \left(1 - \left(\frac{d}{d+4}\right)^i\right)}
%		n^{-\frac{1}{4} \left(1 - \left(\frac{d}{d+4}\right)^i\right)}.
%\end{align*}
%Together with the proved bound $r_{i+1} \leq 1$, we have shown the 
%inequality in Eq~\eqref{eqn:induction-hypothesis-two-zo} for $i+1$. 
Finally, we show the first part of Eq~\eqref{eqn:induction-hypothesis-one-zo} 
for $i+1$. Note that, by definition of $\{r_i\}_{i=1}^M$, 
\begin{equation*}
r_{i+1} = r_1 \cdot \Pi_{j=1}^{i} \frac{r_{j+1}}{r_{j}} = r_1 \cdot 
	B^i \cdot 
	\frac{1 }{\Pi_{j=1}^{i} k_j} = \frac{B^i}{2\Pi_{j=1}^{i} k_j}. 
\end{equation*}
Hence, the first part of Eq~\eqref{eqn:induction-hypothesis-one-zo} 
for $i+1$ is equivalent to
\begin{equation}
\label{eqn:induction-condition-inequality-two-zo}
nB^{i} \geq  16 \cdot (6B)^{2(d+4)} \cdot \Pi_{j=1}^{i} k_j^4. 
\end{equation}
To establish Eq~\eqref{eqn:induction-condition-inequality-two-zo}, note 
first that, by definition of $k_j$, we have, for all $j \leq i$, 
\begin{equation*}
k_j \leq \frac{1}{3} (\gamma(n r_j^4))^\frac{1}{d+4} \leq 
	 \frac{1}{3} (nr_j^4)^{\frac{1}{d+4}}. 
\end{equation*}
Now, using Eq~\eqref{eqn:induction-condition-inequality-two-zo} 
from the induction hypothesis for $j \leq i$, we get that, 
\begin{equation*}
k_j \leq (n r_j^4)^\frac{1}{d+4}  \leq 
	(6B)^{\left(1-\left(\frac{d}{d+4}\right)^i\right)}
	n^{\frac{1}{d+4}(\frac{d}{d+4})^{j-1}} (\log n)^{\frac{1}{d+4}\left(1-\left(\frac{d}{d+4}\right)^i\right)}
	\leq 6 B n^{\frac{1}{d+4}(\frac{d}{d+4})^{j-1}} (\log n).
\end{equation*}
Hence, to prove Eq~\eqref{eqn:induction-condition-inequality-two-zo}, it 
suffices to show that, for all $1\leq i\leq M$, 
\begin{equation*}
nB^i \geq 16  (6B)^{2(d+4)} (6B \log n)^{2i}
	n^{\frac{4}{d+4}\sum_{j=0}^{i-1} \left(\frac{d}{d+4}\right)^j}
	= 16 (6B)^{2(d+4)} (6B \log n)^{2i} n^{1- \left(\frac{d}{d+4}\right)^i}.
\end{equation*}
Note that, it suffices if $n$ is large enough satisfying the bound below, 
\begin{equation*}
n^{\left(\frac{d}{d+4}\right)^M} \geq 16 (6B)^{2(d+4)} (6B\log n)^{2M},
\end{equation*}
which would suffice if
\begin{equation*}
\log \log n \geq M \log \left(1+ \frac{4}{d}\right)
	+ \log \left(2M \log(6B \log n) + (2d+6)\log (6B)\right).
\end{equation*}
\end{proof}
Now, Proposition~\ref{proposition:zeroth-order-smooth-r-rounds} and Lemma 
\ref{lemma:ef-zeroth-order-smooth-r-rounds} together immediately give the 
corollary below. 

\begin{corollary}
\label{corollary:zeroth-order-smooth-r-rounds}
Let $\xdomain = [0, 1]^d$. Consider algorithm~\ref{alg:smoothcvxzoRrounds}. 
Suppose we use the same initialization rule as that in Proposition
\ref{proposition:zeroth-order-smooth-r-rounds} and use Algorithm
\ref{alg:update-rule-smooth-zo-Rrounds} to be the updating rules 
for algorithm~\ref{alg:smoothcvxzoRrounds}, then, when $n$ is large enough 
so that it satisfies Eq~\eqref{eqn:ef-n-condition-zo-smooth-r-rounds}, then
the output $\hat{x}_M$ from algorithm~\ref{alg:smoothcvxzoRrounds} satisfies 
\begin{equation*}
\P\left(f(\hat{x}_M) - f(x_f\opt) \leq \gap_M\opt \right)\geq 1-\delta 
\end{equation*}
with 
\begin{equation*}
\gap_M\opt \defeq \half \lambda (6B)^{\frac{d+4}{2} \left(1-\left(\frac{d}{d+4}\right)^M\right)}
		(\log n)^{\half \left(1 - \left(\frac{d}{d+4}\right)^M\right)}
		n^{-\half \left(1 - \left(\frac{d}{d+4}\right)^M\right)}.
\end{equation*}
\end{corollary}


%
%
%\begin{corollary}
%\label{corollary:zeroth-order-smooth-r-rounds}
%Denote $(c_1, r_1, k_1, T_1)$ the same as that in the statement of 
%proposition~\ref{proposition:zeroth-order-smooth-r-rounds}.
%Let $\hat{x}_M$ denote the output of algorithm~\ref{alg:smoothcvxzoRrounds} 
%with input parameters $(c_1, r_1, k_1, T_1)$. Suppose that $n$ is large 
%enough such that, 
%$n \geq 2^\frac{d+4}{4}\left(\frac{H^2d^2}{10^3\sigma^2 \log \frac{2d}{\delta/M}}\right)^\frac{d}{4}$
%and $\log\log \frac{nH^2d^2}{10^3\sigma^2} \geq M \log \frac{d+4}{d} + \log \frac{d+4}{4} +\residual$, 
%where we define $\residual$ as 
%\begin{align*}
%\residual =  \log \left(\log \left(\log \frac{2d}{\delta/M}+1\right)
%	(4M+2(d+4)) \log \frac{3\sqrt{Hd}}{\sqrt{\lambda}} + \frac{4M}{d+4} \log \log \max 
%		\left\{\frac{nH^2d^2}{10^3\sigma^2}, \frac{2d}{\delta/M}\right\}\right)
%\end{align*}
%Then, for each $1\leq i\leq M$, $k_i$ is well defined. Finally, if we denote 
%$\gap_M\opt$ as
%\begin{equation*}
%\gap_M\opt \defeq \frac{\lambda}{8}\left(\frac{3\sqrt{Hd}}{\sqrt{\lambda}}\right)^\frac{d+4}{2}
%		\left(\frac{10^3\sigma^2\log \max\left\{\frac{nH^2}{10^3\sigma^2}, 
%			\frac{2d}{\delta/M}\right\}}{nH^2}\right)^
%			{\half \left(1-\left(\frac{d}{d+4}\right)^{M}\right)}, 
%\end{equation*}
%then we have, 
%\begin{equation*}
%\P\left(f(\hat{x}_M) - f(x_f\opt) \leq \gap_M\opt \right)\geq 1-\delta.
%\end{equation*}
%\end{corollary}
%\begin{proof}
%Before we start to prove the corollary, we introduce some new notations. 
%Denote $m$ as 
%\begin{equation*}
%m = \frac{nH^2d^2}{10^3\sigma^2}~\text{and}~B = \frac{3\sqrt{Hd}}{\sqrt{\lambda}}.
%\end{equation*}
%Now, we show via induction that the below hypothesis hold for all 
%$1\leq i \leq M$.
%\begin{align}
%%\text{Induction Hypothesis for $1\leq i\leq M$: }
%&(i)mR_i^4 \geq 
%	\left(\log \frac{2d}{\delta/M} + 1\right)B^{2(d+4)}, 
%	(ii)~k_i~\text{well defined}
%		\label{eqn:induction-hypothesis-one-zo} \\
%~\text{and}~~&(iii)~R_i \leq \min
%	\left\{1, 
%		B^{\frac{d+4}{4}\left(1-\left(\frac{d}{d+4}\right)^{i-1}\right)}
%		\left(\frac{\log \max\left\{m, \frac{2d}{\delta/M}\right\}}{m}\right)^
%			{\frac{1}{4} \left(1-\left(\frac{d}{d+4}\right)^{i-1}\right)}\right\}
%\label{eqn:induction-hypothesis-two-zo}.
%\end{align} 
%To prove the base hypotheses ($i=1$), we note that, the first part of
%Eq~\eqref{eqn:induction-hypothesis-one-zo} is ensured by our assumption on $n$, 
%the second part of Eq~\eqref{eqn:induction-hypothesis-one-zo} follows from 
%corollary~\ref{corollary:zeroth-order-smooth-one-round}, and 
%Eq~\eqref{eqn:induction-hypothesis-two-zo} immediately follows from the fact 
%that $R_1 = 1$.  Now, assume that the induction hypothesis 
%holds for all $j = \{1, 2, \ldots, i\}$ where $i < M$. Now, we are going to show that 
%the induction hypothesis also holds for $i+1$. Note that, by definition, we have, 
%\begin{equation*}
%R_{i+1} = \frac{\sqrt{Hd}}{2\sqrt{\lambda}} \cdot \frac{R_i}{k_i}~~\text{and}~~
%k_{i+1} = \max_{k\in \N} \left\{(2k + 1)^d \ceil{\frac{2048\sigma^2 k^4}{H^2 R_{i+1}^4d^2} 
%	\log \frac{2(2k + 1)^d }{\delta/M}} \leq n\right\}.
%\end{equation*}
%Note first that, if we use the induction hypothesis for $j=i$ and our assumption on $n$, 
%then we know that, 
%\begin{equation*}
%\frac{nH^2R_i^4d^2}{10^3\sigma^2} \geq 
%	\left(\log \frac{2d}{\delta/M} + 1 \right)\left(\frac{3\sqrt{Hd}}{\sqrt{\lambda}}\right)^{2(d+4)}
%		~\text{and}~
%			n \geq 2^\frac{d+4}{4}
%				\left(\frac{H^2R_i^4d^2}{10^3\sigma^2 \log \frac{2d}{\delta/M}}\right)^\frac{d}{4}.
%\end{equation*} 
%Hence, we can apply corollary~\eqref{corollary:zeroth-order-smooth-one-round} 
%with $R= R_i$ to show that not only $k_{i+1} \in \N$ is well-defined, but also 
%$k_{i+1} \geq \frac{\sqrt{Hd}}{\sqrt{\lambda}}$. 
%This shows the second part of hypothesis~\eqref{eqn:induction-hypothesis-one-zo} for 
%$j = i+1$. In addition, corollary~\eqref{corollary:zeroth-order-smooth-one-round} also 
%provides the below lower bounds on $k_{i+1}$, 
%\begin{equation}
%\label{eqn:lower-bound-induction-on-k-zo}
%k_{i+1} \geq  \frac{1}{3} \left(\frac{nH^2R_i^4d^2}{10^3\sigma^2 
%	\log \max \{ \frac{2d}{\delta/M}, \frac{nH^2R_i^4d^2}{10^3\sigma^2}\}} \right)^\frac{1}{d+4}.
%\end{equation}
%Therefore, we have, 
%$R_{i+1} = \frac{\sqrt{Hd}}{2\sqrt{\lambda}} \cdot \frac{R_i}{k_{i+1}} \leq  R_i \leq 1$. 
%In addition to that, use Eq~\eqref{eqn:lower-bound-induction-on-k-zo}, we have, 
%\begin{equation*}
%R_{i+1} = \frac{\sqrt{Hd}}{2\sqrt{\lambda}} \cdot \frac{R_i}{k_{i+1}} 
%	\leq   \frac{3R_i^\frac{d}{d+4}\sqrt{Hd}}{2\sqrt{\lambda}} \left(\frac{10^3\sigma^2 
%		\log \max \left\{\frac{2d}{\delta/M}, 
%			\frac{nH^2R_i^4d^2}{10^3\sigma^2}\right\}}{nH^2d^2}\right)^\frac{1}{d+4}
%\end{equation*}
%Now, use the fact that, $R_i \leq 1$, we get that, 
%\begin{equation*}
%R_{i+1} \leq B R_i^\frac{d}{d+4} 
%	\left(\frac{\log \max\left\{m, \frac{2d}{\delta/M}\right\}}{m}\right)^\frac{1}{d+4}.
%\end{equation*}
%Now, use our inductive assumption, we get that $R_i \leq 
%B^{\frac{d+4}{4}\left(1-\left(\frac{d}{d+4}\right)^{i-1}\right)} 
%	\left(\frac{\log \max\left\{m, \frac{2d}{\delta/M}\right\}}{m}\right)^{\frac{1}{4}
%		\left(1-\left(\frac{d}{d+4}\right)^{i-1}\right)}$, thus we have, 
%\begin{equation*}
%R_{i+1} \leq B R_i^\frac{d}{d+4} 
%	\left(\frac{\log \log \max\left\{m, \frac{2d}{\delta/M}\right\}}{m}\right)^\frac{1}{d+4} 
%	\leq B^{\frac{d+4}{4}\left(1-\left(\frac{d}{d+4}\right)^{i}\right)} 
%	\left(\frac{\log \max\left\{m, \frac{2d}{\delta/M}\right\}}{m}\right)^{\frac{1}{4}
%		\left(1-\left(\frac{d}{d+4}\right)^{i}\right)}. 
%\end{equation*}
%This together with the proved bound that $R_{i+1} \leq 1$, shows that hypothesis 
%\eqref{eqn:induction-hypothesis-two-zo} holds for $i+1$. Finally, we show that the 
%first part of Eq~\eqref{eqn:induction-hypothesis-one-zo} holds for $i+1$. To do so, 
%we note that, by definition of $\{R_i\}_{i=1}^M$, we have, 
%\begin{equation*}
%R_{i+1} = \Pi_{j=1}^{i} \frac{R_{j+1}}{R_{j}} = \left(\frac{\sqrt{Hd}}{2\sqrt{\lambda}}\right)^{i} 
%	\frac{R_1}{\Pi_{j=1}^{i} k_j} \geq \frac{1}{\Pi_{j=1}^{i} k_j}. 
%\end{equation*}
%Therefore, to show the first part of Eq~\eqref{eqn:induction-hypothesis-one-zo} holds for $i+1$,
%it suffices to show that, 
%\begin{equation}
%\label{eqn:induction-condition-inequality-two-zo}
%m \geq \left(\log \frac{2d}{\delta/M} + 1\right)B^{2(d+4)} \left(\Pi_{j=1}^{i} k_j^4\right).
%\end{equation}
%Now, since we have $2 \log \frac{2(2k_j + 1)^d }{\delta/M} \geq 2\log(2) > 1$, 
%we have by definition of $k_j$ for $1\leq j\leq i$ that, 
%\begin{equation*}
%n \geq (2k_j+1)^d \frac{2048\sigma^2k_j^4}{H^2 R_j^4d^2}\log \frac{2(2k + 1)^d }{\delta/M} 
%	\geq k_j^{d+4} \frac{10^3\sigma^2}{H^2 R_j^4d^2} 
%		\Rightarrow \left(mR_j^4\right)^\frac{1}{d+4} \geq k_j.
%\end{equation*}
%Now, we use induction hypothesis to get that, for $1\leq j\leq i$, we have, 
%\begin{align*}
%k_j^4 \leq \left(mR_j^4\right)^\frac{4}{d+4} 
%	&\leq \left(mB^{d+4} 
%		\left(\frac{\log \max\left\{m, \frac{2d}{\delta/M}\right\}}
%			{m}\right)^{\left(1-\left(\frac{d}{d+4}\right)^{j-1}\right)}\right)^\frac{4}{d+4} \\
%	&\leq B^4 \left(\log \max\left\{m, \frac{2d}{\delta/M}\right\}\right)^\frac{4}{d+4} m^{\frac{4}{d+4} \left(\frac{d}{d+4}\right)^{j-1}}.
%\end{align*}
%Therefore, we have the following upper bound on $\Pi_{j=1}^{i} k_j^4$,
%\begin{align*}
%\Pi_{j=1}^{i} k_j^4 &\leq B^{4i} \left(\log \max\left\{m, \frac{2d}{\delta/M}\right\}\right)^\frac{4i}{d+4} 
%	m^{\sum_{j=1}^{i}\frac{4}{d+4} \left(\frac{d}{d+4}\right)^{j-1}} \\
%	&\leq  B^{4M} \left(\log \max\left\{m, \frac{2d}{\delta/M}\right\}\right)^\frac{4M}{d+4} m^{\sum_{j=1}^{M}
%		\frac{4}{d+4} \left(\frac{d}{d+4}\right)^{j-1}}.
%\end{align*}
%Now use the fact that, $1-\sum_{j=1}^M \frac{4}{d+4} \left(\frac{d}{d+4}\right)^{j-1} 
%	\geq \frac{4}{d+4} \left(\frac{d}{d+4}\right)^M$. Therefore, to prove 
%Eq~\eqref{eqn:induction-condition-inequality-two-zo}, it suffices to show that, 
%\begin{equation*}
%m^{\left(\frac{4}{d+4}\left(\frac{d}{d+4}\right)^M\right)} 
%	\geq \left(\log \frac{2d}{\delta/M} + 1\right) B^{2(d+4)+4M}
%		\left(\log \max\left\{m, \frac{2d}{\delta/M}\right\}\right)^\frac{4M}{d+4}, 
%\end{equation*}
%which is equivalent to 
%\begin{align*}
%\log \log m &\geq M \log \frac{d+4}{d} + \residual, 
%\end{align*}
%which is indeed equivalent to our first assumption on $n$. Hence, we have 
%shown the desired first part in hypothese~\eqref{eqn:induction-hypothesis-one-zo}. 
%Altogether, by induction principle, the hypothesis in Eq~\eqref{eqn:induction-hypothesis-one-zo} 
%and Eq~\eqref{eqn:induction-hypothesis-two-zo} is proved for all $1\leq i\leq M$.
%Note that, the above deduction also implies that Eq~\eqref{eqn:induction-hypothesis-two-zo}
%holds when $i = R+1$. The second claim of the corollary follows from the 
%upper bound Eq~\eqref{eqn:induction-hypothesis-two-zo} for $r_{M+1}$ and 
%proposition~\ref{proposition:zeroth-order-smooth-r-rounds}.
%\end{proof}
%
% 


\newcommand{\logtwo}{\log_2}

\section{Lipschitz Function with First-Order Oracle}
\label{sec:proof-Lip-fo}
\subsection{Lipschitz Function with First-Order Oracle, $d = 1$}
\subsubsection{Description of Algorithm}
In this section, we propose a generic algorithm: Algorithm~\ref{alg:Lipcvxfo}
to solve the problem when the dimension $d = 1$. The algorithm is a one-round 
algorithm. As will be seen immediately, this one-round algorithm achieves the 
best possible statistical minimax rate (up to logarithmic factors and constants). 
Thus, under the first oracle situtation, \emph{adaptivity gives no advantage} 
for optimization in one dimension.
\begin{algorithm}[H]
\caption{Routine for One Stage Lipschitz Function $\ffamily_\lambda$ 
	(First-Order Oracle)}  % scale here for size
\begin{algorithmic}[1]  % [1] is line numbering
\Statex Input: User's choice of the sampling center $c \in \xdomain$, radius 
	$r\in \R_+$, resolution $M$, grid size parameter $\{k_i\}_{i=1}^M$, 
	sampling times $\{T_i\}_{i=1}^M$ and the precision $\{\Delta_i\}_{i=1}^M$. 
\State Initialize the interval $I = [l_1, l_2] = [c-r, c+r]$.
\FOR{$i = 1$ to $K$}
	\State Compute the grid $G_i = G(c, r, k_i) \cap I$.
	\State At each point $x\in G_i$, query the first-order oracle $T_i$ times. 
		Denote each sample derivative value via $\{\hat{f}^\prime(x)^{(1)}, 
		\hat{f}^\prime(x)^{(2)}, \ldots, \hat{f}^\prime(x)^{(T_i)}\}$. 
	\State Compute the derivative estimate at each point $x\in G_i$ via 
		averaging: 
		\begin{equation*}
			\hat{f}^\prime(x) = \frac{1}{T_i} \sum_{i=1}^{T_i} {\hat{f}^\prime(x)^{(i)}}
		\end{equation*}
		\IF {there exists $x_i\opt \in G_i$ such that $|\hat{f}^\prime(x_i\opt)| \leq \Delta_i$}
			\State Return the estimator $\hat{x} = x_i\opt$.
		\ELSIF{for all $x\in G_i$, $\hat{f}^\prime(x_i\opt) <  -\Delta_i$} 
			\State Return the estimator $\hat{x} = r$
		\ELSIF{for all $x\in G_i$, $\hat{f}^\prime(x_i\opt) >  \Delta_i$} 
			\State Return the estimator $\hat{x} = l$
		\ELSE{~update the interval $I = [l_1, l_2]$ via: 
			\begin{equation*}
				l_1 = \max_{x\in G_i}\{x \in I: \hat{f}^\prime(x) < -\Delta_i\}~~\text{and}~~
				l_2 = \min_{x \in G_i}\{x \in I: \hat{f}^\prime(x) > \Delta_i\}. 
			\end{equation*}
			}
		\ENDIF
\ENDFOR 
%\State $B \defeq \floor {\half \logtwo n}$.
%\FOR{ $i = 1$ to $B$ }
%	\State $l_i \defeq 2^{-i}$, 
%	  	   $a_i \defeq 2^{i/2+1} \sigma n^{-1/2} \sqrt{(\log  \frac{2}{\delta} + \half \log n + \log \logtwo n)\logtwo n}$.
%	\FOR { $j = 0$ to $1/l_i$ }
%		\State $x^i_j \defeq j \cdot l_i$, $g^i_j \defeq 0$
%		\FOR { $p = 1$ to $\floor{\frac{n}{B(l_i^{-1}+1)}}$ }
%			\State $g^i_j \defeq g^i_j + \phi_1(f, x^i_j)$
%		\ENDFOR
%		\State $g^i_j \defeq g^i_j/\floor{n/(B(l_i^{-1}+1))}$
%	\ENDFOR
%\ENDFOR
%\Statex \emph{//Optimization}
%\FOR{ $j = 0$ to $1/l_1$ }
%	\If {$| g^1_j | \leq a_1$} \Return $x^1_j$.
%	\EndIf
%\ENDFOR
%\State $\tilde \alpha_2 \defeq  \sup \{ x^1_j: j \textup{ satisfies } \forall p \in \{0, \dots, j\}, g^1_p < -a_1 \}$, 
%	  $\alpha_2 \defeq \max \{ 0, \tilde \alpha_2 \}$
%\State $\tilde \beta_2 \defeq \inf \{ x^1_j: j \textup{ satisfies } \forall p \in \{j, \dots, 1/l_1\}, g^1_p > a_1 \}$, 
%	  $\beta_2 \defeq \min \{ 1, \tilde \beta_2 \}$
%\If {$\alpha_2 = \beta_2$} \Return $\alpha_2$.
%\EndIf
%\FOR{ $i = 2$ to $B$ }
%	\FOR{ $j = \alpha_i/l_i$ to $\beta_i/l_i$ }
%		\If {$| g^i_j | \leq a_i$} \Return $x^i_j$.
%		\EndIf
%	\ENDFOR
%	\State $\tilde \alpha_{i+1} \defeq  \sup \{ x^i_j: j \textup{ satisfies } \forall p \in \{\alpha_i/l_i, \dots, j\}, g^i_p < -a_i \}$, 
%	  	  $\alpha_{i+1} \defeq \max \{ \alpha_i, \tilde \alpha_{i+1} \}$
%	\State $\tilde \beta_{i+1} \defeq \inf \{ x^i_j: j \textup{ satisfies } \forall p \in \{j, \beta_i/l_i\}, g^i_p > a_i \}$, 
%	 	  $\beta_{i+1} \defeq \min \{ \beta_i, \tilde \beta_{i+1} \}$
%	\If {$\alpha_{i+1} = \beta_{i+1}$} \Return $\alpha_{i+1}$.
%	\EndIf
%\ENDFOR
%\State \Return $(\alpha_{B+1} + \beta_{B+1})/2$.
\State Return the estimator $\hat{x}= (l_1 + l_2)/2$.
\end{algorithmic}
\label{alg:Lipcvxfo}
\end{algorithm}
%
%\subsubsection{Sampling strategy}
%Suppose we are given a budget of $n$ available oracle queries.
%We divide the budget evenly into $B \defeq \defeq \floor {\half \logtwo n}$ ``batches,'' 
%so that each batch is allowed to take $n/B$ samples.
%
%Now, for each batch $i = 1, \dots, B$, define $l_i = 2^{-i}$.
%Then, notice that $l_1 = 2^{-1}$, $l_2 = 2^{-2}$, \dots, $l_B = 2^{-\floor {\half \logtwo n}} \leq 2^{-\half \logtwo n + 1} = 2 n^{-1/2}$.
%For the $i$-th batch, we observe the gradients at the points $x^i_0 = 0, x^i_1 = l_i, x^i_2 = 2l_i, \dots, x^i_{1/l_i} = 1$.
%Each point on the grid is sampled repeatedly $\floor{n / (B(l_i^{-1}+1))}$ times, 
%and the mean is taken as the estimate of the true gradient value.
%Let us denote the estimate of $\nabla f (x^i_j)$ by $i$-th batch as $g^i_j$.
%
%\subsubsection{Optimization}
%Given the estimates obtained by the $B$ batches,
%we first consider the estimates obtained from the first batch. 
%We examine the estimates, and see if there is a point $x^1_j$ satisfying $|g^1_j| \leq a_1$.
%If so, the algorithm returns such $x^1_j$ and terminates.
%
%If there is no such point with small gradient estimates, than we reach the Line 16 of Algorithm~\ref{alg:Lipcvxfo}.
%We consider a set of points $x^1_j$ such that, for this point and all points to the left of $x^1_j$,
%$x^1_0, \dots, x^1_j$, the corresponding estimates all satisfy $g^1_0 < -a_1, \dots, x^1_j < -a_1$.
%We define $\tilde \alpha_2$ to be the supremum of that set. If the set is empty, $\tilde \alpha_2 = -\infty$ by definition of supremum.
%Then, we take maximum of $0$ and $\tilde \alpha_2$ as $\alpha_2$.
%Similarly, we define a set of points $x^1_j$ such that $x^1_j$ and all points to the right of it
%satisfies $g^1_j > a_1, \dots, g^1_{l_1^{-1}} > a_1$.
%The infimum of that set is $\tilde \beta_2$, which is $\infty$ if the set is empty.
%$\beta_2$ is taken to be minimum of $1$ and $\tilde \beta_2$.
%We can check that $0\leq \alpha_2 \leq \beta_2 \leq 1$, and that 
%if $\alpha_2=\beta_2$, either of the two sets are empty and $\alpha_2$ is $0$ or $1$.
%If $\alpha_2=\beta_2$, the algorithm returns $\alpha_2$.
%Otherwise, we must have $\beta_2 - \alpha_2 \geq l_1$, and we only focus on $[\alpha_2, \beta_2]$ afterwards.
%
%For the rest of the algorithm, we iterate through $i = 2, \dots, B$ and examine the $i$-th batch in the following way.
%We look at the estimates $g^i_j$, but only the points in the interval $[\alpha_{i}, \beta_{i}]$.
%Again, if there is a point $x^i_j$ such that $|g^i_j| \leq a_i$, we return that point and terminate.
%If this is no such point, we calculate $\alpha_{i+1}$ and $\beta_{i+1}$ using similarly defined sets,
%return $\alpha_{i+1}$ if they are the same, otherwise move on to the next batch.
%
%When the algorithm does not terminate until we finish the iteration for the $B$-th batch,
%The algorithm just returns the middle point $(\alpha_{B+1} + \beta_{B+1})/2$ of the interval.
%
\subsubsection{Analysis of Algorithm~\ref{alg:Lipcvxfo}}
In this section, we show that, with careful choice of input parameters, algorithm 
\ref{alg:Lipcvxfo} returns some estimator $\hat{x}$ so that its risk is upper bounded
by $\tilde{O}\left(n^{-1/2}\right)$. As before, we slightly generalize the domain of 
interest to be $\xdomain_{c, r} = [c-r, c+r]$. The target of interest now becomes 
$x_{f, c, r}\opt$, the unique minimum of $f$ in the domain $\xdomain_{c, r}$ and 
the risk of interest would be $\E\left[f(\hat{x}) - f(x_{f, c, r}\opt)\right]$.
\begin{proposition}
\label{proposition:first-order-nonsmooth-one-round-onedim}
Given any fix $c\in \R$ and $r\in \R_+$. Let us choose the parameters
\begin{equation}
\label{eqn:existence-of-first-order-nonsmooth-onedim}
k_i = 2^{i-1},~M = \floor{\half \log_2 n},~T_i = \floor{\frac{n}{M(2k_i + 1)}}~\text{and}~~
\Delta_i = \sqrt{\frac{2\sigma^2}{T_i}\log \frac{2M(2k_i +1)}{\delta}}.
\end{equation}
If we denote $\hat{x}$ to be the output of the algorithm~\ref{alg:Lipcvxfo}
with the above input parameters, then, 
\begin{equation*}
\P \left(f(\hat{x}) - f(x_{f, c, r}\opt) \leq \max\{4 \max_{1\leq j\leq M}\left\{\Delta_j k_j^{-1}\right\}, 
	Lk_M^{-1}\} r\right) \geq 1-\delta. 
\end{equation*}
\end{proposition}
\begin{proof}
Now, let us consider the following event: 
\begin{equation*}
\event = \bigcap_{i=1}^M \left\{|\hat{f}^\prime(x) - f^\prime(x)| \leq \Delta_i ~\text{for all $x \in G_i$}\right\}
\end{equation*}
The next lemma shows that $\event$ happens with probability at least $1-\delta$. 
\begin{lemma}
We have $\P(\event) \geq 1-\delta$. 
\end{lemma}
\begin{proof}
First, for each $x \in G_i$, denote $\noise(x) \defeq \what{f}^\prime(x) - f^\prime(x)$.  
Then, since by our assumption, the noise $\{\what{f}^\prime(x) - f^\prime(x)\}_{x=1}^{T_i}$ 
is mean $0$, independent and subgaussian with parameter $\sigma^2$, we have 
$\noise(x)$ is mean $0$ and subgaussian with parameter $\sigma^2/T_i$. 
Therefore, for any fix $x\in G_i$, 
\begin{align*}
\P\left(|\noise(x)| \geq \Delta_i \right) 
		\leq 2\exp\left(-\frac{\Delta_i^2T_i}{2\sigma^2}\right) 
			\leq \delta M^{-1} (2k_i+1)^{-d}, 
\end{align*}
where the first inequality above uses the subgaussianity of $\noise(x)$, and 
the second inequality uses the definition of $k$ and $T$. 
Now, since $|G_i| = (2k+1)^d$, by union bound, we have, 
\begin{equation*}
\P(\event^c) \leq \sum_{i=1}^M \P(\exists x \in G_i: |\noise(x)|\geq \Delta_i) \leq \delta,
\end{equation*} 
which gives the desired claim of the proposition. 
\end{proof}
Now, assuming in the rest of the proof that event $\event$ happens. We now take a 
look into the \emph{first} iteration from line 2 to line 14. Note that, basically, from 
line 6 to line 14, we are checking whether there exist $x_1\in G_1$ such that 
$\hat{f}^\prime(x_1) < -\Delta_1$ or $x_2\in G_2$ such that $\hat{f}^\prime(x_2) > \Delta_2$. 
There are four different circumstances, and here we discuss them one by one: 
\begin{enumerate}
\item Suppose there exist some $x_1\opt \in G_1$ such that 
	$|\hat{f}^\prime(x_1)| < \Delta_1$, then the algorithm terminates at line 7. 
	In this case, we know that 
	$|f^\prime(x_1)| \leq |\hat{f}^\prime(x)| + |f(x_1) - \hat{f}^\prime(x_1)| \leq 2\Delta_1$. 
	By convexity, this gives the following upper bound on $f(x_1\opt) - f(x_{f, c, r}\opt)$: 
	\begin{equation*}
	f(x_1\opt) - f(x_{f, c, r}\opt) \leq f^\prime(x_1\opt) (x_1\opt - x_{f, c, r}\opt) 
		\leq |f^\prime(x_1\opt)| |x_1\opt - x_{f, c, r}\opt|\leq  4\Delta_1 k_1^{-1}r.
	\end{equation*}
	In this case, the algorithm returns $x_1\opt$. 
\item Suppose for all $x\in G_1$, $\hat{f}^\prime(x) < -\Delta_1$, then the algorithm 
	terminates at line 9. Hence, for all $x\in G_1$, we indeed have $f^\prime(x) < 0$, 
	and the function is monotonically decreasing on the interval $[l_1, l_2]$. In this case, 
	we return $r$, the minimum of $f$. 
\item Suppose for all $x\in G_1$, $\hat{f}^\prime(x) > \Delta_1$, then the algorithm 
	terminates at line 11. Hence, for all $x\in G_1$, we indeed have $f^\prime(x) > 0$, 
	and the function is monotonically increasing on the interval $[l_1, l_2]$. In this case, 
	we return $l$, the minimum of $f$. 
\item Lastly, suppose there exists some $x_1, x_2 \in G_1$ such that 
	$\hat{f}^\prime(x_1) < -\Delta_1$ and $\hat{f}^\prime(x_2) > \Delta_1$.
	Thus in this case, $f^\prime(x_1) < 0$ and $f^\prime(x_2) > 0$.
	 As $f$is a convex function on the interval $I = [l_1, l_2]$, its derivative $f^\prime$ 
	 can only flip the sign at most once on $I$. Our way of updating the interval, 
	 will make sure that the minimum $x_{f, c, r}\opt$ lie in the updated
	 interval $I$. Note that, the length of the interval $I$ now decreases to 
	 $k_1^{-1}r = 2k_2^{-1}r$ in the next for loop.
\end{enumerate}
Now, following exactly the same reasoning as above, one can prove via induction that, 
in the $j$th loop, where $1\leq j\leq M$, we have, $|I| = 2 k_j^{-1}r$.
\begin{enumerate}
\item Suppose there exist some $x_j\opt \in G_1$ such that $|\hat{f}^\prime(x_j)| < \Delta_j$.
	Then, the algorithm returns $x_j\opt$, which satisfies, 
	\begin{equation*}
	f(x_j\opt) - f(x_{f, c, r}\opt) \leq 
		 |f^\prime(x_j\opt)| |x_j\opt - x_{f, c, r}\opt|\leq  4\Delta_j k_{j}^{-1}r.
	\end{equation*}
\item Suppose for all $x\in G_j$, $\hat{f}^\prime(x) < -\Delta_j$, then the algorithm returns 
	the minimum of $f$.
\item Suppose for all $x\in G_j$, $\hat{f}^\prime(x) >  \Delta_j$, then the algorithm returns 
	the minimum of $f$.
\item Suppose there exists some $x_1, x_2 \in G_1$ such that $\hat{f}^\prime(x_1) < -\Delta_j$ 
	and $\hat{f}^\prime(x_2) > \Delta_j$. Then $I$ is updated, and its length shrinks to 
	$k_j^{-1} r$. $I$ contains the minimum $x_{f, c, r}\opt$.
\end{enumerate}
Now, when the algorithm does not terminate until we finish the $M$-th loop, the interval 
$I$ now contains $x_{f, c, r}\opt$ with length at most $k_M^{-1} r$. In this case, we 
return the middle point of the interval, so that by Lipschitzness of the objective function $f$,
we have, 
\begin{equation*}
f(\hat{x}) - f(x_{f, c, r}\opt) \leq L |\hat{x} - x_{f, c, r}\opt| \leq L |I| \leq L k_M^{-1} r.
\end{equation*}
The desired claim of the proposition now thus follows. 
\end{proof}

\begin{corollary}
\label{corollary:first-order-nonsmooth-one-round-onedim}
Assume $n \geq 30$, then the parameters in Eq 
\eqref{eqn:existence-of-first-order-nonsmooth-onedim} satisfy
\begin{equation*}
\min_{1\leq j\leq M}T_j \geq 1, k_M \leq n^{-1/2}~\text{and}~
	\max_{1\leq j\leq M}\left\{\Delta_j k_j^{-1}\right\} \leq 3\sigma 
		n^{-\half}(\log_2(n))^\half \left(\log \frac{3}{\delta} + \log n\right)^\half. 
\end{equation*} 
Moreover, the output $\hat{x}$ from algorithm~\ref{alg:Lipcvxfo} with 
the input defined in Eq~\eqref{eqn:existence-of-first-order-nonsmooth-onedim} satisfies, 
\begin{equation*}
\P \left(f(\hat{x}) - f(x_{f, c, r}\opt) \leq 
	\gap\opt  \right) \geq 1-\delta, 
\end{equation*}
where 
\begin{equation*}
\gap\opt \defeq \max\left\{12\sigma (\log_2(n))^\half \left(\log \frac{3}{\delta} + \log n\right)^\half, 2 L\right\}
		n^{-\half}r
\end{equation*}
\end{corollary}
\begin{proof}
Note that, the second part of the corollary follows immediately from the first part
and proposition~\ref{proposition:first-order-nonsmooth-one-round-onedim}. To 
prove the first part, note first that, $k_j \leq k_M \leq n^{\half}$ for $1\leq j \leq M$. 
Since $n\geq 30$, we get for $1\leq j\leq M$, 
\begin{equation*}
\frac{n}{M(2k_j+1)} \geq \frac{n}{\half \log_2 n(2n^\half + 1)} \geq 1~~\Rightarrow~~
T_j \geq 1.
\end{equation*}
Finally, note that, since we have, for all $1\leq j\leq M$: 
\begin{equation*}
T_j \geq \frac{n}{2M(2k_j+1)} \geq \frac{n}{\log_2 (n) \cdot (2k_j+1)} \geq \frac{n}{3k_j \log_2(n)}
	~~\text{and}~~2k_j + 1\leq 3n^\half.
\end{equation*}
this gives us that, when $n\geq 30$, for all $1\leq j\leq M$: 
\begin{equation*}
\Delta_j k_j^{-1} \leq \sqrt{\frac{6\sigma^2\log_2 n}{nk_j} \log \frac{3n^{1/2} \log_2(n)}{\delta}}
	\leq 3\sigma \sqrt{\frac{\log_2(n)}{n} \left(\log \frac{3}{\delta} + \log n\right)}.
\end{equation*}
\end{proof}

%
%\begin{proposition}
%\label{prop:Lipcvxfoub}
%If $n \geq 4$, the output $\what x$ of Algorithm~\ref{alg:Lipcvxfo} satisfies the following with probability at least $1-\delta$:
%\begin{equation}
%\label{eqn:Lipcvxfoub}
%    f(\what x_M) - f(x^*) 
%    \leq \max\left \{ L, 2^{5/2} \sigma \sqrt{\left (\log  \frac{2}{\delta} + \half \log n + \log \logtwo n \right )\logtwo n} \right \} n^{-1/2}.
%\end{equation}
%\end{proposition}
%\begin{proof}
%As seen in Lemma~\ref{lem:Lipcvxfoaccu}, the event $\Gamma$ occurs with probability at least $1-\delta$. 
%For rest of the proof, let us assume that $\Gamma$ is given.
%Define the right hand side of Equation~\ref{eqn:Lipcvxfoub} to be $\rho$.
%We will prove the proposition by showing that when $\Gamma$ is given,
%all the return points $\what x$ satisfies $f(\what x) - f(x^*) \leq \rho$.
%
%In lines 12-15, we examine if any of $x^1_j$'s have $|g^1_j| \leq a_1$. 
%If there is such a point $x^1_j$ satisfying, because of $\Gamma$ the true gradient is small: $|\nabla f(x^1_j)| \leq 2a_1$.
%By convexity,
%\begin{equation*}
%    f(x^1_j) - f(x^*) \leq \nabla f(x^1_j) (x^1_j - x^*) \leq |\nabla f(x^1_j)| |x^1_j - x^*| = 2a_1 \cdot 1 \leq \rho.
%\end{equation*}
%so the algorithm returns $x^1_j$ and terminates.
%
%Otherwise, we move to lines 16-19.
%By $\Gamma$, $g^1_j < -a_1$ implies $\nabla f(x^1_j) < 0$ and $g^1_j > a_1$ implies $\nabla f(x^1_j) > 0$.
%Note that $\alpha_2 = \beta_2$ can only occur when $(\tilde \alpha_2, \tilde \beta_2) = (1, \infty)$ or $(-\infty, 0)$.
%Here, $(\tilde \alpha_2, \tilde \beta_2) = (1, \infty)$ means that $g^1_j < -a_1$ for all $x^1_j$, 
%so the function is monotone decreasing and $1$ is the true optimal point. 
%We get $\alpha_2 = \beta_2 = 1$, so returning $\alpha_2$ gives zero error.
%Similarly, $(\tilde \alpha_2, \tilde \beta_2) = (-\infty, 0)$ means that the function is monotone increasing,
%and returning $\alpha_2 = 0$ gives zero error.
%
%If $\alpha_2 \neq \beta_2$ at this point, it means that $\nabla f (\alpha_2) < 0$ and $\nabla f (\beta_2) > 0$.
%We must have $\beta_2 - \alpha_2 = l_1$, 
%because there can be only one sign flip in the gradient of a convex function,
%and true minimum point must lie in $[\alpha_2, \beta_2]$.
%
%In the for loop starting at line 20, 
%note that by previous iteration we have $\beta_i - \alpha_i = l_{i-1}$ and $x^* \in [\alpha_i , \beta_i]$.
%If we find any $x^i_j \in [\alpha_{i}, \beta_{i}]$ such that $|g^i_j| \leq a_i$, by convexity
%\begin{equation*}
%    f(x^i_j) - f(x^*) \leq |\nabla f(x^i_j)| |x^i_j - x^*| = 2a_i \cdot l_{i-1} \leq \rho .
%\end{equation*}
%If there is no such point, at the end we get an interval $[\alpha_{i+1}, \beta_{i+1}]$ of length $l_i$ 
%that contains the true optimal point, and move on to the next batch.
%
%Note that when $\Gamma$ has occurred, the algorithm never terminates by lines 27-28
%because $\alpha_{i+1} = \beta_{i+1}$ can never happen:
%as seen above, $\alpha_{i+1} = \beta_{i+1}$ implies that the function is 
%either monotone increasing or decreasing in the interval $[\alpha_{i}, \beta_{i}]$.
%However, this contradicts that $\nabla f(\alpha_i) < 0$ and $\nabla f(\beta_i) > 0$
%that was obtained in the previous iteration.
%
%When the algorithm does not terminate until we finish the iteration for the $B$-th batch,
%we now have an interval of length $\beta_{B+1} - \alpha_{B+1} = l_B$, 
%and we know that the minimum point is in the interval.
%Then, we can just return the middle point $\frac{\alpha_B + \beta_B}{2}$ of the interval, 
%and by Lipschitzness
%\begin{equation*}
%    f\left ( \frac{\alpha_B + \beta_B}{2} \right) - f(x^*) 
%    \leq L \left |\frac{\alpha_B + \beta_B}{2} - x^* \right |
%    \leq \half L l_B \leq Ln^{-1/2} \leq \rho.
%\end{equation*}
%\end{proof}
%
%




%\newpage
\section{Lipschitz Function with Zeroth-Order Oracle}
\label{sec:proof-Lip-zo}
\subsection{Lipschitz Function with Zeroth-Order Oracle, $d = 1$ and $M \geq 1$}
\subsubsection{Description of Algorithms}
In this section, we propose two generic algorithms: algorithm~\ref{alg:Lipcvxzo}
and algorithm~\ref{alg:Lipcvxzo-r-round} that builds from algorithm~\ref{alg:Lipcvxzo}.
We note here that, algorithm~\ref{alg:Lipcvxzo-r-round} in essence, builds from 
$M$ times of repeated calls of algorithm~\ref{alg:Lipcvxzo}. As will be shown 
immediately in the later subsections, it turns out that the two algorithms with careful
choice of parameters return the minimax estimator in the single and multi rounds 
respectively. 

\newcommand{\lei}{_-}
\newcommand{\rii}{_+}
\begin{algorithm}[htp]
\caption{Routine for One Stage $d = 1$ Lipschitz Functions 
	$\ffamily_\lambda$ (Zeroth-Order Oracle)}  % scale here for size
\begin{algorithmic}[1]  % [1] is line numbering
\Statex Input:  User's choice of the left point and right point of interval $l_1$ and 
	$l_2$,  grid size parameter $k\in \N$ and sampling times $T\in \N$, 
	value $m_1$ and $m_2$ and function gap ${U} \in \R_+$.
\State Set $c = \half(l_1 + l_2)$ and $r = \half(l_2 - l_1)$. Compute the grid points $G = G(c, r, k)$.
\State At each point $x \in G$, query the zeroth oracle $T$ times and denote each sample 
	function value via $\{f(x)^{(1)}, f(x)^{(2)}, \ldots, f(x)^{(T)}\}$.
\State Compute the function value estimate at each point $x \in G$ via $
		\hat{f}(x) = \frac{1}{T} \sum_{i=1}^T f(x)^{(i)}$. 
\State Compute the estimate $x \in G$ via 
		$\hat{x} = \argmin_{x\in G} \hat{f}(x)$.
\State Compute $\hat{m}_1$ and $\hat{m}_2$ as follows: 
	\begin{equation*}
		\hat{m}_1= \min\{x \in G: \hat{f}(x) -  \hat{f}(\hat{x}) \leq U\}~\text{and}~
		\hat{m}_2 = \max\{x \in G: \hat{f}(x) - \hat{f}(\hat{x}) \leq U\}
	\end{equation*}
\State Compute $\hat{l}_1$ and $\hat{l}_2$ as follows: 
	\begin{equation*}
	\hat{l}_1 = \max \left\{m_1 - \frac{r}{k}, l_1 \right\}~\text{and}~
	\hat{l}_2 = \min \left\{m_2 + \frac{r}{k}, l_1 \right\}	
	\end{equation*}
\State Return the the estimator $\hat{x}$, the value of $\hat{l}_1$, $\hat{l}_2$,
	$\hat{m}_1$ and $\hat{m}_2$.
\end{algorithmic}
\label{alg:Lipcvxzo}
\end{algorithm}

\begin{algorithm}[htp]
\caption{Routine for Multi Stage $d = 1$ Lipschitz Functions 
	$\ffamily_\lambda$ (Zeroth-Order Oracle)}  % scale here for size
\begin{algorithmic}[1]  % [1] is line numbering
\Statex Input:  Initialization of th eparameters $(l_1^{(1)}, l_2^{(1)}, m_1^{(1)}, 
	m_2^{(1)}, k^{(1)}, T^{(1)}, U^{(1)}) \in \R \times \R \times \R \times \R 
		 \times \N \times \N \times \R_+$.
	User specifies the updating rule used in line (3) of the algorithm. 
\FOR{ $i = 1$ to $M$ }
	\State Run algorithm~\ref{alg:Lipcvxzo} with input parameter $(l_1^{(i)}, l_2^{(i)}, 
		m_1^{(i)}, m_2^{(i)}, k^{(i)}, T^{(i)}, U^{(i)})$. Denote the output to be 
		$\hat{l}_1^{(i)}$, $\hat{l}_2^{(i)}$, $\hat{m}_1^{(i)}$, $\hat{m}_2^{(i)}$
		and $\hat{x}^{(i)}$.
	\State Update $(l_1^{(i+1)}, l_2^{(i+1)}, 
		m_1^{(i+1)}, m_2^{(i+1)}, k^{(i+1)}, T^{(i+1)}, U^{(i+1)})$.
\ENDFOR
%\State $x = (x_0, \dots, x_k)$, $g = (g_0, \dots, g_k)$
\State \Return $\hat{x}_M$ as the estimate of $x_f\opt$, and $\hat{U} = U^{(M+1)}$.
\end{algorithmic}
\label{alg:Lipcvxzo-r-round}
\end{algorithm}

\subsubsection{Analysis of Algorithm~\ref{alg:Lipcvxzo}: Single-Stage Analysis}
In this section, we show that, with careful choice of input parameters, algorithm 
\ref{alg:Lipcvxzo} returns some estimator $\hat{x}$ so that its associated risk 
$\risk$ is upper bounded by $\tilde{O}\left(n^{-\frac{1}{3}}\right)$. For purpose 
of convenience in later discussion on upper bounds for multi-stage algorithm, 
in this section, we slightly generalize the domain of interest by considering the 
domain $\xdomain = [l_1, l_2]$ (while noting that $l_1 = 0$ and $l_2 = 1$ 
corresponds to the original domain $\xdomain = [0, 1]$). In this sense, 
the algorithm discussed in this section seeks to estimate $x_{f, l_1, l_2}\opt$, 
the unique minimum of $f$ in the domain $\xdomain_{l_1, l_2}$, while the risk of 
interest would be $\E \left[f(\hat{x}) - f(x_{f, l_1, l_2}\opt)\right]$. 

\begin{proposition}
\label{proposition:zeroth-order-nonsmooth-one-round-one-dim}
Given any fix $c\in \R^d$ and $r$, suppose there exists some $k \in \N$ satisfying 
\begin{equation}
\label{eqn:existence-of-k-one-round-zo-one-dim-ns}
(2k + 1) \ceil{2k^2 \log (2k+1)} \leq nr^2~~\text{and}~~k \geq 6. 
\end{equation}
Set $T= \floor{\frac{n}{2k+1}}$ and $U = 2Lr$. Then, if we denote $\hat{l}_1$, 
$\hat{l}_2$, $\hat{m}_1$, $\hat{m}_2$ and $\hat{x}$ to be the output from 
algorithm~\ref{alg:Lipcvxzo-r-round} with input $l_1 = m_1 = c-r$, 
$l_2 = m_2 = c+r$, and $k, T, U$ set up as above, we have, with probability at least $1-\delta$: 
\begin{enumerate}
\item The output set $[\hat{m}_1, \hat{m}_2]$ is amenable to  
	$[\hat{l}_1, \hat{l}_2]$ with parameter $(k, t) = \left(6, \frac{1}{6}\right)$.
\item The output estimator $\hat{x}$ satisfies $f(\hat{x}) - f(x_{f, l_1, l_2}\opt)
	\leq k^{-1} r\left(6L + 2\sigma \left(\log \frac{2}{\delta} + 1\right)^\half \right)$. 
\item For any $x \in \{\hat{m}_1, \hat{m}_2\}$, we have, 
	$f(x) - f(x_{f, l_1, l_2}\opt) \leq k^{-1} r\left(18 L + 2\sigma \left(\log \frac{2}{\delta} + 1\right)^\half \right)$. 
\item The output $[\hat{l}_1, \hat{l}_2]$ contains the minimum:
	 any minimum $x_{f, l_1, l_2}\opt$ satisfies $x_{f, l_1, l_2}\opt \in [\hat{l}_1, \hat{l_2}]$. 
\end{enumerate} 
\end{proposition}
\begin{proof}
The proof of the proposition relies on the following two critical lemma. The 
first lemma bounds the deviation of the function value $f(x)$ and its estimate 
$\hat{f}(x)$ on the grid $G$. The second lemma establishes a critical property 
of the algorithm, which turns out to be very useful in the analysis of multi-stage 
algorithm. We start with the first lemma. Consider the following event, 
\begin{equation*}
\event \defeq \left\{\left|\hat{f}(x) - f(x)\right| \leq r^a \defeq \sigma \sqrt{\frac{2}{T} 
	\log \frac{2(2k+1)}{\delta}}~~\text{for all $x\in G$}\right\}.
\end{equation*}
The next lemma shows that $\event$ happens with probability at least $1-\delta$. 
\begin{lemma}
We have $\P(\event) \geq 1-\delta$. 
\end{lemma}
\begin{proof}
First, for each $x \in G$, denote $\noise(x) \defeq \what{f}(x) - f(x)$.  Then, since 
by our assumption, the noise $\{\what{f}(x) - f(x)\}_{x=1}^{T}$ is mean $0$, independent 
and subgaussian with parameter $\sigma^2$, we have that $\noise(x)$ is mean $0$ 
and subgaussian with parameter $\sigma^2/T$. Therefore, for any fix $x\in G$, 
\begin{align*}
\P\left(|\noise(x)| \geq r^a \right) 
		\leq 2\exp\left(-\frac{(r^a)^2T}{2\sigma^2}\right) 
			\leq \delta (2k+1)^{-1}, 
\end{align*}
where the first inequality above uses the subgaussianity of $\noise(x)$, and 
the second inequality uses the definition of $k$ and $T$. 
Now, the desired claim of the lemma follows from the fact that 
$|G| = 2k+1$ and the union bound of the above events. 
\end{proof}
Before we introduce the next lemma, we introduce the following concept. 
\begin{definition}
An interval $[m_1, m_2]$ is amenable to another interval $[l_1, l_2]$ with 
parameter $k\opt\in \N, t\opt \in \R_+$ if it satisfies the following two conditions: 
\begin{enumerate}
\item The set $[m_1, m_2] \subseteq [l_1, l_2]$.
\item For any $k \in \N$, denote $G_k$ to be the grid points 
	$G_k = G(c, r, k)$ with $c = \half(l_1 + l_2)$ and $r = \half(l_2 - l_1)$. 
	Then, for any $x \in [l_1, l_2]$ and $k \geq k\opt$, there exists three 
	consecutive grids $x_0 \leq x_1 \leq x_2 \in G_k$ such that, either 
	$m_1 \leq x_0 \leq x_1 \leq x \leq x_2, x- m_1 \geq t\opt (l_2 - l_1)$ 
	or $x_0 \leq x \leq x_1 \leq x_2 \leq m_2, m_2 - x \geq t\opt (l_2 - l_1)$. 
\end{enumerate}
\end{definition}\noindent
The lemma below is helpful in understanding the above concept. 
\begin{lemma}
\label{lemma:structure-amenable}
Let $ [l_1, l_2]$ be some interval on $\R$. Suppose $m_1, m_2$ satisfy
the following condition: 
\begin{equation}
\label{eqn:structure-amenable-cond}
m_1 \leq l_1 \leq l_2 \leq  m_2~\text{and}~m_2 - m_1 \geq \frac{1}{3}(l_2 - l_1).
\end{equation}
Then, the interval $[m_1, m_2]$ is amenable to $[l_1, l_2]$ with parameter 
$\left(6, \frac{1}{6} \right)$
\end{lemma}
\begin{proof}
By definition, we need to show that, $[m_1, m_2]$ satisfies the following conditions:
\begin{enumerate}
\item The set $[m_1, m_2] \subseteq [l_1, l_2]$.
\item For any $k \geq 6$, denote $G_k$ to be the grid points 
	$G_k = G(c, r, k)$ with $c = \half(l_1 + l_2)$ and $r = \half(l_2 - l_1)$. 
	Then, for any $x \in [l_1, l_2]$ and $k \geq 6$, there exists three 
	consecutive grids $x_0 \leq x_1 \leq x_2 \in G_k$ such that, either 
	$m_1 \leq x_0 \leq x_1 \leq x \leq x_2, x- m_1 \geq \frac{1}{6}(l_2 - l_1)$ 
	or $x_0 \leq x \leq x_1 \leq x_2 \leq m_2, m_2 - x \geq \frac{1}{6}(l_2 - l_1)$. 
\end{enumerate}
The first condition is satisfied according to the first group of inequality of 
Eq~\eqref{eqn:structure-amenable-cond}. To show the second inequality, 
pick any $x \in [l_1, l_2]$, and we divide our discussion into two cases.
(i) $x \geq \half (m_1 + m_2)$. Then, since $k \geq 6$, we can take three 
consecutive grids $x_0, x_1, x_2 \in G_k$ such that $x \in [x_1, x_2]$. Then, 
by definition,we know that, $x_2 - x_1 = x_1 - x_0 = \frac{1}{2k}(l_2 - l_1)$. 
This gives $x_0 \geq  x - \frac{1}{6} \left(l_2 - l_1\right)$. 
Since $m_2 - m_1 \geq \frac{1}{3} (l_2 - l_1)$ by Eq~\eqref{eqn:structure-amenable-cond}, 
we get that, 
\begin{equation*}
x - m_1 \geq \half (m_2 - m_1) \geq \frac{1}{6} (l_2 - l_1)~\text{and}~
	x_0 \geq x - \frac{1}{6} \left(l_2 - l_1\right) \geq m_1. 
\end{equation*}
The desired claim of the lemma now thus follows. (ii)$x \leq \half (m_1 + m_2)$.
One can similarly show that, for $k \geq 6$. there exists consecutive grids 
$x_0, x_1, x_2 \in G_k$ such that $x_0 \leq x \leq x_1 \leq x_2 \leq m_2, m_2 - x \geq \frac{1}{6}(l_2 - l_1)$. 
The proof under this case is essentially the same as that under the case 
where $x \geq \half (m_1 + m_2)$. 
\end{proof}

Now, we are ready to introduce the following lemma. It is deterministic in nature. 
\begin{lemma}
\label{lemma:crucial-one-dim-zo-ns}
 Assume that, the following four conditions hold: 
\begin{enumerate}
\item The set $[m_1, m_2]$ is amenable to the set $[l_1, l_2]$ with parameter $(k, t) = \left(6, \frac{1}{6}\right)$.
\item For any $x \in [m_1, m_2]$, we have, $f(x) - f(x_{f, l_1, l_2}\opt) \leq U^\prime$ for some $U^\prime > 0$.
\item The grid $G = G(c, r, k)$ with $c = \half(l_1 + l_2)$, $r = \half(l_2 - l_1)$ and $k \geq 6$.
\item For any $x\in G$, we have, $|\hat{f}(x) - f(x)| \leq r^a$. 
\end{enumerate}
Now, let us denote $U$ to be $U =9U^\prime k^{-1} + 2r^a > 0$. Then, the output of 
algorithm~\ref{alg:Lipcvxzo} with input parameters $l_1, l_2, m_1, m_2, k, T$ 
and $U$ satisfies: 
\begin{enumerate}
\item The output set $[\hat{m}_1, \hat{m}_2]$ is amenable to  
	$[\hat{l}_1, \hat{l}_2]$ with parameter $(k, t) = \left(6, \frac{1}{6}\right)$.
\item The output estimator $\hat{x}$ satisfies $f(\hat{x}) - f(x_{f, l_1, l_2}\opt)
	\leq 3U^\prime k^{-1} + 2r^a$. 
\item For any $x \in [\hat{m}_1, \hat{m}_2]$, we have, 
	$f(x) - f(x_{f, l_1, l_2}\opt) \leq 9 U^\prime k^{-1} + 2r^a = U$. 
\item The output $[\hat{l}_1, \hat{l}_2]$ contains the minimum:
	 any minimum $x_{f, l_1, l_2}\opt$ satisfies $x_{f, l_1, l_2}\opt \in [\hat{l}_1, \hat{l_2}]$. 
\end{enumerate} 
\end{lemma}
\begin{proof}
First note that, $[m_1, m_2]$ is amenable to $[l_1, l_2]$ with parameters $(6, \frac{1}{6})$.
Since $x_{f, l_1, l_2}\opt \in [l_1, l_2]$, and the grid size $k \geq 6$, thus, by definition, 
we know that, there exist three consecutive grid points $x_0 \leq x_1 \leq x_2 \in G_k$ such 
that, either $m_1 \leq x_0 \leq x_1 \leq x_{f, l_1, l_2}\opt \leq x_2$, $x_{f, l_1, l_2}\opt -m_1 \geq 
\frac{1}{6}(l_2 - l_1)$ or $x_0 \leq x_{f, l_1, l_2}\opt \leq x_1 \leq x_2 \leq m_2$ and 
$m_2 - x_{f, l_1, l_2}\opt \geq \frac{1}{6}(l_2-l_1)$. We only prove the desired claim of the 
lemma under the first situation, that is $m_1 \leq x_0 \leq x_1 \leq x_{f, l_1, l_2}\opt \leq x_2$, 
$x_{f, l_1, l_2}\opt -m_1 \geq \frac{1}{6}(l_2 - l_1)$, while noting that the desired result 
of the lemma can be proved in a totally similar way under the other situation. 

We start by showing the first claim of the desired result. To do so, 
we show that, $\hat{m_1} \leq x_0 \leq x_1 \leq \hat{m_2}$. 
%To do so, we only need 
%to show that, 
%\begin{equation*}
%f(x_0) - f(x_{f, l_1, l_2}\opt) \leq U~~\text{and}~~f(x_1) - f(x_{f, l_1, l_2}\opt) \leq U.
%\end{equation*}
Indeed whenever $z \in \{x_0, x_1\}$, we know that, $m_1 \leq z \leq x_{f, l_1, l_2}\opt$,
and therefore, by convexity of $f$, we have,   
\begin{equation*}
f(z) \leq \frac{z_1 - m_1}{x_{f, l_1, l_2}\opt - m_1} f(x_{f, l_1, l_2}\opt) + 
	\frac{x_{f, l_1, l_2}\opt - z_1}{x_{f, l_1, l_2}\opt - m_1} f(m_1).
\end{equation*}
Hence, the value $f(z) - f(x_{f, l_1, l_2}\opt)$ for $z\in \{x_0, x_1\}$ can be upper bounded by: 
\begin{equation*}
f(z) - f(x_{f, l_1, l_2}\opt) \leq \frac{x_{f, l_1, l_2}\opt - z}{x_{f, l_1, l_2}\opt - m_1}
	\left(f(m_1) - f(x_{f, l_1, l_2}\opt)\right). 
\end{equation*}
Now, note that, since we have $0\leq x_{f, l_1, l_2}\opt - x_0 \leq \frac{1}{k}\left(l_2 - l_1\right)$, 
$0\leq x_{f, l_1, l_2}\opt - x_1 \leq \frac{1}{2k}\left(l_2 - l_1\right)$, 
$\frac{1}{6} \left(l_2 - l_1\right) \leq x_{f, l_1, l_2}\opt - m_1$ and 
$f(m_1) - f(x_{f, l_1, l_2}\opt)\leq U^\prime$, we get that, 
\begin{equation}
\label{eqn:how-close-is-x-1}
f(x_1) - f(x_{f, l_1, l_2}\opt) \leq 3U^\prime k^{-1}~~\text{and}~~
	f(x_0) - f(x_{f, l_1, l_2}\opt)  \leq 6U^\prime k^{-1}.
\end{equation}
Thus, whenever $z \in \{x_0, x_1\}$, we get, 
\begin{equation*}
\hat{f}(z) - \hat{f}(x_{f, l_1, l_2}\opt) \leq \left|\hat{f}(z) - f(z)\right| + f(z) - 
	f(x_{f, l_1, l_2}\opt) + \left|\hat{f}(x_{f, l_1, l_2}\opt) - f(x_{f, l_1, l_2}\opt)\right|
		\leq 2r^a + 6U^\prime k^{-1} = U.
\end{equation*}
This gives $\hat{m}_1 \leq x_0 \leq x_1 \leq \hat{m}_2$. As an immediate consequence, 
we get $\hat{m}_2 \geq \hat{m_1} + k^{-1} r$. Since by definition, we know 
$\hat{l_1} \leq \hat{m}_1 \leq \hat{l}_1 + k^{-1} r$ and 
$\hat{m}_2 \leq \hat{l}_2 \leq \hat{m}_2 + k^{-1} r$, we get that, 
\begin{equation*}
\hat{l}_1 \leq \hat{m}_1 \leq \frac{2}{3}\hat{l}_1 + \frac{1}{3} \hat{l}_2~~\text{and}~~
	\frac{1}{3} \hat{l}_1 + \frac{2}{3} \hat{l}_2 \leq \hat{m}_2 \leq \hat{l}_2. 
\end{equation*}
Thus, by lemma~\ref{lemma:structure-amenable}, we get that,  
$[\hat{m}_1, \hat{m}_2]$ is amenable to $[\hat{l}_1, \hat{l}_2]$ with parameter 
$(6, \frac{1}{6})$.

Next, we show the second and third claim of the desired result. Indeed, 
note first that, by definition, $\hat{f}(\hat{x}) \leq \hat{f}(x_1)$. In addition, 
we know from the condition $4$ of the lemma that, $f(\hat{x}) - \hat{f}(\hat{x}) \leq r^a$ 
and $\hat{f}(x_1) - f(x_1) \leq r^a$. Lastly, we know from Eq~\eqref{eqn:how-close-is-x-1}
that, $f(x_1) - f(x_{f, l_1, l_2}\opt) \leq 3U^\prime k^{-1}$. Thus, combining 
all these bounds together, we have, 
\begin{equation*}
f(\hat{x}) - f(x_{f, l_1, l_2}\opt) \leq 
	f(\hat{x}) - \hat{f}(\hat{x})
	+ \hat{f}(\hat{x}) -\hat{f}(x_1)
	+ \hat{f}(x_1) - f(x_1)
	+ f(x_1) - f(x_{f, l_1, l_2}\opt)
		\leq 3U^\prime k^{-1} + 2r^a. 
\end{equation*}
This gives us the second claim of the desired result. Similarly, notice that, 
for any $z \in \{\hat{m}_1, \hat{m}_2\}$, we have $f(z) - \hat{f}(z) \leq r^a$. 
and the definition of $\hat{m}_1$, $\hat{m}_2$ and $\hat{x}$ implies that, 
$\hat{f}(z) - \hat{f}(x_1) \leq \hat{f}(z) - \hat{f}(\hat{x}) \leq 6U^\prime k^{-1} + 2r^a$.
Thus, combining these bounds together, we get for $z\in \{\hat{m}_1, \hat{m}_2\}$, 
\begin{equation*}
f(z) - f(x_{f, l_1, l_2}\opt) \leq 
	f(z) - \hat{f}(z)
	+ \hat{f}(z) -\hat{f}(x_1)
	+ \hat{f}(x_1) - f(x_1)
	+ f(x_1) - f(x_{f, l_1, l_2}\opt)
		\leq 9U^\prime k^{-1} + 2r^a. 
\end{equation*}
This gives us the third claim of the desired result.

In the last step, we show the fourth claim of the desired result. To do so, we 
need to show that, $\hat{l}_1 \leq x_{f, l_1, l_2}\opt \leq \hat{l}_2$. First, 
we know that $x_{f, l_1, l_2}\opt \geq \hat{m}_1 \geq \hat{l_1}$. Next, 
we show that $x_{f, l_1, l_2}\opt \leq \hat{m}_2$. We divide our discussion 
into two cases: (i) $\hat{m}_2 = l_2$. In this case, the result is trivial. 
(ii) $\hat{m}_2 < l_2$. In this case, by definition of $\hat{l}_2$, we know 
that in this case $\hat{l}_2 = \hat{m}_2 + k^{-1} r$. Since we have already 
shown that $\hat{m}_2 \geq x_1$, the fact that, $x_{f, l_1, l_2}\opt 
\leq x_1 + k^{-1}r$ gives us that, $x_{f, l_1, l_2}\opt  \leq \hat{l}_2$.  
Now, altogether, we have shown the fourth claim of the desired result. 
\end{proof}

Now, we are ready to show the desired claim of the proposition. Since 
the function $f(x)$ is known to be Lipschitz with parameter $L$ on the 
interval $[l_1, l_2]$, thus, it is known that, for all $x \in [l_1, l_2]$, we have, 
$f(x) - f(x_{f, l_1, l_2}\opt) \leq L(l_2 - l_1) \leq 2Lr$. Now that, the 
condition on $k$ shows that $T \geq 2k^2 \log(2k+1) r^{-2}$. As a consequence,
this gives us that, 
\begin{equation*}
r^a = \sigma \sqrt{\frac{2}{T}\log \frac{2(2k+1)}{\delta}} 
	\leq \frac{\sigma r}{k} \cdot \sqrt{\log \frac{2}{\delta} + 1}.
\end{equation*}
Now, the desired claim of the proposition follows from lemma 
\ref{lemma:crucial-one-dim-zo-ns} and above bound on $r^a$. 
\end{proof}

Motivated by proposition~\ref{proposition:zeroth-order-nonsmooth-one-round-one-dim},
it becomes important to understand when such $k$ exists in Eq
\eqref{eqn:existence-of-k-one-round-zo-one-dim-ns} and how large it is. 
\begin{lemma}
\label{lemma:ef-zeroth-order-ns-one-round-one-dim}
Assume $n$ is large enough satisfying $nr^2 \geq 6^{12}$. 
Denote $k(r) = (\gamma(nr^2))^\frac{1}{3}$, 
and $k\opt = \floor{\frac{1}{3} k(r)}$. Then $k \opt \in \N$ 
and it satisfies Eq~\eqref{eqn:existence-of-k-one-round-zo-one-dim-ns}.
\end{lemma}
\begin{proof}
Note that, the second part of the corollary follows easily from the first part,
and proposition~\ref{proposition:zeroth-order-nonsmooth-one-round-one-dim}.
To show the first part, note that, $\gamma(x) \geq \sqrt{x}$ when $x\geq 3$. 
Therefore, whenever $nr^2 \geq 6^{12}$, we have, $\gamma(nr^2) \geq 6^6$. 
As a consequence, $k(r) \geq 36$, and hence, $k\opt \geq 6$. Now, we show 
that $k\opt$ satisfies Eq~\eqref{eqn:existence-of-k-one-round-zo-one-dim-ns}. 
Indeed, when $k = k\opt$, 
\begin{equation*}
(2k+1) \floor{2k^2\log(2k+1)} \leq (3k)^3 \log(3k)^3 \leq k(r)^3 \log (k(r))^3 
	\leq nr^2,
\end{equation*}
where the last inequality follows from the fact that, for any $x > 0$, $\gamma(x)
\log \gamma(x) \leq x$. 
\end{proof}
Now, proposition~\ref{proposition:zeroth-order-nonsmooth-one-round-one-dim}
and Lemma~\ref{lemma:ef-zeroth-order-ns-one-round-one-dim} immediately 
give us the following corollary. 


\begin{corollary}
\label{corollary:zeroth-order-nonsmooth-one-round-one-dim}
Given any fix $c \in \R^d$ and $r \in (0, 1]$, set $k = \floor{\frac{1}{3} \gamma(nr^2)^\frac{1}{3}}$
and $T = \floor{\frac{n}{2k+1}}$.  Assume $n$ is large enough satisfying 
$nr^2 \geq 6^{12}$. Then, if we denote $\hat{x}$ to be the output from 
algorithm~\ref{alg:Lipcvxzo-r-round} with input $l_1 = m_1 = c-r$, 
$l_2 = m_2 = c+r$, and $k, T, U$ set up as above, we get, 
\begin{equation*}
\P\left(f(\hat{x}) - f(x_{f, l_1, l_2}\opt) \leq 12B n^{-\frac{1}{3}} r^\frac{1}{3}
	\log(nr^2)^\frac{1}{3}  \right) \geq 1-\delta~
\text{where}~B = 3L + \sigma \left(\log \frac{2}{\delta} + 1\right)^\half.
\end{equation*}
\end{corollary}
%\begin{proof}
%Note that, the second part of the corollary follows easily from the first part,
%and proposition~\ref{proposition:zeroth-order-nonsmooth-one-round-one-dim}.
%To show the first part, note that, $\gamma(x) \geq \sqrt{x}$ when $x\geq 3$. 
%Therefore, whenever $nr^2 \geq 6^{12}$, we have, $\gamma(nr^2) \geq 6^6$. 
%As a consequence, $k(r) \geq 36$, and hence, $k\opt \geq 6$. Now, we show 
%that $k\opt$ satisfies Eq~\eqref{eqn:existence-of-k-one-round-zo-one-dim-ns}. 
%Indeed, when $k = k\opt$, 
%\begin{equation*}
%(2k+1) \floor{2k^2\log(2k+1)} \leq (3k)^3 \log(3k)^3 \leq k(r)^3 \log (k(r))^3 
%	\leq nr^2,
%\end{equation*}
%where the last inequality follows from the fact that, for any $x > 0$, $\gamma(x)
%\log \gamma(x) \leq x$. 
%\end{proof}

\subsubsection{Analysis of Algorithm~\ref{alg:Lipcvxzo-r-round}: Multi-Stage Analysis}
In this section, we show that with careful choice of input parameters $l_1$, 
$l_2$, $k_1$, $T_1$ and $U_1$, and appropriate updating rule, then algorithm 
returns some estimator $\hat{x}$ that is minimax optimal. The idea is from the crucial 
lemma~\ref{lemma:crucial-one-dim-zo-ns}: lemma~\ref{lemma:crucial-one-dim-zo-ns}
helps locate an interval such that all point in this new interval has smaller function 
value gap to the minimum value $f(x_{f, l_1, l_2}\opt)$. The idea is to apply this 
technique in multiple rounds, and after each round, get a better convergence rate 
of the function value.  

To be clear about how we specify the updating rule in line 3 of the algorithm 
\ref{alg:Lipcvxzo-r-round}, we summarize it as follows: given the output 
parameters $\hat{l_1}^{(i)}$, $\hat{l}_2^{(i)}$, $\hat{m}_1^{(i)}$, $\hat{m}_2^{(i)}$, 
we update $l_1^{(i+1)}$, $l_2^{(i+1)}$, $m_1^{(i+1)}$, $m_2^{(i+1)}$, 
$k^{(i+1)}$, $T^{(i+1)}$ and $U^{(i+1)}$ as described in Algorithm 
\ref{alg:update-rule-Lipcvxzo-r-round}. Our update also requires
knowledge of $\{k^{(j)}\}_{j=1}^i$ and $\{U^{(j)}\}_{j=1}^i$.

\begin{algorithm}[htp]
\caption{Updating Rule in Algorithm~\ref{alg:Lipcvxzo-r-round}}
\begin{algorithmic}
%\Procedure{updating rule}{}
  \Statex Input:$l_1^{(i)}$, $l_2^{(i)}$, $m_1^{(i)}$, $m_2^{(i)}$, 
  	$\{k^{(j)}\}_{j=1}^i$ and $\{U^{(j)}\}_{j=1}^i$.
  \State (i) Update $l_1^{(i+1)} = \hat{l}_1^{(i)}$, $l_2^{(i+1)} = \hat{l}_2^{(i)}$, 
  	$m_1^{(i+1)} = \hat{m}_1^{(i)}$, $m_2^{(i+1)} = \hat{m}_2^{(i)}$. 
  \State (ii) Update $k^{(i+1)}$ to be the largest $k \in \N$ such that, 
  	\begin{equation}
	\label{eqn:existence-of-k-r-round-zo-one-dim-ns}
		(2k + 1) \Pi_{j=1}^{i} \left(k^{(j)}\right)^2 \ceil{2 \log (2k+1) } \leq n
	\end{equation}
	 If no such $k$ exists, return FAIL. 
%	where $c_{i, j}$ and $c_{i+1, j}$ denote the $j$th coordinate of 
%		$c_i$ and $c_{i+1}$. 
  \State (iii) Update $T^{(i+1)}$ to be $T^{(i+1)} = \floor{\frac{n}{2k^{(i)}+1}}$.
%  \State (iii) Update $k_{i+1}$ to be the largest $k \in \N$ such that 
%  	\begin{equation} 
%		\label{eqn:existence-of-k-i+1-r-rounds-fo}
%		(2k + 1)^d \ceil{2k^2 \log (2k+1)} \leq nr_i^2.
%	\end{equation}
%  	 If no such $k$ exists, return FAIL. 
  \State (iv) Update $U^{(i+1)}$ to be $U^{(i+1)} = 9U^{(i)} \left(k^{(i)}\right)^{-1} + 
	2\sigma \Pi_{j=1}^i (k^{(j)})^{-1} \left(\log \frac{2}{\delta} + 1\right)^\half$.
%\EndProcedure
\end{algorithmic}
\label{alg:update-rule-Lipcvxzo-r-round}
\end{algorithm}

\begin{proposition}
\label{proposition:zeroth-order-nonsmooth-r-round-one-dim}
Let $\xdomain = [0, 1]$ and we are given the confidence level $\delta > 0$.
Set the initial parameter $l_1^{(1)} = m_1^{(1)}= 0$ and $l_2^{(1)} = m_2 ^{(1)}= 1$. 
Set the parameters $k^{(1)}$ to be the largest $k \in \N$ such that,  
$(2k+1) \ceil{2\log (2k+1)} \leq n$. Set the initial parameters 
$T^{(1)} = \floor{\frac{n}{2k+1}}$ and $U^{(1)} = L$. Consider algorithm 
\ref{alg:Lipcvxzo-r-round} that uses algorithm~\ref{alg:update-rule-Lipcvxzo-r-round}
to be the updating rule. Now, assuming that the $\{k^{(i)}\}_{i=1}^M$ exist 
and satisfy $k^{(i)} \geq 6$ for all $1\leq i\leq M$. Then, if we denote
$\hat{x}$ and $\hat{U}$ to be the output from algorithm~~\ref{alg:Lipcvxzo-r-round}, 
we get,  
\begin{equation*}
\P \left(f(\hat{x}) - f(x_{f, l_1, l_2}\opt) \leq U^{(M+1)} \right) \geq 1-\delta. 
\end{equation*}
%with probability at least $1-\delta$: 
%\begin{enumerate}
%\item The output set $[\hat{m}_1, \hat{m}_2]$ is amenable to  
%	$[\hat{l}_1, \hat{l}_2]$ with parameter $(k, t) = \left(6, \frac{1}{6}\right)$.
%\item The output estimator $\hat{x}$ satisfies $f(\hat{x}) - f(x_{f, l_1, l_2}\opt)
%	\leq k^{-1} r\left(6L + 2\sigma \left(\log \frac{2}{\delta} + 1\right)^\half \right)$. 
%\item For any $x \in \{\hat{m}_1, \hat{m}_2\}$, we have, 
%	$f(x) - f(x_{f, l_1, l_2}\opt) \leq k^{-1} r\left(18 L + 2\sigma \left(\log \frac{2}{\delta} + 1\right)^\half \right)$. 
%\item The output $[\hat{l}_1, \hat{l}_2]$ contains the minimum:
%	 any minimum $x_{f, l_1, l_2}\opt$ satisfies $x_{f, l_1, l_2}\opt \in [\hat{l}_1, \hat{l_2}]$. 
%\end{enumerate} 
\end{proposition}
\begin{proof}
The proof of the proposition relies on the following two components. The first 
component is the following probabilistic lemma that bounds the deviation of the 
function value $f(x)$ and its estimate $\hat{f}(x)$ on the grid $\{G_i\}_{i=1}^M$. 
The second component is the crucial lemma~\ref{lemma:structure-amenable} 
that we have shown before. We start with the first component. Consider the 
following event $\event$, 
\begin{equation*}
\event = \bigcap_{i=1}^M \left\{|\hat{f}(x) - f(x)| \leq r_i^a \defeq \sigma \sqrt{\frac{2}{T_i} 
	\log \frac{2M(2k_i + 1)}{\delta}}~~\text{for all $x\in G_i$}\right\}.
\end{equation*}
The following lemma shows that $\event$ happens with probability at least $1-\delta$. 
\begin{lemma}
We have $\P(\event) \geq 1-\delta$. 
\end{lemma}
\begin{proof}
First, fix $1\leq i\leq M$. Note that, for each $x \in G_i$, denote 
$\noise(x) \defeq \what{f}(x) - f(x)$.  Then, since by our assumption, 
the noise $\{\what{f}(x) - f(x)\}_{x=1}^{T_i}$ is mean $0$, independent 
and subgaussian with parameter $\sigma^2$, we have that $\noise(x)$ is mean $0$ 
and subgaussian with parameter $\sigma^2/T_i$. Therefore, for any fix $x\in G_i$, 
\begin{align*}
\P\left(|\noise(x)| \geq r^a \right) 
		\leq 2\exp\left(-\frac{(r_i^a)^2T_i}{2\sigma^2}\right) 
			\leq \delta M^{-1}(2k_i+1)^{-1}, 
\end{align*}
where the first inequality above uses the subgaussianity of $\noise(x)$, and 
the second inequality uses the definition of $r^a$. Since $|G_i| = 2k_i + 1$, 
after taking a union bound, we get for $1\leq i\leq M$, 
\begin{equation*}
\P \left(\exists x \in G_i~\text{such that}~|\hat{f}(x) - f(x)| \geq r_i^a\right) 
	\leq \delta M^{-1}.
\end{equation*} 
Now, the desired claim of the lemma follows from a union bound on $1\leq i\leq M$. 
\end{proof}
We are now ready to prove the desired claim of the proposition. Indeed, we can 
prove the following via induction on the rounds $1\leq i\leq M$: 
\begin{enumerate}
\item The output $[\hat{m}_1^{(i+1)}, \hat{m}_2^{(i+1)}]$ is amenable to  
	$[\hat{l}_1^{(i+1)}, \hat{l}_2^{(i+1)}]$ with parameter $(k, t) = \left(6, \frac{1}{6}\right)$.
\item The output estimator $\hat{x}^{(i+1)}$ satisfies $f(\hat{x}^{(i)}) - f(x_{f, l_1, l_2}\opt)
	\leq 3U^{(i)} (k^{(i)})^{-1} + 2r_i^a$. 
\item For any $x \in [\hat{m}_1^{(i+1)}, \hat{m}_2^{(i+1)}]$, we have, 
	$f(x) - f(x_{f, l_1, l_2}\opt) \leq 9U^{(i)} (k^{(i)})^{-1} + 2r_i^a \leq U^{(i+1)}$. 
\item The output $[\hat{l}_1, \hat{l}_2]$ contains the minimum:
	 any minimum $x_{f, l_1, l_2}\opt$ satisfies $x_{f, l_1, l_2}\opt \in [\hat{l}_1, \hat{l_2}]$. 
\end{enumerate} 
The only trick is to apply lemma~\ref{lemma:crucial-one-dim-zo-ns} repeatedly and notice the 
fact that $T^{(i)} \geq 2\log(2k^{(i)}+1) \Pi_{j=1}^i (k^{(j)})^2$ implies the following upper bound
on $r_i^a$: 
\begin{equation*}
r_i^a \defeq \sigma \sqrt{\frac{2}{T^{(i)}} \log \frac{2M(2k^{(i)} + 1)}{\delta}} 
	\leq \sigma \Pi_{j=1}^i ({k^{(j)}})^{-1} \left(\log \frac{2M}{\delta} + 1\right)^\half.
\end{equation*}
\end{proof}

Motivated by Proposition~\ref{proposition:zeroth-order-nonsmooth-r-round-one-dim}, 
it becomes important to understand how large $\{k^{(i)}\}_{i=1}^M$ and $\{U^{(i)}\}_{i=1}^M$
are. 

\begin{lemma}
\label{lemma:ef-zeroth-order-nonsmooth-r-round-one-dim}
Suppose that $n$ is large enough satisfying
\begin{equation}
\label{eqn:condition-n-zeroth-order-ns-r-round-1dim}
\log \log n \geq M\log 3 + \log 12 +\log \log 6. 
\end{equation}
Then, the sequence $\{k^{(i)}\}_{i=1}^M$, $\{T^{(i)}\}_{i=1}^M$ and 
$\{U^{(i)}\}_{i=1}^M$ are well-defined via algorithm 
\ref{alg:update-rule-Lipcvxzo-r-round}. In addition, $k^{(i)} \geq 6$
for all $1\leq i\leq M$. Finally, if we denote the output $U^{(M+1)}$
satisfies, 
%Then if we recursively define the sequence $\{k^{(i)}\}_{i=1}^M$ as follows, 
%$k_{i} = \left[\frac{1}{6}(\gamma(n \Pi_{j=1}^{i-1}(k^{(i)})^{-2}))^\frac{1}{3}\right]$, 
%then we have, $\{k^{(i)}\}_{i=1}^M$ satisfies 
%Eq~\eqref{eqn:existence-of-k-r-round-zo-one-dim-ns} and $k^{(i)} \geq 6$. 
%and it also satisfies, 
%\begin{equation}
%\label{eqn:zo-ns-r-first-part}
%\Pi_{j=1}^M k^{(j)} \geq \left(12\right)^{-\frac{3}{2}\left(1-\frac{1}{3^M}\right)}
%	(\log n)^{-\frac{1}{2}\left(1-\frac{1}{3^M}\right)} n^{\frac{1}{2}\left(1-\frac{1}{3^M}\right)}.
%\end{equation}
%Now, if denote the sequence $\left\{U^{(i)}\right\}_{i=1}^{M+1}$ recursively as 
%that appears in Eq~\eqref{eqn:existence-of-u-r-round-zo-one-dim-ns}, we get that, 
\begin{equation}
\label{eqn:zo-ns-r-second-part}
U^{(M+1)} %\leq 9^{M} \Pi_{j=1}^M (k^{(j)})^{-1} \cdot  \left(2L  + \sigma \left(\log \frac{2}{\delta}+1\right)^\half\right)
 \leq 9^{M+2}B (\log n)^\half \cdot  n^{\frac{1}{2}\left(1-\frac{1}{3^M}\right)}~~\text{where}~B = 
 	L  + \sigma \left(\log \frac{2}{\delta}+1\right)^\half. 
\end{equation}
%As a consequence, if we denote $\gap_M\opt$ to be: 
%\begin{equation}
%\label{eqn:zo-ns-r-third-part}
%\gap_M\opt = 9^{M+2} (\log n)^\half \cdot  n^{\frac{1}{2}\left(1-\frac{1}{3^M}\right)} \cdot
% 	\left(L  + \sigma \left(\log \frac{2}{\delta}+1\right)^\half\right), 
%\end{equation}
%we get the bound below, 
%\begin{equation*}
%\P(f(\hat{x}) - f(x\opt) \leq \gap_M\opt) \geq 1-\delta. 
%\end{equation*}
\end{lemma}
\begin{proof}
%We start by showing the first part, i.e, Eq~\eqref{eqn:zo-ns-r-first-part} of the corollary. 
%To show the first part, our strategy is to show via induction that the below 
First of all, we show that $\{k^{(i)}\}_{i=1}^M$ are well defined via algorithm 
\ref{alg:update-rule-Lipcvxzo-r-round}. To do so, we show that the following 
hypothesis hold for all $1\leq i\leq M$:
\begin{align}
&(i) n \geq 6^{12} \Pi_{j=1}^i (k^{(j)})^2 ~~
(ii)k^{(i)} \in \N~\text{and}~k^{(i)} \geq 6	\label{eqn:induction-hypothesis-one-zo-ns} \\
&(iii)\Pi_{j=1}^i k^{(j)} \geq \left(12\right)^{-\frac{3}{2}\left(1-\frac{1}{3^i}\right)}
	(\log n)^{-\frac{1}{2}\left(1-\frac{1}{3^i}\right)} n^{\frac{1}{2}\left(1-\frac{1}{3^i}\right)}
\label{eqn:induction-hypothesis-two-zo-ns} \\
&(iv)\Pi_{j=1}^i k^{(j)} \leq 6^{-\frac{3}{2}\left(1-\frac{1}{3^i}\right)}
	n^{\frac{1}{2}\left(1-\frac{1}{3^i}\right)}
\label{eqn:induction-hypothesis-three-zo-ns}
\end{align}
We first show the base case $i=1$. Note that, the first part of Eq
\eqref{eqn:induction-hypothesis-one-zo-ns} is implied by the assumption 
on $n$, both the second part of Eq~\eqref{eqn:induction-hypothesis-one-zo-ns}, 
Eq~\eqref{eqn:induction-hypothesis-two-zo-ns} and 
Eq~\eqref{eqn:induction-hypothesis-three-zo-ns}
 follow from the first part
of Eq~\eqref{eqn:induction-hypothesis-one-zo-ns} and corollary 
\ref{corollary:zeroth-order-nonsmooth-one-round-one-dim}. 
Now, for some $i < M$, assuming that the induction hypothesis holds for 
all $j \in \{1, 2, \ldots, i\}$, we show that the hypothesis holds for $i+1$.  
We first show the second part of Eq~\eqref{eqn:induction-hypothesis-one-zo-ns}
for $i+1$. Indeed, by induction hypothesis, we know that, the first 
inequality of Eq~\eqref{eqn:induction-hypothesis-one-zo-ns} is true for $i$,
and thus the second part of Eq~\eqref{eqn:induction-hypothesis-one-zo-ns}
follows from a direct application of corollary~\ref{corollary:zeroth-order-nonsmooth-one-round-one-dim}
(by substituting $\delta$ by $\delta/M$ and $r$ by $\Pi_{j=1}^{i} k_j^{-1}$ there).
%Note that corollary~\ref{corollary:first-order-smooth-one-round} also provides 
%the following lower bound on $k_{i+1}$: 
%\begin{equation}
%\label{eqn:lower-bound-induction-on-k-fo}
%k_{i+1} \geq  \frac{1}{3} \left(\frac{nH^2r_i^2}
%	{\max \left\{ \sigma^2 \log \frac{2d}{\delta/M}, 
%		\sigma^2 \log \frac{nH^2r_i^2}{25\sigma^2}, 
%			H^2 r_i^2\right\}} \right)^\frac{1}{d+2}.
%\end{equation}
Next, we show Eq~\eqref{eqn:induction-hypothesis-two-zo-ns} holds for $i+1$. 
In fact, note that, by definition of $k^{(i)}$, we know that, 
\begin{equation*}
 k^{(i+1)} \geq \frac{1}{12} \left[\gamma \left(n \Pi_{j=1}^i (k^{(j)})^{-2}\right)\right]^\frac{1}{3}
	\geq \frac{1}{12} \left(n \Pi_{j=1}^i (k^{(j)})^{-2}\right)^\frac{1}{3} (\log n)^{-\frac{1}{3}}, 
\end{equation*}
and therefore, we get that, 
\begin{equation*}
\Pi_{j=1}^{i+1} k^{(j)} = \Pi_{j=1}^{i} k^{(j)} \cdot k^{(i+1)} 
	\geq 12^{-1} n^{\frac{1}{3}}(\log n)^{-\frac{1}{3}}
	 \left(\Pi_{j=1}^i k^{(j)}\right)^\frac{1}{3} 
\end{equation*}
Now, using the induction hypothesis Eq~\eqref{eqn:induction-hypothesis-two-zo-ns}, 
we get, 
\begin{align*}
\Pi_{j=1}^{i+1} k^{(j)}&\geq 12^{-1} n^{-\frac{1}{3}}(\log n)^\frac{1}{3}
	 \left(\left(12\right)^{-\frac{3}{2}\left(1-\frac{1}{3^i}\right)}
		(\log n)^{-\frac{1}{2}\left(1-\frac{1}{3^i}\right)} 
			n^{\frac{1}{2}\left(1-\frac{1}{3^i}\right)} \right)^\frac{1}{3}\\
	&= \left(12\right)^{-\frac{3}{2}\left(1-\frac{1}{3^{i+1}}\right)}
		(\log n)^{-\frac{1}{2}\left(1-\frac{1}{3^{i+1}}\right)} 
			n^{\frac{1}{2}\left(1-\frac{1}{3^{i+1}}\right)} 
\end{align*}
This gives Eq~\eqref{eqn:induction-hypothesis-two-zo-ns} for the case $i+1$. 
In the third step, we show Eq~\eqref{eqn:induction-hypothesis-three-zo-ns}
for $i+1$. The proof idea is similar to that of Eq~\eqref{eqn:induction-hypothesis-two-zo-ns}.
In fact, by definition of $k^{(i)}$, we know that, 
\begin{equation*}
 k^{(i+1)} \leq \frac{1}{6} \left[\gamma \left(n \Pi_{j=1}^i (k^{(j)})^{-2}\right)\right]^\frac{1}{3}
	\leq \frac{1}{6} \left(n \Pi_{j=1}^i (k^{(j)})^{-2}\right)^\frac{1}{3}, 
\end{equation*}
and hence immediately we can get that, 
\begin{equation*}
\Pi_{j=1}^{i+1} k^{(j)} = \Pi_{j=1}^{i} k^{(j)} \cdot k^{(i+1)} 
	\leq 6^{-1} n^{\frac{1}{3}} \left(\Pi_{j=1}^i k^{(j)}\right)^\frac{1}{3}.
\end{equation*}
Now, using the induction hypothesis Eq~\eqref{eqn:induction-hypothesis-three-zo-ns}, 
we get, 
\begin{align*}
\Pi_{j=1}^{i+1} k^{(j)}\leq 6^{-1} n^{-\frac{1}{3}}
	 \left(6^{-\frac{3}{2}\left(1-\frac{1}{3^i}\right)}
			n^{\frac{1}{2}\left(1-\frac{1}{3^i}\right)} \right)^\frac{1}{3}
	\leq 6^{-\frac{3}{2}\left(1-\frac{1}{3^{i+1}}\right)}	
		n^{\frac{1}{2}\left(1-\frac{1}{3^{i+1}}\right)} 
\end{align*}
Finally, we show the first part of Eq~\eqref{eqn:induction-hypothesis-one-zo-ns}. Note 
that, by the proven fact of Eq~\eqref{eqn:induction-hypothesis-three-zo-ns} for $i+1$, 
it suffices to show that, $n^{1/3^{i+1}} \geq 6^{12}$, which would suffice if $n$ is large 
enough so that, 
\begin{equation*}
\log \log n \geq M\log 3 + \log 12 +\log \log 6. 
\end{equation*}

Now, we are ready to show the rest of the corollary. We first show Eq
\eqref{eqn:zo-ns-r-second-part}. Indeed, it is easy to use 
induction argument to show the following result: for all $0\leq i\leq M-1$: 
\begin{equation*}
U^{(M+1)} = 9^{i+1} U^{(M-i)} \Pi_{j=0}^i k_{M-j}^{-1} + \frac{1}{4}(9^{i+1}-1)
	\sigma \left(\log \frac{2}{\delta} + 1\right)^\half \Pi_{j=0}^M k_j^{-1}.
\end{equation*}
Therefore, if we plug in $i = M-1$ in the above argument, we get that, 
\begin{equation*}
U^{(M+1)} \leq 9^{M} \left(U_1 + \sigma \left(\log \frac{2}{\delta} + 1\right)^\half \right)
	 \Pi_{j=0}^M k_j^{-1} = 9^{M}
	 \Pi_{j=0}^M k_j^{-1} \left(2L + \sigma \left(\log \frac{2}{\delta} + 1\right)^\half \right)
\end{equation*}
%The above result and Eq~\eqref{eqn:zo-ns-r-first-part} gives the inequality 
%in Eq~\eqref{eqn:zo-ns-r-second-part}. Finally, we note that, Eq~\eqref{eqn:zo-ns-r-third-part}
%follows easily from proposition~\ref{proposition:zeroth-order-nonsmooth-r-round-one-dim}
%and Eq~\eqref{eqn:zo-ns-r-second-part}. 
%
%note that, $\gamma(x) \geq \sqrt{x}$ when $x\geq 3$. 
%Therefore, whenever $nr^2 \geq 6^{12}$, we have, $\gamma(nr^2) \geq 6^6$. 
%As a consequence, $k(r) \geq 36$, and hence, $k\opt \geq 6$. Now, we show 
%that $k\opt$ satisfies Eq~\eqref{eqn:existence-of-k-one-round-zo-one-dim-ns}. 
%Indeed, when $k = k\opt$, 
%\begin{equation*}
%(2k+1) \floor{2k^2\log(2k+1)} \leq (3k)^3 \log(3k)^3 \leq k(r)^3 \log (k(r))^3 
%	\leq nr^2,
%\end{equation*}
%where the last inequality follows from the fact that, for any $x > 0$, $\gamma(x)
%\log \gamma(x) \leq x$. 
\end{proof}
Now, Proposition~\ref{proposition:zeroth-order-nonsmooth-r-round-one-dim}
and lemma~\ref{lemma:ef-zeroth-order-nonsmooth-r-round-one-dim} together 
immediately give us the corollary below. 

\begin{corollary}
\label{corollary:zeroth-order-nonsmooth-r-round-one-dim}
Let $\xdomain = [0, 1]$. Consider algorithm 
\ref{alg:Lipcvxzo-r-round}. Suppose we use the same initialization 
rule and use algorithm~\ref{alg:update-rule-Lipcvxzo-r-round} to 
be the updating rule as that in Proposition~\ref{proposition:zeroth-order-nonsmooth-r-round-one-dim}.
Then, when $n$ is large enough satisfying Eq~\eqref{eqn:condition-n-zeroth-order-ns-r-round-1dim},
then the output $\hat{x}$ from algorithm \ref{alg:Lipcvxzo-r-round} satisfies, 
\begin{equation*}
\P(f(\hat{x}) - f(x\opt) \leq \gap_M\opt) \geq 1-\delta. 
\end{equation*}
where
\begin{equation}
\label{eqn:zo-ns-r-third-part}
\gap_M\opt = 9^{M+2} (\log n)^\half \cdot  n^{\frac{1}{2}\left(1-\frac{1}{3^M}\right)} \cdot
 	\left(L  + \sigma \left(\log \frac{2}{\delta}+1\right)^\half\right), 
\end{equation}
%
%to be the updating rule
%Suppose that $n$ is large enough satisfying
%\begin{equation*}
%\log \log n \geq M\log 3 + \log 12 +\log \log 6. 
%\end{equation*}
%Then if we recursively define the sequence $\{k^{(i)}\}_{i=1}^M$ as follows, 
%$k_{i} = \left[\frac{1}{6}(\gamma(n \Pi_{j=1}^{i-1}(k^{(i)})^{-2}))^\frac{1}{3}\right]$, 
%then we have, $\{k^{(i)}\}_{i=1}^M$ satisfies 
%Eq~\eqref{eqn:existence-of-k-r-round-zo-one-dim-ns} and $k^{(i)} \geq 6$. 
%and it also satisfies, 
%\begin{equation}
%\label{eqn:zo-ns-r-first-part}
%\Pi_{j=1}^M k^{(j)} \geq \left(12\right)^{-\frac{3}{2}\left(1-\frac{1}{3^M}\right)}
%	(\log n)^{-\frac{1}{2}\left(1-\frac{1}{3^M}\right)} n^{\frac{1}{2}\left(1-\frac{1}{3^M}\right)}.
%\end{equation}
%Now, if denote the sequence $\left\{U^{(i)}\right\}_{i=1}^{M+1}$ recursively as 
%that appears in Eq~\eqref{eqn:existence-of-u-r-round-zo-one-dim-ns}, we get that, 
%\begin{equation}
%\label{eqn:zo-ns-r-second-part}
%U^{(M+1)} \leq 9^{M} \Pi_{j=1}^M (k^{(j)})^{-1} \cdot  \left(2L  + \sigma \left(\log \frac{2}{\delta}+1\right)^\half\right)
% \leq 9^{M+2} (\log n)^\half \cdot  n^{\frac{1}{2}\left(1-\frac{1}{3^M}\right)} 
% 	\left(L  + \sigma \left(\log \frac{2}{\delta}+1\right)^\half\right).
%\end{equation}
%As a consequence, if we denote $\gap_M\opt$ to be: 
%\begin{equation}
%\label{eqn:zo-ns-r-third-part}
%\gap_M\opt = 9^{M+2} (\log n)^\half \cdot  n^{\frac{1}{2}\left(1-\frac{1}{3^M}\right)} \cdot
% 	\left(L  + \sigma \left(\log \frac{2}{\delta}+1\right)^\half\right), 
%\end{equation}
%we get the bound below, 

\end{corollary}
%\begin{proof}
%We start by showing the first part, i.e, Eq~\eqref{eqn:zo-ns-r-first-part} of the corollary. 
%To show the first part, our strategy is to show via induction that the below 
%hypothesis hold for all $1\leq i\leq M$:
%\begin{align}
%&(i) n \geq 6^{12} \Pi_{j=1}^i (k^{(j)})^2 ~~
%(ii)k^{(i)} \in \N~\text{and}~k^{(i)} \geq 6	\label{eqn:induction-hypothesis-one-zo-ns} \\
%&(iii)\Pi_{j=1}^i k^{(j)} \geq \left(12\right)^{-\frac{3}{2}\left(1-\frac{1}{3^i}\right)}
%	(\log n)^{-\frac{1}{2}\left(1-\frac{1}{3^i}\right)} n^{\frac{1}{2}\left(1-\frac{1}{3^i}\right)}
%\label{eqn:induction-hypothesis-two-zo-ns} \\
%&(iv)\Pi_{j=1}^i k^{(j)} \leq 6^{-\frac{3}{2}\left(1-\frac{1}{3^i}\right)}
%	n^{\frac{1}{2}\left(1-\frac{1}{3^i}\right)}
%\label{eqn:induction-hypothesis-three-zo-ns}
%\end{align}
%We first show the base case $i=1$. Note that, the first part of Eq
%\eqref{eqn:induction-hypothesis-one-zo-ns} is implied by the assumption 
%on $n$, both the second part of Eq~\eqref{eqn:induction-hypothesis-one-zo-ns}, 
%Eq~\eqref{eqn:induction-hypothesis-two-zo-ns} and 
%Eq~\eqref{eqn:induction-hypothesis-three-zo-ns}
% follow from the first part
%of Eq~\eqref{eqn:induction-hypothesis-one-zo-ns} and corollary 
%\ref{corollary:zeroth-order-nonsmooth-one-round-one-dim}. 
%Now, for some $i < M$, assuming that the induction hypothesis holds for 
%all $j \in \{1, 2, \ldots, i\}$, we show that the hypothesis holds for $i+1$.  
%We first show the second part of Eq~\eqref{eqn:induction-hypothesis-one-zo-ns}
%for $i+1$. Indeed, by induction hypothesis, we know that, the first 
%inequality of Eq~\eqref{eqn:induction-hypothesis-one-zo-ns} is true for $i$,
%and thus the second part of Eq~\eqref{eqn:induction-hypothesis-one-zo-ns}
%follows from a direct application of corollary~\ref{corollary:zeroth-order-nonsmooth-one-round-one-dim}
%(by substituting $\delta$ by $\delta/M$ and $r$ by $\Pi_{j=1}^{i} k_j^{-1}$ there).
%%Note that corollary~\ref{corollary:first-order-smooth-one-round} also provides 
%%the following lower bound on $k_{i+1}$: 
%%\begin{equation}
%%\label{eqn:lower-bound-induction-on-k-fo}
%%k_{i+1} \geq  \frac{1}{3} \left(\frac{nH^2r_i^2}
%%	{\max \left\{ \sigma^2 \log \frac{2d}{\delta/M}, 
%%		\sigma^2 \log \frac{nH^2r_i^2}{25\sigma^2}, 
%%			H^2 r_i^2\right\}} \right)^\frac{1}{d+2}.
%%\end{equation}
%Next, we show Eq~\eqref{eqn:induction-hypothesis-two-zo-ns} holds for $i+1$. 
%In fact, note that, by definition of $k^{(i)}$, we know that, 
%\begin{equation*}
% k^{(i+1)} \geq \frac{1}{12} \left[\gamma \left(n \Pi_{j=1}^i (k^{(j)})^{-2}\right)\right]^\frac{1}{3}
%	\geq \frac{1}{12} \left(n \Pi_{j=1}^i (k^{(j)})^{-2}\right)^\frac{1}{3} (\log n)^{-\frac{1}{3}}, 
%\end{equation*}
%and therefore, we get that, 
%\begin{equation*}
%\Pi_{j=1}^{i+1} k^{(j)} = \Pi_{j=1}^{i} k^{(j)} \cdot k^{(i+1)} 
%	\geq 12^{-1} n^{\frac{1}{3}}(\log n)^{-\frac{1}{3}}
%	 \left(\Pi_{j=1}^i k^{(j)}\right)^\frac{1}{3} 
%\end{equation*}
%Now, using the induction hypothesis Eq~\eqref{eqn:induction-hypothesis-two-zo-ns}, 
%we get, 
%\begin{align*}
%\Pi_{j=1}^{i+1} k^{(j)}&\geq 12^{-1} n^{-\frac{1}{3}}(\log n)^\frac{1}{3}
%	 \left(\left(12\right)^{-\frac{3}{2}\left(1-\frac{1}{3^i}\right)}
%		(\log n)^{-\frac{1}{2}\left(1-\frac{1}{3^i}\right)} 
%			n^{\frac{1}{2}\left(1-\frac{1}{3^i}\right)} \right)^\frac{1}{3}\\
%	&= \left(12\right)^{-\frac{3}{2}\left(1-\frac{1}{3^{i+1}}\right)}
%		(\log n)^{-\frac{1}{2}\left(1-\frac{1}{3^{i+1}}\right)} 
%			n^{\frac{1}{2}\left(1-\frac{1}{3^{i+1}}\right)} 
%\end{align*}
%This gives Eq~\eqref{eqn:induction-hypothesis-two-zo-ns} for the case $i+1$. 
%In the third step, we show Eq~\eqref{eqn:induction-hypothesis-three-zo-ns}
%for $i+1$. The proof idea is similar to that of Eq~\eqref{eqn:induction-hypothesis-two-zo-ns}.
%In fact, by definition of $k^{(i)}$, we know that, 
%\begin{equation*}
% k^{(i+1)} \leq \frac{1}{6} \left[\gamma \left(n \Pi_{j=1}^i (k^{(j)})^{-2}\right)\right]^\frac{1}{3}
%	\leq \frac{1}{6} \left(n \Pi_{j=1}^i (k^{(j)})^{-2}\right)^\frac{1}{3}, 
%\end{equation*}
%and hence immediately we can get that, 
%\begin{equation*}
%\Pi_{j=1}^{i+1} k^{(j)} = \Pi_{j=1}^{i} k^{(j)} \cdot k^{(i+1)} 
%	\leq 6^{-1} n^{\frac{1}{3}} \left(\Pi_{j=1}^i k^{(j)}\right)^\frac{1}{3}.
%\end{equation*}
%Now, using the induction hypothesis Eq~\eqref{eqn:induction-hypothesis-three-zo-ns}, 
%we get, 
%\begin{align*}
%\Pi_{j=1}^{i+1} k^{(j)}\leq 6^{-1} n^{-\frac{1}{3}}
%	 \left(6^{-\frac{3}{2}\left(1-\frac{1}{3^i}\right)}
%			n^{\frac{1}{2}\left(1-\frac{1}{3^i}\right)} \right)^\frac{1}{3}
%	\leq 6^{-\frac{3}{2}\left(1-\frac{1}{3^{i+1}}\right)}	
%		n^{\frac{1}{2}\left(1-\frac{1}{3^{i+1}}\right)} 
%\end{align*}
%Finally, we show the first part of Eq~\eqref{eqn:induction-hypothesis-one-zo-ns}. Note 
%that, by the proven fact of Eq~\eqref{eqn:induction-hypothesis-three-zo-ns} for $i+1$, 
%it suffices to show that, $n^{1/3^{i+1}} \geq 6^{12}$, which would suffice if $n$ is large 
%enough so that, 
%\begin{equation*}
%\log \log n \geq M\log 3 + \log 12 +\log \log 6. 
%\end{equation*}
%
%Now, we are ready to show the rest of the corollary. We first show Eq
%\eqref{eqn:zo-ns-r-second-part}. Indeed, it is easy to use 
%induction argument to show the following result: for all $0\leq i\leq M-1$: 
%\begin{equation*}
%U^{(M+1)} = 9^{i+1} U^{(M-i)} \Pi_{j=0}^i k_{M-j}^{-1} + \frac{1}{4}(9^{i+1}-1)
%	\sigma \left(\log \frac{2}{\delta} + 1\right)^\half \Pi_{j=0}^M k_j^{-1}.
%\end{equation*}
%Therefore, if we plug in $i = M-1$ in the above argument, we get that, 
%\begin{equation*}
%U^{(M+1)} \leq 9^{M} \left(U_1 + \sigma \left(\log \frac{2}{\delta} + 1\right)^\half \right)
%	 \Pi_{j=0}^M k_j^{-1} = 9^{M}
%	 \Pi_{j=0}^M k_j^{-1} \left(2L + \sigma \left(\log \frac{2}{\delta} + 1\right)^\half \right)
%\end{equation*}
%The above result and Eq~\eqref{eqn:zo-ns-r-first-part} gives the inequality 
%in Eq~\eqref{eqn:zo-ns-r-second-part}. Finally, we note that, Eq~\eqref{eqn:zo-ns-r-third-part}
%follows easily from proposition~\ref{proposition:zeroth-order-nonsmooth-r-round-one-dim}
%and Eq~\eqref{eqn:zo-ns-r-second-part}. 
%%
%%note that, $\gamma(x) \geq \sqrt{x}$ when $x\geq 3$. 
%%Therefore, whenever $nr^2 \geq 6^{12}$, we have, $\gamma(nr^2) \geq 6^6$. 
%%As a consequence, $k(r) \geq 36$, and hence, $k\opt \geq 6$. Now, we show 
%%that $k\opt$ satisfies Eq~\eqref{eqn:existence-of-k-one-round-zo-one-dim-ns}. 
%%Indeed, when $k = k\opt$, 
%%\begin{equation*}
%%(2k+1) \floor{2k^2\log(2k+1)} \leq (3k)^3 \log(3k)^3 \leq k(r)^3 \log (k(r))^3 
%%	\leq nr^2,
%%\end{equation*}
%%where the last inequality follows from the fact that, for any $x > 0$, $\gamma(x)
%%\log \gamma(x) \leq x$. 
%\end{proof}

%\newpage 
\subsection{Lipschitz Function with Zeroth-Order Oracle, $d \geq 1$ and $M = 1$}
\subsubsection{Description of Algorithms}
In this section, we introduce the one stage algorithm for minimization of Lipschitz function 
under zeroth-order oracle: algorithm~\ref{alg:Lipcvxzomultidim} parameterized by 
$(c, r, k, T) \in \xdomain \times \R_+ \times \N \times \N$. Note that, algorithm 
\ref{alg:Lipcvxzomultidim} is in essence the same as algorithm~\ref{alg:smoothcvxzo}. 

\begin{algorithm}[htp]
\caption{Routine for One Stage Lipschitz Function $\ffamily_\lambda$ (Zeroth-Order Oracle)}  % scale here for size
\begin{algorithmic}[1]  % [1] is line numbering
\Statex Input: User's choice of the sampling center $c \in \xdomain$, radius $r\in \R_+$, 
	grid size parameter $k\in \N$ and the sampling times $T \in \N$.
\State Compute the grid points $G = G(c, r, k)$.
\State At each point $x\in G$, query the zeroth-order oracle $T$ times and denote each 
	sample function value via $\{\what{f}(x)^{(1)}, \what{f} (x)^{(2)}, \ldots, 
		\what{f} (x)^{(T)}\}$.
\State Compute the function value estimate at each point $x\in G$ via 
	$\hat{f}(x) = \frac{1}{T} \sum_{i=1}^T \what{f}(x)^{(i)}$.
\State Compute the estimate $\hat{x} \in G$, defined by
	$\hat{x} \defeq \argmin_{x\in G}\hat{f}(x)$.
\State Return the estimator $\hat x$.
\end{algorithmic}
\label{alg:Lipcvxzomultidim}
\end{algorithm}

\subsubsection{Analysis of Algorithm~\ref{alg:Lipcvxzomultidim}}
In this section, we show that, with careful choice of input parameters $(c, r, k, T)$, 
algorithm~\ref{alg:Lipcvxzomultidim} returns some estimator $\hat{x}$ that achieves 
the minimax risk (up to constants and logarithmic factors). The analysis 
is pretty close to that of algorithm~\ref{alg:smoothcvxzo}. For convenience, we slightly generalize 
the domain of interest by considering $\xdomain_{c, r} = \{x: \norm{x-c}_\infty \leq r\}$
parameterized by $c\in \R^d$ and $r\in \R$. The target now becomes $x_{f, c, r}\opt$,
the unique minimum of $f$ in the domain $\xdomain_{c, r}$, and the risk of interest 
would be $\E \left[f(\hat{x}) - f(x_{f, c, r}\opt)\right]$. 

\begin{proposition}
\label{proposition:zeroth-order-nonsmooth-one-round-multidim}
Given any fix $c\in \R^d$ and $r \in (0, 1]$, suppose there exists $k \in \N$ satisfying 
\begin{equation}
\label{eqn:existence-of-k-one-round-zo-multidim-ns}
(2k + 1)^d \ceil{2k^2 \log (2k+1)} \leq nr^2.
\end{equation}
Then, pick any $k \in \N$ satisfying Eq~\eqref{eqn:existence-of-k-one-round-zo-multidim-ns}, 
and set $T= \floor{\frac{n}{(2k+1)^d}}$. Denote $\hat{x}$ to be the output from 
algorithm~\ref{alg:Lipcvxzomultidim} when we input $(c, r, k, T)$ as the input parameters.
Then, we have 
\begin{equation*}
\P\left(f(\hat{x}) - f(x_{f, c, r}\opt) \leq \frac{r}{k} \cdot \left(2\sigma 
	\left(\sqrt{2 \log \frac{2}{\delta} + d} \right)+ L\sqrt{d}\right)
		\right)\geq 1-\delta.
\end{equation*}
\end{proposition}

%\rfcomment{I am going to re-write things below}
\begin{proof}
Denote $\bar{x} = \argmin_{x\in G} \ltwo{\bar{x} - x_{f, c, r}\opt}$. Then, by 
construction of the grids $G$ and Lipschitzness of the function $f$, we know 
that, 
\begin{equation*}
\ltwo{\bar{x} - x_{f, c, r}\opt} \leq \frac{r\sqrt{d}}{k}~~\text{and}~~
	f(\bar{x}) - f(x_{f, c, r}\opt) \leq L \ltwo{\bar{x} - x_{f, c, r}\opt}  
		\leq \frac{Lr\sqrt{d}}{k}.
\end{equation*}
Now, let us consider the following event: 
\begin{equation*}
\event = \left\{\left|\what{f}(x) - f(x)\right| \leq r^a \defeq
	\sigma \sqrt{\frac{2}{T} \log \frac{2(2k+1)^d}{\delta}}
	~\text{for all}~x \in G\right\}, 
\end{equation*}
The next lemma shows that $\event$ happens with probability at least $1-\delta$. 
\begin{lemma}
We have $\P(\Gamma) \geq 1-\delta$. 
\end{lemma}
\begin{proof}
First, for each $x \in G$, denote $\noise(x) \defeq \what{f}(x) - f(x)$.  Then, since 
by our assumption, the noise $\{\what{f}(x) - f(x)\}_{x=1}^{T}$ is mean $0$, independent 
and subgaussian with parameter $\sigma^2$, we have that $\noise(x)$ is mean $0$ 
and subgaussian with parameter $\sigma^2/T$. Therefore, for any fix $x\in G$, 
\begin{align*}
\P\left(|\noise(x)| \geq r^a \right) 
		\leq 2\exp\left(-\frac{(r^a)^2T}{2\sigma^2}\right) 
			\leq \delta (2k+1)^{-d}, 
\end{align*}
where the first inequality above uses the subgaussianity of $\noise(x)$, and 
the second inequality uses the definition of $r^a$. 
Now, the desired claim of the lemma follows from the fact that 
$|G| = (2k+1)^d$ and the union bound of the above events. 
\end{proof}
Note that, since by definition $\hat{f}(\hat{x}) \leq \hat{f}(\bar{x})$, we get 
the below upper bound of $f(\hat{x})$ on $\event$, 
\begin{equation*}
f(\hat{x}) - f(x_{f, c, r}\opt) = \underbrace{f(\hat{x}) - \hat{f}(\hat{x})}_{\leq r^a}
	+ \underbrace{\hat{f}(\hat{x}) - \hat{f}(\bar{x})}_{\leq 0}
	+ \underbrace{\hat{f}(\bar{x}) - f(\bar{x})}_{\leq  r^a} 
	+ \underbrace{f(\bar{x}) - f(x_{f, c, r}\opt)}_{\leq \frac{Lr\sqrt{d}}{k}} 
		\leq 2r^a + \frac{Lr\sqrt{d}}{k}.
\end{equation*}
Now, the assumption on $k, T$ shows that, $T \geq 2k^2\log(2k+1)r^{-2}$ and, 
\begin{equation*}
r^a = \sigma \left(\sqrt{\frac{2\log \frac{2}{\delta} + 2d\log(2k+1)}{T}} \right)
	\leq \frac{\sigma r}{k} \cdot \left(\sqrt{2 \log \frac{2}{\delta} + d}\right)
\end{equation*}
This gives the desired claim of the proposition. 
\end{proof}

Motivated by Proposition~\ref{proposition:zeroth-order-nonsmooth-one-round-multidim}, 
it becomes important to understand when such $k$ exists in Eq 
\eqref{eqn:existence-of-k-one-round-zo-multidim-ns} and how large it is. 

\begin{lemma}
\label{lemma:ef-zeroth-order-smooth-multidim-ns}
Assume $n$ is large enough satisfying $nr^2 \geq 3^{2{d+2}}$. 
Denote $k(r)= \left(\gamma\left(nr^2\right)\right)^\frac{1}{d+2}$
and $k = \floor{\frac{1}{3} k(r)}$. Then $k\opt\in \N$,
and $k\opt$ satisfies Eq~\eqref{eqn:existence-of-k-one-round-zo-multidim-ns}.
\end{lemma}


\begin{proof}
%Note that, the second claim of the corollary follows immediately from its 
%first part and Proposition~\ref{proposition:zeroth-order-nonsmooth-one-round-multidim}.
%To prove the first part, 
Note that, $\gamma(x) \geq \sqrt{x}$ whenever $x \geq 3$. 
Thus, by assumption that $nr^2 \geq 3^{2(d+2)}$, we get that $\gamma(nr^2) \geq 3^{{d+2}}$ 
and hence $k\opt \geq 1$. To show that $k\opt$ satisfies Eq 
\eqref{eqn:existence-of-k-one-round-zo-multidim-ns}, note that, when 
$k = k\opt$, we have, 
\begin{equation*}
(2k+1)^d \ceil{2k^2 \log(2k+1)} \leq (3k)^{d+2} \log (3k)^{d+2} \leq
	 (k(r))^{d+2} \log (k(r))^{d+2} \leq nr^2,
\end{equation*}
where the last inequality follows from the fact that, for any $x > 0$, 
$\gamma(x) \log \gamma(x) \leq x$.
\end{proof}
Proposition~\ref{proposition:zeroth-order-nonsmooth-one-round-multidim}
and lemma~\ref{lemma:ef-zeroth-order-smooth-multidim-ns} immediately 
give us the corollary below. 

\begin{corollary}
\label{corollary:zeroth-order-nonsmooth-one-round-multidim}
Given any fix $c \in \R^d$ and $r\in (0, 1]$, set 
$k = \floor{\frac{1}{3} (\gamma(nr^2))^\frac{1}{d+2}}$ and 
$T = \floor{\frac{n}{2k+1}}$. Assume $n$ is large enough satisfying 
$nr^2 \geq 3^{2{d+2}}$. Then if we denote $\hat{x}$ to be the output 
from algorithm~\ref{alg:Lipcvxzomultidim} when we input 
$(c, r, k, T)$ as the input parameters, we get, 
%\begin{equation*}
%\P\left(f(\hat{x}) - f(x_{f, c, r}\opt) \leq \frac{r}{k} \cdot \left(2\sigma 
%	\left(\sqrt{2 \log \frac{2}{\delta} + d} \right)+ L\sqrt{d}\right)
%		\right)\geq 1-\delta.
%\end{equation*}
%Assume $n$ is large enough satisfying $nr^2 \geq 3^{2{d+2}}$. 
%Denote $k(r)= \left(\gamma\left(nr^2\right)\right)^\frac{1}{d+2}$
%and $k = \floor{\frac{1}{6} k(r)}$. Then $k\opt\in \N$,
%and $k\opt$ satisfies Eq~\eqref{eqn:existence-of-k-one-round-zo-multidim-ns}.
%Moreover, we have, 
\begin{equation*}
\P\left(f(\hat{x}) - f(x_{f, c, r}\opt) \leq \gap\opt \right) \geq 1 - \delta, 
\end{equation*}
where 
\begin{equation*}
\gap\opt 
	%= \frac{r}{k\opt} \cdot \left(2\sigma \sqrt{2\log \frac{2}{\delta} + d} + L\sqrt{d}\right)
	= 6r^\frac{d}{d+2} n^{-\frac{1}{d+2}} \log (nr^2)^\frac{1}{d+2} \cdot 
		\left(2\sigma \sqrt{2\log \frac{2}{\delta} + d} + L\sqrt{d}\right).
\end{equation*}

\end{corollary}

%\begin{proof}
%Note that, the second claim of the corollary follows immediately from its 
%first part and Proposition~\ref{proposition:zeroth-order-nonsmooth-one-round-multidim}.
%To prove the first part, note first that, $\gamma(x) \geq \sqrt{x}$ whenever $x \geq 3$. 
%Thus, by assumption that $nr^2 \geq 6^{d+2}$, we get that $\gamma(nr^2) \geq 6^{2{d+2}}$ 
%and hence $k\opt \geq 1$. To show that $k\opt$ satisfies Eq 
%\eqref{eqn:existence-of-k-one-round-zo-multidim-ns}, note that, when 
%$k = k\opt$, we have, 
%\begin{equation*}
%(2k+1)^d \ceil{2k^2 \log(2k+1)} \leq (3k)^{d+2} \log (3k)^{d+2} \leq
%	 (k(r))^{d+2} \log (k(r))^{d+2} \leq nr^2,
%\end{equation*}
%where the last inequality follows from the fact that, for any $x > 0$, 
%$\gamma(x) \log \gamma(x) \leq x$.
%\end{proof}
%
%
%\begin{corollary}
%\label{corollary:zeroth-order-smooth-one-round}
%Suppose $n$ is large enough so that 
%\begin{equation*}
%nH^2 r^4 d^2 \geq 
%	\left(\sigma^2\left(\log \frac{2d}{\delta} + 1\right) + H^2 r^4d^2\right)
%		\left(\frac{Hd}{\lambda}\right)^{(d+4)}
%%		~\text{and}~
%%			n \geq 2^\frac{d+2}{2}
%%				\left(\frac{H^2R^2}{25\sigma^2 \log \frac{2d}{\delta}}\right)^\frac{d}{2}
%\end{equation*} 
%then $k\opt = \ceil{k(r)}$ satisfies Eq~\eqref{eqn:existence-of-k-one-round-fo}.
%Moreover, we have,  
%\begin{equation*}
%\P\left(f(\hat{x}) - f(x_{f, c, r}\opt) \leq \gap\opt \right)\geq 1-\delta, 
%\end{equation*}
%where, $\gap\opt$ is defined by, 
%\begin{equation*}
%\gap\opt = \frac{r^2 Hd}{(k\opt)^2} \leq
%	36 r^2 Hd 
% \left(\frac{\max\left\{\sigma^2 \log \frac{2d}{\delta}, 
% 	\sigma^2 \log \frac{nH^2r^4d^2}{\sigma^2}, H^2 r^4d^2\right\}}{nH^2r^4 d^2} 
%		\right)^\frac{2}{d+4} 
%\end{equation*}
%\end{corollary}
%\begin{proof}
%Note that, the last claim of the lemma follows immediately from 
%proposition~\ref{proposition:zeroth-order-smooth-one-round}. Now, we show 
%that $k\opt \geq \frac{4Hd}{\lambda} \geq 1$ under our 
%assumptions. Note that, by elementary inequality that $x/\log(x) \geq \sqrt{x}$ 
%for all $x\geq 3$,  our first assumption on $n$ gives us that, 
%\begin{equation*}
%\frac{nH^2r^4d^2}{\sigma^2 \log \frac{nH^2r^4d^2}{\sigma^2}} 
%	\geq \left(\frac{nH^2r^4d^2}{\sigma^2}\right)^\half
%		\geq \left(\frac{Hd}{\lambda}\right)^\frac{d+4}{2}, ~
%			\frac{nH^2r^4d^2}{\sigma^2 \log \frac{2d}{\delta}} \geq 
%				\left(\frac{Hd}{\lambda}\right)^\frac{d+4}{2}
%	~~\text{and}~~ \frac{nH^2r^4d^2}{H^2 r^4d^2} \geq  
%		\left(\frac{Hd}{\lambda}\right)^\frac{d+4}{2}
%\end{equation*}
%which immediately leads to the elementary fact, 
%\begin{equation*}
%\left(\frac{nH^2r^4d^2}{\max\left\{\sigma^2 
%	\log \frac{2d}{\delta}, \sigma^2 \log \frac{nH^2r^4d^2}{\sigma^2}, 
%		H^2 r^4\right\}}\right)^{\frac{1}{d+4}}
%\geq \left(\frac{Hd}{\lambda}\right)^\half.
%\end{equation*}
%Hence, we get that, $k\opt \geq k(r) \geq \left(\frac{Hd}{\lambda}\right)^\half \geq 1$ by definition of 
%$k\opt$. Now, we show that $k \geq k\opt$. Indeed, it suffices to show that, 
%$k = k\opt$ satisfies below three inequalities, 
%\begin{equation}
%\label{eqn:inequality-k-one-zo}
%2(2k+1)^d \leq n,~
%	(2k+1)^d \log \frac{2d}{\delta}\leq \frac{nH^2r^4d^2}{8\sigma^2k^4} 
%		~\text{and}~
%			(2k+1)^d \log (2k+1)^d \leq \frac{nH^2r^4d^2}{8\sigma^2k^4}. 
%\end{equation}
%In fact, if the above three inequalities hold for $k = k\opt$, then we have, 
%when $k = k\opt$, 
%\begin{align*}
%&(2k+1)^d \ceil{\frac{2\sigma^2k^4}{H^2r^4d^2} \log \frac{2d(2k+1)^d}{\delta}} 
%\leq (2k+1)^d \left(1+\frac{2\sigma^2k^4}{H^2r^4d^2} \log \frac{2d(2k+1)^d}{\delta}\right) \\
%&=  \underbrace{(2k+1)^d}_{\leq \frac{n}{2}}
% + \underbrace{(2k+1)^d \frac{2\sigma^2k^4}{H^2r^4d^2} \log \frac{2d}{\delta}}_{\leq \frac{n}{4}}
% + \underbrace{(2k+1)^d \frac{2\sigma^2k^4}{H^2r^4d^2}\log (2k+1)^d}_{\leq \frac{n}{4}}\leq n, 
%\end{align*}
%and hence $k$ by its definition must satisfy $k \geq k\opt$. 
%We first show $k\opt$ satisfies the first inequality in 
%Eq~\eqref{eqn:inequality-k-one-zo}. Indeed, by definition of $k(r)$,
%one can easily see that, $k(r) \leq \frac{1}{6} n^{\frac{1}{d+4}}$. Hence, 
%\begin{equation*}
%2(2k\opt+1)^d \leq (6k(r))^{d+4} \leq n.
%\end{equation*}
%%In fact, notice that,
%%our second assumption on $n$ gives us that, 
%%\begin{equation*}
%%n \geq 2^\frac{d+2}{2}\left(\frac{H^2R^2}{25\sigma^2 \log \frac{2d}{\delta}}\right)^\frac{d}{2} 
%%~\Leftrightarrow \left(\frac{nH^2R^2}{25\sigma^2 
%%	\log \frac{2d}{\delta}}\right)^{\frac{d}{d+2}} \leq \frac{n}{2}.
%%\end{equation*}
%%Therefore, we get that, for $k = k_1\opt$, we have, 
%%\begin{equation*}
%%2(2k+1)^d \leq 2(3k)^d \leq \left(\frac{nH^2R^2}{25\sigma^2 
%%	\log \frac{2d}{\delta}}\right)^{\frac{d}{d+2}} \leq \frac{n}{2}, 
%%\end{equation*}
%%which proves the desired first inequality in Eq~\eqref{eqn:inequality-k-one}.
%Next, we show that $k = k\opt$ satisfies the second inequality in 
%Eq~\eqref{eqn:inequality-k-one-zo}. Again, by definition of $k(r)$, we have, 
%$k(r) \leq \frac{1}{6}\left(\frac{nH^2r^4d^2}{\sigma^2 \log \frac{2d}{\delta}}\right)^{\frac{1}{d+4}}$.
%Hence, we have, 
%\begin{equation*}
%(2k\opt+1)^d (k\opt)^4 \log \frac{2d}{\delta}\leq \frac{1}{81} 
%	(6k(r))^{d+4} \log \frac{2d}{\delta} 
%	\leq \frac{nH^2r^4d^2}{8\sigma^2}, 
%\end{equation*}
%which gives the second inequality in  Eq~\eqref{eqn:inequality-k-one-zo}. 
%Finally, we show that $k = k\opt$ satisfies the third (and also the last) 
%inequality in Eq~\eqref{eqn:inequality-k-one-zo}. To do so, denote
%$m = nH^2 r^4d^2/\sigma^2$, then $m \geq 3$ by our 
%first assumption on $n$. Note that, our definition of $k(r)$ satisfies 
%$k(r) \leq \frac{1}{6}\left(\frac{m}{\log m}\right)^{\frac{1}{d+4}}$, 
%or equivalently, $(6k(r))^{d+4} \leq \frac{m}{\log m}$. Hence, 
%\begin{equation*}
%(2k\opt+1)^d (k\opt)^4 \log (2k+1)^d \leq \frac{1}{81}(6k(r))^{d+4} 
%	\log (6k(r))^{d+4} \leq \frac{m}{81\log m} \log \frac{m}{\log m} 
%		\leq \frac{m}{81} \leq \frac{nH^2r^4 d^2}{8\sigma^2}, 
%\end{equation*}
%which gives the last inequality of Eq~\eqref{eqn:inequality-k-one-zo}. The
%desired claim of the corollary now follows. 
%\end{proof}
%



%
%In order to analyze this algorithm, let us first define the event $\Gamma$ as done in the previous section.
%Let $\Gamma$ be a good event that all the estimates obtained by sampling are within some accuracy.
%\begin{equation*}
%    \Gamma = \left \{|f(x) - g_x| \leq a \textup{ for all } x \in P \right \},
%\end{equation*}
%where $a$ is defined as
%\begin{equation*}
%    a \defeq 2^{d+1} \sigma n^{-\frac{1}{d+2}} \sqrt{\log \frac{2^{2d+1}}{\delta} + \log n}.
%\end{equation*}
%\begin{lemma}
%    If we run Algorithm~\ref{alg:Lipcvxzomultidim} with $n \geq \max\{d^d, 9\}$, $\P(\Gamma) \geq 1-\delta$.
%\end{lemma}
%\begin{proof}
%In order to make the sampling strategy reasonable, 
%the number of sampled points $(k+1)^d$ must be less than or equal to the sampling budget $n$.
%So $n$ must satisfy $(\ceil{n^{1/(d+2)}}+1)^d \leq n$. 
%For $d \geq 6$, we can check that $(n^{1/(d+2)}+2)^d \leq n$ holds whenever $n \geq d^d$, so $(\ceil{n^{1/(d+2)}}+1)^d \leq n$.
%For $1 \leq d \leq 5$, we can directly compute $n_0$ such that $(\ceil{n^{1/(d+2)}}+1)^d \leq n$ is satisfied whenever $n \geq n_0$.
%With a bit of calculation we get $n_0 = 3, 9, 3^3, 4^4, 5^5$ for $d = 1,2,3,4,5$, respectively.
%Therefore, $n$ must be at least $\max\{d^d, 9\}$ to satisfy $n \geq (k+1)^d$.
%
%Given $n \geq (k+1)^d$, by sub-Gaussian property of the noise, we have from Hoeffding's inequality that
%\begin{equation*}
%    \P\left (\left | g_x - f(x) \right | \ge a \right ) \leq 2 \exp \left ( - \frac{a^2}{2 \sigma^2} \floor{\frac{n}{(k+1)^d}} \right).
%\end{equation*}
%for any accuracy $a>0$. Our goal is to show that with 
%$a \defeq 2^{d+1} \sigma n^{-\frac{1}{d+2}} \sqrt{\log \frac{2^{2d+1}}{\delta} + \log n}$, we have
%\begin{align*}
%    2 \exp \left ( - \frac{a^2}{2 \sigma^2} \floor{\frac{n}{(k+1)^d}} \right) \leq \frac{\delta}{(k+1)^d},
%\end{align*}
%so that by union bound we have $1-\P(\Gamma) \leq \delta$.
%As done in Lemma~\ref{lem:Lipcvxzoaccu}, the condition is equivalent to
%\begin{align*}
%    a \geq \sqrt{\frac{2\sigma^2}{\floor{n/(k+1)^d}} \log \left ( \frac{2 (k+1)^d}{\delta} \right )}.
%\end{align*}
%Since $n \geq (k+1)^d$ and $k \geq 1$, we can use inequalities $\floor{\frac{n}{(k+1)^d}} \geq \frac{n}{2(k+1)^d} \geq \frac{n}{2^{d+1} k^d}$, and $k = \ceil{n^{1/(d+2)}} \leq 2n^{1/(d+2)}$, to get
%\begin{align*}
%    \sqrt{\frac{2\sigma^2}{\floor{n/(k+1)^d}} \log \left ( \frac{2 (k+1)^d}{\delta} \right )}
%    &\leq \sqrt{\frac{2^{d+2} k^d \sigma^2}{n} \log\left( \frac{2^{d+1} k^d}{\delta}\right)}\\
%    &\leq \sqrt{\frac{2^{2d+2} n^{d/(d+2)} \sigma^2}{n} \log\left( \frac{2^{2d+1} n^{d/(d+2)}}{\delta}\right)}\\
%    &\leq 2^{d+1} \sigma n^{-\frac{1}{d+2}}\sqrt{\log \frac{2^{2d+1}}{\delta} + \log n} =: a.
%    \end{align*}
%\end{proof}
%The following proposition proves that this simple algorithm is indeed minimax optimal, up to sub-polynomial factors.
%\begin{proposition}
%If $n \geq \max\{d^d, 9\}$, the output $\what x$ of Algorithm~\ref{alg:Lipcvxzomultidim} satisfies
%\begin{equation*}
%    f(\what x) - f(x^*) \leq \left (L \sqrt d + 2^{d+2} \sigma \sqrt{\log \frac{2^{2d+1}}{\delta} + \log n} \right ) n^{-\frac{1}{d+2}},
%\end{equation*}
%with probability at least $1-\delta$.
%\end{proposition}
%\begin{proof}
%Suppose the event $\Gamma$ is given. Define $\tilde x \defeq \argmin_{x \in P} \ltwo{x-x^*}$. Then,
%\begin{equation*}
%    f(\what x) - f(\tilde x)
%    \leq | f(\what x) - g_{\what x} | + g_{\what x} - g_{\tilde x} + |g_{\tilde x} - f(\tilde x)|
%    \leq 2a.
%\end{equation*}
%Also, by Lipschitzness
%\begin{equation*}
%    f(\tilde x)- f(x^*) \leq L \ltwo{\tilde x - x^*} \leq \frac{L \sqrt d}{k} \leq L\sqrt{d} n^{-\frac{1}{d+2}}.
%\end{equation*}
%Combining these two inequalities proves the proposition.
%\end{proof}

%\newpage

%\bibliography{template}
%\end{document}
