\newcommand{\Conthm}{C}
\newcommand{\conthm}{c}

\section{Lower Bounds}

With our sketches and ``big ideas'' implemented, we turn to formal
statements of our results. We begin with the lower bounds on the minimax
risk~\eqref{eqn:minimax-risk-definition}.  We defer the proof of
Theorem~\ref{thm:lbmain} to appendices~\ref{sec:lbmain-roadmap}
through~\ref{sec:defer-proof-const}, with apologies for the extraordinary
length.
%\subsection{Main Result}
\begin{theorem}
  \label{thm:lbmain}
  % Recall that $\ofamilyz$ and $\ofamilyf$ denote the classes of zeroth- and first-order oracles with $\sigma^2$-subgaussian noise.
  Consider the case when the domain $\xdomain = [0, 1]^d$. 
  \begin{enumerate}[1.]
  \item When the function class $\ffamily = \ffamily_L$ and $M \leq \log
    \log n/\log \left(1+ \frac{2}{d}\right)$, then there exist
    constants $\conthm_1, \conthm_2>0$ depending solely on $d, \sigma,$ and $L$ such that
    \begin{equation*}
      \begin{array}{l}
	\minimax_M(\ffamily_L, \ofamilyz) \geq \conthm_1
	n^{-\half \left(1 - \left( \frac{d}{d+2} \right)^M \right)  } 
	e^{-\sqrt{2\log n}} \log^{-\conthm_2} n.
      \end{array}
    \end{equation*}
  \item When the function class $\ffamily = \ffamily_{H, \lambda}$ and $M
    \leq \log \log n/\log \left(1+ \frac{4}{d}\right)$, then there exist
    constants $\conthm_1, \conthm_2 > 0$ depending solely on $d, \sigma, H,$ and $\lambda$ such
    that
    \begin{align*}
      \minimax_M(\ffamily_{H, \lambda}, \ofamilyz) \geq \conthm_1
      n^{-\half \left(1 - \left( \frac{d}{d+4} \right)^M \right)  } 
      e^{-\sqrt{2\log n}} \log^{-\conthm_2} n
    \end{align*}
  \item When the function class $\ffamily = \ffamily_{H, \lambda}$ and $M
    \leq \log \log n/\log \left(1+ \frac{2}{d}\right)$, then there exist
    constants $\conthm_1, \conthm_2 > 0$ depending solely on $d, \sigma, H,$ and $\lambda$ such
    that
    \begin{align*}
      \minimax_M(\ffamily_{H, \lambda}, \ofamilyf) \geq \conthm_1
      n^{- \left(1 - \left( \frac{d}{d+2} \right)^M \right)  } 
      e^{-\sqrt{8\log n}} \log^{-\conthm_2} n.
    \end{align*}
  \end{enumerate}
  %	If the function class $\ffamily = \ffamily_{H,\lambda}$ is $H$-smooth and $\lambda$-strongly convex functions, where $H > \lambda$, there exist constants $K_{H,\lambda}^{(0)}$ and $K_{H,\lambda}^{(1)}$ that depend only on $M$, $d$, $\sigma$, $H$, and $\lambda$, such that
  %	\begin{align*}
  %		&\epsilon(\ffamily_{H,\lambda}, \ofamilyz) 
  %		\geq
  %		K_{H,\lambda}^{(0)}
  %		n^{-\half \left(1 - \left( \frac{d}{d+4} \right)^M\right)  } 
  %		\exp\left ( - \sqrt{2} \sqrt{\log n} \right ) ,\\
  %		&\epsilon(\ffamily_{H,\lambda}, \ofamilyf) 
  %		\geq
  %		K_{H,\lambda}^{(1)}
  %		n^{- \left(1 - \left( \frac{d}{d+2} \right)^M\right)  } 
  %		\exp\left ( - 2\sqrt{2} \sqrt{\log n} \right ).
  %	\end{align*}
  %
  %Moreover, the leading constants $\conthm_1$, $K_{H,\lambda}^{(0)}$, and $K_{H,\lambda}^{(1)}$ have lower bounds of the form $c \cdot r^M$, where $c>0$ and $0 < r < 1$.
\end{theorem}

We provide some contextualizing remarks on this result.
\begin{enumerate}[1.]
\item 
In Theorem~\ref{thm:lbmain}, the lower bounds
using $n$ observations
for $M$ rounds take the form $\tilde{\Omega} (n^{-\gamma})$, where
\begin{equation}
  \label{eq:lbform}
  \gamma = \frac{\kappa}{2(\kappa-\zeta)} \left(1 - \left(
    \frac{d}{d+2(\kappa-\zeta)} \right)^M\right),
\end{equation}
and $\tilde \Omega$ hides leading dimension-dependent constants and
sub-polynomial terms in $n$. The constant $\kappa$ corresponds to the
function class, with $\kappa = 1$ for $\ffamily_{L}$ and $\kappa
= 2$ for $\ffamily_{H,\lambda}$, while $\zeta \in \{0,1\}$
indicates the order of the information oracle.  
The theorem shows that the rate is worse than the
fully-sequential rate $n^{-\frac{\kappa}{2(\kappa-\zeta)}}$ for $M$ 
smaller than $\log \log n/\log \left(1+ 2(\kappa-\zeta)/d\right)$.

%% for $M$ larger than $\log\log n/\log \left(1+ 2(\kappa-\zeta)/d\right)$, we achieve the fully-sequential minimax rate,
%% which is interesting because with this limited amount of
%% adaptivity we can still perform nearly at the rate-optimal convergence
%% rate---in $n$---as the fully sequential or adaptive regime.

\item 
  It is also interesting to see how the rate of algorithm depends on $d$ for
  fixed $M$. As $d$ increases to infinity, $\frac{d}{d+2(\kappa-\zeta)}
  \to 1$,
  meaning that the minimax rates show \emph{exponential degradation} as $d$
  increases. We cannot observe this phenomenon in fully sequential
  problems. (Our lower bounds have dimension-dependent
  leading constants, which we lave as an important open question.)
\item 
  Our proof technique gives a generic approach of achieving these lower
  bounds whenever we can construct functions satisfying recursive closeness
  properties similar to equations~\eqref{eqn:function-pair-dist},
  \eqref{eqn:function-value-same}, and
  \eqref{eqn:function-value-difference}.
  (Condition~\ref{con:ftnconstrunified} in Appendix~\ref{sec:proof-lbmain}
  makes this precise.) If we can construct the set of functions satisfying
  these, we immediately obtain a lower bound of the form~\eqref{eq:lbform}.
\end{enumerate}

%\subsection{High-level proof idea of Theorem \ref{thm:lbmain}}
%(Recall the function construction part... the functions have the same value outside some small set...)
%
%The key idea of the proof is pigeon hole principle: if there are $n$ pigeons and $k$ holes, there must be at least one hole that contains at most $n/k$ pigeons, because otherwise every hole will have more than $n/k$ pigeons and the sum of all pigeons will be larger than $n$.
%The same principle works for the sampling strategy $\Qone$; regardless of the strategy or probability distribution you pick, there must be at least one ``hole'' $\Bone$ such that less than certain number of points are sampled with constant probability.
%Then, for that $u_1$, think of a function $\ftnuv$ defined with this parameter $u_1$; its global minimizer is attained in $\Bone$, 
%but we have small number of points in there, so it would be difficult to minimize well.
%
%The next sampling strategy $\Qtwo$ will sample the points based on observations at $X^{(1)}_{1:n}$, but again, pigeon hole principle works and there is at least one ``hole'' $\Btwo$ such that small number of points are sampled (with constant probability).
%We can repeat this process and find out a set of parameters $u_{1:M}$ whereby the sampling strategy $\Qone, \dots, \QM$ 
%has difficulty in sampling many points in the hole $\BM$, which contains the global optimum of $\ftnuv$.
%Recall that $\ftnuvp$ and $\ftnuvm$ are exactly the same outside $\BM$; 
%only samples in $\BM$ can help distinguish them, but due to the procedure we picked $u_{1:M}$ 
%we do not have many samples in $\BM$, so optimizing $\ftnuv$ must be very hard 
%and must produce some amount of error with constant probability, which gives the lower bound.

%\section{Function Construction}
%\subsection{Lipschitz Convex}
%\label{sec:multiconstlipcvx}
%\label{sec:constLipcvx}
%\paragraph{Constructing recursive maximal packings.}
%Consider $\delta_1, \delta_2, \dots, \delta_M$, which are real valued quantities that decays to zero as 
%the number of samples $n$ goes to infinity.
%The values will be fixed by lower bound proof later,
%but for now $\delta_t = o(\delta_{t-1})$ suffices to construct the functions.
%
%Given $\delta_t$'s, we recursively define the packings as follows:
%\begin{description}
%	\item[$1.$] Let $\Uone$ to be any maximal $2\delta_1$-packing of $[\delta_1, 1-\delta_1]^d$ with respect to $\ell_\infty$ norm.
%	\item[$2.$] For any $u_1 \in \Uone$, let $\Utemp{1}{2}$ to be any maximal $2\delta_2$-packing of $\ballinf{u_1}{\tfrac{\delta_1}{2}-\delta_2}$ w.r.t.\ $\ell_\infty$ norm.
%	\item[$3.$] For any $u_2 \in \Utemp{1}{2}$, let $\Utemp{2}{3}$ to be any maximal $2\delta_3$-packing of $\ballinf{u_2}{\tfrac{\delta_2}{2}-\delta_3}$ w.r.t.\ $\ell_\infty$ norm.\\ $\cdots$
%	\item[$M.$] For any $u_{M-1} \in \Utemp{M-2}{M-1}$, let $\Utemp{M-1}{M}$ to be any maximal $2\delta_M$-packing of $\ballinf{u_{M-1}}{\tfrac{\delta_{M-1}}{2}-\delta_M}$ w.r.t.\ $\ell_\infty$ norm.
%	\item[$M+1.$] For any $u_{M} \in \Utemp{M-1}{M}$, let $\V = \{-1, +1\}$.
%\end{description}
%In the first stage, we construct a set of points $\Uone$ so that any $u_1 \in \Uone$ satisfies $\ballinf{u_1}{\delta_1} \subset \domain$.
%Starting from second stage, we construct maximal packings for \emph{all} points in the previous stage;
%for example, $\Utemp{1}{2}$ is defined for all $u_1 \in \Uone$ and $\Utemp{1}{2}$ depends on which $u_1$ we choose.
%We continue in this recursive way until we define $\Utemp{M-1}{M}$. After that stage, we define the final set $\V = \{ \pm 1\}$.
%Also note that by construction we have $\ballinf{u_t}{\delta_t} \subset \ballinf{u_{t-1}}{\delta_{t-1}/2}$ for $t \in 2:M$.
%Notice that for any $t \in 2:M$, $\ballinf{u_{t}}{\tfrac{\delta_{t}}{2}} \cap \ballinf{\tilde u_{t}}{\tfrac{\delta_{t}}{2}} = \emptyset$,
%for different $u_t, \tilde u_t \in \Utemp{t-1}{t}$.
%This means that a point $u_t \in \Utemp{t-1}{t}$ uniquely maps to all their ``ancestors'' $u_{t-1}, u_{t-2}, \dots, u_1$,
%because the neighborhoods of their ancestors never overlap with the other ancestors at the corresponding level.
%
%\paragraph{Defining a function for each set of parameters.}
%Given the maximal packings, suppose we chose any $u_1 \in \Uone$. 
%Once we chose $u_1$, we can choose $u_2 \in \Utemp{1}{2}$ from the packing constructed for $u_1$.
%We can recurse this way and choose all the points up to $u_M$, and then $v \in \V$.
%Using these points $u_1, \dots, u_M$ and $v$ as parameters, Algorithm~\ref{alg:ftnconstrLipsconv}
%constructs a function $\ftnuv(x)$ that corresponds to the specific choice of $u_1, \dots, u_M$ and $v$.
%
%\begin{algorithm}[t]
%	\caption{Construction of Lipschitz convex $\ftnuv$.}  \label{alg:ftnconstrLipsconv}
%	\begin{algorithmic}[1]  % [1] is line numbering
%		\STATEx Given parameters $u_1, u_2, \dots, u_M, v$, size $\delta_1, \dots, \delta_M$, and Lipschitzness parameter $L$,
%		\STATE Start with $\ftnuone (x) = \ftninuone(x) \defeq L\linf{x-u_1}$.
%		\FOR{ $t = 2$ to $M$ }
%		\STATE $\ftninut (x) \defeq \frac{L}{3^{t-1}} \linf{x-u_{t}} + \sum_{m=1}^{t-1} \frac{L \delta_m}{2 \cdot 3^{m-1}}.$
%		\STATE $\ftnut (x) \defeq \max \{ \ftnutmone (x), \ftninut (x) \}$.
%		\ENDFOR
%		\STATE $\ftninuv (x) \defeq \frac{L}{3^M} \linf{x-u_M-\frac{v \delta_M}{2} \ones} 
%		+ \sum_{m=1}^{M} \frac{L \delta_m}{2 \cdot 3^{m-1}}$. 
%		\STATE Return $\ftnuv(x) \defeq \max \{ \ftnuM(x), \ftninuv(x) \}$.
%	\end{algorithmic}
%\end{algorithm}
%
%Algorithm~\ref{alg:ftnconstrLipsconv} defines a series of convex functions $\ftninut(x)$ and repeatedly takes maximum with previous ones to get the final function.
%From this way of defining the functions, we can achieve an interesting property that helps us prove the minimax lower bound later on: two functions with ``similar'' parameter values have ``similar'' function values.
%More concretely, if the two functions share the same parameter values from $u_1$ up to $u_{t-1}$ ($t \in 2:M$),
%but their parameter deviated from each other after $t$, say $u_t, \dots, u_M, v$ and $\tilde u_t, \dots, \tilde u_M, \tilde v$, 
%then the two functions $\ftnuv(x)$ and $\ftnuvtildet(x)$ are completely identical far away from $u_{t-1}$,
%and differ only ``slightly'' at points near $u_{t-1}$.
%We illustrate this property more concretely in the following lemma.
%\begin{lemma}
%	\label{lem:ftnconstrLipcvx}
%	The functions constructed with Algorithm~\ref{alg:ftnconstrLipsconv} have the following property: whenever the two functions have the same parameters from $u_1$ up to $u_{t-1}$ ($t \in 2:M$), they satisfy, for any values of remaining parameters $u_{t:M}$, $v$, $\tilde u_{t:M}$, and $\tilde v$,
%	\begin{enumerate}
%		\item $\ftnuv(x) = \ftnuvtildet(x),~ \forall x \notin \Binftmone$.
%		\item $| \ftnuv(x) - \ftnuvtildet(x) | \leq \frac{ L \delta_{t-1}}{2 \cdot 3^{t-2}},~ \forall x \in \Binftmone$. 
%		\item $\lone{\grduv - \grduvtildet} \leq \frac{ 2 L }{3^{t-2}},~\forall x \in \Binftmone$, \\
%		$\forall \grduv \in \sgrduv(x), \forall \grduvtildet \in \sgrduvtildet(x).$ 
%	\end{enumerate}
%	Similarly, when $u_{1:M}$ are all the same and only $v, \tilde v \in \V$ are different, 
%	\begin{enumerate}
%		\setcounter{enumi}{3}
%		\item $\ftnuvm(x) = \ftnuvp(x),~ \forall x \notin \BinfM$. 
%		\item $| \ftnuvm(x) - \ftnuvp(x) | \leq \frac{ L \delta_M}{2 \cdot 3^{M-1}},~ \forall x \in \BinfM$. 
%		\item $\lone{\grduvm - \grduvp} \leq \frac{ 2 L }{3^{M-1}}, \forall x \in \Binftmone, \forall \grduvm \in \sgrduvm(x), \forall \grduvp \in \sgrduvp(x)$.
%	\end{enumerate}
%	Finally, we have the following: 
%	\begin{enumerate}
%		\setcounter{enumi}{6}
%		\item $\inf_{x\in \domain} [\ftnuvm(x) + \ftnuvp(x)] - \inf_{x\in \domain} \ftnuvm(x) - \inf_{x\in \domain} \ftnuvp(x) = \frac{ L \delta_M}{3^M}$.
%	\end{enumerate}
%\end{lemma}
%The proof of Lemma~\ref{lem:ftnconstrLipcvx} is deferred to Appendix~\ref{sec:proof-ftnconstrLipcvx}.
%
%We also want to check whether the functions $\ftnuv(x)$ we are constructing are indeed in our function class of interest:
%$L$-Lipschitz convex functions. The following lemma addresses this points, whose simple proof is deferred to Appendix~\ref{sec:proof-ftnconstrLipcvx2}.
%\begin{lemma}
%	\label{lem:ftnconstrLipcvx2}
%	The functions constructed with Algorithm~\ref{alg:ftnconstrLipsconv} are $L$-Lipschitz and convex.
%\end{lemma}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%NEW SMOOTH STRONGLY CONVEX%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\subsection{Smooth strongly convex}
%\label{sec:constsscvx}
%The agenda of construction for smooth strongly convex functions is the same,
%but the technicality involved in the construction is significantly more complicated.
%In this section, we first describe how to take a ``smooth maximum'' of two different quadratic functions.
%After that, as done in the Lipschitz convex case, we describe 
%how to recursively construct smooth strongly convex functions such that
%similar parameter values result in the same function values outside some region
%while having different values in the same region.
%
%\subsubsection{Smooth interpolation of two quadratic functions: a 1D example}
%Before we describe all the details, we start with an easy 
%1D example that illustrates our key approach.
%In Lipschitz convex case, the goal of getting two functions that have the same values 
%outside some set while having different values in that set was simply achieved by
%taking $\max$ operations. 
%We want to do the same for smooth strongly convex functions, 
%but the difficulty is that the maximum of two functions $f$ and $g$ is not smooth on the intersection set $\intset{f}{g}$.
%To remedy this problem, we can \emph{interpolate} or \emph{smooth} the functions near the intersection set using \emph{suprema of infinitely many hyperplanes.}
%
%To illustrate the idea, let us start with a simple 1D example; 
%consider taking the maximum of 
%two quadratic functions $f_1(x) = 2x^2$ and $f_2(x) = (x-1)^2 + 14$ defined on $[0, \infty)$. 
%Their intersection set is $\intset{f_1}{f_2}= \{ 3 \}$, 
%which contains the only non-smooth point of $\max\{f_1, f_2\}$.
%Note that $\max\{f_1, f_2\} = f_1$ for $x \geq 3$ and $\max\{f_1, f_2\} = f_2$ for $x \leq 3$.
%Smoothing is done by linearly interpolating the gradient in the vicinity 
%of non-smooth point, for example $x \in [2,4]$, 
%while leaving the function values outside $[2,4]$ the same.
%We define constants $\dot g_-, \dot g_0, \dot g_+$:
%\begin{equation*}
%	\dot g_- \defeq \dot f_2(2) = 2, ~
%	\dot g_0 \defeq \frac{\dot f_2(3)+\dot f_1(3)}{2} = \frac{4+12}{2} = 8, ~
%	\dot g_+ \defeq \dot f_1(4) = 16,
%\end{equation*}
%and then define the linearly interpolated gradients
%\begin{align*}
%	\dot h_-(x) &\defeq \dot g_- + (\dot g_0 - \dot g_-) (x-2) = 6x-10 & \text{ for } x \in [2,3],\\
%	\dot h_+(x) &\defeq \dot g_+ - (\dot g_0 - \dot g_+) (x-4) = 8x-16 & \text{ for } x \in [3,4].
%\end{align*}
%Since the gradient in $[2,4]$ changed, also calculate interpolated function value accordingly:
%\begin{align*}
%	h_-(x) & \defeq f_2(2) + \int_2^x \dot h_-(t) dt = 3x^2-10x+23 & \text{ for } x \in [2,3],\\
%	h_+(x)& \defeq f_1(4) - \int_x^4 \dot h_+(t) dt = 4x^2-16x+32 & \text{ for } x \in [3,4].
%\end{align*}
%Note that $f_2(2) = h_-(2) = 15$, $h_-(3) = h_+(3) = 20$, and $h_+(4) = f_1(4) = 32$,
%so the functions $f_2$, $h_-$, $h_+$, $f_1$ can be ``connected'' to make a continuous function.
%Lastly define an infinite number of affine functions using previously calculated $\dot h_-$, $\dot h_+$, $h_-$ and $h_+$:
%\begin{align*}
%	f_-^\rho(x) \defeq \dot h_-(\rho)(x-\rho)+h_-(\rho) & \text{ for } \rho \in [2,3],\\
%	f_+^\rho(x) \defeq \dot h_+(\rho)(x-\rho)+h_+(\rho) & \text{ for } \rho \in [3,4].
%\end{align*}
%Here, we are defining one affine function $f_-^\rho(x)$ for \emph{each} value of $\rho \in [2,3]$. Same applies to $f_+^\rho$ for $\rho \in [3,4]$.
%
%Now, we can define the interpolated function, which is the supremum of the original functions
%$f_1$ and $f_2$, and affine functions $f_-^\rho$ and $f_+^\rho$:
%\begin{equation}
%\label{eq:ftn1d}
%f(x) = \max \left \{ 
%f_1(x), f_2(x), 
%\sup_{\rho \in [2,3]} f_-^\rho(x),
%\sup_{\rho \in [3,4]} f_+^\rho(x) 
%\right \}.
%\end{equation}
%We can prove that this $f(x)$, defined as the maximum of many functions,
%is actually a smooth interpolation of maximum of $f_1$ and $f_2$. We state this in 
%the following lemma, although the proof is rather tedious case analysis,
%which we defer to Appendix~\ref{sec:proof-ssftn1d}.
%\begin{lemma}
%	\label{lem:ssftn1d}
%	The function $f(x)$ defined in Eq~\eqref{eq:ftn1d} satisfies the following:
%	\begin{equation*}
%		f(x) = 
%		\begin{cases}
%			f_2(x) & \text{if } 0 \leq x \leq 2\\
%			h_-(x) & \text{if } 2 \leq x \leq 3\\
%			h_+(x) & \text{if } 3 \leq x \leq 4\\
%			f_1(x) & \text{if } x \geq 4,
%		\end{cases}
%		\text{ and }
%		\dot f(x) = 
%		\begin{cases}
%			\dot f_2(x) & \text{if } 0 \leq x \leq 2\\
%			\dot h_-(x) & \text{if } 2 \leq x \leq 3\\
%			\dot h_+(x) & \text{if } 3 \leq x \leq 4\\
%			\dot f_1(x) & \text{if } x \geq 4.
%		\end{cases}
%	\end{equation*}
%	Also, $f(x)$ is a $8$-smooth and $2$-strongly convex function.
%\end{lemma}
%
%By Lemma~\ref{lem:ssftn1d}, we saw that the maximum of 
%two quadratic functions can be smoothly interpolated by using infinite number of hyperplanes. 
%Also note that this function is $8$-smooth and $2$-strongly convex,
%whereas $f_1$ and $f_2$ are in the class of $4$-smooth and $2$-strongly convex functions; the interpolation cause the ``increase'' of the smoothness constant,
%which we will observe in the multi-dimensional example as well.
%
%\subsubsection{Smooth interpolation of two quadratic functions: multi-dimension}
%\paragraph{Choosing the right parameters.}
%Now, we extend the domain to $d$-dimension, and consider taking the maximum of 
%two quadratic functions $\fxone(x)$ and $\fxtwo(x)$, each minimized at $x_1$ and $x_2$, respectively, where $x_2 \in \balltwo{x_1}{\eta\delta}$:
%\begin{equation*}
%	\fxone(x) \defeq \ltwosqr{x-x_1 } ~~\text{and}~~
%	\fxtwo(x) \defeq \alpha \ltwosqr{x-x_2} + \beta \delta^2,
%\end{equation*}
%where the parameters satisfy $0<\eta<1$, $\delta > 0$, $0<\alpha<1$, and $0<\beta<1$.
%
%By solving $\fxone(x) = \fxtwo(x)$, we can see that their intersection set $\intset{\fxone}{\fxtwo} \defeq \{ x \in \R^d \mid \fxone(x) = \fxtwo(x)\}$ is a sphere:
%\begin{equation*}
%	\intset{\fxone}{\fxtwo} = \{ x \mid \ltwosqr {x-c} = r^2 \},
%\end{equation*}
%where 
%\begin{align}
%	c &= \frac{1}{1-\alpha} x_1 - \frac{\alpha}{1-\alpha} x_2 = x_1 - \frac{\alpha}{1-\alpha} (x_2-x_1), \label{eq:defcent}\\
%	r &= \sqrt{\frac{\alpha}{(1-\alpha)^2} \ltwosqr{x_1-x_2} + \frac{\beta\delta^2}{1-\alpha}}. \label{eq:defradi}
%	%	=\sqrt{\frac{(1+\eta)^2 \delta^2}{4}-\frac{\alpha}{(1-\alpha)^2} (\eta^2 \delta^2-\ltwosqr{x_1-x_2})}.
%\end{align}
%Since $\frac{\alpha}{(1-\alpha)^2} \ltwosqr{x_1-x_2} + \frac{\beta\delta^2}{1-\alpha} > 0$ by assumptions on parameters, this sphere exists.
%
%As seen in the 1D example,
%we need some ``margin'' for interpolation near non-smooth points.
%In the 1D example the ``margin'' or what we call ``interpolation set'' was the interval $[2,4]$, on which we alter the function values to do smooth interpolation.
%So, after taking the maximum between $\fxone$ and $\fxtwo$, 
%we will smooth the non-smooth points in the intersection set 
%$\intset{\fxone}{\fxtwo}$ by linearly interpolating the gradients
%on a set $\itpset$ called ``interpolation set'':
%\begin{equation*}
%	\itpset \defeq \left \{ x \mid (1-\theta)r \leq \ltwo{x-c} \leq (1+\theta)r \right \},
%\end{equation*}
%where $0<\theta<1$ will be chosen shortly.
%
%We now state a lemma that chooses the parameters in the ``right'' way 
%that makes our construction of smooth maximum easier.
%\begin{lemma}
%	\label{lem:ssftnmdparam}
%	Recall the conditions $0<\eta<1$, $\delta > 0$, $0<\alpha<1$, $0<\beta<1$, and $0<\theta<1$ on parameters of $\fxone$, $\fxtwo$, and $\itpset$.
%	Choose parameters that satisfy
%	\begin{align}
%		&\eta+\alpha+\alpha\eta < 1,\label{eq:constraints}\\
%		&\beta = \frac{(1-\alpha)(1+\eta)^2}{4} - \frac{\alpha \eta^2}{1-\alpha}, \label{eqn:smthconstbetaval}\\
%		&\theta = \frac{1-\eta-\alpha-\alpha\eta}{1+\eta-\alpha-\alpha\eta}. \label{eqn:smthconstthetaval}
%	\end{align}
%	Then, the following statements hold: 
%	\begin{align}
%		&\itpset \subset \cl(\balltwo{x_1}{\eta\delta}^c \cap \balltwo{x_1}{\delta}) ~\text{ for any } x_2 \in \balltwo{x_1}{\eta\delta}, \label{eqn:itpsubset}%\\
%		%&\fxone(x) \leq \fxtwo(x) ~\text{ for any } x \in \balltwo{x_1}{\eta\delta},\label{eqn:fxtwobig}\\
%		%&\fxone(x) \geq \fxtwo(x) ~\text{ for any } x \notin \balltwo{x_1}{\delta}.\label{eqn:fxonebig}
%	\end{align}
%\end{lemma}
%The proof of Lemma~\ref{lem:ssftnmdparam} is provided in Appendix~\ref{sec:proof-ssftnmdparam}.
%With the parameters satisfying Eqs~\eqref{eq:constraints}--\eqref{eqn:smthconstthetaval}, we can ensure that
%the interpolation set is a subset of $\cl(\balltwo{x_1}{\eta\delta}^c \cap \balltwo{x_1}{\delta})$,
%so any point $x \notin \cl(\balltwo{x_1}{\eta\delta}^c \cap \balltwo{x_1}{\delta})$ will not be affected by the interpolation.
%
%\paragraph{Interpolating the maximum of two functions.}
%
%\begin{algorithm}[t]
%	\caption{Algorithm $\algmaxsmth(\fxone, \fxtwo, \alpha, \eta, \delta)$.}  
%	\label{alg:algmaxsmth}
%	\begin{algorithmic}[1]  % [1] is line numbering
%		\STATEx Assume $0<\alpha<1, 0<\eta<1, \eta+\alpha+\alpha\eta<1, \delta>0$. Let $\beta \defeq \frac{(1-\alpha)(1+\eta)^2}{4} - \frac{\alpha \eta^2}{1-\alpha}$.
%		\STATEx Assume $\fxone(x) = s\ltwosqr{x-x_1 } + t$, $\fxtwo(x) = s\alpha \ltwosqr{x-x_2} + s\beta \delta^2 + t$, $s>0$, $x_2 \in \balltwo{x_1}{\eta\delta}$
%		\STATE $\theta \defeq \frac{1-\eta-\alpha-\alpha\eta}{1+\eta-\alpha-\alpha\eta},~c \defeq x_1 - \frac{\alpha}{1-\alpha} (x_2-x_1),~ r \defeq \sqrt{\frac{\alpha}{(1-\alpha)^2} \ltwosqr{x_1-x_2} + \frac{\beta\delta^2}{1-\alpha}}$.
%		\STATE For all $\rho \in [(1-\theta)r,r]$ and all unit vectors $\ltwo w = 1$, 
%		\begin{align*}
%			\dot h_-(\rho,w) 
%			\defeq& \frac{2\alpha}{1-\alpha}(x_1-x_2) 
%			-	\frac{(1-\alpha)(1-\theta)r}{\theta} w 
%			+ \left ( \frac{1-\alpha}{\theta}+2\alpha \right ) \rho w,\\
%			h_-(\rho,w) 
%			\defeq &\frac{\alpha}{(1-\alpha)^2} \ltwosqr{x_1-x_2}
%			+ \beta \delta^2 + \frac{(1-\alpha)(1-\theta)^2 r^2}{2\theta}\\
%			&+ \left ( \frac{2\alpha}{1-\alpha} \< x_1-x_2, w \> - \frac{(1-\alpha)(1-\theta)r}{\theta} \right ) \rho
%			+ \left ( \frac{1-\alpha}{2\theta} + \alpha \right ) \rho^2,\\
%			\fhypm (x) 
%			\defeq& s \< \dot h_-(\rho,w), x-(c+\rho w) \> + s h_-(\rho,w) + t.
%		\end{align*}
%		\STATE For all $\rho \in [r, (1+\theta)r]$ and all unit vectors $\ltwo w = 1$,
%		\begin{align*}
%			\dot h_+(\rho,w) 
%			\defeq& \frac{2\alpha}{1-\alpha}(x_1-x_2) 
%			- \frac{(1-\alpha)(1+\theta)r}{\theta} w 
%			+ \left ( \frac{1-\alpha}{\theta}+2 \right ) \rho w,\\
%			h_+(\rho,w) 
%			\defeq &\frac{\alpha^2}{(1-\alpha)^2} \ltwosqr{x_1-x_2}
%			+ \frac{(1-\alpha)(1+\theta)^2 r^2}{2\theta}\\
%			&+ \left ( \frac{2\alpha}{1-\alpha} \< x_1-x_2, w \> - \frac{(1-\alpha)(1+\theta)r}{\theta} \right ) \rho
%			+ \left ( \frac{1-\alpha}{2\theta} + 1 \right ) \rho^2,\\
%			\fhypp (x) 
%			\defeq & s \< \dot h_+(\rho,w), x-(c+\rho w) \> + s h_+(\rho,w) + t.
%		\end{align*}
%		\STATE Return $f(x) \defeq \max \left \{ 
%		\fxone(x), \fxtwo(x), 
%		\sup_{\rho \in [(1-t)r,r], \ltwo w = 1} \fhypm(x),
%		\sup_{\rho \in [r,(1+t)r], \ltwo w = 1} \fhypp(x)
%		\right \}.$
%	\end{algorithmic}
%\end{algorithm}
%
%We now describe how the smooth interpolation of $\max\{\fxone, \fxtwo\}$ is done in $\itpset$. 
%Notice that $\itpset$ can be expressed in a ``polar'' form:
%\begin{equation*}
%	\itpset 
%	\defeq \left \{ x \mid (1-\theta)r \leq \ltwo{x-c} \leq (1+\theta)r \right \}
%	= \left \{ 
%	c + \rho w 
%	\mid
%	(1-\theta) r \leq \rho \leq (1+\theta) r, 
%	\ltwo{w} = 1
%	\right \},
%\end{equation*}
%and we will specify the new interpolated gradient and function values for all $(1-\theta) r \leq \rho \leq (1+\theta) r$ and $\ltwo{w} = 1$.
%For each fixed direction $w$,
%interpolated gradients $\dot h_-(\rho, w)$ and $\dot h_+(\rho, w)$ are obtained by
%linearly interpolating the gradients along $w$.
%After that, we obtain interpolated function values $h_-(\rho, w)$ and $h_+(\rho, w)$ 
%by integrating directional derivatives along $w$, starting from $\fxtwo(c+(1-\theta)r)$ and $\fxone(c+(1+\theta)r)$, respectively.
%For details of the construction, please refer to Appendix~\ref{sec:proof-algmaxsmth}.
%In Algorithm~\ref{alg:algmaxsmth}, we summarize the outcomes of the construction, 
%and present the process of getting the ``smooth maximum'' of two quadratic functions,
%for a slightly more general case where $\fxone(x)$ and $\fxtwo(x)$ are defined in the form
%\begin{equation*}
%	\fxone(x) \defeq s\ltwosqr{x-x_1 } + t ~~\text{and}~~
%	\fxtwo(x) \defeq s\alpha \ltwosqr{x-x_2} + s\beta \delta^2 + t,
%\end{equation*}
%where $s > 0$ and $t \in \R$. That is, $s$ and $t$ are scale and translation in the range space.
%
%\paragraph{Correctness of smooth maximum.}
%We now prove that the output of Algorithm~\ref{alg:algmaxsmth} is
%indeed the smooth interpolation of $\max \{\fxone, \fxtwo\}$,
%and the interpolated function values attain the maximum/suprimum as originally intended.
%We prove this by the following lemma, whose technical proof is deferred to Appendix~\ref{sec:proof-smthconstvalues}.
%\begin{lemma}
%	\label{lem:smthconstvalues}
%	Let 
%	\begin{equation*}
%		\fxone(x) \defeq \ltwosqr{x-x_1 } ~~\text{and}~~
%		\fxtwo(x) \defeq \alpha \ltwosqr{x-x_2} + \beta \delta^2,
%	\end{equation*}
%	where parameters satisfy $0<\alpha<1$, $0<\eta<1$, $\eta+\alpha+\alpha\eta<1$, $\delta>0$, $\beta = \frac{(1-\alpha)(1+\eta)^2}{4} - \frac{\alpha \eta^2}{1-\alpha}$, and $x_2 \in \balltwo{x_1}{\eta\delta}$.
%	Then, the output $f(x)$ of $\algmaxsmth(\fxone, \fxtwo, \alpha, \eta, \delta)$
%	satisfies, for all $\rho \geq 0$ and $\ltwo w = 1$,
%	\begin{align*}
%		f(c+\rho w) &=
%		\begin{cases}
%			\fxtwo(c+\rho w) 	& \text{if } \rho \in [0, (1-\theta)r]\\
%			h_-(\rho, w) 		& \text{if } \rho \in [(1-\theta)r,r]\\
%			h_+(\rho, w) 		& \text{if } \rho \in [r,(1+\theta) r]\\
%			\fxone(c+\rho w) 	& \text{if } \rho \in [(1+\theta) r, \infty)
%		\end{cases}\\
%		\nabla f(c+\rho w) &=
%		\begin{cases}
%			\nabla \fxtwo(c+\rho w) 	& \text{if } \rho \in [0, (1-\theta)r]\\
%			\dot h_-(\rho, w) 		& \text{if } \rho \in [(1-\theta)r,r]\\
%			\dot h_+(\rho, w) 		& \text{if } \rho \in [r,(1+\theta) r]\\
%			\nabla \fxone(c+\rho w) 	& \text{if } \rho \in [(1+\theta) r, \infty).
%		\end{cases}
%	\end{align*}
%\end{lemma}
%Given Lemma~\ref{lem:smthconstvalues}, we showed that the interpolated function
%$f(x)$ has function values and gradients as specified in the lemma,
%which agrees with our intended construction in Appendix~\ref{sec:proof-algmaxsmth}.
%Note that positive scaling and translation does not hurt the correctness of interpolation.
%
%Now, we prove the smoothness and strong convexity constants of $f(x)$.
%\begin{lemma}
%	\label{lem:smthconstparams}
%	Under the same setting as Lemma~\ref{lem:smthconstvalues},
%	the output $f(x)$ of $\algmaxsmth(\fxone, \fxtwo, \alpha, \eta, \delta)$ is $\left (2+\frac{1-\alpha}{\theta}\right )$-smooth and $2\alpha$-strongly convex.
%\end{lemma}
%The proof is in Appendix~\ref{sec:proof-smthconstparams}. 
%Note that, as in the 1D case, the smoothness constant increased after interpolation while the strong convexity constant stayed the same.
%
%We end this subsection with a lemma on the range of $f(x)$, which will prove useful for multistage construction as well as the reader's comprehension of the interpolation.
%The proof is deferred to Appendix~\ref{sec:proof-smthconstrange}.
%\begin{lemma}
%	\label{lem:smthconstrange}
%	Let 
%	\begin{equation*}
%		\fxone(x) \defeq s\ltwosqr{x-x_1 } + t~~\text{and}~~
%		\fxtwo(x) \defeq s\alpha \ltwosqr{x-x_2} + s \beta \delta^2 + t,
%	\end{equation*}
%	where parameters satisfy $0<\alpha<1$, $0<\eta<1$, $\eta+\alpha+\alpha\eta<1$, $\delta>0$, $\beta = \frac{(1-\alpha)(1+\eta)^2}{4} - \frac{\alpha \eta^2}{1-\alpha}$, $s>0$ and $x_2 \in \balltwo{x_1}{\eta\delta}$.
%	Then, the output $f(x)$ of $\algmaxsmth(\fxone, \fxtwo, \alpha, \eta, \delta)$ satisfies
%	\begin{enumerate}
%		\item $f(x) = \fxone(x) \quad \text{for all } x\in \cl(\balltwo{x_1}{\delta}^c)$.
%		\item $f(x) = \fxtwo(x) \quad \text{for all } x\in \balltwo{x_1}{\eta\delta}$.
%		\item $s \beta \delta^2 + t\leq f(x) \leq s \delta^2 + t	\quad \text{for all } x \in \balltwo{x_1}{\delta}$.
%		\item $\ltwo{\nabla f(x)} \leq 2 s\delta \quad \text{for all } x \in \balltwo{x_1}{\delta}$.
%		\item $\nabla f(x) = \nabla \fxone(x) \quad \text{for all } x\in \cl(\balltwo{x_1}{\delta}^c)$.
%		\item $\nabla f(x) = \nabla \fxtwo(x) \quad \text{for all } x\in \balltwo{x_1}{\eta\delta}$.
%	\end{enumerate}
%\end{lemma}
%
%\subsubsection{Multi-stage recursive construction}
%\label{sec:multiconstsscvx}
%Now let us consider applying $\algmaxsmth(\cdot)$ many times in a recursive way, 
%as done in the Lipschitz convex case; we will iteratively apply $\algmaxsmth$ 
%while zooming into narrower portions of the domain.
%The outline of the construction is the same, except a bit of difference in details.
%
%\paragraph{Constructing recursive maximal packings.}
%Assume that values of $0<\alpha<1$ and $0<\eta<1$ satisfying $\eta + \alpha + \alpha\eta < 1$
%are given. Consider $\delta_1, \delta_2, \dots, \delta_M$, which are real valued quantities that decays to zero as 
%the number of samples $n$ goes to infinity.
%The values will be fixed by lower bound proof later,
%but for now $\delta_t = o(\delta_{t-1})$ suffices to construct the functions.
%
%Given $\delta_t$'s, we recursively define the packings, this time with respect to $\ell_2$ norm:
%\begin{description}
%	\item[$1.$] Let $\Uone$ to be any maximal $2\delta_1$-packing of $[\delta_1, 1-\delta_1]^d$ with respect to $\ell_2$ norm.
%	\item[$2.$] For any $u_1 \in \Uone$, let $\Utemp{1}{2}$ to be any maximal $2\delta_2$-packing of $\balltwo{u_1}{\eta \delta_1 -\delta_2}$ w.r.t.\ $\ell_2$ norm.
%	\item[$3.$] For any $u_2 \in \Utemp{1}{2}$, let $\Utemp{2}{3}$ to be any maximal $2\delta_3$-packing of $\balltwo{u_2}{\eta \delta_2-\delta_3}$ w.r.t.\ $\ell_2$ norm.\\ $\cdots$
%	\item[$M.$] For any $u_{M-1} \in \Utemp{M-2}{M-1}$, let $\Utemp{M-1}{M}$ to be any maximal $2\delta_M$-packing of $\balltwo{u_{M-1}}{\eta\delta_{M-1}-\delta_M}$ w.r.t.\ $\ell_2$ norm.
%	\item[$M+1.$] For any $u_{M} \in \Utemp{M-1}{M}$, let $\V = \{-1, +1\}$.
%\end{description}
%This construction is the same as Lipschitz convex case except that the norm is $\ell_2$ instead of $\ell_\infty$. 
%%In the first stage, we construct a set of points $\Uone$ so that any $u_1 \in \Uone$ satisfies $\balltwo{u_1}{\delta_1} \subset \domain$.
%%Starting from second stage, we construct maximal packings for \emph{all} points in the previous stage;
%%for example, $\Utemp{1}{2}$ is defined for all $u_1 \in \Uone$ and $\Utemp{1}{2}$ depends on which $u_1$ we choose.
%%We continue in this recursive way until we define $\Utemp{M-1}{M}$. After that stage, we define the final set $\V = \{ \pm 1\}$.
%Note that by construction we have $\balltwo{u_t}{\delta_t} \subset \balltwo{u_{t-1}}{\eta \delta_{t-1}}$ for $t \in 2:M$.
%%Also, for any $t \in 2:M$, $\balltwo{u_{t}}{\eta\delta_{t}} \cap \ballinf{\tilde u_{t}}{\eta\delta_{t}} = \emptyset$,
%%for different $u_t, \tilde u_t \in \Utemp{t-1}{t}$.
%%This means that a point $u_t \in \Utemp{t-1}{t}$ uniquely maps to all their ``ancestors'' $u_{t-1}, u_{t-2}, \dots, u_1$,
%%because the neighbors of their ancestors never overlap with the other ancestors at the corresponding level.
%
%\paragraph{Defining a function for each set of parameters.}
%Given the maximal packings, in the same way as Lipschitz convex case,
%we can recursively choose points $u_1, \dots, u_M$ and $v$ as parameters. Algorithm~\ref{alg:ftnconstrSmthstrcvx}
%constructs a function $\ftnuv(x)$ that corresponds to the specific choice of $u_1, \dots, u_M$ and $v$.
%\begin{algorithm}[t]
%	\caption{Construction of smooth strongly convex $\ftnuv$.}  \label{alg:ftnconstrSmthstrcvx}
%	\begin{algorithmic}[1]  % [1] is line numbering
%		\STATEx Given parameters $u_1, u_2, \dots, u_M, v$, size $\delta_1, \dots, \delta_M$, 
%		\STATEx $\alpha, \eta \in (0,1)$ satisfying $\eta + \alpha + \alpha\eta < 1$, and $C>0$,
%		\STATE Let $\beta \defeq \frac{(1-\alpha)(1+\eta)^2}{4} - \frac{\alpha \eta^2}{1-\alpha}$
%		\STATE Start with $\ftnuone (x) = \ftninuone(x) \defeq C\ltwosqr{x-u_1}$.
%		\FOR{ $t = 2$ to $M$ }
%		\STATE $\ftninut (x) \defeq C\alpha^{t-1} \ltwosqr{x-u_{t}} + C \beta \sum_{m=1}^{t-1} \alpha^{m-1}\delta_m^2$
%		\STATE $\gut(x) \defeq \algmaxsmth(\ftninutmone, \ftninut, \alpha, \eta, \delta_{t-1})$.
%		\STATE $\ftnut (x) \defeq \max \{ \ftnutmone (x), \gut(x) \}$.
%		\ENDFOR
%		\STATE $\ftninuv (x) \defeq C\alpha^M \ltwosqr{x-u_M-v \eta \delta_M \unitvec} + C \beta \sum_{m=1}^{M}  \alpha^{m-1} \delta_m^2$.
%		\STATE $\guv(x) \defeq \algmaxsmth(\ftninuM, \ftninuv,\alpha,\eta,\delta_M)$.
%		\STATE Return $\ftnuv(x) \defeq \max \{ \ftnuM(x), \guv(x) \}$.
%	\end{algorithmic}
%\end{algorithm}
%
%Algorithm~\ref{alg:ftnconstrSmthstrcvx} defines a series of quadratic functions $\ftninut(x)$ and repeatedly takes smooth maximum with previous ones to get the final function.
%With Algorithm~\ref{alg:ftnconstrSmthstrcvx}, we can achieve the same property that 
%if the two functions share the same parameter values from $u_1$ up to $u_{t-1}$ ($t \in 2:M$),
%but their parameter deviated from each other after $t$, say $u_t, \dots, u_M, v$ and $\tilde u_t, \dots, \tilde u_M, \tilde v$, 
%then the two functions $\ftnuv(x)$ and $\ftnuvtildet(x)$ are completely identical far away from $u_{t-1}$,
%and differ only slightly at points near $u_{t-1}$.
%The difference is that the output function $\ftnuv(x)$ is now smooth and strongly convex,
%which we will prove shortly.
%
%We illustrate this property more concretely in the following lemma.
%\begin{lemma}
%	\label{lem:ftnconstrsscvx}
%	The functions constructed with Algorithm~\ref{alg:ftnconstrSmthstrcvx} have the following property: whenever the two functions have the same parameters from $u_1$ up to $u_{t-1}$ ($t \in 2:M$), they satisfy, for any values of remaining parameters $u_{t:M}$, $v$, $\tilde u_{t:M}$, and $\tilde v$,
%	\begin{enumerate}
%		\item $\ftnuv(x) = \ftnuvtildet(x),~\forall x \notin \Btwotmone$.
%		\item $| \ftnuv(x) - \ftnuvtildet(x) | \leq C(1-\beta) \alpha^{t-2} \delta_{t-1}^2,~\forall x \in \Btwotmone$.
%		\item $\ltwo{\nabla \ftnuv(x) - \nabla \ftnuvtildet(x)} \leq 4C\alpha^{t-2} \delta_{t-1},~\forall x \in \Btwotmone$.
%	\end{enumerate}	
%	Similarly, when $u_{1:M}$ are all the same and only $v, \tilde v \in \V$ are different, 
%	\begin{enumerate}
%		\setcounter{enumi}{3}
%		\item $\ftnuvm(x) = \ftnuvp(x),~ \forall x \notin \BtwoM$.
%		\item $| \ftnuvm(x) - \ftnuvp(x) | \leq C(1-\beta) \alpha^{M-1} \delta_M^2,~ \forall x \in \BtwoM$.
%		\item $\ltwo{\nabla \ftnuvm(x) - \nabla \ftnuvp(x)} \leq 4C\alpha^{M-1} \delta_{M},~ \forall x \in \BtwoM$.
%	\end{enumerate}
%	Finally, we have the following: 
%	\begin{enumerate}
%		\setcounter{enumi}{6}
%		\item $\inf_{x\in \domain} [\ftnuvm(x) + \ftnuvp(x)] - \inf_{x\in \domain} \ftnuvm(x) - \inf_{x\in \domain} \ftnuvp(x) = 2C\alpha^M \eta^2\delta_M^2$.
%	\end{enumerate}
%\end{lemma}
%The proof of Lemma~\ref{lem:ftnconstrsscvx} is deferred to Appendix~\ref{sec:proof-ftnconstrsscvx}.
%
%We also want to check whether the functions $\ftnuv(x)$ we are constructing are indeed smooth and strongly convex. Especially, one might wonder if the $\max$ operations at Lines~6 and 10 can hurt the smoothness. The following lemma addresses this points, whose proof is deferred to Appendix~\ref{sec:proof-ftnconstrsscvx2}.
%\begin{lemma}
%	\label{lem:ftnconstrsscvx2}
%	The functions constructed with Algorithm~\ref{alg:ftnconstrSmthstrcvx} are $\left (C \left ( 2 + \frac{1-\alpha}{\theta} \right )\right )$-smooth and $2C\alpha^M$-strongly convex.
%	Moreover, when $H/5 \geq \lambda$, with the parameter choice 
%	\begin{equation}
%	\alpha = \left (\half \right)^{\frac{1}{M}},~\eta = \frac{1-\alpha}{2},~C = \frac{H}{5}
%	\label{eqn:smthconstdesignparams}
%	\end{equation}
%	the constructed functions are $H$-smooth and $\lambda$-strongly convex, and they satisfy 
%	\begin{equation}
%	\inf_{x\in \domain} [\ftnuvm(x) + \ftnuvp(x)] - \inf_{x\in \domain} \ftnuvm(x) - \inf_{x\in \domain} \ftnuvp(x) 
%	= \frac{H}{20}\left(1-\left(\half\right)^{\frac{1}{M}}\right)^2 \delta_M^2.
%	\end{equation}
%	For $H/5 < \lambda < H$, there also exists choice of parameters that returns $H$-smooth and $\lambda$-strongly convex functions, although more complicated.
%\end{lemma}
%
%\section{Proof of Theorem~\ref{thm:lbmain}}
%\label{sec:proof-lbmain}
%\subsection{Construction of functions revisited}
%Suppose we are given $\delta_1, \dots, \delta_M$ which satisfy $\delta_1 = o(1)$ and $\delta_t = o(\delta_{t-1})$ as $n$ grows to infinity.
%We described in Sections~\ref{sec:multiconstlipcvx} and \ref{sec:multiconstsscvx}
%how to recursively define maximal packings. 
%To put two cases into a unified framework, let us rewrite the procedure as follows:
%Given $\delta_t$'s, we recursively define the packings as follows:
%\begin{description}
%	\item[$1.$] Let $\Uone$ to be any maximal $2\delta_1$-packing of $[\delta_1, 1-\delta_1]^d$ with respect to $\ell_p$ norm.
%	\item[$2.$] For any $u_1 \in \Uone$, let $\Utwo$ to be any maximal $2\delta_2$-packing of $\ballp{u_1}{\eta\delta_1-\delta_2}$ w.r.t.\ $\ell_p$ norm.
%	\item[$3.$] For any $u_2 \in \Utwo$, let $\Utemp{2}{3}$ to be any maximal $2\delta_3$-packing of $\ballp{u_2}{\eta\delta_2-\delta_3}$ w.r.t.\ $\ell_p$ norm.\\ $\cdots$
%	\item[$M.$] For any $u_{M-1} \in \Utemp{M-2}{M-1}$, let $\UM$ to be any maximal $2\delta_M$-packing of $\ballp{u_{M-1}}{\eta\delta_{M-1}-\delta_M}$ w.r.t.\ $\ell_p$ norm.
%	\item[$M+1.$] For any $u_{M} \in \UM$, let $\V = \{-1, +1\}$.
%\end{description}
%We can readily check that $p = \infty$, $\eta = \half$ for Lipschitz convex functions in Section~\ref{sec:multiconstlipcvx}, and $p = 2$ for smooth strongly convex functions in Section~\ref{sec:multiconstsscvx}.
%
%By a volumetric argument, we can show that the packing number is bounded below by a number depending on $\delta_t$'s, which is presented in the following lemma.
%\begin{lemma}
%\label{lem:packingno}
%For large enough $n$, the cardinality of maximal packings satisfy
%\begin{equation*}
%	| \Uone | \geq \left(\frac{1}{8\delta_1} \right)^d, \text{ and }
%	| \Ut | \geq \left( \frac{\eta \delta_{t-1}}{4\delta_t} \right )^d \text{ for } t \in 2:M,
%\end{equation*}
%regardless of the choice $u_{t-1}$.
%\end{lemma}
%The proof of Lemma~\ref{lem:packingno} is provided in Appendix~\ref{sec:proof-packingno}.
%
%In Section~\ref{sec:multiconstlipcvx} and \ref{sec:multiconstsscvx}, we saw that
%we can choose parameters $u_1 \in \Uone, u_2 \in \Utwo, \dots, u_M \in \UM$ and $v \in \V$,
%and then run Algorithms~\ref{alg:ftnconstrLipsconv} and \ref{alg:ftnconstrSmthstrcvx}
%using the parameters $u_{1:M}$ and $v$ to construct functions
%that have similar function values for similar parameter values.
%Lemmas~\ref{lem:ftnconstrLipcvx} and \ref{lem:ftnconstrsscvx} summarize this property, 
%but here we provide a unified version of Lemmas~\ref{lem:ftnconstrLipcvx} and \ref{lem:ftnconstrsscvx}
%that works for both function classes.
%\begin{lemma}
%\label{lem:ftnconstrunified}
%Suppose the function class is either $L$-Lipschitz convex, or $H$-smooth $\lambda$-strongly convex, where $H> \lambda$.
%For $t \in 2:M$, if we have two sets of parameters $(u_1, \dots, u_{t-1}, u_t, \dots, u_M, v)$ and
%$(u_1, \dots, u_{t-1}, \tilde u_t, \dots, \tilde u_M, \tilde v)$, with these parameters 
%we can construct functions $\ftnuv$ and $\ftnuvtildet$ that have the following property:
%\begin{enumerate}
%	\item $\ftnuv(x) = \ftnuvtildet(x),~\forall x \notin \Bptmone$.
%	\item $| \ftnuv(x) - \ftnuvtildet(x) | \leq C(1-\beta) \alpha^{t-2} \delta_{t-1}^\kappa,~\forall x \in \Bptmone$.
%	\item $\ltwo{\nabla \ftnuv(x) - \nabla \ftnuvtildet(x)} \leq 2 \kappa C\alpha^{t-2} \delta_{t-1}^{\kappa-1},~\forall x \in \Bptmone$.
%\end{enumerate}	
%Similarly, when $u_{1:M}$ are all the same and only $v, \tilde v \in \V$ are different, 
%\begin{enumerate}
%	\setcounter{enumi}{3}
%	\item $\ftnuvm(x) = \ftnuvp(x),~ \forall x \notin \BpM$.
%	\item $| \ftnuvm(x) - \ftnuvp(x) | \leq C(1-\beta) \alpha^{M-1} \delta_M^{\kappa},~ \forall x \in \BpM$.
%	\item $\ltwo{\nabla \ftnuvm(x) - \nabla \ftnuvp(x)} \leq 2 \kappa C\alpha^{M-1} \delta_{M}^{\kappa-1},~ \forall x \in \BpM$.
%	\item $\inf_{x\in \domain} [\ftnuvm(x) + \ftnuvp(x)] - \inf_{x\in \domain} \ftnuvm(x) - \inf_{x\in \domain} \ftnuvp(x) = 2C\alpha^M \eta^\kappa \delta_M^\kappa$.
%\end{enumerate}
%Here, whenever the function $\ftnuv$ is non-differentiable at $x$,
%we replace $\nabla \ftnuv(x)$ with any subgradient of $\ftnuv(x)$ at $x$.
%This abuse of notation is to simplify the presentation.
%\end{lemma}
%From Lemma~\ref{lem:ftnconstrunified}, notice that we can get 
%Lemma~\ref{lem:ftnconstrLipcvx} as a special case by substituting
%\begin{equation*}
%(C, \alpha, \eta, \beta, \kappa, p) = \left (L, \frac{1}{3}, \half, \half, 1, \infty \right ),
%\end{equation*}
%and Lemma~\ref{lem:ftnconstrLipcvx2} ensures that the constructed functions $\ftnuv$ are $L$-Lipschitz convex.
%Also, by Lemma~\ref{lem:ftnconstrsscvx} and \ref{lem:ftnconstrsscvx2}, if $H/5 \geq \lambda$ we can observe that with
%\begin{equation*}
%(C, \alpha, \eta, \beta, \kappa, p) = 
%\left ( \frac{H}{5} , \left (\half \right)^{\frac{1}{M}}, \frac{1-\alpha}{2},
%\frac{(1-\alpha)(1+\eta)^2}{4} - \frac{\alpha \eta^2}{1-\alpha}, 2, 2 \right ),
%\end{equation*}
%we can get $H$-smooth and $\lambda$-strongly convex functions $\ftnuv$.
%If $H/5 < \lambda < H$, we can also choose parameters as described in Appendix~\ref{sec:proof-ftnconstrsscvx2}.
%
%The rest of our proof is based solely on the assumption that Lemma~\ref{lem:ftnconstrunified} holds for 
%\emph{some} function class we are interested in, i.e.\ the construction algorithm for $\ftnuv$ and 
%constants $(C, \alpha, \eta, \beta, \kappa, p)$ exist for that function class.
%This means that our analysis that follows can be used \emph{universally} for any other function classes
%once we can construct functions $\ftnuv$ that satisfy Lemma~\ref{lem:ftnconstrunified}.
%
%\subsection{Notation}
%For the rest of the proof, let $\probuv$ and $\exptuv$ denote the probability of an event and expectation of any quantity, respectively, based on the event that the true objective function is $\ftnuv$.
%
%Our proof depends on whether the oracle is zeroth-order or first-order.
%Let $\zeta$ denote the order of the oracle: $\zeta = 0$ whenever we are using zeroth-order oracles, 
%$\zeta = 1$ for first-order. 
%Our proof strategy assumes $\kappa > \zeta$; 
%this means that our proof does not apply to Lipschitz convex functions ($\kappa = 1$) 
%with first-order oracles ($\zeta = 1$).
%
%Using $\zeta$, for example, we can define
%\begin{equation}
%\label{eq:deftildeC}
%\tilde C_\zeta = C(1-\beta)^{1-\zeta}(2\kappa)^\zeta,
%\end{equation}
%with which we can simplify the notations quite a bit. For example, Lemma~\ref{lem:ftnconstrunified}.2 and \ref{lem:ftnconstrunified}.3 can be now written as
%\begin{align*}
%| \ftnuv(x) - \ftnuvtildet(x) | &\leq \tilde C_0 \alpha^{t-2} \delta_{t-1}^\kappa,~\forall x \in \Bptmone\\
%\ltwo{\nabla \ftnuv(x) - \nabla \ftnuvtildet(x)} &\leq \tilde C_1 \alpha^{t-2} \delta_{t-1}^{\kappa-1},~\forall x \in \Bptmone
%\end{align*}
%Next, define the ``exponent'' constants
%\begin{equation}
%\label{eq:defgammat}
%	\gamma_t \defeq \frac{1}{d+2(\kappa-\zeta)} \sum_{m=0}^{t-1} \left(\frac{d}{d+2(\kappa-\zeta)}\right)^m
%	\text{ for } t \in 0:M.
%\end{equation}
%Note that because $\kappa-\zeta > 0$, this is expressed as
%\begin{equation}
%\label{eq:defgammat2}
%	\gamma_t =
%	\frac{1}{2(\kappa-\zeta)} \left[1 - \left( \frac{d}{d+2(\kappa-\zeta)} \right)^t\right].
%\end{equation}
%Using this exponent constants, define $\delta_1, \dots, \delta_M$ as
%\begin{equation}
%\label{eq:defdeltat}
%\begin{array}{ll}
%\delta_t \defeq 
%D_t n^{ - \gamma_t } \exp \left( -2 \sqrt{2} \gamma_{t-1} \sqrt{\log n} \right) & \text{ for } t = 1:M-1,\\
%\delta_M \defeq 
%D_M n^{ -\gamma_M } \exp \left( -2 \sqrt{2} \gamma_{M-1} \sqrt{\log n} \right) \log^{-\nu/\kappa} n,
%\end{array}
%\end{equation}
%where $\nu > 0$ is any arbitrarily small number.
%The leading constants are defined recursively as
%\begin{equation}
%\label{eq:defDt}
%\begin{array}{ll}
%D_1 \defeq \left ( \frac{\sigma^2}{2 \cdot 8^{d} \tilde C_\zeta^2 } \right )^{\frac{1}{d+2(\kappa-\zeta)}}\\
%D_t \defeq \left ( \frac{D_{t-1} \eta}{4} \right )^{\frac{d}{d+2(\kappa-\zeta)}} 
%			\left ( \frac{\sigma^2} {8e \tilde C_\zeta^2 \alpha^{2t-2}} \right)^{\frac{1}{d+2(\kappa-\zeta)}}
%&\text{ for } t \in 2:M.
%\end{array}
%\end{equation}
%Solving the recurrence of $D_t$ gives
%\begin{equation}
%\label{eq:defDt2}
%D_t = D_1^{\left( \frac{d}{d+2(\kappa-\zeta)} \right)^{t-1} }
%\left ( \frac{\eta^d \sigma^2}{8e\cdot 4^d \tilde C_\zeta^2} \right)
%				^{\frac{1}{d}\sum_{m=1}^{t-1} \left (\tfrac{d}{d+2(\kappa-\zeta)}\right )^m}
%\alpha^{-\frac{2}{d} \sum_{m=1}^{t-1} (t-m) \left (\tfrac{d}{d+2(\kappa-\zeta)}\right )^m }
%\end{equation}
%Also, define 
%\begin{equation}
%\label{eq:defht}
%h_t \defeq \left ( \frac{\sigma}{\tilde C_\zeta \alpha^{t-1} D_t^{\kappa-\zeta}} \right )^2 
%		n^{2(\kappa-\zeta) \gamma_t} 
%		\exp \left( 4 \sqrt{2} (\kappa-\zeta) \gamma_{t-1} \sqrt{\log n} \right)
%\text{ for } t= 1:M,
%\end{equation}
%so that
%\begin{equation}
%\label{eq:defht2}
%\begin{array}{ll}
%\tilde C_\zeta^2 \alpha^{2t-2} h_t \delta_t^{2(\kappa-\zeta)} = \sigma^2
%&\text{ for } t \in 1:M-1,
%\\
%\tilde C_\zeta^2 \alpha^{2M-2} h_M \delta_M^{2(\kappa-\zeta)} = \sigma^2 \log^{-\frac{2\nu(\kappa-\zeta)}{\kappa}}n.
%\end{array}
%\end{equation}
%Note that $h_t$ is an increasing quantity as $n$ grows. Also, the exponent of $n$ in $h_t$ is 
%$1-(\frac{d}{d+2(\kappa-\zeta)})^t$, so we have $h_1, \dots, h_M \leq n$ for large enough $n$.
%Define events $\EvSmSplm$ as
%\begin{align*}
%\EvSmSplm &\defeq \left\{ \sum_{i=1}^n \IND{ X_{i}^{(m)}\in \Bpm } \leq h_m \right\} & \textup{ for } m = 1:M.
%\end{align*}
%These are probabilistic events that, among the points sampled in the $m$-th round, the number of points within $\delta_m$ distance from $u_m$ is no greater than $h_m$.
%In the pigeon hole principle style argument, this event corresponds to the case that
%the hole $\Bpm$ has small number of pigeons in it.
%
%Given a fixed ground-truth function $\ftnuv$, $\EvSmSplm$ is an event that only a small number $h_m$ of 
%sampled points $X_i^{(m)}$ during the $m$-th round are in the region $\Bpm$,
%which contains the global minimum of $\ftnuv$.
%So, if this occurs, the amount of information to distinguish between $\ftnuv$ and other functions $\ftnuvtildem$ is small, so it is hard to optimize $\ftnuv$ to global optimality.
%Recall that $\probuv$ is the probability measure when underlying true function is $\ftnuv$.
%So, if $\probuv(\EvSmSplm)$ happens with high probability, it means that there is high chance of
%sampling strategy $\Qm$ failing to sample good enough amount of informative sample points.
%
%\subsection{Proof by induction}
%The key step of the proof is to show that, for any $\Qone, \Qtwo, \dots, \QM$, 
%there exists $u_{1:M}$ such that both
%$\probuvp (\bigcap_{m=1}^{M} \EvSmSplm )$ and $\probuvm (\bigcap_{m=1}^{M} \EvSmSplm )$
%are bounded below by some constant.
%This means that if the true objective function is either one of $\ftnuvp$ or $\ftnuvm$,
%with constant probability we do not have enough informative samples needed to 
%distinguish between $\ftnuvp$ and $\ftnuvm$, hence leading to error in optimization.
%
%The proof is done by mathematical induction. 
%First we fix any sampling strategy $\Qone, \dots, \QM$.
%Starting from $t = 1$ to $t = M$, we prove the statement 
%\begin{displayquote}
%	$\stmt_t$ : There exists parameters $u_1 \in \Uone, u_2 \in \Utwo, \dots, u_M \in \UM$ and $v \in \V$ such that $\probuv (\bigcap_{m=1}^t \EvSmSplm ) \geq \frac{1}{4^{t}}$ for sufficiently large $n$.
%\end{displayquote}
%
%\paragraph{Base case ($t=1$).}
%Since the first stage observations are sampled without any information about the true function, the sampling strategies are all identical regardless of the true function $\ftnuv$.
%This is, in other words, $\probuv(X_{i}^{(1)}\in \Bpone) = \Qone(X_{i}^{(1)}\in \Bpone)$, 
%for any $u_{1:M}$ and $v$.
%Recall that interiors of balls $\interior(\Bpone)$ are disjoint for different $u_1 \in \Uone$.
%So, for our fixed $\Qone$,
%\begin{equation}
%\label{eq:smallthann}
%\sum_{u_1 \in \Uone} \sum_{i=1}^n \probuv(X_{i}^{(1)}\in \Bpone)
%=\sum_{i=1}^n \sum_{u_1 \in \Uone} \Qone(X_{i}^{(1)}\in \Bpone) \leq n.
%\end{equation}
%Now, there must exist at least one $u_1 \in \Uone$ such that 
%\begin{equation*}
%\sum_{i=1}^n \Qone(X_{i}^{(1)}\in \Bpone) \leq \frac{n}{| \Uone |}.
%\end{equation*}
%Otherwise, $\sum_{i=1}^n \Qone(X_{i}^{(1)}\in \Bpone) > \frac{n}{| \Uone |}$ for all $u_1$, so summing up for all $u_1$'s will contradict Eq~\eqref{eq:smallthann}. Thus, there must exist such $u_1 \in \Uone$.
%Now recall $| \Uone | \geq \left( \frac{1}{8\delta_1} \right )^d$ from Lemma~\ref{lem:packingno}.
%Given $u_1$, regardless of how we choose the next parameters $u_{2:M}$ and $v$,
%\begin{equation*}
%\sum_{i=1}^n \Qone(X_{i}^{(1)}\in \Bpone) = \exptuv \left [\sum_{i=1}^n \IND{ X_{i}^{(1)}\in \Bpone } \right ] \leq (8\delta_1)^d n.
%\end{equation*}
%Then, for that specific $u_{1:M}$ and $v$, by Markov's inequality,
%\begin{align*}
%\probuv \left ((\EvSmSplone)^c \right ) 
%&= \probuv \left (\sum_{i=1}^n \IND{ X_{i}^{(1)}\in \Bpone } > h_1 \right ) 
%\leq \frac{(8 \delta_1)^d n}{h_1} \\
%&= \frac{8^d \tilde C_\zeta^2}{\sigma^2}\delta_1^{d+2(\kappa-\zeta)} n
%= \frac{8^d \tilde C_\zeta^2}{\sigma^2} D_1^{d+2(\kappa-\zeta)} n^{-\gamma_1(d+2(\kappa-\zeta))} n
%= \half,
%\end{align*}
%by definition of $D_1$ and $\gamma_1$ in Eqs~\eqref{eq:defDt} and \eqref{eq:defgammat}.
%This implies $\stmt_1$.
%In words, for the first-stage sampling strategy $\Qone$, there exist parameter $u_{1:M}$ and $v$ such that,
%no more than $h_1$ sample points are in $\Bpone$ with probability at least 1/2 (and of course 1/4).
%
%%\paragraph{Inductive step ($t =2$).}
%%From the previous step, we saw that for any fixed $\Qone$, there exists a $u_1 \in \Uone$ such that
%%for any choice of the next parameters $u_2 \in \Utwo, \dots, u_{M} \in \UM$ and $v \in \V$,
%%$\probuv(\EvSmSplone) \geq \frac{1}{4}$.
%%Note that this holds for \emph{any} choice of $u_{2:M}$ and $v$.
%%Given that 
%
%\paragraph{Inductive step ($2 \leq t \leq M$).}
%Recall that $\probuv$ is a probability measure when the ground-truth function is $\ftnuv$.
%At step $t$, by the inductive assumption $\stmt_{t-1}$ we know that there exist $u_{1:M}$ and $v$ 
%such that $\probuv (\bigcap_{m=1}^{t-1} \EvSmSplm ) \geq \frac{1}{4^{t-1}}$.
%This means that for the sampling strategies $\Qone, \Qtwo, \dots, \Qtmone$,
%there is constant probability for them to fail to sample sufficient amount of samples 
%in $\Bpone, \Bptwo, \dots, \Bptmone$.
%Note that these are the balls containing the global minimizer of $\ftnuv$.
%Since the function $\ftnuv$ outside $\Bptmone$ decreases towards $\Bptmone$,
%the algorithm may now have noticed that we need more samples in $\Bptmone$.
%So, the next sampling strategy $\Qt$ will likely focus on sampling more points in/near $\Bptmone$.
%However, by the following lemmas, we make pigeon hole principle work again, 
%and find a $\tilde u_t \in \Ut$ such that $\EvSmSpltildet$ occurs; 
%i.e.\ find a ``hole'' $\ballp{\tilde u_t}{\delta_t}$ such that small number of points are sampled with constant probability.
%
%The procedure of this step is as follows.
%We leave the first $t-1$ parameters $u_{1:t-1}$ fixed, 
%re-choose the rest of them to define another set of parameters,
%say $\tilde u_t \in \Utemp{t-1}{t}, \tilde u_{t+1} \in \mathcal{U} _{\tilde u_t}^{(t+1)}, \dots, \tilde u_{M} \in \mathcal{U} _{\tilde u_{M-1}}^{(M)}$ and $\tilde v \in \V$.
%Then, note by Lemma~\ref{lem:ftnconstrunified}.1 that $\ftnuv$ and $\ftnuvtildet$ satisfy
%$\ftnuv(x) = \ftnuvtildet(x)$, for all $x \notin \Bptmone$.
%Since the two functions $\ftnuv(x)$ and $\ftnuvtildet(x)$ look exactly the same outside $\Bptmone$
%and it is known that $\Qone, \dots, \Qtmone$ are likely to 
%fail to sample sufficient amount of samples in $\Bpone, \dots, \Bptmone$ when true function is $\ftnuv(x)$,
%it is plausible to guess that similar thing might happen to $\ftnuvtildet(x)$ as well.
%The next lemma formalizes and proves this idea:
%\begin{lemma}
%	\label{lem:lblem1}
%	Suppose there exist $u_{1:M}$ and $v$ 
%	such that $\probuv (\bigcap_{m=1}^{t-1} \EvSmSplm ) \geq \frac{1}{4^{t-1}}$ for sufficiently large $n$.
%	For \emph{any} $\tilde u_{t:M}$ and $\tilde v$ re-chosen as above, the following inequality holds: 
%	\begin{equation}
%	\label{eqn:lblem1induc}
%	\probuvtildet \left (\bigcap\nolimits_{m=1}^{t-1} \EvSmSplm \right ) 
%	\geq \frac{1}{2 \cdot 4^{t-1}},
%	\end{equation}
%	for sufficiently large $n$.
%\end{lemma}
%Notice that this lemma holds for \emph{any} new choice of $\tilde u_{t:M}$ and $\tilde v$.
%The proof of Lemma~\ref{lem:lblem1} is presented in Appendix~\ref{sec:proof-lblem1}.
%
%Next, consider the conditional probability of $\EvSmSpltildet$ given $\bigcap_{m=1}^{t-1} \EvSmSplm$,
%where $\tilde u_t \in \Ut$.
%Given that $\bigcap_{m=1}^{t-1} \EvSmSplm$ already occurred, as described above,
%the distribution $\Qt(X_{1:n}^{(t)} \mid \bigcap_{m=1}^{t-1} \EvSmSplm)$ is likely 
%to sample more points in/near $\Bptmone$.
%However, recall that the sampling strategy was fixed before we start, 
%so the conditional distribution $\Qt(X_{1:n}^{(t)} \mid \bigcap_{m=1}^{t-1} \EvSmSplm)$ 
%is also a fixed probability distribution.
%We can then apply pigeon hole principle here and find out one particular $\tilde u_t$ such that
%the number of samples in $\ballp{\tilde u_t}{\delta_t}$ are smaller than $h_t$ with constant probability.
%The next lemma proves this idea.
%\begin{lemma}
%	\label{lem:lblem2}
%	Suppose there exist $u_{1:M}$ and $v$ 
%	such that $\probuv (\bigcap_{m=1}^{t-1} \EvSmSplm ) \geq \frac{1}{4^{t-1}}$ for sufficiently large $n$.
%	Then, there \emph{exist} re-chosen parameters $\tilde u_{t:M}$ and $\tilde v$ such that 
%	the following lower bound is satisfied:
%	\begin{equation}
%	\label{eqn:lblem2induc}
%	\probuvtildet \left (\EvSmSpltildet \mid \bigcap\nolimits_{m=1}^{t-1} \EvSmSplm \right ) \geq \half
%	\end{equation}
%\end{lemma}
%The proof of Lemma~\ref{lem:lblem2} is in Appendix~\ref{sec:proof-lblem2}.
%By Lemma~\ref{lem:lblem2}, there exists a function $\ftnuvtildet$ such that Eq~\eqref{eqn:lblem2induc} holds. 
%Then, by Lemma~\ref{lem:lblem1}, Eq~\eqref{eqn:lblem1induc} holds for any such $\ftnuvtildet$.
%Combining the two equations, we can prove $\stmt_t$.
%
%\paragraph{Final step.}
%The proof of the final step is similar to the inductive step.
%From $\stmt_M$, let $u_{1:M}$ and $v$ be the parameter values 
%satisfying $\probuv (\bigcap_{m=1}^{M} \EvSmSplm ) \geq \frac{1}{4^{M}}$.
%We can state another lemma:
%\begin{lemma}
%	\label{lem:lblem3}
%	Suppose there exist $u_{1:M}$ and $v$ 
%	such that $\probuv (\bigcap_{m=1}^{M} \EvSmSplm ) \geq \frac{1}{4^{M}}$ for sufficiently large $n$.
%	Then, for $\tilde v \neq v$, the following lower bound is satisfied for $n$ large enough:
%	\begin{equation}
%	\label{eqn:lblem3eq1}
%	\probuvtildetfin \left (\bigcap\nolimits_{m=1}^{M} \EvSmSplm  \right ) \geq \frac{1}{2 \cdot 4^{M}}.
%	\end{equation}
%	Also, the total variation distance between two conditional probability $\probuvm$ and $\probuvp$ 
%	given the event $\bigcap\nolimits_{m=1}^{M} \EvSmSplm$ satisfies
%	\begin{equation}
%	\label{eqn:lblem3eq2}
%	\tvnorm{ \probuvm \left (\cdot \mid \bigcap\nolimits_{m=1}^{M} \EvSmSplm \right ) -
%		\probuvp \left (\cdot \mid \bigcap\nolimits_{m=1}^{M} \EvSmSplm \right ) } \leq \frac{1}{2},
%	\end{equation}
%	for sufficiently large $n$.
%\end{lemma}
%The proof is also deferred to Appendix~\ref{sec:proof-lblem3}.
%
%Now, we are left with the final step of our proof. 
%Recall that the minimax error we were after was
%\begin{equation*}
%\sup_{\phi \in \ofamilyzeta} \inf_{\what{X} \in \sfamily} \sup_{f \in \ffamily} \E[f(\what{X}) - f^\star].
%\end{equation*}
%Now, we are going to provide a lower bound for $\sup_{f \in \ffamily} \E[f(\what{X}) - f^\star]$.
%This finishes the proof because
%our lower bound technique will work for \emph{any} algorithm $\what{X}$, 
%i.e.\ sampling strategy $\Qone, \dots, \QM$ and estimation algorithm $\QMpone$ of minimum $\what X$,
%Recall from Lemma~\ref{lem:lblem3} that
%there exist $u_{1:M}$ such that for both $\ftnuvm$ and $\ftnuvp$, 
%the event $\bigcap_{m=1}^{M} \EvSmSplm$ occurs with probability at least $\frac{1}{2\cdot4^{M}}$. 
%For simplicity in notation, let us denote $\EvSmSplFrmOneTo{M} \defeq \bigcap_{m=1}^{M} \EvSmSplm$.
%
%Then, since reducing $\ffamily$ to $\{\ftnuvm, \ftnuvp\}$ gives a lower bound, we have
%\begin{align*}
%&\sup_{f\in \ffamily} \E[f(\what X) - f^\star] \\
%\geq &\max_{v \in \V} \exptuv[ \ftnuv(\what X) - (\ftnuv)^\star \IND{\EvSmSplFrmOneTo{M}}]\\
%\geq &\frac{1}{2\cdot 4^M} \max_{v \in \V} \exptuv [ \ftnuv(\what X) - (\ftnuv)^\star \mid \EvSmSplFrmOneTo{M}].
%\numberthis \label{eq:finallb}
%\end{align*}
%We now reduce the optimization problem to hypothesis testing problem,
%which enables us to use the classical inequality on hypothesis testing and total variation distance.
%This is a standard technique in minimax lower bounds, and we will defer this step to the Appendix~\ref{sec:proof-opt2hyptest}.
%\begin{lemma}
%	\label{lem:opt2hyptest}
%	Define 
%	\begin{equation*}
%	\Phi \defeq \inf_{x\in \domain} [\ftnuvm(x) + \ftnuvp(x)] - \inf_{x\in \domain} \ftnuvm(x) - \inf_{x\in \domain} \ftnuvp(x).
%	\end{equation*}
%	Then,
%	\begin{equation*}
%	\max_{v \in \V} \exptuv [ \ftnuv(\what X) - (\ftnuv)^\star \mid \EvSmSplFrmOneTo{M}]
%	\geq
%	\frac{\Phi}{2}
%	\left( 1-\tvnorm{ \probuvm(\cdot \mid \EvSmSplFrmOneTo{M}) -\probuvp(\cdot \mid \EvSmSplFrmOneTo{M}) } \right).
%	\end{equation*}
%\end{lemma}
%From the definition of $\Phi$,
%we know from Lemma~\ref{lem:ftnconstrunified}.7 that $\Phi = 2C\alpha^M \eta^\kappa \delta_M^\kappa$.
%Substituting the result of Lemmas~\ref{lem:opt2hyptest} and \ref{lem:ftnconstrunified}.7 to Eq~\eqref{eq:finallb} and then using Eq~\eqref{eqn:lblem3eq2} of Lemma~\ref{lem:lblem3},
%\begin{align*}
%\sup_{f\in \ffamily} \E[f(\what X) - f^\star]
%&\geq \frac{C\alpha^M \eta^\kappa \delta_M^\kappa}{4^{M+1}}\\
%&=\frac{C\alpha^M \eta^\kappa D_M^\kappa}{4^{M+1}} n^{-\kappa \gamma_M} \exp\left ( - 2 \sqrt{2} \gamma_{M-1} \sqrt{\log n} \right ) \log^{-\nu} n.
%\numberthis \label{eq:generalbd}
%\end{align*}
%The sub-polynomial factors such as
%$\exp\left ( - 2 \sqrt{2} \gamma_{M-1} \sqrt{\log n} \right )$ and $\log^{-\nu} n$
%unfortunately appear during the course of analysis (while proving lemmas),
%and whether they are artifacts of analysis or inevitable factors is not clear for now.
%
%Recall the definition of exponent constant $\gamma_M$ in Eqs~\eqref{eq:defgammat} and \eqref{eq:defgammat2}:
%\begin{equation*}
%\gamma_M =
%\frac{1}{2(\kappa-\zeta)} \left[1 - \left( \frac{d}{d+2(\kappa-\zeta)} \right)^M\right]. 
%\end{equation*}
%Given this general bound, let us now substitute $\kappa$ and $\zeta$ to get the exponents for each case.
%Recall that $\kappa = 1$ for Lipschitz convex functions, and $\kappa = 2$ for smooth strongly convex functions.
%Also, $\zeta = 0$ for zeroth order oracle, and $\zeta = 1$ for first order oracle.
%This gives
%\begin{equation*}
%	\sup_{f\in \ffamily} \E[f(\what X) - f^\star]
%	= \begin{cases}
%	\tilde \Omega \left ( n^{-\half \left[1 - \left( \frac{d}{d+2} \right)^M\right]  }\right ) & \text { for } (\kappa, \zeta) = (1,0),\\
%	\tilde \Omega \left ( n^{-\half \left[1 - \left( \frac{d}{d+4} \right)^M\right]  }\right ) & \text { for } (\kappa, \zeta) = (2,0),\\
%	\tilde \Omega \left ( n^{- \left[1 - \left( \frac{d}{d+2} \right)^M\right]  }\right ) & \text { for } (\kappa, \zeta) = (2,1),
%	\end{cases}
%\end{equation*}
%where $\tilde \Omega$ hides all leading constants and sub-polynomial factors of $n$.
%
%From the leading constant $\frac{C\alpha^M \eta^\kappa D_M^\kappa}{4^{M+1}}$ that appeared
%in Eq~\eqref{eq:generalbd} and the definition of $D_M$ in Eq~\eqref{eq:defDt2},
%we can check that the leading constant is a dependent on
%$M$, $d$, $\sigma$, $C$, $\alpha$, $\eta$, $\beta$, $\kappa$, and $\zeta$.
%Note that $C$, $\alpha$, $\eta$, $\beta$, $\kappa$ are defined according to function classes, and $\zeta$ depends on oracle order.
%So, in Lipschitz convex case ($\kappa = 1$), the leading constant depends only on $M$, $d$, $\sigma$, $L$. In smooth strongly convex case ($\kappa = 2$), the leading constant depends only on $M$, $d$, $\sigma$, $H$, and $\lambda$.
