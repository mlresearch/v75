% -*- Mode: latex -*- %

\section{Upper Bounds}

We now present our upper bounds for the stochastic \emph{batched} convex
optimization problem. We note that, while we present convergence guarantees,
these algorithms are impractical, so it remains of substantial interest to
understand \emph{practical} but low-round optimization schemes. We can
construct algorithms that achieve the lower bounds established in
Theorem~\ref{thm:lbmain} over the functions $f\in \ffamily$ to within
constants, which depend on the dimension $d$ in possibly onerous ways, and
sub-polynomial factors in the sample size $n$. We defer the proof of
Theorem~\ref{thm:ubmain} into appendices~\ref{sec:ubmain-roadmap} through
\ref{sec:proof-Lip-zo}.
% \subsection{Main Results}
\begin{theorem}
\label{thm:ubmain}
Consider the case when the domain $\xdomain = [0, 1]^d$. Fix $\delta > 0$.
\begin{enumerate}[1.]
\item When $\ffamily = \ffamily_{H, \lambda}$ and $\ofamily = \ofamilyf$, then there exists an algorithm (detailed 
in section~\ref{sec:proof-Smooth-fo}) and constants
$\Conthm_1, \Conthm_2 > 0$ 
depending solely on $d, \sigma, H, \lambda, \delta$ such that,  for all
%independent of sample size $n$, l 
$\orc \in \ofamilyf$, $f\in \ffamily_{H, \lambda}$, and
$M \leq \log \log n/\log (1+\frac{2}{d}) - \Conthm_2
	\log\log\log n$, 
the output $\what{X}$ of the algorithm satisfies
\begin{equation*}
\P_{f, \orc} \left( f(\what{X}) - f\opt \leq \Conthm_1
	n^{-\left(1-\left(\frac{d}{d+2}\right)^M \right)} \log (n)
		\right) \geq 1-\delta.
\end{equation*}
\item When $\ffamily = \ffamily_{H, \lambda}$ and $\ofamily = \ofamilyz$, then there exists an algorithm (detailed 
in section~\ref{sec:proof-Smooth-zo}) and constants
$\Conthm_1, \Conthm_2 > 0$ depending solely on $d, \sigma, H, \lambda, \delta$ 
%and independent of sample size $n$ 
such that, for all $\orc \in \ofamilyz$, $f\in \ffamily_{H, \lambda}$, and 
$M \leq \log \log n/\log (1+\frac{4}{d}) -
	\Conthm_2 \log\log\log n$, the output $\what{X}$ of the algorithm satisfies
\begin{equation*}
\P_{f, \orc} \left( f(\what{X}) - f\opt \leq \Conthm_1
	n^{- \half \left(1-\left(\frac{d}{d+4}\right)^M \right)} \log n \right) \geq 1-\delta.
\end{equation*}
\item When $\ffamily = \ffamily_L$ and $\ofamily = \ofamilyf$, then in the case when $d = 1$, there 
	exists an algorithm (detailed in section~\ref{sec:proof-Lip-fo})
	and constant $\Conthm > 0$ depending solely on $\sigma, L, 
	\delta$ 
	%and independent of sample size $n$ 
	such that, for all $\orc \in \ofamilyf$, $f\in \ffamily_L$, and
	$M \geq 1$,
	the output $\what{X}$ of the algorithm satisfies
\begin{equation*}
\P_{f, \orc} \left(f(\what{X}) - f\opt \leq \Conthm n^{-\half} \log n \right) \geq 1-\delta.
\end{equation*}	
\item When $\ffamily = \ffamily_L$ and $\ofamily = \ofamilyz$, then, 
\begin{enumerate}[(a)]
\item in the case when $d = 1$, 
	there exists an algorithm (detailed in section~\ref{sec:proof-Lip-zo})
	and constants $\Conthm_1, \Conthm_2 > 0$ depending solely on $\sigma, L, \delta$ 
	% and independent of sample size $n$ 
	such that, for all $\orc \in \ofamilyz$, $f\in \ffamily_L$, 
	and $M \leq \log \log n/\log 3 - \Conthm_2 \log\log\log n$,
	the output $\what{X}$ of the algorithm satisfies
\begin{equation*}
\P_{f, \orc} \left(f(\what{X}) - f\opt  \leq  \Conthm_1
	n^{-\half \left(1 - \left(\frac{1}{3}\right)^M\right)} \log n\right) \geq 1-\delta.
\end{equation*}	
\item in the case when $M = 1$ (but $d \geq 1$ arbitrary),  
	there exists an algorithm (detailed in section~\ref{sec:proof-Lip-zo})
	and some constant $\Conthm > 0$ depending solely on $d, \sigma, L, 
	\delta$ 
	%and independent of sample size $n$ 
	such that, for all
	$\orc \in \ofamilyz$ and $f \in \ffamily_L$, the output $\what{X}$ of 
	the algorithm satisfies
\begin{equation*}
\P_{f, \orc} \left(f(\what{X}) - f\opt  \leq  \Conthm n^{-\frac{1}{d+2}}
	(\log n)^\frac{1}{d+2} \right) \geq 1-\delta.
\end{equation*}	
\end{enumerate}
%Here, in above, the notation $\tilde{O}^\prime$ neglects constants that depend solely 
%on the parameters $\lambda, L, H, \sigma, \delta$ (thus \emph{independent} of the function 
%$f$) and lower order term on $n$ and $d$ including $\log \log \log n$ e.t.c, and the notation 
%$\tilde{O}$ neglects constants that depend solely on the parameters 
%$\lambda, L, H, \sigma, \delta$ (thus \emph{independent} of the function $f$)
%and lower order term on $n$ and $d$ including universally constant power of $\log n$ e.t.c.
\end{enumerate}
\end{theorem}

We provide some remarks on this theorem before concluding.
Theorems~\ref{thm:lbmain} and \ref{thm:ubmain} together give a
tight minimax rate (up to dimension-dependent
constants and sub-polynomial factors of $n$) for
the \emph{batched} convex optimization problems for the
strongly convex and smooth function class $\ffamily_{H, \lambda}$
and the $1$-dimensional Lipschitz convex class $\ffamily_L$ 
(for both zeroth and first order oracles).
In these settings, Theorems~\ref{thm:lbmain}
and~\ref{thm:ubmain} imply that $M = \tilde{O} (d \log \log n)$ sequential
rounds is 
necessary and sufficient to achieve the fully sequential rate in $n$.

%the number of rounds $M = \tilde{O} (\log \log n)$ is sufficient and 
%necessary to achieve the fully sequential rate $\tilde{O}(n^{-1/2})$ under the 
%zeroth-order oracle and the fully sequential rate $\tilde{O}(n^{-1})$ under the 
%first-order oracle. 
%
%
%when $d$ is fixed, Theorems~\ref{thm:lbmain}
%and \ref{thm:ubmain} also show that, for the aforementioned cases, 
%the number of rounds $M = \tilde{O} (\log \log n)$ is sufficient and 
%necessary to achieve the fully sequential rate $\tilde{O}(n^{-1/2})$ under the 
%zeroth-order oracle and the fully sequential rate $\tilde{O}(n^{-1})$ under the 
%first-order oracle. 

The theorems also leave open a number of important questions.  First, we do
not have the tight rates for batched convex optimization on the Lipschitz
function class $\ffamily_L$ when $d, M > 1$; it is unclear whether
$\tilde{O}(d \log \log n)$ rounds are sufficient to achieve the fully
sequential rate.  The construction of multi-round algorithms for the smooth
and strongly convex function class $\ffamily_{H, \lambda}$ uses the ideas we
elaborate in Section~\ref{sec:big-ideas}: the algorithms are recursive
algorithms based on ideas of grid search.  However, the same idea does not
apply to the Lipschitz function class $\ffamily_L$ because local information
is less useful for nonsmooth functions.  Solving the general Lipschitz
function case $\ffamily_L$ calls for new ideas and techniques.
Moreover, the algorithms for the proof of Theorem~\ref{thm:ubmain} require
prior knowledge of the parameters $\sigma, \lambda, H, L$ and $\delta$;
we have completely ignored questions of adaptivity (which still provide
challenge even in fully sequential settings).

%Novel techniques are needed to treat the general Lipschitz function case 
%$\ffamily_L$ when $d > 1$. 
%Since the underlying function is smooth and strongly convex, we can always base 
%on the information of function values/gradients to localize our search of the minimum 
%$x_f\opt$ as the number of round $M$ increases. The same idea, however, does not apply 
%to the Lipschitz function class $\ffamily_L$, since the function value/gradient being 
%large/small does not provide any useful information onto the distance to the minimum 
%$x_f\opt$. Our construction of algorithm for the $1$-dimensional Lipschitz 
%function case $\ffamily_L$ takes advantage of the special properties of convex functions in $1$
%dimension. It might be the case that a lot more sophisticated technique is 
%needed to treat the general Lipschitz function case $\ffamily_L$ when $d > 1$. 


%Consider the following problem: given a convex function $f: \R^d \to \R$ 
%on a compact set $\xdomain = [0, 1]^d$, an experimenter needs to
%estimate $\argmin_{x\in \xdomain} f(x)$ through $M$ rounds of experiments.
%In each round we are given a budget of $n$ observations on the function 
%values and/or gradient values contaminated with $\sigma$-subgaussian 
%noise. Given some convex function $f$ on domain $\xdomain$, we 
%denote $x_f\opt$ to be the minimum of $f$ in $\xdomain$. In this paper, 
%we consider two function classes. The first class of function consists 
%of strongly convex and smooth functions. This function class is 
%parameterized by two parameters $H, \lambda$, and is defined as below: 
%\begin{equation*}
%\ffamily_{H, \lambda} = \left\{f: \lambda \ltwo{x-x^\prime}^2 \leq 
%	\left<\grad f(x) - \grad f(x^\prime), x - x^\prime\right> \leq 
%		H\ltwo{x-x^\prime}^2 ~\text{for all}~x, x^\prime \in \xdomain\right\}.
%\end{equation*}
%The second class of function consists of Lipschitz convex functions. This 
%function class is parameterized by one single parameter $L$, and is defined 
%as below: 
%\begin{equation*}
%\ffamily_{L} = \left\{f: \left|f(x) - f(x^\prime)\right| \leq L \ltwo{x-x^\prime}	
%	~\text{for all}~x, x^\prime \in \xdomain\right\}.
%\end{equation*}
%Now, for given function $f$, we measure the performance of any given 
%algorithm of this particular function $f$ by its pointwise risk 
%$\risk(\hat{x}, f) = \E_f \left[f(\hat{x}) - f(x_f\opt)\right]$, and measure its performance
%over a family of functions $\ffamily$ via $\risk(\hat{x}, \ffamily) = 
%\sup_{f\in \ffamily} \risk(\hat{x}, f)$. In the rest of the report, we describe 
%algorithms that achieves the minimax rate for the above two classes 
%of functions. More specifically, we show that, 
%\begin{enumerate}
%    \item When $\ffamily = \ffamily_{H, \lambda}$, $M \leq \tilde{O}^\prime\left(\log \log n/\log (1+\frac{4}{d})\right)$, 
%    	and the oracle is zeroth-order oracle, then, there exists some algorithm such that, 
%	$\risk(\hat{x}, \ffamily) = \tilde O \left (n^{- \half \left(1-\left(\frac{d}{d+4}\right)^M \right)} \right)$.
%    \item When $\ffamily = \ffamily_{H, \lambda}$, $M \leq \tilde{O}^\prime\left(\log \log n/\log (1+\frac{2}{d})\right)$, 
%    	and the oracle is zeroth-order oracle, then, there exists some algorithm such that, 
%	$\risk(\hat{x}, \ffamily) = \tilde O \left (n^{- \left(1-\left(\frac{d}{d+2}\right)^M \right)} \right)$.
%    \item When $\ffamily = \ffamily_{L}$, $M \leq \tilde{O}^\prime\left(\log \log n/\log 3\right)$, $d = 1$, 
%    	and the oracle is zeroth-order oracle, then, there exists some algorithm such that, 
%	$\risk(\hat{x}, \ffamily) = \tilde O \left (n^{- \half \left(1-\left(\frac{1}{3}\right)^M \right)} \right)$.
%    \item When $\ffamily = \ffamily_{L}$, $M = 1$ and the oracle is zeroth-order oracle, then, there exists 
%    	some algorithm such that, $\risk(\hat{x}, \ffamily) = \tilde O \left (n^{-\frac{1}{d+2}} \right)$.
%    \item When $\ffamily = \ffamily_{L}$, $d = 1$ and the oracle is first-order oracle, then, there exists 
%    	some algorithm such that $\risk(\hat{x}, \ffamily) = \tilde O \left (n^{-\half} \right)$.
%\end{enumerate}
%Here, we stress out that $\tilde{O}$ neglects lower order terms including $\log n, \log\log n$ etc, 
%and $\tilde{O}^\prime$ neglects lower order terms such as $\log\log\log n$.

%Note that $|G(c, r, k)| = (2k+1)^d$. 
%\newcommand{\new}{^{\text{new}}}
%\newpage
