%!TEX root = colt-camera-ready.tex

\section{Proofs regarding Smoothing}
\subsection{Proof of Lemma \ref{thm:smoothingmain}}
\begin{proof}
  As stated before $f$ being $\Gamma$-invariant implies that there exists a function $g$ such that $f(x) = g(\Gamma x)$. Therefore we have that
  \[ f_{\delta, \Gamma}(x) = \mathbb{E}_{v \in \Gamma, \|v\| \leq 1}[ f(x + \delta v)] = \mathbb{E}_{v \in \Gamma, \|v\| \leq 1}[ g(M_{\Gamma} x + \delta M_{\Gamma} v)] = g_{\delta}(M_{\Gamma} x)\]
  where $g_{\delta}(x) \defeq S_{\delta, \reals^r} g(x)$. The representation of $\smooth{f}$ as $g_{\delta}(M_{\Gamma} x)$ implies that $f_{\delta, \Gamma}$ is $\Gamma$-invariant. Further the above equality implies that $\nabla \smooth{f}(x) = M_{\Gamma}^T \nabla g_{\delta}(M_{\Gamma} x)$. A standard argument using Stokes' theorem shows that $g_{\delta}$ is differentiable even when $g$ is not \footnote{We need $g$ to be not differentiable in a measure 0 set which is always the case with our constructions} and that $\nabla g_{\delta}(y) = \frac{r}{\delta}\mathbb{E}_{v \sim S_r} \left[ g(y + \delta v) v\right]$(Lemma 1 \cite{flaxman2005online}),
  where $S_r$ is the $r$-dimensional sphere, i.e. $S_r = \{x \in \reals^r | \|x\| = 1\}$
  \begin{align*}
    \| \nabla g_{\delta}(x) - \nabla g_{\delta}(y) \| &=  \frac{r}{\delta}\|  \mathbb{E}_{v \sim S_r} \left[ g(x + \delta v) v\right] - \mathbb{E}_{v \sim S_r} \left[ g(y + \delta v) v\right] \|  \\ 
    &\leq   \frac{r}{\delta} \mathbb{E}_{v \sim S_r} \left[ | g(x + \delta v)  -  g(y + \delta v) | \| v \| \right] \leq \frac{rG}{\delta} \|x - y\|. 
  \end{align*}

  The first inequality follows from Jensen's inequality and the second inequality follows from noticing that $f$ being $G$-Lipschitz implies that $g$ is $G$-Lipschitz. We now have that 
  \[ \|\nabla \smooth{f}(x) - \nabla \smooth{f}(y)\| \leq \|M_{\Gamma} \left( \nabla g_{\delta}(M_{\Gamma} x) - \nabla g_{\delta}(M_{\Gamma} y) \right)\| \leq \frac{r G}{\delta} \| M_{\Gamma}\|\|(x - y)\| \leq \frac{r G}{\delta} \|(x - y)\|.\]
  $f$ being $G$-Lipschitz immediately gives us $\forall \;x: |f_{\delta,\Gamma}(x) - f(x)| \leq \delta G$.
\end{proof}


\subsection{Proof of Corollary \ref{cor:maincor}}

\begin{proof}
We will argue inductively. The base case ($k=0$) is a direct consequence of the function $f$ being $G$-Lipschitz. Suppose the theorem holds for $k-1$. To argue about $\|\nabla^{i} S_{\delta,\Gamma}^k f(x) - \nabla^{i} S_{\delta,\Gamma}^k f(y)\|$ we will consider the function $q_{i,v}(x) = \nabla^{i} S_{\delta,\Gamma}^{k-1} f(x)[v^{\otimes i}]$ for $i \in [k]$ and for a unit vector $v$. 
We will first consider the case $i < k$. Using the inductive hypothesis and the fact that smoothing and derivative commute for differentiable functions we have that 
\[S_{\delta, \Gamma}\;q_{i,v}(x) = \nabla^i S_{\delta, \Gamma}^k  f(x)[v^{\otimes i}]\enspace.\]
Note that the inductive hypothesis implies that $q_{i,v}(x)$ is $\left(\frac{r}{\delta}\right)^{i}G$-Lipschitz and so is $S_{\delta, \Gamma}\;q_{i,v}(x)$ via Lemma \ref{thm:smoothingmain}. Therefore we have that 
\[ \forall i \in [k-1] \;\; \| \nabla^{i} S_{\delta,\Gamma}^k f(x) - \nabla^{i} S_{\delta,\Gamma}^k f(y)\| \leq \left(\frac{r}{\delta} \right)^iG\|x-y\|.\]
We now consider the case when $i = k$. By Lemma \ref{thm:smoothingmain} we know that $S_{\delta, \Gamma}\;q_{k-1,v}(x) = \nabla^{k-1} S_{\delta, \Gamma}^k  f(x)[v^{\otimes i}]$ is differentiable and therefore we have that $S_{\delta, \Gamma}^k  f(x)$ is $k$ times differentiable. Further we have that 
\[\nabla S_{\delta, \Gamma} \; q_{k-1,v}(x) = \nabla^{k} S_{\delta,\Gamma}^k f(x)[v^{\otimes k-1}].\]
A direct application of Lemma $\ref{thm:smoothingmain}$ and the inductive hypothesis which implies that $q_{k-1,v}$ is $\left( \frac{r}{\delta}\right)^{k-1}G$- Lipschitz gives that 
\[ \|\nabla^{k} S_{\delta,\Gamma}^k f(x) - \nabla^{k} S_{\delta,\Gamma}^k f(y)\| \leq \left(\frac{r}{\delta}\right)^{k}G\|x-y\|.\]
Further it is immediate to see that 
\[ \inf_{y: \|y - x\| \leq k\delta} f(y) \leq S_{\delta, \Gamma}^k f(x) \leq \sup_{y: \|y - x\| \leq k\delta} f(y)\]
which implies using the fact that $f$ is $G$ Lipschitz that 
\[| S_{\delta,\Gamma}^k f(x) - f(x)| \leq G\delta k .\]


\end{proof}



\section{Lower bound against Randomized Algorithms}

\subsection{Proof of Theorem \ref{thm:mainthmrandomized}}

\begin{proof}
  We provide a randomized construction for the function $\hardf$.
  The construction is the same as in Section \ref{sec:construction} but we repeat it here for clarity. 
  We sample a random $T$ dimensional orthonormal basis $\{a_1 \ldots a_T\}$ from the uniform distribution on the orthonormal group $O(T)$. Let $A_i$ be the subspace spanned by $\{a_1 \ldots a_i\}$ and $A_i^{\perp}$ be the orthogonal complement of $A_i$. Further define an auxiliary function 
\[f(x) \defeq \max_i f_i(x) \text{ where } f_i(x) \defeq a_i^T x\enspace.\]
Given a parameter $\gamma$, now define the following functions 
\[ \tilde{f}(x) \defeq \max_i \tilde{f}_i(x) \text{ where } \tilde{f}_i(x) \defeq f_i(x) + \left(1 - \frac{i}{T} \right)\gamma \defeq a_i^T x + \left(1 - \frac{i}{T} \right)\gamma,\] 
\begin{equation}
  \label{eqn:hardfdef}
  \hardf(k, \gamma, \delta_T) \defeq S^k_{\delta_T, A_T} \;\tilde{f},
\end{equation}
i.e. smoothing $\tilde{f}$ with respect to $\delta_T, A_T$. The hard function we propose is the random function $\hardf$ with parameters set as $\gamma = \frac{1}{3\sqrt{T}}$ and $\delta_T = \frac{1}{20kT^{1.5}}$. 
  We restate facts which can be derived analogously to those derived in Section \ref{sec:construction} (c.f. Equations \eqref{eqn:errorbound},\eqref{eqn:funcminbound}).
  \begin{equation}
  \label{eqn:errorboundrand}
  \forall x \;\;|\hardf(x) - \tf(x)| \leq k\delta \;\;\text{ and }\;\; \min_{x \in B_d} \hardf(x) \leq \frac{-1}{\sqrt{T}} + \gamma + k\delta.
  \end{equation}
The following key lemma will be the main component of the proof. 
\begin{lemma}
\label{lemma:randmainlemma}
  Let $\{x_1 \ldots x_T\}$ be the points queried by a randomized algorithm throughout its execution on the function $\hardf$. With probability at least $1 - \delta$ (over the randomness of the algorithm and the selection of $\hardf$) the following event $\mathcal{E}$ happens
  \[ \mathcal{E} = \left\{ \forall i \in [T] \;\; \forall j \geq i\;\;|a_j^Tx_i| \leq \smallq \right\}.\]  
\end{lemma}
Using the above lemma we first demonstrate the proof of Theorem \ref{thm:mainthmrandomized}. We will assume the event $\mathcal{E}$ in Lemma \ref{lemma:randmainlemma} happens.
\\
\\

\noindent \textbf{Bounded Lipschitz Constants} Using Corollary \ref{cor:maincor}, the fact that $\hardf$ has Lipschitz constant bounded by 1 and that $\tilde{f}$ is invariant with respect to the $T$ dimensional subspace $A_T$, we get that the function $\hardf_T$ has higher order Lipschitz constants bounded above as
\[ \forall i \leq k \;\;\; L_{i+1} \leq \left(\frac{T}{\delta_T}\right)^i \leq \left(20kT^{2.5}\right)^i.\] 

\noindent\textbf{Sub-optimality} : The event $\mathcal{E}$ in the lemma implies that $\tilde{f}_i(x_i) \geq -\smallq + (1 - \frac{i}{T})\gamma$ which implies that $\tilde{f}(x_i) \geq -\smallq + (1 - \frac{i}{T})\gamma$ and from Equation \eqref{eqn:errorboundrand} we get that 
\[ \forall i \in [T]\;\; \hardf(x_i) \geq -\smallq + \left(1 - \frac{i}{T}\right)\gamma - k\delta_T.\]
Now using Equation \eqref{eqn:errorboundrand} we get that every $x_i$ is such that
\[ \forall i \in [T]\;\;\;\; \hardf(x_i) - \min_{x \in B} \hardf(x) \geq \frac{1}{\sqrt{T}} - \frac{i}{T}\gamma - 2k\delta_T - \smallq \geq \frac{1}{2\sqrt{T}}.\]
The last inequality follows by the choice of parameters.
This finishes the proof of Theorem \ref{thm:mainthmrandomized}.
\end{proof}
\begin{proof}[Proof of Lemma \ref{lemma:randmainlemma}]
  We will use the following claims to prove the lemma. For any vector $x$, define the event $\mathcal{E}_i(x) = \big\{ \forall j \geq i\;\;|a_j^Tx| \leq \smallq \big\}$. The event we care about then is 
  \[ \mathcal{E} \defeq \bigcap_{i = 1 \rightarrow T} \mathcal{E}_i(x_i).\]

  \begin{claim}
  \label{claim:splineclaim}
      Let $x \in \reals^d$. If $\mathcal{E}_i(x)$ holds, then $[\hardf(x), \nabla \hardf(x) \ldots \nabla^k \hardf(x)]$ all depend only on $\{ a_1 \ldots a_i\}$. 
    \end{claim}
  \begin{claim}
  \label{claim:inductiveclaim}
      Let $i \in [T]$. Assume that $\forall j < i$, $\mathcal{E}_j(x_j)$ holds. Then $\mathcal{E}_{i}(x_{i})$ holds with probability at least $1 - \frac{\delta}{T}$(over the choice of $a_i$ and the randomness of the algorithm).
    \end{claim}
    Claim \ref{claim:splineclaim} is a robust version of the argument presented in the proof of Theorem \ref{thm:mainthm}. Claim \ref{claim:inductiveclaim} is a byproduct of the fact that in high dimensions the correlation between a fixed vector and a random small basis is small. Claim \ref{claim:splineclaim} is used to prove the Claim \ref{claim:inductiveclaim}.

  Lemma \ref{lemma:randmainlemma} now follows via a simple inductive argument using Claim \ref{claim:inductiveclaim} which is as follows
      \[ \mathrm{Pr}(\mathcal{E}) = \mathrm{Pr} \left(\bigcap_{i = 1 \rightarrow T} \mathcal{E}_i(x_i)\right) = \prod_{i=1 \rightarrow T}\mathrm{Pr}\left(\mathcal{E}_i(x_i) \big| \bigcap_{j < i} \mathcal{E}_j(x_j)\right) \geq \left(1 - \frac{\delta}{T}\right)^T \geq 1 - \delta.\] 
\end{proof}
\begin{proof}[Proof of Claim \ref{claim:splineclaim}]
   As noted before the smoothing operator $S^k_{\delta}$ is such that at any point $x$ all the $k$ derivatives of $S^k_{\delta} f$ depend only on the value of $f$ in a ball of radius $k \delta$ around the point $x$. Therefore  it is sufficient to show that for the function $\tf$, for every $y$ such that $\|y - x\| \leq k\delta_T$ we have that $\tf(y)$ depends only on $\{a_1 \ldots a_{i}\}$. To ensure this, it is enough to ensure that for every such $y$ we have that $\argmin_{j \in [T]} \tf_j(y) \leq i$ which is what we prove next.

Let us first note the following facts. By the definition of $\mathcal{E}_i(x)$ we have that $\forall j > i, f_j(x) \leq \smallq$. This immediately implies that             
\begin{equation}
\max_{j > i} \;\; \tf_j(x_i) \leq \smallq + \left(1 - \frac{i+1}{T} \right) \gamma.
\end{equation}
Now since we know each $\tf_i$ is $1$-Lipschitz \footnote{$\|a_i\| = 1$}, this also gives us 
\begin{equation}
\forall y \;\;\text{s.t.}\;\;\|y - x\| \leq k\delta_T \;\;\text{we have}\;\;\max_{j > i} \;\; \tf_j(y) \leq \smallq + \left(1 - \frac{i+1}{T} \right) \gamma + k\delta_T. 
\end{equation}
By the event $\mathcal{E}_i(x)$ we also know that $\tf_i(x) \geq -\smallq + (1 - \frac{i}{T})\gamma$.
This implies as above 
\begin{equation}
\forall y \;\;\text{s.t.}\;\;\|y - x\| \leq k\delta_T \;\;\text{we have}\;\;\max_{j \leq i} \;\; \tf_j(y) \geq - \smallq + \left(1 - \frac{i}{T} \right) \gamma - k\delta_T. 
\end{equation}
The above equations imply that as long as $2k\delta_T + \frac{1}{10T^{1.5}} < \frac{\gamma}{T}$ (which is true by the choice of parameters), we have that  
\begin{equation}
\forall y \;\;\text{s.t.}\;\;\|y - x\| \leq k\delta_T \;\; \argmin_{j \in [t]} \tf_j(y) \leq i. 
\end{equation}
which is sufficient to prove Claim \ref{claim:splineclaim}.
 \end{proof} 
\begin{proof}[Proof of Claim \ref{claim:inductiveclaim}]
  Consider any $i \in [T]$. Given $\mathcal{E}_j(x_j)$ is true for all $j < i$, applying Claim \ref{claim:splineclaim} for all $j < i$, implies that all the information that the algorithm possesses is only a function of $\{a_1 \ldots a_{i-1}\}$ and the internal randomness of the algorithm. Since $\{a_1 \ldots a_T\}$ was sampled from the uniform distribution on the orthonormal group, we can assume that it was sampled by the inductive process which picks $a_i$ as a uniformly random unit vector from the subspace $A_{i-1}^{\perp}$, which is defined to be the orthogonal component of $A_{i-1}$, the subspace spanned by $\{a_1, \ldots a_{i-1}\}$.

  The above inductive procedure for sampling implies that conditioned on $\{a_1 \ldots a_{i-1}\}$, the distribution of the remaining vectors $\{a_i \ldots a_T\}$ is uniform on the orthogonal group $O(T - i + 1)$, lying in the $d-i + 1$-dimensional subspace $A_{i-1}^{\perp}$. The above implies that the distribution over $\{a_i \ldots a_T\}$ is conditionally independent of any $x_i$ the algorithm might play. Since we wish to bound the absolute value of the inner product we can assume $\|x_i\| = 1$ \footnote{Otherwise the absolute value of the inner product is only lower}.

  Following the above arguments, proving the lemma now reduces to the following. Consider a fixed unit vector $y$ in a $\reals^{d-i+1}$. The vector $y$ corresponds to the vector $x_i$ played by the algorithm. Further, consider picking a uniformly random $T - i + 1$ dimensional subspace of $\reals^{d - i + 1}$ represented by the basis $\{y_1 \ldots y_{T-i+1}\}$. $\{y_1 \ldots y_{T - i +1}\}$ represent the vectors $\{a_i \ldots a_T\}$. The lemma now reduces to bounding $\forall\;j$, the probability 
  \[\mathrm{Pr}\left(|\innerprod{y}{y_j}| > \smallq\right) .\]
  The rest of the argument follows the argument by \cite{WoodworthSrebro2016}(Proof of Lemma 7). Note that 
  for $y_1$ this probability amounts to the ratio between the surface area of a sphere above the caps of radius $\sqrt{1 - (\smallq)^2}$ and the surface area of the unit sphere. This surface area is smaller than the ratio between the surface area of a sphere of radius $\sqrt{1 - (\smallq)^2}$ and the surface area of a unit sphere. Formally this gives us 
  \[ \mathrm{Pr}\left(|y_1^T y| \geq \frac{1}{20T^{1.5}}\right) \leq \sqrt{\left(1 - \left(\smallq\right)^2\right)^{d-i+1}}.\]
  Applying the argument inductively we get that
  \[ \forall j \in [1, T-i+1] \;\;\; \mathrm{Pr}\left(|y_j^T y| \geq \frac{1}{20T^{1.5}}\right) \leq \sqrt{\left(1 - \left(\smallq\right)^2\right)^{d-i-j+2}}\enspace.\]
  Using the union bound we have that
  \begin{multline}
    \mathrm{Pr}(\mathcal{E}_i(x_i)) \geq 1 - \left( \mathrm{Pr} \left(\bigcup_{j=1 \rightarrow T-i+1} \left(|y_j^T y| \geq \frac{1}{20T^{1.5}}\right)\right)\right) \geq 1 - (T - i)\left(1 - \left(\smallq\right)^2\right)^{\frac{d-T+1}{2}} \\ \geq 1 - (T - i)e^{- (\smallq)^2\frac{d-T}{2}} \geq 1 - \frac{\delta}{T} \enspace.
  \end{multline}
   \[ \] 
   The last line follows from the choice of $d = \Omega\left(T^3\log(T^2/\delta)\right)$.
\end{proof}