% Reinforcement Learning (RL) has gained wide acceptance as a general paradigm for solving stochastic control problems, especially when the underlying mechanics of the system are unknown. One of the core problems in RL is policy evaluation, that involves using data sampled from a fixed policy to estimate the policy's value function without necessarily knowing the dynamics of the underlying Markov Decision Process (MDP). The Temporal Difference (TD) algorithm, introduced by \cite{sutton1988learning}, is the most widely used approach for policy evaluation. The original TD algorithm stores the value function of each state in the system ({\em tabular representation}). However, modern day applications of TD have large number of states, rendering the tabular approach impractical and giving rise to parametric approximations. Given the recent promising empirical results of such parametric approaches \citep{mnih2015human, silver2016mastering}, it is of theoretical interest to understand the reason behind such success. With this goal in mind, we perform a finite time analysis of the Online TD(0) \rs{(projected? TD or TD(0)? should tweak the title and the rest of the paper accordingly; say that by TD, we refer to TD(0) throughout the paper)} algorithm with a linear function parameterization in order to better understand its convergence properties. 

% \textbf{Related literature}: Although TD(0) was introduced in 1988, it took almost a decade in proving its convergence \citep{tsitsiklis1997analysis}. Following that, \cite{konda2002thesis} showed the asymptotic normality of TD(0). In recent years, much of the research in this domain has transitioned from asymptotic results to finite time bounds. To our knowledge, there exist only two directly relevant papers that provide finite time bounds for the TD(0) algorithm \citep{korda2015td, dalal2017finite}, each deriving finite time convergence rates in both expectation and high probability. \cite{korda2015td} claim to prove an $\mathcal{O}(1/\sqrt{T})$ rate but doubts have been raised concerning the correctness of their work \citep{2017kordaissues}. Moreover, the constant terms in their bounds are hard to interpret. \cite{dalal2017finite} show the rate to be $\mathcal{O}(1/\text{exp}(T^{1-\sigma}) + 1/T^{\sigma})$ for $\sigma \in (0,1)$ with constant terms that are challenging to understand and a complicated analysis.  \rs{We need to promote our 1/T rate to show dominance. Does Dalal's step size depend on problem instance?} 

% Though not directly relevant to TD(0), it is worthwhile to mention the literature corresponding to Stochastic Approximation (SA) since it has provided generic tools for analyzing TD algorithms. Some related work includes \cite{borkar2000ode, borkar2008stochastic, kamal2010convergence, frikha2012concentration, fathi2013transport}, and \cite{thoppe2015concentration}. \rs{Maybe we should mention the papers that we use such as Lacoste-Julien and Nemirovski? (Note that we do mention them in Section 4.)}

% There also exists convergence results for other policy evaluation algorithms that differ from TD(0) in terms of the objective function they minimize or the optimization technique they employ \citep{dann2014policy}. In particular, various algorithms such as LSTD \citep{lazaric2010finite, ghavamzadeh2010lstd, ghavamzadeh2011finite, lazaric2012finite, tagorti2015rate, antos2008learning, pires2012statistical, prashanth2014fast}, LSPE \citep{yu2009convergence}, GTD \citep{liu2015finite}, and ATD \citep{pan2017accelerated} have been analyzed. We emphasize that all of these algorithms are {\em batch} methods and the study of {\em incremental} methods such as TD(0) has proved to be much more challenging. Finally, we note that the correctness of \cite{prashanth2014fast} is debatable \citep{2017kordaissues}. 

% \textbf{Our approach}: We start simple and then, add complexity. Specifically, we consider two sampling models. In the first model, we assume the samples are generated iid from the stationary distribution of the underlying Markov chain and provide finite time bounds in expectation and in high probability under different choices of step sizes. We add complexity in the second model by relaxing the stationary distribution assumption and show how our analysis for the first model extends gracefully to this setting. \rs{Extensions?} In all the main results, our guiding approach has been to derive the convergence rate in the value function space (as opposed to the parameter space). The key ingredient for the success of such an approach is a result from \cite{tsitsiklis1997analysis}, which states that at all time steps of the TD(0) algorithm, the expected gradient points in the descent direction, more so when the gap between the current and the true value function is large (Lemma \ref{lemma:expected_gradient}). Though \cite{tsitsiklis1997analysis} used this fact as a supporting lemma to prove the convergence of TD(0), we believe this result to be much more powerful. Finally, we note that one main motivation for this work is to provide a finite time analysis for TD(0) in a fashion similar to that for Stochastic Gradient Descent (SGD) algorithms, that is, we want the analysis to be clean and the constant terms appearing in the bound to be easily interpretable, which is not the case in current literature \citep{korda2015td, dalal2017finite} \rs{(should this pitch go eariler?)}. \rs{Should we mention projection here? Can cite Nemirovski (2009) to justify projection. Also, from Liu et al (2015), ``this [projection] is standard in SA algorithms and has been used in off-policy TD (Yu and Bertsekas 2008) and actor-critic algorithms (Bhatnagar et al 2009)''.} 

% \textbf{Contributions}: asas

% \rs{Contributions:}  \rs{Markov chain sampling model (the two competing papers do not have this though Dalal recognizes it is possible using a similar mixing assumption; need to double-check Prashanth) in addition to the stationary distribution expectation and concentration bounds -- state the bounds crisply and refer to Theorem numbers explicitly; \hspace{1cm} Tsitsiklis-Van Roy result (novel) + V-space instead of $\theta$-space (not novel); \hspace{1cm} simple analysis and easy to interpret constants; \hspace{1cm} step sizes are simple and require no problem specific knowledge (as argued for in Nemirovski (2009)); \hspace{1cm} no dependence on dimensionality (what about the two competing papers?)}

% \vspace{1cm}
% This paper is organized as follows: Section (\ref{sec:algorithm}) outlines the set-up for our analysis. We introduce the classic MDP model with some additional assumptions and give the Online Projected TD algorithm. In this work, we show results under two sampling models: the IID sampling model and the Markov chain sampling model; convergence analysis for which is sketched out in Section (\ref{sec:iid_sampling}) and Section (\ref{sec:markov_chain_analyis}) respectively. \jb{We discuss relationship to closely related previous work in Section (\ref{sec:disc}) and comment about some aspects of our work; specifically the choice of step-size and the projection step.} We conclude with some future directions of work in Section (\ref{sec:conc}).

