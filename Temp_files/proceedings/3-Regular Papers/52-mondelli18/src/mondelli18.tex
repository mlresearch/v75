\documentclass[final,12pt]{colt2018} 
% \documentclass{colt2017} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e



\usepackage{color}
%\usepackage{cite}
\usepackage{fixltx2e}
\usepackage{amsmath}
\usepackage{amssymb}
\interdisplaylinepenalty=2500
\usepackage{array}
\usepackage{wasysym}
\usepackage{dsfont}
%\usepackage[hyperindex]{hyperref}
\usepackage[latin1]{inputenc}                       
\usepackage[english]{babel}                         
\usepackage[T1]{fontenc}
\usepackage{mathtools}
\usepackage{amssymb} 
\usepackage{enumerate}
\usepackage{bbm}
\usepackage{syntonly}
\usepackage{verbatim,times}
\usepackage{epstopdf}
\usepackage{graphicx}
%\usepackage{accents}
\usepackage{latexsym,fancyhdr,bm}
%\usepackage{subfigure}
\usepackage{caption}
\usepackage{url}
\usepackage{bbm}
\usepackage{dsfont}
%\usepackage{amsthm}
\usepackage{bigints}
%\usepackage{subfigure}
% \usepackage[nospread,keeplastbox]{flushend}
% \usepackage{flushend}


\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}


%\pdfminorversion=5

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\newtheorem*{theorem*}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{proposition*}{Proposition}
\newtheorem*{definition*}{Definition}
\newtheorem*{example*}{Example}
\newtheorem*{remark*}{Remark}
\newtheorem*{corollary*}{Corollary}



\newcommand{\code}{{\mathcal C}}
\newcommand{\PB}{P_{\text{B}}^{\text{MAP}}}
\newcommand{\pset}{{\mathcal P}}
\newcommand{\Ldens}{{}}
\newcommand{\shell}{{\mathcal S}}
\newcommand{\eset}{{\mathcal E}}
\newcommand{\vset}{{\mathcal V}}
\newcommand{\prob}{{\mathbb P}}
\newcommand{\field}{{\mathbb F}_2}
\newcommand{\naturals}{{\mathbb N}}
\newcommand{\cw}{c}
\newcommand{\poset}{{\mathcal P}}

\newcommand{\OPT}{{\textup{\rm OPT}}}
\newcommand{\SDP}{{\textup{\rm SDP}}}
\def\ddiag{\text{ddiag}}
\def\group{{\mathcal G}}
\def\mZt{{\mathbb Z_2}}
\def\mC{{\Crit}}
\def\mR{{\mathbb R}}
\def\SO{{\rm SO}}
\def\bz{{\boldsymbol z}}
\def\sT{{\mathsf T}}
\def\ed{\stackrel{{\rm d}}{=}}
%\def\id{{\id}}

\def\det{{\rm det}}
\def\tu{{\tilde u}}
\def\tsigma{{\tilde \sigma}}
\def\tD{{\tilde D}}
\def\tLambda{{\tilde \Lambda}}
\def\tgrad{{\textup{grad}}}
\def\thess{{\textup{Hess}}}
\def\cO{{\mathcal O}}
\def\rank{{\rm rank}}
\def\func{{f}}
\def\complex{{\mathbb C}}
\def\reals{{\mathbb R}}
\def\integers{{\mathbb Z}}
\def\<{\langle}
\def\>{\rangle}
\def\projp{{\boldsymbol P}^{\perp}}
\def\hphi{\hat{\varphi}}
\def\reals{{\mathbb R}}

\def\baq{\bar{q}}
\def\ba{{\boldsymbol a}}
\def\bq{{\boldsymbol q}}
\def\be{{\boldsymbol e}}
\def\bv{{\boldsymbol v}}
\def\bx{{\boldsymbol x}}
\def\bw{{\boldsymbol w}}
\def\by{{\boldsymbol y}}
\def\bd{{\boldsymbol d}}
\def\bs{{\boldsymbol s}}
\def\br{{\boldsymbol r}}
\def\hbs{\hat{\boldsymbol s}}
\def\hbr{\hat{\boldsymbol r}}
\def\cF{{\mathcal F}}
\def\cH{{\mathcal H}}
\def\hh{\hat{h}}
\def\sB{{\sf B}}
\def\hsB{\hat{\sf B}}
\def\bM{{\boldsymbol M}}
\def\bK{{\boldsymbol K}}
\def\bH{{\boldsymbol H}}
\def\bA{{\boldsymbol A}}
\def\bB{{\boldsymbol B}}
\def\bC{{\boldsymbol C}}
\def\bY{{\boldsymbol Y}}
\def\bX{{\boldsymbol X}}
\def\bW{{\boldsymbol W}}
\def\bZ{{\boldsymbol Z}}
\def\bI{{\boldsymbol I}}
\def\bU{{\boldsymbol U}}
\def\bSigma{{\boldsymbol \Sigma}}
\def\bV{{\boldsymbol V}}
\def\bD{{\boldsymbol D}}
\def\bP{{\boldsymbol P}}
\def\bO{{\boldsymbol O}}
\def\bJ{{\boldsymbol J}}
\def\bL{{\boldsymbol L}}
\def\b0{{\boldsymbol 0}}
\def\bzero{{\boldsymbol 0}}
\def\bS{{\boldsymbol S}}
\def\hbx{\hat{\boldsymbol x}}

\def\bu{{\boldsymbol u}}
\def\bg{{\boldsymbol g}}
\def\bv{{\boldsymbol v}}

\def\sF{{\sf F}}
\def\sG{{\sf G}}
\def\bz{{\boldsymbol z}}
\def\hbz{\hat{\boldsymbol z}}
\def\hx{\hat{x}}
\def\hbx{\hat{\boldsymbol x}}
\def\hz{\hat{z}}
\def\hsigma{\hat{\sigma}}
\def\sb{{\sf b}}
\def\hnu{\hat{\nu}}


\def\normal{{\sf N}}
\def\cnormal{{\sf CN}}
\def\id{{\boldsymbol I}}
\def\E{{\mathbb E}}

\def\de{{\rm d}}
\def\sE{{\sf E}}
\def\Sphere{{\sf S}}
\def\MMSE{{\sf MMSE}}
\def\Unif{{\rm Unif}}
\def\op{\overline{p}}
\def\complex{{\mathbb C}}
\def\cT{{\mathcal T}}


\title[Fundamental Limits of Weak Recovery with Applications to Phase Retrieval]{Fundamental Limits of Weak Recovery\\ with Applications to Phase Retrieval}

\usepackage{times}
 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % Two authors with the same address
  % \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
  %  \Name{Author Name2} \Email{xyz@sample.com}\\
  %  \addr Address}

 % Three or more authors with the same address:
 % \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
 %  \Name{Author Name2} \Email{an2@sample.com}\\
 %  \Name{Author Name3} \Email{an3@sample.com}\\
 %  \addr Address}


 % Authors with different addresses:
 \coltauthor{\Name{Marco Mondelli} \Email{mondelli@stanford.edu}\\
 \addr Department of Electrical Engineering, Stanford University
 \AND
 \Name{Andrea Montanari} \Email{montanari@stanford.edu}\\
 \addr Department of Electrical Engineering and Department of Statistics, Stanford University
 }

\begin{document}

\maketitle

\begin{abstract}
In phase retrieval we want to recover an unknown signal $\bx\in\complex^d$ from $n$ quadratic measurements of the form 
$y_i = |\<\ba_i,\bx\>|^2+w_i$ where $\ba_i\in \complex^d$ are known sensing vectors and  $w_i$ is measurement noise. We ask the following
\emph{weak recovery} question: what is the minimum number of measurements $n$ needed to produce an estimator $\hat{\bx}(\by)$ that is positively correlated with 
the signal $\bx$?
We consider the case of Gaussian  vectors $\ba_i$. We prove that -- in the high-dimensional limit -- a sharp phase transition takes place, and we locate the threshold in the regime of vanishingly small noise.
For $n\le d-o(d)$ no estimator can do significantly better than random and achieve a strictly positive correlation.
For $n\ge d+o(d)$ a simple spectral estimator achieves a positive correlation. 
Surprisingly, numerical simulations with the same spectral estimator  demonstrate promising performance with realistic sensing matrices. 
Spectral methods are used to initialize non-convex optimization algorithms in phase retrieval, and our approach can boost the performance in this setting as well.

Our impossibility result is based on classical information-theory arguments. The spectral algorithm computes the leading eigenvector
of a weighted empirical covariance matrix. We obtain a sharp
characterization of the spectral properties of this random matrix  using tools from free probability and generalizing
a recent result by Lu and Li. 
Both the upper and lower bound generalize beyond phase retrieval to measurements $y_i$ produced according to a generalized linear model. 
As a byproduct of our analysis, we compare the threshold of the proposed spectral method with that of a message passing algorithm. 
\end{abstract}

\begin{keywords}
Spectral initialization, phase transition, mutual information, second moment method, phase retrieval, free probability
\end{keywords}

%----------------------------------------------------
%\section{Introduction} \label{sec:intro}
%----------------------------------------------------

% thresholds for high-dimensional statistical estimation problems

In this work\footnote{Extended abstract. Full version appears as [arXiv:1708.05932, v3].}, we consider the problem of recovering a signal $\bx$ of dimension $d$, given $n$ \emph{generalized linear measurements}. More specifically, the measurements are drawn independently according to the conditional distribution
\begin{equation}\label{eq:model}
y_i\sim p(y\mid |\langle\bx, \ba_i \rangle|), \qquad i\in\{1, \ldots, n\},
\end{equation}
where $\langle\cdot, \cdot\rangle$ denotes the inner product, $\{\ba_i\}_{1\le i\le n}$ is a set of known sensing vector, and $p(\cdot \mid \langle\bx, \ba_i \rangle)$ is a known probability density function. For the problem of \emph{phase retrieval}, the model \eqref{eq:model} is specialized to 
\begin{equation}\label{eq:phretr}
y_i = |\langle\bx, \ba_i\rangle|^2+w_i, \qquad i\in\{1, \ldots, n\}\, ,
\end{equation}
where $w_i$ is noise. 
%Applications of phase retrieval arise in several areas of science and engineering, including X-ray crystallography \citep{M90, H93}, microscopy \citep{miao2008extending}, astronomy \citep{fienup1987phase}, optics \citep{walther1963question}, acoustics \citep{balan2006signal}, interferometry \citep{demanet2017convex}, and quantum mechanics \citep{corbett2006pauli}.
 
Popular methods to solve the phase retrieval problem are based on semi-definite programming relaxations \citep{candes2015phase,
  candes2015phase2, candes2013phaselift,  waldspurger2015phase}. However, these algorithms rapidly become prohibitive from a computational point of view when the dimension $d$
of the signal increases, which makes them impractical in most of the real-world applications. For this reason, several algorithms have been
developed in order to solve directly the non-convex least-squares problem, including the error reduction schemes dating back to
Gerchberg-Saxton and Fienup \citep{gerchberg1972practical, phFienup}, alternating minimization \citep{netrapalli2013phase}, 
approximate message passing \citep{schniter2015compressive}, Wirtinger Flow \citep{candes2015wirt}, iterative projections \citep{li2015phase}, the Kaczmarz method
\citep{wei2015solving}, and a number of other approaches \citep{chen2017solving, zhang2016reshaped, cai2016optimal, wang2016solving, wang2016solvingnips, soltanolkotabi2017structured,duchi2017solving, wang2017solving}. Furthermore, recently a convex relaxation that operates in the natural domain of the signal was independently proposed by two groups of authors \citep{goldstein2016phasemax, bahmani17a}. All these techniques require an initialization step, whose goal is to provide a solution $\hat{\bx}$ that is positively correlated with the unknown signal $\bx$. To do so, spectral methods are widely employed: the estimate $\hat{\bx}$ is given by the principal eigenvector of a suitable matrix constructed from the data. 
A similar stategy (initialization step followed by an iterative algorithm) has proved successful for many other estimation problems, e.g., matrix completion \citep{keshavan2010matrix, jain2013low}, blind deconvolution \citep{lee2017blind, li2016rapid}, sparse coding \citep{arora2015simple} and joint alignment from pairwise noisy observations \citep{chen2016projected}. 

We focus on a regime in which both the number of measurement $n$ and the dimension of the signal $d$ tend to infinity, but their ratio $n/d$ 
tends to a positive constant $\delta$. The \emph{weak recovery} problem requires to provide an estimate $\hat{\bx}(\by)$ that has a positive correlation with the unknown vector $\bx$:
%
\begin{equation}\label{eq:poscorr}
\liminf_{n\to\infty}\E\bigg\{\frac{|\langle\hat{\bx}(\by), \bx\rangle|}{\norm{\hat{\bx}(\by)}_2 \norm{\bx}_2} \bigg\}> \epsilon,
\end{equation}
for some $\epsilon >0$. 

In this paper, we consider either $\bx\in \reals^d$ or $\bx\in\complex^d$ and assume that the measurement vectors
$\ba_i$ are standard Gaussian (either real or complex). In the general setting of model (\ref{eq:model}), we present two types of results:
%
\begin{enumerate}
\item We develop an \emph{information-theoretic lower bound} $\delta_{\ell}$: for $\delta < \delta_{\ell}$, no estimator can output non-trivial estimates.
In other words, the weak recovery problem cannot be solved. 
%
\item We establish an  \emph{upper bound} $\delta_{\rm u}$ based on a \emph{spectral algorithm}:
for $\delta > \delta_{\rm u}$, we can achieve weak recovery (see~\eqref{eq:poscorr}) by letting $\hat{\bx}$ be the principal
eigenvector of a matrix suitably constructed from the data.
%
\end{enumerate}
%
The values of the thresholds $\delta_{\ell}$ and $\delta_{\rm u}$ depend on the conditional distribution $p(\cdot \mid |\langle\bx, \ba_i \rangle|)$, and we provide analytic formulas to compute them. More formally, consider the function $f : [0, 1]\to {\mathbb R}$, given by 
\begin{equation}\label{eq:fnorm}
f(m) = \bigintssss_{\mathbb R}\frac{{\mathbb E}_{G_1, G_2}\left\{p(y\mid |G_1|)p(y\mid |G_2|)\right\}}{{\mathbb E}_{G}\left\{p(y\mid |G|)\right\}} \,{\rm d}y,
\end{equation}
with 
\begin{equation}
G\sim \cnormal(0, 1),\qquad (G_1, G_2)\sim\cnormal\left(\b0_2, \left[\begin{array}{ll}
1 & c \\ c^* & 1\\ 
\end{array}\right]\right),
\end{equation}
and $m=|c|^2$. Note that the RHS of \eqref{eq:fnorm} depends only on $m=|c|^2$. Indeed, by applying the transformation $(G_1, G_2)\to (e^{i\theta_1}G_1, e^{i\theta_2}G_2)$, $f(m)$ does not change, but the correlation coefficient $c$ is mapped into $ce^{i(\theta_1-\theta_2)}$. Furthermore, set
\begin{equation}\label{eq:defF}
F_{\delta}(m) = \delta \log f(m) + \log (1-m).
\end{equation}
Note that, when $m=0$, $G_1$ and $G_2$ are independent. Hence, $f(0)=1$, which implies that $F_{\delta}(0)=0$ for any $\delta >0$. We define the information-theoretic threshold $\delta_{\ell}$ as the largest value of $\delta$ such that the maximum of $F_{\delta}(m)$ is attained at $m=0$, i.e., 
\begin{equation}\label{eq:defdelta}
\delta_{\ell} = \sup \{\delta \mid F_{\delta}(m) < 0 \mbox{ for }m\in (0, 1]\}.
\end{equation}
The spectral threshold $\delta_{\rm u}$ is defined as
\begin{equation}\label{eq:defdeltau}
\delta_{\rm u} = \frac{1}{\displaystyle\bigintssss_{\mathbb R}\frac{\left({\mathbb E}_{G}\left\{p(y\mid |G|)(|G|^2-1)\right\}\right)^2}{{\mathbb E}_{G}\left\{p(y\mid |G|)\right\}} \,{\rm d}y},
\end{equation}
with $G\sim \cnormal(0, 1)$.

The main result of this paper can be summarized as follows.


\begin{theorem*} 
Let $\bx\in \mathbb C^d$ be chosen uniformly at random on the $d$-dimensional complex sphere with radius $\sqrt{d}$ and assume that $\{\ba_i\}_{1\le i\le n}\sim_{i.i.d.}\cnormal(\b0_d,\id_d/d)$.
Let $\by\in \mathbb R^n$ be drawn independently according to \eqref{eq:model}, and $n, d\to \infty$ with $n/d\to \delta \in (0, +\infty)$. Then,
\begin{itemize}
\item For $\delta <\delta_\ell$, no algorithm can provide non-trivial estimates on $\bx$;

\item For $\delta > \delta_{\rm u}$, there exists a spectral algorithm that returns an estimate $\hat{\bx}$ satisfying \eqref{eq:poscorr}. 
\end{itemize}
\end{theorem*}

For the special case of phase retrieval (see \eqref{eq:phretr}), we evaluate the thresholds $\delta_\ell$ and $\delta_{\rm u}$, and we show that they coincide
in the limit of vanishing noise.

%
\begin{theorem*} 
Let $\bx\in \mathbb C^d$ be chosen uniformly at random on the $d$-dimensional complex sphere with radius $\sqrt{d}$, and assume that $\{\ba_i\}_{1\le i\le n}\sim_{i.i.d.}\cnormal(\b0_d,\id_d/d)$.
Let $\by\in \mathbb R^n$ be drawn independently according to \eqref{eq:phretr}, with $\{w_i\}_{1\le i\le n}\sim\normal(0,\sigma^2)$, and $n, d\to \infty$ with $n/d\to \delta \in (0, +\infty)$. Then,
\begin{itemize}
\item For $\delta <1$, no algorithm can provide non-trivial estimates on $\bx$;

\item For $\delta > 1$, there exists $\sigma_0(\delta)>0$ and  a spectral algorithm that returns an estimate $\hat{\bx}$ satisfying \eqref{eq:poscorr}, for any $\sigma\in [0,\sigma_0(\delta)]$. 
\end{itemize}
\end{theorem*}
%
When $\bx$ is chosen uniformly at random on the $d$-dimensional \emph{real} sphere with radius $\sqrt{d}$ and $\{\ba_i\}_{1\le i\le n}\sim_{i.i.d.}\normal(\b0_d,\id_d/d)$, we show that analogous results hold and that the threshold for phase retrieval moves from $1$ to $1/2$. This is reminiscent of how the injectivity thresholds
are $\delta=4$ and $\delta=2$ in the complex and the real case, respectively \citep{balan2006signal, bandeira2014saving, conca2015algebraic}. A possible intuition for this halving phenomenon comes from the fact that the complex problem has twice as many variables but the same amount of equations of the real problem. Hence, it is reasonable that the complex case requires twice the amount of data with respect to the real case. 

The lower bound is proved by estimating the conditional entropy via the second moment method.

The spectral algorithm computes the eigenvector  corresponding to the largest eigenvalue of a matrix of the form:
%
\begin{equation}
\bD_n = \frac{1}{n}\sum_{i=1}^n \mathcal T (y_i) \ba_i \ba_i^*\, ,\label{eq:D_Matrix}
\end{equation}
%
where $\mathcal T: \mathbb R\to \mathbb R$ is a pre-processing function. 
For $\delta$ large  enough (and a suitable choice of $\mathcal T$), we expect the resulting eigenvector $\hat{\bx}(\by)$ to be positively 
correlated with the true signal $\bx$. 
The recent paper \citep{lulispectral_arxiv}  computed exactly the threshold value $\delta_{\rm u}$, 
under the assumption that the measurement vectors are real Gaussian, and $\cT$ is non-negative.




Here, we generalize the result of \citep{lulispectral_arxiv} by removing the assumption that  $\mathcal T(y)\ge 0$ and by considering the complex case. 
Armed with this result, we compute the optimal\footnote{Here optimality is understood with respect to  the weak recovery threshold.} 
pre-processing function $\cT^*_{\delta}(y)$ for the general model (\ref{eq:model}). Our upper bound
$\delta_u$ is the phase transition location for this optimal spectral method.  
In the  case of phase retrieval (as $\sigma\to 0$),  this pre-processing function is given by
%
\begin{equation}
\mathcal T^*_{\delta}(y) = \frac{y-1}{y+\sqrt{\delta}-1}, \label{eq:T-Phase-Retrieval}
\end{equation}
%
and achieves weak recovery for any $\delta>\delta_u=1$. 
In the limit $\delta\downarrow 1$, this converges to the limiting function $\cT^*(y) = 1-(1/y)$.

While the expression (\ref{eq:T-Phase-Retrieval}) is remarkably simple, it is somewhat counter-intuitive. Earlier methods 
\citep{candes2015wirt,chen2015solving,lulispectral_arxiv}  use $\cT(y)\ge 0$ and try to extract information from the large values of $y_i$. The function (\ref{eq:T-Phase-Retrieval})
has a large negative part for small $y$, in particular when $\delta$ is close to $1$. Furthermore, it extracts useful information from data
points with $y_i$ small. One possible interpretation is that the points in which the measurement vector is basically orthogonal to the unknown signal are not informative, hence we penalize them. 

%A popular method to solve statistical estimation problems consists in employing approximate message-passing (AMP) algorithms \citep{DMM09} that apply ideas from graphicals models (belief propagation \citep{pearl2014probabilistic}) and statistical physics (mean field or TAP equations \citep{mezard1987spin, MezardMontanari}). In particular, when the data is sampled according to \eqref{eq:model}, a generalized AMP (GAMP) algorithm was proposed in \citep{RanganGAMP}. We show that the threshold $\delta_{\rm u}$ of the proposed spectral method is intimately related to the performance of GAMP, when the signal and the measurement vectors are real. We consider generalized linear models in which the state evolution (SE) that tracks the performance of GAMP has a fixed point at $0$. Then, we show the following results: 
%\begin{enumerate}

%\item For $\delta < \delta_{\rm u}$, the GAMP algorithm converges to the trivial fixed point at $0$ even if the initial condition given to the algorithm has a positive correlation with the unknown vector $\bx$;

%\item For $\delta > \delta_{\rm u}$, the linear system obtained from the GAMP equations in a neighborhood of $0$ is unstable, which means that we escape from the fixed point at $0$. 
%\end{enumerate}


Our analysis applies to Gaussian measurement matrices. However, the proposed spectral method works well also on real images and realistic
measurement matrices. %To illustrate this fact, in Figure \ref{fig:venrec} we test our algorithm on a digital photograph of the painting ``The birth of Venus'' by Sandro Botticelli. We consider a type of measurements that falls under the category of coded diffraction patterns (CDP) \citep{candes2015phase2, chen2017solving}: the measurement matrix is given by the product of $\delta$ copies of a Fourier matrix and a diagonal matrix with entries i.i.d. and uniform in $\{1, -1, i, -i\}$, where $i$ denotes the imaginary unit. We compare our method with the truncated spectral initialization proposed in \citep{chen2017solving}, which consists in discarding the measurements larger than an assigned threshold and leaving the others untouched. The proposed choice of the pre-processing function allows to recover a good estimate of the original image already when $\delta = 4$, while the truncated spectral initialization of \citep{chen2017solving} requires $\delta=12$ to obtain similar results.  As mentioned above, these estimates are to be used as initializations in a non-convex optimization algorithm that  improves the reconstruction error.

We also compare our spectral approach to message passing algorithms. In particular, we prove that, for $\delta<\delta_{\rm u}$ (i.e. in the regime in which the spectral approach fails), message passing converges to an un-informative fixed point, even if initialized in a state that is correlated with the true signal $\bx$. Vice versa, for $\delta>\delta_{\rm u}$ (i.e. in the regime in which the spectral approach achieves weak recovery), we consider a linearized message passing algorithm, and prove that the un-informative fixed point is unstable.

%The rest of the paper is organized as follows. In Section \ref{sec:main_complex}, after introducing the necessary notation, we define formally the problem. We then state our general information-theoretic lower bound and our spectral upper bound for the case of complex signal $\bx$ and complex measurement vectors $\{\ba_i\}_{1\le i\le n}$. In Section \ref{sec:sketch} we provide a sketch of the proofs. 

%The main results for the real case are stated in Appendix \ref{sec:main_real}. The full proof of the information-theoretic lower bound is presented in Appendix \ref{sec:mainproof}, and the full proof of the spectral lower bound is presented in Appendix \ref{sec:spectralproof}. Some auxiliary lemmas are stated and proved in Appendix \ref{app:distribution}. The comparison between the spectral approach and message passing is contained in Appendix \ref{sec:amp}. Eventually, in Appendix \ref{sec:num}, we present some numerical simulations that illustrate the behavior of the proposed spectral method for the phase retrieval problem. 


\bibliography{all-bibliography}


\end{document}
