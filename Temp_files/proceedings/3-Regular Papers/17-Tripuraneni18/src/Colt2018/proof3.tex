%!TEX root = main.tex
\subsection{Proofs in \mysec{pfsketch3}}
Finally, we would like to asymptotically understand the evolution of the averaged vector $\tilde{\Delta}_n = R^{-1}_{x_\star}(\tilde{x}_n)$, where
$\tilde{x}_n$ is the online, streaming iterate average.
From \eq{ave_grad_desc} we see that $\tilde{\Delta}_{n+1} = F_{\bar{x}_n, x_\star}[\frac{1}{n+1} F^{-1}_{\bar{x}_n, x_\star}(\Delta_{n+1})] = \tilde{F}(\Delta_{n+1})$, defining
$\tilde{F}(\cdot) = F_{\bar x_n,x_\star}[\frac{1}{n+1} F_{\bar x_n,x_\star}^{-1}(\cdot)]$.

We first start with a lemma controlling $\Vert \tilde{\Delta}_n \Vert$, when $x_n$ locally converges to $x_\star$.
\begin{lemma} \label{lem:avg_iters}
  Let Assumptions  \ref{assump:slowrate} and \ref{assump:manifold}  hold. Consider $x_n$ and $\tilde{x}_n$, which are a sequence of iterates evolving as in  \eq{grad_desc} and \eq{ave_grad_desc}, and define
  $\tilde {\Delta}_{n} = R_{x_\star}^{-1}(\tilde {x}_{n})$. Then,
  $\E [\Vert \tilde{\Delta}_n \Vert^2] = O(\gamma_n)$ as well.
\end{lemma}
\begin{proof}
  By Assumption \ref{assump:manifold}, the function $x \to \Vert R_{x_\star}^{-1}(x) \Vert^2$ is retraction convex in $x$.
  Then,
  \begin{align}
    \Vert R_{x_\star}^{-1}(\tilde{x}_n) \Vert^2 = \Vert R_{x_\star}^{-1} \left(R_{\tilde{x}_{n-1}}(\frac{1}{n} R^{-1}_{\tilde{x}_{n-1}}(x_n)) \right) \Vert^2 \leq \frac{n-1}{n} \Vert R_{x_\star}^{-1} \left(x_{n-1}\right) \Vert^2 + \frac{1}{n} \Vert R_{x_\star}^{-1} \left(\tilde{x}_{n-1}\right) \Vert^2. \notag
  \end{align}
  A simple inductive argument then shows that $\Vert R_{x_\star}^{-1}(\tilde{x}_n) \Vert^2 \leq \frac{1}{n} \sum_{i=0}^{n} \Vert R^{-1}_{x_\star}(x_i)\Vert^2$. Using that $\E \Vert \Delta_n \Vert^2 = O(\gamma_n)$ (from Assumption \ref{assump:slowrate}),
  and taking expectations shows $\E[\Vert \tilde{\Delta}_n \Vert^2] \leq \frac{C}{n} \sum_{i=0}^{n} \gamma_i \leq C \gamma_n$ when we choose a step-size sequence of the form $\gamma_n = \frac{C}{n^{\alpha}}$.
  \end{proof}
Finally using an asymptotic expansion we can show that $\tilde{\Delta}_n$ and $\bar{\Delta}_n$ approach each other:
\begin{lemma} \label{lem:stream_avg_iters}
  Let Assumptions \ref{assump:slowrate} and \ref{assump:manifold} hold. As before, consider $x_n$ and $\tilde{x}_n$, which are a sequence of iterates evolving as in  \eq{grad_desc} and \eq{ave_grad_desc}, and define
  $\tilde {\Delta}_{n} = R_{x_\star}^{-1}(\tilde {x}_{n})$. Then,
  \[ \tilde{\Delta}_n=\bar{\Delta} + e_n, \]
where  $\E [\Vert e_n \Vert] = O(\gamma_n)$.
\end{lemma}
\begin{proof}
  A similar chain rule computation to Lemma \eqref{lem:tangent_rec} shows that $d \tilde{F}(\Delta) = \frac{1}{n+1}\idm_{T_{x_\star}\M}$.
  Now, in addition to $\tilde {\Delta}_{n+1} = F_{\bar{x}_n, x_\star}[\frac{1}{n+1} F^{-1}_{\bar{x}_n, x_\star}(\Delta_{n+1})] = \tilde{F}(\Delta_{n+1})$,
  we also have that $\tilde{\Delta}_n = \tilde{F}(\tilde{\Delta}_n)$ identically.
  As $\tilde{F}(\cdot)$ is a mapping between vector spaces applying a Taylor expansion to the first expression about
  $\tilde{\Delta}_n$
  gives:
  \begin{align}
    \tilde{\Delta}_{n+1} & = \tilde {\Delta}_{n} + \frac{1}{n+1}(\Delta_{n+1}-\tilde{\Delta}_n) + O(D^2 \tilde{F}(\Delta) \Vert \Delta_{n+1}-\tilde{\Delta}_n \Vert^2).
  \end{align}
  for $\Delta \in R_{x_\star}^{-1}(\X)$.
  Since $\tilde{F}$ is twice-continuously differentiable and $R_{x_\star}^{-1}(\X)$ is compact,
  direct computation of the Hessian using the chain and Leibniz rules shows
  \begin{align}
  \tilde{e}_n = O \left((n+1) D^2 \tilde{F}(\Delta) \Vert \Delta_{n+1}-\tilde{\Delta}_n \Vert^2 \right) =
  O\left((n+1) \left(\frac{1}{(n+1)^2} + \frac{1}{n+1} \right) \cdot \Vert \Delta_{n+1} - \tilde{\Delta}_n \Vert^2 \right),
  \notag
  \end{align}
  which implies that
  \[
   \E \Vert \tilde{e}_n \Vert= O({\gamma_n}),\]
  since both $\E [\Vert \Delta_n \Vert^2] = O(\gamma_n)$ and $\E [\Vert \tilde{\Delta}_n\Vert^2] = O(\gamma_n)$ by Lemma \ref{lem:avg_iters}.
  Therefore $ (n+1)  \tilde{\Delta}_{n+1}  = n   \tilde{\Delta}_{n}+ \Delta_{n+1}+e_n = \sum_{k=0}^{n+1}\Delta_k + \sum_{k=0}^{n+1} \tilde e_k \implies
  \tilde{\Delta}_{n+1} = \bar{\Delta}_{n+1} +  e_{n+1}$ where $e_{n+1}=\frac{\sum_{k=0}^{n+1} \tilde e_k}{n+1}$, and $\E[\Vert e_{n+1} \Vert]= \E\big[\big\Vert \frac{\sum_{k=0}^{n+1} \tilde e_k}{n+1} \big\Vert\big] \leq \frac{1}{n+1} \sum_{i=0}^{n} \E[\Vert \tilde e_k \Vert] = O(\gamma_n)$.
\end{proof}
This result states that the distance between the streaming average $\tilde{\Delta}_n = R_{x_\star}^{-1}(\tilde{x}_n)$ is close to the computationally intractable $\bar{\Delta}_n$ up to $O(\gamma_n)$ error.

We can prove a slightly stronger statement under a 4th-moment assumption on the iterates that follows identically to the above.
\begin{lemma} \label{lem:stream_avg_iters_4mom}
  Let Assumption \ref{assump:manifold} hold, and assume the 4th-moment bound $\E[\Vert \Delta_n \Vert^4] = O(\gamma_n^2)$. As before, consider $x_n$ and $\tilde{x}_n$, which are a sequence of iterates evolving as in  \eq{grad_desc} and \eq{ave_grad_desc}, and define
  $\tilde {\Delta}_{n} = R_{x_\star}^{-1}(\tilde {x}_{n})$. Then,
  \[ \E \left[ \Vert \tilde{\Delta}_n-\bar{\Delta} \Vert^2 \right] = O(\gamma_n^2). \]
\end{lemma}
\begin{proof}
  The proof is almost identical to the proofs of Lemma \ref{lem:avg_iters} and \ref{lem:stream_avg_iters} so we will be brief. Since the function $x \to x^2$ is convex
  and nondecreasing over positive support, using Assumption~\ref{assump:manifold}, the composition $x \to \Vert R_{x_\star}^{-1}(x) \Vert^4$ is also retraction-convex in $x$. An identical argument to the proof
  of Lemma \ref{lem:avg_iters} then shows that $\E[\Vert \Delta_n \Vert^4] = O(\gamma_n^2)$ implies $\E\left[\Vert \tilde{\Delta}_n \Vert^4\right] = O(\gamma_n^2)$. Using that $\E\left[\Vert \tilde{\Delta}_n \Vert^4\right] = O(\gamma_n^2)$, a nearly identical calculation
  to Lemma \ref{lem:stream_avg_iters} and an application of Minkowski's inequality (in $L_2$) shows that $\sqrt{\E \left[\Vert \bar{\Delta}_n - \tilde{\Delta}_n \Vert^2\right]} = O(\gamma_n)$. The conclusion follows.
\end{proof}
