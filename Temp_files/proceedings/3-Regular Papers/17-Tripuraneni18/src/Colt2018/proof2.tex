%!TEX root = main.tex
\subsection{Proofs in \mysec{pfsketch2}} \label{sec:conv_rates}
Here we provide proofs, in the Euclidean setting, of both asymptotic and non-asymptotic Polyak-Ruppert-type averaging results. We apply these results to the tangent vectors $\Delta \in T_{x_\star}\M$ as described in \mysec{pfsketch1}.
\subsubsection{Asymptotic Convergence}
Throughout this section, we will consider a general linear recursion perturbed by a remainder term $e_n$ of the form:
\begin{align}
  \D_{n}=\D_{n-1} -\gamma_n A \D_{n-1}+ \gamma_n (\eps_n+\xi_{n}+e_{n}), \label{eq:rec_with_error1}
\end{align}
for which we will show an asymptotic convergence result under appropriate conditions.  

Note that we eventually apply these convergence results to iterates $\Delta_n \in T_{x_\star} \M$, which is a finite-dimensional vector space. In this setting, a probability measure can be defined on a vector space (with inner product) with a covariance operator implicitly depending on the inner product (via the dual map). 

We make the following assumptions on the structure of the recursion:
\begin{assumption} \label{assump:psd}
  $A$ is symmetric positive-definite matrix.
\end{assumption}
\begin{assumption} \label{assump:noise1}
    The noise process $\{ \eps_{n} \}$ is a martingale-difference process (with  $\E[\eps_n\vert\mathcal F_{n-1}]=0$  and  $\sup_n \E[\eps_n^2] < \infty$), for which    there exists $C>0$ such that $\E [\Vert \eps_n \Vert^4 | \mathcal{F}_{n-1}] \leq C$ for all $n \geq 0$ and a matrix $\Sigma \succ 0$ such that
    \[\E[\eps_n\eps_n^\top\vert\mathcal F_{n-1}]\overset{P}{\to} \Sigma.\]
\end{assumption}
\begin{assumption} \label{assump:noise2}
    The noise process $\{ \xi_{n} \}$ is a martingale-difference process (with  $\E[\xi_n\vert\mathcal F_{n-1}]=0$ and $\sup_n \E[\xi_n^2] < \infty$), and for sufficiently large $n \geq N$, there exists $K > 0$ such that
    \[ \E[\Vert \xi_n\Vert^2\vert\mathcal F_{n-1}] \leq K\gamma_n \text{ a.s. } \]
    with $\gamma_n \to 0$ as $n \to \infty$.
\end{assumption}
\begin{assumption} \label{assump:remainder}
  For $n \geq 0$
    \[
    \E[\Vert e_n \Vert] = O(\gamma_n).
    \]
\end{assumption}
\begin{assumption}\label{assump:step_size}
  $\gamma_n \to 0$, $\frac{\gamma_n-\gamma_{n-1}}{\gamma_n} = o(\gamma_n)$ and $\sum_{j=1}^{\infty} \frac{\gamma_j}{\sqrt{j}} < \infty$.
\end{assumption}
The first two conditions in Assumption \ref{assump:step_size} require that $\gamma_n$ decrease sufficiently slowly. For example $\gamma_n = \gamma t^{-\alpha}$ with $ \frac{1}{2} < \alpha < 1$ satisfy these two conditions
but the sequence $\gamma = \gamma t^{-1}$ does not.

We can now derive the asymptotic convergence rate,
\begin{theorem} \label{thm:asymp_ave}
  Let Assumptions \ref{assump:psd}, \ref{assump:noise1}, \ref{assump:noise2}, \ref{assump:remainder} and \ref{assump:step_size}
  hold for the perturbed linear recursion in Equation \eqref{eq:rec_with_error1}.
  Then,
\[
\sqrt n \bar{\Delta}_n  \overset{D}{\to} \mathcal N (0,  A^{-1}\Sigma A^{-1}).
\]
\end{theorem}
\begin{proof}
The argument mirrors the proof of Theorem 2 in \citet{polyak1992acceleration} so we only
sketch the primary points. Throughout we will use $C$ to denote an unimportant, global constant that may change line to line.

Consider the purely linear recursion of the form:
\begin{align}
  & \D^1_{n}=\D^1_{n-1} -\gamma_n A \D^1_{n-1}+ \gamma_n (\eps_n+\xi_{n}) \label{eq:linear_rec} \\
  & \bar{\D}^1_{n} = \frac{1}{n} \sum_{i=0}^{n-1} \D^1_{i}, \nonumber
\end{align}
which satisfies $\D^1_{0} = \D_{0}$, and approximates the perturbed recursion in Equation \eqref{eq:rec_with_error1},
\begin{align}
  & \D_{n}=\D_{n-1} -\gamma_n A \D_{n-1}+ \gamma_n (\eps_n+\xi_{n}) + \gamma_n e_{n} \label{eq:linear_rec_perturb} \\
  & \bar{\D}_{n} = \frac{1}{n} \sum_{i=0}^{n-1} \D_{i}. \nonumber
\end{align}
Now, note that we can show that
$\lim_{K \to \infty} \lim \sup_{n} \E \left[\Vert \eps_n \Vert^2 \mathbb{I}[\Vert \eps_n \Vert > K]|\F_{n-1} \right] \overset{p}{\to} 0$, using our (conditional) 4th-moment bound and the (conditional) Cauchy-Schwarz/Markov inequalities, so the relevant assumption in \citet{polyak1992acceleration} is satisfied. Then as the argument in Part 3 of the proof of Theorem 2 in
\citet{polyak1992acceleration} shows, under Assumptions \ref{assump:psd}, \ref{assump:noise1}, \ref{assump:noise2}
the conditions of Proposition (a) of Theorem 1 in \citet{polyak1992acceleration} also hold. This implies the linear process
satisfies:
\[
\sqrt n \bar{\D}^1_n  \overset{D}{\to} \mathcal N (0,  A^{-1}\Sigma A^{-1}). \notag
\]
We now argue that the process $\bar{\D}^1_n$ and $\bar{\D}_n$ are asymptotically equivalent in distribution. First,
since the noise process is coupled between Equations \ref{eq:linear_rec} and \ref{eq:linear_rec_perturb}, the differenced process
obeys a simple (perturbed) linear recursion,
\[
  \D_n - \D^1_{n} = (I-\gamma_j A)(\D_{n-1}-\D^1_{n-1}) - \gamma_n e_n. \notag
\]
Expanding and averaging this recursion (defining $\delta_n = \bar{\D}_n-\bar{\D}^1_{n}$) gives:
\begin{align}
   & \D_n-\D^1_{n} = \sum_{j=1}^{n} \Pi_{i=j+1}^{n}(I-\gamma_j A) \gamma_j e_j \implies \delta_n = \frac{1}{n} \sum_{k=1}^{n-1} \sum_{j=1}^{k}[\Pi_{i=j+1}^{k}(I-\gamma_i A)] \gamma_j e_j \notag \\
   \implies
   & \delta_n = \frac{1}{n} \sum_{j=1}^{n-1} \left[ \sum_{k=j}^{n-1} \Pi_{i=j+1}^{k}(I-\gamma_i A) \right] \gamma_j e_j. \notag
\end{align}
We can rewrite the recursion for this averaged differenced process as:
\[
  \sqrt{n}\delta_n =  \frac{1}{\sqrt{n}} \sum_{j=1}^{n-1}(A^{-1} + w_j^n)e_j,
\]
defining,
\[
  w_j^n = \gamma_j \sum_{i=j}^{n-1} \Pi_{k=j+1}^{i} (I-\gamma_k A) - A^{-1}.
\]
Now if the step-size sequence satisfies the first two conditions of Assumption \ref{assump:step_size}, by Lemma 1 and 2 in \citet{polyak1992acceleration}
we have that $\Vert w_j^n \Vert \leq C$ uniformly. So using Assumption \ref{assump:psd} we obtain that:
\[
  \sum_{j=1}^{\infty} \frac{1}{\sqrt{j}} \Vert (A^{-1}+w_j^t) e_j\Vert \leq C \sum_{j=1}^{\infty} \frac{1}{\sqrt{j}} \Vert e_j \Vert.
\]
An application of the Tonelli-Fubini theorem and Assumption \ref{assump:remainder} then shows that
\[
  \E[\sum_{j=1}^{\infty} \frac{1}{\sqrt{j}} \Vert e_j \Vert] = \sum_{j=1}^{\infty} \frac{1}{\sqrt{j}} \E [\Vert e_j \Vert] \leq C \sum_{j=1}^{\infty} \frac{\gamma_j}{\sqrt{j}} < \infty,
\]
by choice of the step-size sequence in Assumption \ref{assump:step_size}. Since $\sum_{j=1}^{\infty} \frac{1}{\sqrt{j}} \Vert e_j \Vert \geq 0$ and has finite expectation it must be that,
\[
  \sum_{j=1}^{\infty} \frac{1}{\sqrt{j}} \Vert e_j \Vert < \infty \implies \sum_{j=1}^{\infty} \frac{1}{\sqrt{j}}\Vert (A^{-1} + w_j^n)e_j \Vert < \infty.
\]
An application of the Kronecker lemma then shows that
\[
  \frac{1}{\sqrt{n}} \sum_{j=1}^{n-1} \Vert (A^{-1} + w_j^n)e_j \Vert \to 0 \implies \sqrt{n} \delta_n \to 0 \text{ a.s.}
\]
The conclusion of theorem follows by Slutsky's theorem.
\end{proof}
\subsubsection{Nonasymptotic Convergence}
Throughout this section, we will consider a general linear recursion perturbed by remainder terms $e_n$ of the form:
\begin{align}
  \D_{n}=\D_{n-1} -\gamma_n A \D_{n-1}+ \gamma_n (\eps_n+\xi_{n}+e_{n}). \label{eq:rec_with_error2}
\end{align}
We make the following assumptions on the structure of the recursion:
\begin{assumption} \label{assump:psd2}
  $A$ is symmetric positive-definite matrix, such that $A\succcurlyeq \mu \idm$ for $\mu > 0$.
\end{assumption}
\begin{assumption} \label{assump:noise1na}
    The noise process $\{ \eps_{n} \}$ is a martingale-difference process (with  $\E[\eps_n\vert\mathcal F_{n-1}]=0$  and  $\sup_n \E[\eps_n^2] < \infty$) and a matrix $\Sigma \succ 0$ such that
    \[\E[\eps_n\eps_n^\top\vert\mathcal F_{n-1}] \preccurlyeq \Sigma.\]
\end{assumption}
\begin{assumption} \label{assump:noise2na}
    The noise process $\{ \xi_{n} \}$ is a martingale-difference process (with  $\E[\xi_n\vert\mathcal F_{n-1}]=0$ and $\sup_n \E[\xi_n^2] < \infty$), and there exists $K > 0$ such that for $n\geq 0$
    \[ \E[\Vert \xi_n\Vert^2\vert\mathcal F_{n-1}] \leq K\gamma_n \text{ a.s. } \]
\end{assumption}
\begin{assumption} \label{assump:remainderna}
There exists $M$ such that for $n \geq 0$, $\E[\Vert e_n \Vert^2] \leq M \gamma_n^2$.
\end{assumption}
\begin{assumption} \label{assump:step_sizena}
  The step-sizes take the form $\gamma_n=\frac{C}{n^\alpha}$
  for $C>0$ and $\alpha\in[1/2,1)$.
\end{assumption}
\begin{assumption} \label{assump:slowrate2}
  There exists $C' > 0$ such that for $n \geq 0$, we have that
  \[\sqrt{\E[\norm{\Delta_n}^2]} = O(\sqrt{\gamma_n}) = C' n^{-\alpha/2} \]
\end{assumption}
Using these Assumptions we can derive the non-asymptotic convergence rate:
\begin{theorem} \label{thm:nonasymp_ave}
 Let Assumptions \ref{assump:psd2}, \ref{assump:noise1na}, \ref{assump:noise2na}, \ref{assump:remainderna}, \ref{assump:step_sizena} and \ref{assump:slowrate2} hold for the recursion in Equation \ref{eq:rec_with_error2},
  \[
      \mathbb{E}[\Vert \bar{\Delta}_n \Vert ^2] \leq \frac{1}{n} \tr [A^{-1} \Sigma A^{-1}] +  O(n^{-2\alpha}) + O(n^{\alpha-2}).
      \]
\end{theorem}
\begin{proof}
The argument mirrors the proof of Theorem 3 in \citet{moulines2011non} so we only sketch the key points.
First, since $A$ is invertible due to Assumption \ref{assump:psd2}, from Equation \ref{eq:rec_with_error2}:
\[
\Delta_{n-1}= \frac{A^{-1}(\Delta_{n-1}-\Delta_{n})}{\gamma_{n}} + A^{-1}\eps_{n}+A^{-1}\xi_{n}+A^{-1}e_{n},
\]
We now analyze the average of each of the 4 terms separately. Throughout we will use $C$ to denote an unimportant, numerical constant that may change line to line.
\begin{itemize}
\item Summing the first term by parts we obtain,
\[
\frac{1}{n}\sum_{k=1}^n\frac{A^{-1} (\Delta_{k-1}-\Delta_{k})}{\gamma_{k}}=\frac{1}{n}\sum_{k=1}^{n-1}A^{-1}\Delta_k \left(\frac{1}{\gamma_{k+1}}-\frac{1}{\gamma_{k}}\right)-\frac{1}{n\gamma_n}A^{-1}\Delta_n+\frac{1}{n\gamma_1}A^{-1}\Delta_0,
\]
and using Minkowski's inequality (in $L_2$) gives,
\[
\sqrt{\E \norm{ \frac{1}{n}\sum_{k=1}^n\frac{A^{-1}(\Delta_{k-1}-\Delta_{k})}{\gamma_{k}} }^2}  \leq
\frac{1}{n \mu} \sum_{k=1}^{n-1}\sqrt{\E \Vert \Delta_k\Vert^2} \abs{\frac{1}{\gamma_{k+1}}-\frac{1}{\gamma_{k}}} +\frac{\sqrt{\E \Vert \Delta_n\Vert^2}}{n\gamma_n  \mu } +\frac{\Vert \Delta_0 \Vert}{n\gamma_1 \mu}.
\]
Since we choose a sequence of decreasing step-sizes of the form $\gamma=\frac{C}{n^{\alpha}}$ for $\alpha \in [\frac{1}{2}, 1)$, an application of the
Bernoulli inequality shows that $\vert \gamma_{k+1}^{-1}-\gamma_{k}^{-1}\vert = C^{-1} [(k+1)^\alpha-k^\alpha]\leq  C^{-1} \alpha k^{\alpha-1}$. By assumption, we have that
$\sqrt{\E \Vert \Delta_n\Vert^2}\leq Cn^{-\alpha/2}$ so,
\BEAS
\sqrt{\E \norm{\frac{1}{n}\sum_{k=1}^n\frac{A^{-1}(\Delta_{k}-\Delta_{k})}{\gamma_{k}} }^2}  &\leq& \frac{C\alpha }{n \mu } \sum_{k=1}^{n-1}  k^{\alpha/2-1}+\frac{C}{\mu} n^{\alpha/2-1}+\frac{C}{n  \mu}\Vert \Delta_0 \Vert \\
 &\leq& \frac{2C n^{\alpha/2-1}}{ \mu }+\frac{Cn^{\alpha/2-1}}{\mu} +\frac{C\Vert \Delta_0 \Vert}{n  \mu}\\
 & \leq &\frac{3Cn^{\alpha/2-1}}{\mu} +\frac{C\Vert \Delta_0 \Vert}{n  \mu}.
\EEAS
This implies that,
\[
\E \norm{ \frac{1}{n}\sum_{k=1}^n\frac{A^{-1}(\Delta_{k-1}-\Delta_{k})}{\gamma_{k}} }^2 = O(n^{\alpha-2}). \]
\item Using the  Assumption  \ref{assump:noise1na} and the orthogonality of martingale increments we immediately obtain the leading order term as,
\[
\E \Vert A^{-1} \bar \eps_n\Vert ^2\leq\frac{1}{n}\tr [A^{-1} \Sigma A^{-1}].
\]
\item Using Assumption  \ref{assump:noise2na} and the orthogonality of martingale increments we obtain,
\[
\E \Vert  A^{-1} \bar  \xi_n \Vert^2 =\frac{1}{n^2 \mu^2} \sum_{k=1}^n \E \Vert \xi_k \Vert^2 \leq  \frac{C}{n^2 \mu^2} \sum_{k=0}^{n-1} k^{-\alpha} = O(n^{-(\alpha+1)}).
\]
\item Using the Minkowski inequality (in $L_2$), and Assumption  \ref{assump:remainderna},
we have that
\[
\E \Vert A^{-1} \bar e_{n-1}\Vert^2  \leq \left(\frac{1}{n\mu}\sum_{k=1}^n  \sqrt{\E \Vert e_k \Vert^2 }\right)^2\leq \frac{M^2}{(n \mu)^2} \left(\sum_{k=1}^n k^{-\alpha}\right)^2 \leq \frac{M^2}{\mu^2}n^{-2\alpha}.
\]
\end{itemize}
The conclusion follows.
\end{proof}
\subsubsection{On the Riemannian Center of Mass} \label{sec:com}
Note that $\bar{\Delta}_n$ is \textit{not} computable, but has an interesting interpretation as an upper bound on the Riemannian center of mass (or Karcher mean),
\[
K_n = \arg \min_{x \in \M} \frac{1}{n} \sum_{i=1}^{n} \norm{R_{x}^{-1}(x_i)}^2
\]
 of a set of iterates $\{ x_i \}_{n \geq 0}$ in $\M$. When it exists, computing $K_n$ is itself a nontrivial geometric optimization problem since it does not admit a closed-form solution in general.
See  \citet[][]{moakher2002means,bini2013computing,hosseini2015matrix} for more background on the Karcher mean problem.
If we consider a ``symmetric'' retraction $R$ satisfying for $x,y\in\X$ that $\Vert R_{x}^{-1}(y)\Vert^2=\Vert R_{y}^{-1}(x)\Vert^2$ (which is the case for the exponential map for example), then
\begin{lemma} \label{lem:karcher_mean}
  Let $\{ x_i \}_{i=0}^{n}$ be a sequence of iterates contained in $\M$ and let the retraction $R$ be symmetric, then
  \[\Vert R_{x_\star}^{-1}(K_n)\Vert^2\leq2\Vert\bar \Delta_n\Vert^2.\]
\end{lemma}
\begin{proof}
  The first-order optimality condition requires that $\nabla D(K_n) = 0$ where the manifold gradient is
  given by $\nabla D(x) = \frac{1}{n} \sum_{i=1}^{n} R_{x}^{-1}(x_i)$. Thus, $\nabla D(x_\star)=\bar \Delta_n$. By Assumption \ref{assump:manifold}, the function $D$ is $1$-retraction strongly convex. Defining the function  $g: t\mapsto D \left(R_{x_\star} (t \frac{R_{x_\star}^{-1}(K_n)}{\Vert R_{x_\star}^{-1}(K_n)\Vert })\right)$, we have that at $t_0= \Vert R_{x_\star}^{-1}(K_n)\Vert$,
  \[
     2\Vert \nabla D(x_\star)\Vert^2=2(g'(t_0)-g'(0))^2 \geq  {t_0^2} =\Vert R_{x_\star}^{-1}(K_n)\Vert^2.
     \]
\end{proof}
Therefore the Riemannian center of mass will enjoy the same convergence rate as $\bar \Delta_n$ itself.
