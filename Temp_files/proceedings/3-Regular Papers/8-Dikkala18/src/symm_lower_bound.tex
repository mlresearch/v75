
\label{sec:symm-lower-bound}
In this section we provide an information theoretic lower bound to the identity testing problem on Markov chains defined in Section~\ref{sec:symmetric}. 
%will show that any algorithm that tests identity of a symmetric Markov chain requires a word of length $\Omega\left({\frac{n}{\eps}}\right)$ from the Markov chain. 
%where $n$ is the number of states in the Markov chain.
%We will use Le Cam's two point method to show our lower bound.
\begin{theorem}
\label{thm:symm-lower-bound}
There exists a constant $c > 0$ and an instance of the identity testing problem for symmetric Markov chains such that any tester on this instance requires
a word of length at least $c\frac{n}{\eps}$ as input to produce the correct output with probability $> 0.99$. %(\ref{def:mc-id-testing})
%There is an instance of the identity testing problem for symmetric Markov chain $Q$ on $n$ states 
%that requires a word of length at least $\Omega(\frac{n}{\eps})$ from $P$ to distinguish between cases 
%$P=Q$ vs. $1-\specr{\srprod{P}{Q}} \ge \eps$  with probability at least $\frac{99}{100}$, for $\eps=\omega(n^{-1/3})$.
%
%There exists a constant $c>0$,such that any algorithm which is given the description of a symmetric Markov chain $Q$ and 
%sample access to words from a symmetric Markov chain $P$ on $n$ states, and distinguishes between the cases $P \equiv Q$ 
%and $1-\specr{\srprod{P}{Q}} \ge \eps$ with probability at least $99/100$ requires at least $k \geq \frac{cn}{\eps}$ samples.
\end{theorem}
The full proof of Theorem~\ref{thm:symm-lower-bound} is given in Appendix~\ref{app:sec_symm_lb}. The high level idea is to construct a Markov chain $Q$ and a family of chains $\mathcal{P}$ such that it is hard to distinguish $Q$ from a randomly chosen $\bar{P} \in \mathcal{P}$ by only looking at trajectories of length $o(n/\eps)$. The chain $Q$ and the family $\mathcal{P}$ we work with are described below (we think of symmetric Markov chains as weighted undirected graphs with multi-edges allowed).
\begin{description}
	\item[Markov Chain $Q$:] complete double graph on $n$ vertices with uniform weights, i.e., 
	\[
	\forall \quad i\neq j \quad\quad (ij)_1, (ij)_2\in E  \quad\quad  Q_{(ij)_1}=Q_{(ij)_2}=\frac{1}{2(n-1)}.
	%Q_{(ij)_1}=\frac{1}{2(n-1)} \quad\quad Q_{(ij)_2}=\frac{1}{2(n-1)}\quad\quad \text{ for any } i\neq j.
	\]
	\item[Family $\mathcal{P}$:] for any pair of vertices $i\neq j$ there are two bidirectional edges $(ij)_1$, $(ij)_2$
	with weights randomly (and independently for each pair of $(i,j)$) chosen to be either
	\[
	P_{(ij)_1}, P_{(ij)_2}=\frac{1\pm \sqrt{8\eps}}{2(n-1)},
	\quad\quad\text{ or }\quad\quad
	P_{(ij)_1},P_{(ij)_2}=\frac{1\mp \sqrt{8\eps}}{2(n-1)}.
	\]
	%\[
	%\begin{cases}
	%P_{(ij)_1}&=\frac{1+4\sqrt{\eps}}{2n-2}, \\
	%P_{(ij)_2}&=\frac{1-4\sqrt{\eps}}{2n-2}
	%\end{cases}
	%\text{ or }
	%\begin{cases}
	%P_{(ij)_1}&=\frac{1-4\sqrt{\eps}}{2n-2}, \\
	%P_{(ij)_2}&=\frac{1+4\sqrt{\eps}}{2n-2}
	%\end{cases}  
	%\]
	%
	%
	%for every state $i$, bidirectional transition probabilities to states $j$ and $j'$, for each $j > i$, 
	%are randomly chosen to be either $\frac{1+4\sqrt{\eps}}{2n-2},\frac{1-4\sqrt{\eps}}{2n-2}$ or 
	%$\frac{1-4\sqrt{\eps}}{2n-2},\frac{1+4\sqrt{\eps}}{2n-2}$ respectively. The family consists of every 
	%Markov chain which can be generated by this process.
	%for every state $i$, bidirectional transition probabilities to states $j$ and $j'$, for each $j > i$, 
	%are randomly chosen to be either $\frac{1+4\sqrt{\eps}}{2n-2},\frac{1-4\sqrt{\eps}}{2n-2}$ or 
	%$\frac{1-4\sqrt{\eps}}{2n-2},\frac{1+4\sqrt{\eps}}{2n-2}$ respectively. The family consists of every 
	%Markov chain which can be generated by this process.
\end{description}
From the construction above it is clear that one needs to observe a number of collisions to distinguish $Q$ from a randomly chosen member of $\mathcal{P}$. The proof proceeds by a careful analysis of these collision probabilities to bound the TV distance between words of length $k$ from $Q$ and from a randomly chosen $\bar{P} \in \mathcal{P}$.
