%\subsubsection{The Lower Bound Proof}
Here we present the proof of Theorem~\ref{thm:symm-lower-bound}.


\begin{prevproof}{Theorem}{thm:symm-lower-bound}
We use Le Cam's two point method and construct a symmetric Markov chain $Q$ and a class of symmetric Markov chains $\mathcal{P}$ s.t.  
(i) every $P \in \mathcal{P}$ is at least $\eps$ far from $Q$ for a given constant $\eps$. That is $1-\specr{\srprod{P}{Q}} \geq \eps$ for any $P \in \mathcal{P}$;
(ii) there is a constant $c > 0$, s.t. it is impossible to distinguish a word of length $m$ generated by a randomly chosen 
Markov chain $\bar{P} \sim \mathcal{P}$, from a word of length $m$ produced by $Q$ with probability equal to or greater than $\frac{99}{100}$ for $m\le\frac{cn}{\eps}$.
To prove (ii) we show that the total variation distance between the $m$-word distributions obtained from the two processes, $Q$ and $\bar{P}$, is small when $m<\frac{cn}{\eps}$ for 
some constant $c$. We denote distribution of length $m$ words obtained from $Q$ by $\word{Q}{m}$, and from MC $\bar{P}\sim\mathcal{P}$ by $\word{\mathcal{P}}{m}$. 
%\begin{itemize}
%\item The word distribution of words of length $t$ obtained from $Q$ (represented by $\word{Q}{t}$).
%\item The word distribution of words of length $t$ obtained from a Markov chain $\bar{P}$ chosen uniformly at random from the family $\mathcal{P}$ (represented by $\word{\bar{P}}{t}$).
%\end{itemize}
We represent symmetric MC as undirected weighted graphs $G=(V,E)$. 
We allow graph to have multi-edges (this is helpful to provide an intuitive understanding of the lower bound construction and is not 
essential). We can ultimately remove all multi-edges and give a construction with only simple edges by 
doubling the number of states.

\begin{description}
	\item[Markov Chain $Q$:] complete double graph on $n$ vertices with uniform weights, i.e., 
	\[
	\forall \quad i\neq j \quad\quad (ij)_1, (ij)_2\in E  \quad\quad  Q_{(ij)_1}=Q_{(ij)_2}=\frac{1}{2(n-1)}.
	%Q_{(ij)_1}=\frac{1}{2(n-1)} \quad\quad Q_{(ij)_2}=\frac{1}{2(n-1)}\quad\quad \text{ for any } i\neq j.
	\]
	\item[Family $\mathcal{P}$:] for any pair of vertices $i\neq j$ there are two bidirectional edges $(ij)_1$, $(ij)_2$
	with weights randomly (and independently for each pair of $(i,j)$) chosen to be either
	\[
	P_{(ij)_1}, P_{(ij)_2}=\frac{1\pm \sqrt{8\eps}}{2(n-1)},
	\quad\quad\text{ or }\quad\quad
	P_{(ij)_1},P_{(ij)_2}=\frac{1\mp \sqrt{8\eps}}{2(n-1)}.
	\]
	%\[
	%\begin{cases}
	%P_{(ij)_1}&=\frac{1+4\sqrt{\eps}}{2n-2}, \\
	%P_{(ij)_2}&=\frac{1-4\sqrt{\eps}}{2n-2}
	%\end{cases}
	%\text{ or }
	%\begin{cases}
	%P_{(ij)_1}&=\frac{1-4\sqrt{\eps}}{2n-2}, \\
	%P_{(ij)_2}&=\frac{1+4\sqrt{\eps}}{2n-2}
	%\end{cases}  
	%\]
	%
	%
	%for every state $i$, bidirectional transition probabilities to states $j$ and $j'$, for each $j > i$, 
	%are randomly chosen to be either $\frac{1+4\sqrt{\eps}}{2n-2},\frac{1-4\sqrt{\eps}}{2n-2}$ or 
	%$\frac{1-4\sqrt{\eps}}{2n-2},\frac{1+4\sqrt{\eps}}{2n-2}$ respectively. The family consists of every 
	%Markov chain which can be generated by this process.
	%for every state $i$, bidirectional transition probabilities to states $j$ and $j'$, for each $j > i$, 
	%are randomly chosen to be either $\frac{1+4\sqrt{\eps}}{2n-2},\frac{1-4\sqrt{\eps}}{2n-2}$ or 
	%$\frac{1-4\sqrt{\eps}}{2n-2},\frac{1+4\sqrt{\eps}}{2n-2}$ respectively. The family consists of every 
	%Markov chain which can be generated by this process.
\end{description}
To make this instance a simple graph with at most one bidirectional edge between any pair of vertices we apply 
a standard graph theoretic transformation: we make a copy $i'$ for each vertex $i$; for each pair of double edges 
$e_1=(ij)_1, e_2=(ij)_2$ construct $4$ edges $(ij),(ij'),(i'j),(i'j')$ with weights $w(ij)=w(i'j')=w(e_1)$ and $w(ij')=w(i'j)=w(e_2)$. 
%See Appendix~\ref{app:sec_symm_lb} for complete proof.
%We complete the rest of the proof in Section \ref{app:sec_symm_lb}.
%We apply standard graph theoretic transformation to make this instance a simple graph with one bidirectional edge
%between any pair of vertices. To this end we make a copy $i'$ for each vertex $i$ and for each pair of double edges 
%$e_1=(ij)_1, e_2=(ij)_2$ construct $4$ edges $(ij),(ij'),(i'j),(i'j')$ with weights $w(ij)=w(i'j')=w(e_1)$ and $w(ij')=w(i'j)=w(e_2)$.
%In fact, we will ultimately remove this assumption and give a construction with only simple edges by 
%increasing the number of states by a factor of 2. \nishanth{intuition about construction goes here}
%All Markov chains in our lower bound construction will have the same set $S$ of states and $|S| = 2n$. Hence each 
%transition matrix is a $2n \times 2n$ square matrix whose row $i$ contains the transition probabilities from state 
%$i$ to the other states.

As all Markov chains $Q$ and $P\in\mathcal{P}$ are symmetric with respect to the starting state, we can assume without loss of generality that word $w$ starts from
the state $i=1$. First, we observe that for the simple graph $2n$-state representation  
\begin{lemma}
	\label{clm:p-far-from-q}
	Every Markov chain $P \in \mathcal{P}$ is at least $\eps$-far from $Q$.
\end{lemma}
\begin{proof}
	For any $P \in \mathcal{P}$, it can be seen that 
	$$\srprod{P}{Q}\circ \onev = \left(\frac{\sqrt{1+\sqrt{8\eps}}+\sqrt{1-\sqrt{8\eps}}}{2}\right)\cdot\onev.$$
	By Perron-Frobenius theorem $\onev$ is the unique eigenvector corresponding to the largest absolute value eigenvalue.  
	%and since all its entries are non-negative, by Perron-Frobenius theorem for non-negative matrices, we get that $\onev$ is the 
	%eigenvector corresponding to the eigenvalue with the largest absolute value. 
	Hence, $\specr{\srprod{P}{Q}} = \frac{\sqrt{1+\sqrt{8\eps}}+\sqrt{1-\sqrt{8\eps}}}{2}$ which by Taylor series expansion implies $1-\specr{\srprod{P}{Q}}\ge\eps+\frac{5}{2}\eps^2+o(\eps^2)
	\ge \eps$ for any $\eps<\frac{1}{8}$.
	%\begin{align}
	%1-\rho\left(\srprod{P}{Q} \right) &= 1-\frac{\sqrt{1+4\sqrt{\eps}}+\sqrt{1-4\sqrt{\eps}}}{2} \notag\\
	%&\geq 1-\left(\frac{1+\frac{4\sqrt{\eps}}{2}-\frac{16\eps}{8}+\frac{64\eps^{1.5}}{16}+1-\frac{4\sqrt{\eps}}{2}-\frac{16\eps}{8}+\frac{64\eps^{1.5}}{16}}{2}\right) \label{eq:bytaylor}\\
	%&\geq 2\eps - 4\eps^{1.5} \notag\\
	%&\geq \eps \label{eq:eps-small}
	%\end{align}
	%where (\ref{eq:bytaylor}) follows from Taylor's inequality and (\ref{eq:eps-small}) follows for small enough $\eps$\nishanth{technically inaccurate}.
\end{proof}

%\nishanth{Collision definition changes now due to duplication of states.}
We say that a given word $w=s_1\ldots s_m$ from a Markov chain $P$ represented as a multi-edge graph on $n$ states has a $(ij)$ {\em collision}, 
if any state transition between states $i$ and $j$ (in any direction along any of the edges $(ij)_1,(ij)_2$) occurs more than once in $w$. We now 
state and prove the following claims about the Markov chain family $\mathcal{P}$.


\begin{lemma}
	\label{clm:num-collisions}
	Consider a word $w$ of length $m$ drawn from $Q$. The expected number of collisions in $w$ is
	at most $\Ocomplex{\frac{m^2}{n^2}}=\Ocomplex{\frac{1}{\eps^2}}$. 
	%and the probability of $w$ having a collision is $\leq \frac{t^2}{2(n-1)^2}$.
\end{lemma}
\begin{prevproof}{Lemma}{clm:num-collisions}
	Let $I_w(t_1,t_2,(ij))$ indicate the event that in the multi-edge interpretation of the Markov chain $P$, the transition along $(ij)$ edge occurs at times $t_1<t_2$  in $w$. 
	%That is, $I_w(t_1,t_2,i \rightarrow j) = 1$ if $s_{t_1} = i, s_{t_1+1} = j$ and $s_{t_2} = i, s_{t_2+1} = j$. $I_w(t_1,t_2,i \rightarrow j) = 0$ otherwise. 
	First, we observe that $\Prx{s_{t_1}=s | s_{t_1-1}=x} \leq \frac{1}{n-1}$ and $\Prx{s_{t_2}=s | s_{t_1-1}=x} \leq \frac{1}{n-1}$ for all $x$ and both $s=i$ or $s=j$.
	Thus for any $t_2\ge t_1+2$ by a union bound for all four possible cases of $s_{t_1},s_{t_1+1},s_{t_2},s_{t_2+1}\in\{i,j\}$ we have 
	$$\E\left[I_w(t_1,t_2,(ij)) \right] \leq \frac{4}{(n-1)^4}.$$
	Similarly, for the case $t_2=t_1+1$ we can obtain 
	$$\E\left[I_w(t_1,t_2,(ij)) \right] \leq \frac{2}{(n-1)^3}.$$
	%$$\Prx{s_{t_1}=i} \leq \frac{1}{n-1}$$
	%when considering the worst possible distribution of state $s_1$. This is because $\Prx{s_{t_1}=s | s_{t_1-1}=x} \leq 1/(n-1)$ for all $x$. 
	%Similarly $\Prx{s_{t_2} = i} \leq \frac{1}{n-1}$. Since $\Prx{s_{t_1+1}=j | s_{t_1}=i} = \frac{1}{n-1}$,
	%$$\E\left[I_w(t_1,t_2,i \rightarrow j) \right] \leq \frac{1}{(n-1)^4}.$$
	Let $X$ denote the random variable which is equal to the total number of collisions in the word $w$. Then,
	\begin{align*}
	\Ex{X} &= \sum_{t_2 \ge t_1+2}\sum_{i\neq j} \Ex{I_w(t_1,t_2,(ij))} + \sum_{t_1=1}^{m-1}\sum_{i\neq j} \Ex{I_w(t_1,t_1+1,(ij))}\\
	& \le \frac{4}{(n-1)^4}\cdot\frac{m^2}{2}\cdot\frac{n(n-1)}{2}+ \frac{2}{(n-1)^3}\cdot m\cdot\frac{n(n-1)}{2}
	=\Ocomplex{\frac{m^2}{n^2}}
	\end{align*}
	%\begin{align*}
	%X &= \sum_{t_1 \neq t_2}\sum_{i,j} I_w(t_1,t_2,i \rightarrow j) \\
	%\implies \E[X] &= \sum_{t_1 \neq t_2}\sum_{i,j} \E\left[ I_w(t_1,t_2,i \rightarrow j) \right] \\
	%\implies \E[X] &\leq \frac{1}{(n-1)^4}\frac{t^2}{2}n^2 \\
	%&= \frac{t^2}{2(n-1)^2}
	%\end{align*}
	%Applying Markov's inequality on $X$ gives us that,
	%$$\Prx{ X > 0} = \Prx{X \geq 1} \leq \frac{t^2}{2(n-1)^2}.$$
\end{prevproof}

We also consider 3-way collisions which are collisions where there was at least 3 different transition between a pair of states $i$ and $j$ in the word $w$. 

\begin{lemma}
	\label{cl:3way-collisions}
	Consider a word $w$ of length $m$ drawn from $Q$. The probability of $w$ having a $3$-way collision is at most $O(\frac{m^3}{n^4})=o(1).$
\end{lemma}
\begin{prevproof}{Lemma}{cl:3way-collisions}
	Similar to the proof of Lemma~\ref{clm:num-collisions} we can give a sharp upper bound on the expected number of $3$-way collisions with the most significant term
	being $\frac{8m^3}{6 (n-1)^6}\cdot\frac{n(n-1)}{2}$, i.e., the expected number of $3$-way collisions is $\Ocomplex{\frac{m^3}{n^4}}$. By Markov inequality we  
	obtain the required bound on the probability of a $3$-way collision.   
\end{prevproof}

Now consider a typical word $w$ generated by $Q$. 
%$\bar{P}\sim\mathcal{P}$. 
As we know from Lemma~\ref{cl:3way-collisions} it has no 3-way collisions and by Markov inequality
and Lemma~\ref{clm:num-collisions} has at most $O(\frac{1}{\eps^2})$ collisions with high probability. As we show next a typical word $w$ has similar probabilities under $Q$ or 
$\bar{P}\sim\mathcal{P}$ models.

\begin{lemma}
	\label{clm:no-collision-analysis}
	For $m=O(\frac{n}{\eps})$ at least $\frac{1}{2}$ fraction of words $w=s_1\cdots s_m$ generated by $Q$ satisfy 
	\[
	\frac{1}{2}\cdot\prob[Q]{w}<\prob[\bar{P}\sim \mathcal{P}]{w} < 2\cdot \prob[Q]{w}
	\]
\end{lemma}
\begin{prevproof}{Lemma}{clm:no-collision-analysis}
	For each feasible word $w$ in $Q$, i.e., $w$ such that $\prob[Q]{w}>0$
	\[
	\Prlong[Q]{w}=\left(\frac{1}{2(n-1)}\right)^{m-1}
	\quad\quad 
	\Prlong[\bar{P}\sim\mathcal{P}]{w}=\prod_{j>i}\sum_{\bar{P}_{(ij)_1}=\frac{1\pm\sqrt{8\eps}}{2(n-1)}} \bar{P}_{(ij)_1}^{|\{(ij)_1\in w\}|} \cdot\bar{P}_{(ij)_2}^{|\{(ij)_2\in w\}|} 
	\]
	
	First, if $w$ has only one transition along edge $(ij)$, then the corresponding term in $\prob[\bar{P}\sim\mathcal{P}]{w}$ 
	\[
	\sum_{\bar{P}_{(ij)_1}} \bar{P}_{(ij)_1}^{|\{(ij)_1\in w\}|} \cdot\bar{P}_{(ij)_2}^{|\{(ij)_2\in w\}|}=\frac{1}{2}\left(\frac{1+\sqrt{8\eps}}{2(n-1)}+\frac{1-\sqrt{8\eps}}{2(n-1)}\right)=
	\frac{1}{2(n-1)}.
	\]
	From Lemma~\ref{cl:3way-collisions}, we know that probability of a $3$-way collision in $w$ is $o(1)$ under $Q$ model. We observe that for a $2$-way collision $(ij)$
	(a collision which is not a $3$-way collision), the corresponding term in $\prob[\bar{P}\sim\mathcal{P}]{w}$  for the case of transition along two different 
	edges $(ij)_1$ and $(ij)_2$ is
	\[
	\sum_{\bar{P}_{(ij)_1}} \bar{P}_{(ij)_1}^{|\{(ij)_1\in w\}|} \cdot\bar{P}_{(ij)_2}^{|\{(ij)_2\in w\}|}=\frac{1+\sqrt{8\eps}}{2(n-1)}\cdot\frac{1-\sqrt{8\eps}}{2(n-1)}=
	\frac{(1-8\eps)}{4(n-1)^2}.
	\] 
	We call this type of collision {\em type I} collision. For the other case ({\em type II} collisions) of transition along the same edges the respective probability is 
	$\frac{(1+8\eps)}{4(n-1)^2}$. By Lemma~\ref{clm:num-collisions} and by Markov inequality the total number of collisions is $O(\frac{1}{\eps^2})$ with probability $\frac{3}{4}$. 
	We can also make sure that out of these collisions number of type I and type II collisions is roughly the same. More precisely, the difference between numbers of 
	type I and type II collisions is at most $O(\frac{1}{\eps})$ with probability of at least $\frac{3}{4}$. Indeed, the choice of edge collision type in $w$ is uniform between type I and type II, and is
	independent across all collision edges. Now, for small enough $m$ we can make sure that at least $\frac{1}{2}$ fraction of words $w$ has number of collisions at most 
	$\frac{c_1}{\eps^2}$ and the difference between number of type I and II collisions is at most $\frac{c_2}{\eps}$, for some small constants $c_1,c_2>0$. Thus the corresponding density 
	functions can be related as follows.
	\[
	2>\left(1+8\eps\right)^{\frac{c_2}{\eps}}>\frac{\prob[\bar{P}\sim\mathcal{P}]{w}}{\prob[Q]{w}}>\left(1-64\eps^2\right)^{\frac{c_1}{2\eps^2}}\cdot \left(1-8\eps\right)^{\frac{c_2}{\eps}}>1/2
	\]
	%First, we observe that if a word $w$ has no collisions then $\prob[Q]{w} = \prob[\bar{P}]{w}$. From Claim \ref{cl:3way-collisions}, 
	%we know that the probability of a word $w$ of length $m = o(n/\eps)$ having a 3-way collision is $< 1/20$ under either $Q$ or $\bar{P}$ 
	%processes. Therefore, the claim holds true for such words. It remains to show the claim to be true for words with at least one 2-way 
	%collision and at most $1/\eps^2$ 2-way collisions. Since collisions are of two types as explained in the description, each one can be 
	%regarded as a coin toss. If the Markov chain was indeed $Q$, then it would be a fair coin toss between the two collision types. If it 
	%was $\bar{P}$ then each collision would correspond to a coin toss with the bias of the coin as $\pm O(\eps)$. Therefore the maximum 
	%$\frac{\prob[Q]{w}}{\prob[\bar{P}]{w}}$ can be is
	%$$(1+\eps)^{(1/\eps^2)} \leq \exp(1/\eps)$$
	%Consider a word $w$ with no collisions. We have,
	%$$\Prx{w=s_1 \ldots s_t} = \Prx{s_1}\Pr{s_2 | s_1} \ldots \Prx{s_t | s_{t-1}}.$$
	%Under $Q$, $\Prx[Q]{w} = 1/n^{t}$.
	%We will now bound $\Prx[\bar{P}]{w}$. Any transition $s_i \rightarrow s_j$ exists in exactly $(1-\eps)^2$ fraction of the Markov chains in family $\mathcal{P}$. And given any sequence of states with no collisions, they will have TBF
	%$\Prx[\bar{P}]{s_{i+1} |s_i}\leq (1-\eps)^2\frac{1}{n(1-\eps)^2} = \frac{1}{n}$.
	%
	%Therefore,
	%$$\Prx[\bar{P}]{w} \leq \frac{1}{n^{t}} = \Prx[Q]{w}.$$
\end{prevproof}


Lemma~\ref{clm:no-collision-analysis} shows that $\dtv{\word{Q}{m}}{\word{\mathcal{P}}{m}} \leq \frac{3}{4}$, which implies that
no algorithm can successfully distinguish $Q$ from the family $\mathcal{P}$ with probability greater than $\frac{3}{4}$ for some
$m=\Omega(\frac{n}{\eps})$.
\end{prevproof}
