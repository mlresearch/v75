%\noindent \nick{Basic intro and notations on Markov chains.}
We list the general notational conventions used in this paper. We denote vectors by small letters 
such as $\vect{v}$ and matrices by capital letters such as $A$,$B$,$P$,$Q$. The $i^{th}$ entry 
of vector $\vect{v}$ is denoted by $v_i$ or $v[i]$ and the $(ij)^{th}$ entry of matrix $A$ ($i^{th}$ row, $j^{th}$ column) is 
denoted by $A_{ij}$ or $A[ij]$; $\vect{e}_i$ denotes the standard basis vector with 
$1$ in its $i^{th}$ coordinate and $0$ elsewhere; $\onev$ denotes the vector of all ones. 
The ``entrywise'' $L_1$ and $L_2$ norms of a matrix  $A$ are respectively denoted as $\onenorm{A}=\sum_{i,j}|A_{ij}|$ and $\twonorm{A}=\sqrt{\sum_{i,j}A_{ij}^2}$; 
$\specr{A}$ denotes the spectral radius of matrix $A$, i.e., the maximum absolute eigenvalue of $A$. 
The eigenvalues of $A$ are denoted by $\eigi[1], \dots, \eigi, \dots, \eigi[n]$ and 
the respective right eigenvectors by $\eigvi[1],\dots,\eigvi,\dots,\eigvi[n]$ (left 
eigenvectors by $\eigvli[1],\dots,\eigvli[n])$\footnote{If matrix $A$ is not symmetric, we allow $\eigi\in\Complex$ and $\eigvi,\eigvli\in\Complex^n$. Then,
	we will only use $\eigi[1]\in\R$ and $\eigvi[1],\eigvli[1]\in\R^n$.}; for symmetric matrix $A$ we assume that 
$\eigi[1]\ge \dots \ge \eigi \ge \dots \ge \eigi[n]$.\\

\noindent Two popular notions of distance between distributions will be used heavily in this paper. We state their formal definitions below and also specify the relation between them.
\begin{definition}
	\label{def:stat_dist}
	The {\em total variation} and {\em Hellinger} distances between distributions $p,q$ over $[n]$ are defined as : 
	$\dtv{p}{q}\eqdef\frac{1}{2}\sum\limits_{i\in[n]}\abs{p_i-q_i}$;\quad\quad\quad\quad 
	$\hellingersq{p}{q}\eqdef\frac{1}{2}\sum\limits_{i\in[n]}\left(\sqrt{p_i}-\sqrt{q_i}\right)^2= 1-\sum\limits_{i\in[n]}\sqrt{p_i\cdot q_i}$;
	The following relation between these notions of distance is well known (see, e.g., \cite{GibbsS02}):
	\be
	\label{eq:tv hellinger relation}
	\sqrt{2}\cdot\hellinger{p}{q}\ge\dtv{p}{q}\ge\hellingersq{p}{q}. 
	\ee
	%\begin{align}
	%\dtv{p}{q}\eqdef\sup_{A\subset[k]}|p(A)-q(A)|=\frac{1}{2}\sum_{i\in[k]}\abs{p_i-q_i} \notag\\
	%\hellingersq{p}{q}\eqdef\frac{1}{2}\sum_{i\in[k]}\left(\sqrt{p_i}-\sqrt{q_i}\right)^2= 1-\sum_{i\in[k]}\sqrt{p_i\cdot q_i} \notag\\
	%\sqrt{2}\cdot\hellinger{p}{q}\ge\dtv{p}{q}\ge\hellingersq{p}{q}. \label{eq:tv hellinger relation}
	%\end{align}
\end{definition}




%\nishanth{Add description about Poisson Sampling}
%\noindent As already stated, we will use capital letters $P$, $Q$ to denote Markov chains. We will also use $P$,$Q$ to denote their respective transition matrices. \nishanth{TBF}
%Before formulating the precise question we study, we need a notion of distance between Markov chains to work with.

%%%%%%%%%%%%%%%%%%%%  %Notations for Markov chains %%%%%%%%%%%%%%%%%%%%
\subsection{Markov Chains}
A {\em discrete-time} Markov chain is a stochastic process $\{X_t\}_{t \in \{0,1,\ldots \}}$ over a state space $S$ which satisfies the Markov property: 
the probability of being in state $s$ at time $t+1$ depends only on the state at previous time $t$. In this paper, we
only consider Markov chains with the {\em finite state space} $[n]$. Such Markov chains can be completely specified 
by a $n \times n$ transition matrix (\emph{kernel}) that contains probabilities of transitioning from state $i$ to state $j$ 
in the $i^{th}$ row and $j^{th}$ column, and a description of the distribution of their starting state. The transition matrix has non-negative entries and is a stochastic matrix. We use capital letters $P,Q,M$ to represent Markov 
chains as well as their respective transition matrices. The stationary distribution $\pi$ of a Markov chain $P$ is a distribution over the state space $S$ such that it satisfies $\trans{\pi}\circ P = \trans{\pi}$. 
Another important parameter is the distribution of the starting state $s_0$ which we denote by $\vect{p}$ (for the Markov chain $P$). It may or not may not be the stationary distribution.\\
%The state space of a Markov chain can be partitioned into communicating classes which groups together those states that are all reachable via a trajectory starting from any state in the group.
The state space of a Markov chain can be partitioned into communicating classes which are groups of states reachable from each other with positive probability.
%Such partitioning is useful when studying irreducible Markov chains. Below, we define formally what communicating classes are and what classifies them as essential.
The formal definition of essential communicating classes is as follows.
\begin{definition}[Essential Communicating Classes]
	\label{def:comm-class}
Given a Markov chain $M$ over the state space $[n]$, we define $x \rightarrow y$ if there exists an integer $r>0$ such that $M^r(x,y) > 0$. Similarly, we define equivalence relation 
$x \leftrightarrow y$ iff $x \rightarrow y$ and $y \rightarrow x$. The equivalence classes under relation $\leftrightarrow$ are called communicating classes. Any communicating class $C$ with the property 
that $y$ must be in $C$ for any $x\in C$ and $x \rightarrow y$ is said to be an {\em essential} communicating class\footnote{An essential communicating class can be intuitively thought of as a strong connected component of the underlying directed graph with no outgoing edges.}.
\end{definition}

\subsubsection{Hitting Times and Mixing Times}
Two commonly studied random variables associated with Markov chains which are relevant to this paper are their mixing times and hitting times. 
%We state their definitions as used in this paper here.
\begin{definition}[Hitting Time $\hitt{P}$ of a Markov chain $P$]
	Given a Markov chain $P$ over a state space $[n]$, let $s_t$ denote the state at time $t$. The hitting time $\hitt{P}$ is 
	\begin{align*}
	\hitt{P} = \max_{r,s \in [n]} \{\E\left[\min \{t \ge 0 : s_t = r \text{ given } s_0 = s\}\right] \}
	\end{align*}
\end{definition}

\begin{definition}[Mixing Time $\mixt{P}$ of a Markov chain $P$]
	Given a Markov chain $P$ with a stationary distribution $\pi$ and a starting state distribution $\vect{p}$, 
	\begin{align*}
	\mixt{P} = \max_{\vect{p}}~\min \{t \ge 0 : \onenorm{ P^t \vect{p} - \pi} \le 1/4 \}
	\end{align*}
\end{definition}



%Hitting time for a Markov chain $Q$ is a defined as a random variable that represent number of steps needed to reach any state in a 
%specified set $S$ of states from a state $s$. \emph{Hitting time} is also used sometimes to refer to the maximum taken over all pairs of states $r$ and $s$ of the 
%expected time needed to reach $r$ from $s$. We denote such number for Markov chain $Q$ as $\hitt{Q}$. 
%Similarly, we define cover time, which is the expected time needed to visit every state at least once (random walk can start from any state).