\iffalse
\begin{lemma}
	\label{lem:hitting-vs-mixing}
	For a symmetric Markov chain $M$ on a discrete state space of size $n$, 
	$$\hitt{M} \le \mixt{M}n.$$
\end{lemma}
\begin{proof}
	
\end{proof}
\fi


\begin{prevproof}{Lemma}{lem:large_deviation_bound}
%\begin{proof}
	To simplify notations we denote by $\Delta\eqdef 2\hitt{Q}$. By union bound over all states $i$ it is enough to show that $\Prx{|\{t: i=s_t\in w\}|<\frac{m}{8e\cdot n}}\le\frac{\eps^2}{n^2}$ 
	for each fixed state $i$. We can make sure that in the first $\frac{m}{2}$ steps state $i$ is visited at least once with probability 
	at least $1-\frac{\eps^2}{n^3}$. Once we visited state $i$, instead of hitting time for state $i$ we can analyze the {\em return time} $\returnt$ for $i$.
	Note that for symmetric Markov chains $\frac{1}{n}\onev$ (uniform distribution) is a stationary distribution. Therefore, every state appears at average once 
	in every $n$ steps in an infinite word from $\word{Q}{\infty}$. In other terms, the expectation of $\returnt$ for each 
	state $i$ is exactly $n$. By definition of hitting time we have that in $\frac{\Delta}{2}$ steps the probability 
	of reaching a particular state $i$ from any other state $j$ is greater than $1-1/e$ (or any other given constant). It implies that $\Prx{\returnt \ge \frac{\Delta}{2}\cdot C}\le e^{-C}$
	for any $C\in\N$. Indeed, one can show this by induction on parameter $C$. 
	Notice that if the random walk did not return to $i$ after $C-1$ steps it has stopped at some state $j\neq i$. Then for any choice of $j$ by definition of the hitting time 
	the random walk will return to $i$ with probability at least $1/e$ in the next $\frac{\Delta}{2}$ steps. It is not hard to get a similar bound $\Prx{\returnt \ge \Delta\cdot C}\le e^{-C}$ for any $C\ge 1, C\in\R$. To simplify notations we use $X$ to denote the random variable $\returnt$ 
	and $X_1,\dots,X_\ell$ to denote $\ell$ i.i.d. samples of $X$. We have  
	
	\be
	X\ge 0\quad\text{and}\quad\forall C\in\R_{\ge 1},\Prx{X \ge \Delta\cdot C}\le e^{-C} \quad\text{and}\quad \Ex{X}=n.
	\label{eq:ineq_return}
	\ee
	
	We only need to show that $\Prx{X_1+\cdots+X_\ell > m/2} \le \frac{\eps^2}{n^2}$ for $\ell=\frac{m}{8e\cdot n}$.
	To this end, we use a standard technique for large deviations and apply Markov's inequality to the moment generating function of $X_1 + \cdots + X_\ell$,
	
	%\begin{multline*}
	\be
	\Prx{X_1+\cdots+X_\ell > m/2}=\Prx{e^{\theta\cdot(X_1+\cdots+X_\ell)} > e^{\theta\cdot m/2}}\le\frac{\Ex{e^{\theta\cdot(X_1+\cdots+X_\ell)}}}{e^{\theta\cdot m/2}} \\
	=\frac{\Ex{e^{\theta X}}^\ell}{e^{\theta\cdot m/2}}
	\label{eq:MGF_markov}
	\ee
	%\end{multline*}
	We note that given restrictions \eqref{eq:ineq_return} on $X$ maximum of $\Ex{e^{\theta X}}$ for any fixed $\theta>0$ is attained at 
	\[
	X^*\sim
	\begin{cases}
	\Delta\cdot x  & x\in[C_0,\infty) \text{ with probability density function } e^{-x}\\
	0 & \text{with remaining probability }1-e^{-C_0},
	\end{cases}
	\]
	where constant $C_0>1$ is such that $\Ex{X^*}=n$. Indeed, distribution $X^*$ maximizes \eqref{eq:MGF_markov} due to simple variational inequality:
	$\epsilon\cdot e^{\theta\cdot a}+\epsilon\cdot e^{\theta\cdot b}<\epsilon\cdot e^{\theta\cdot (a-c)}+\epsilon\cdot e^{\theta\cdot (b+c)}$ for any $b \ge a \ge c > 0$, and probability mass $\epsilon>0$.
	This inequality allows us to increase $\Ex{e^{\theta\cdot X}}$ and not change $\Ex{X}$ by tweaking density function $f(x)$ of $X$ 
	$f'(a-c)=f(a-c)+\epsilon$, $f'(a)=f(a)-\epsilon$, $f'(b)=f(b)-\epsilon,$ $f'(b+c)=f'(b+c)+\epsilon$, ($f'(x)=f(x)$ for all other $x$), for some $c\le a$. 
	The only time we cannot apply this incremental change is when $X=X^*$.
	
	We have 
	\be
	\label{eq:C0}
	\Ex{X^*}=\Delta(C_0+1)e^{-C_0} = n.
	\ee 
	We set $\theta\eqdef\frac{1}{2\Delta\log\Delta}$ in \eqref{eq:MGF_markov}. Now we are ready to estimate $\Ex{e^{\theta\cdot X}}$.
	To simplify notations we denote $\gamma\eqdef\frac{1}{2\log\Delta}$.
	\begin{multline}
	\Ex{e^{\theta\cdot X}}=1-e^{-C_0}+\int_{C_0}^{\infty}e^{\theta\cdot\Delta\cdot x}\cdot e^{-x}\,\mathrm{d}x=
	1-e^{-C_0}+\int_{C_0}^{\infty}e^{-x\cdot(1-\frac{1}{2\log\Delta})}\,\mathrm{d}x\\
	=1-e^{-C_0}+\frac{e^{-C_0(1-\gamma)}}{1-\gamma}=1+e^{-C_0}\left(\frac{e^{C_0\gamma}}{1-\gamma}-1\right).
	\label{eq:expectation_MGF}
	\end{multline}
	We notice that $\gamma C_0<1$, since from \eqref{eq:C0} we can conclude that $\frac{e^{C_0}}{C_0+1}=\frac{\Delta}{n}\implies C_0<2\log\Delta=1/\gamma$. The last implication can be obtained as follows: for $C_0 > 2.52,$ we have $C_0 - \frac{C_0}{2}\le C_0-\ln(1+C_0)=\ln(\frac{\Delta}{n})$. Now, we can estimate $e^{\gamma C_0}\le 1 + e\cdot\gamma C_0$ in \eqref{eq:expectation_MGF}. Furthermore, since $\gamma<1/2$ we have
	the term $\frac{e^{C_0\gamma}}{1-\gamma}-1$ in \eqref{eq:expectation_MGF} to be at most $2e\gamma(C_0+1)$. With this estimate we continue \eqref{eq:expectation_MGF}
	\be
	\Ex{e^{\theta\cdot X}}\le 1+e^{-C_0}2e\gamma(C_0+1)=1+\frac{e\cdot n}{\Delta\log\Delta}.
	\label{eq:expectation_MGF2}
	\ee
	We apply estimate~\eqref{eq:expectation_MGF2} and formula $\theta=\frac{1}{2\Delta\log\Delta}$ to~\eqref{eq:MGF_markov} to obtain
	\[
	\Prx{X_1+\cdots+X_\ell > m/2}\le \frac{\left(1+\frac{e\cdot n}{\Delta\log\Delta}\right)^\ell}{e^{m/4\Delta\log\Delta}}
	\le\frac{e^{m/8\Delta\log\Delta}}{e^{m/4\Delta\log\Delta}}=e^{\frac{-m}{8\Delta\log\Delta}}<\frac{\eps^2}{n^2},
	\]
	where in the second inequality we used the fact $\left(1+\frac{e\cdot n}{\Delta\log\Delta}\right)^{\frac{\Delta\log\Delta}{e\cdot n}}<e$, and to get the last inequality
	we used $m=\wOm{\Delta\log\Delta}$ (where in $\widetilde{\Omega}$ the hidden dependency is only on  $\log \eps$ and $\log n$).
%\end{proof}
\end{prevproof}


%\begin{lemma}
%	Consider two symmetric Markov chains $P$ and $Q$ on a finite state space $[n]$. Denote by $\frac{1}{n}P$ the distribution over $n^2$ elements obtained by scaling down every entry of the transition matrix $P$ by a factor $1/n$. We have,
%\begin{align}
%\frac{1}{2}\sum\limits_{i,j\in[n]}\left(\sqrt{\frac{P_{ij}}{n}}-\sqrt{\frac{Q_{ij}}{n}}\right)^2=
%\hellingersq{\frac{1}{n}P}{\frac{1}{n}Q}\ge\eps.
%\end{align}
%%\label{lem:rho_l1_distances}
%\end{lemma}
\begin{prevproof}{Lemma}{lem:rho_l1_distances}
	%\begin{proof}
	We note that, as $P$ and $Q$ are symmetric matrices, so is $\srprod{P}{Q}$. Thus we have
	\be
	\label{eq:sv_def}
	1-\eps = \specr{\srprod{P}{Q}} = \max_{\twonorm{\vev}=1} \vevt\circ\srprod{P}{Q}\circ\vev.
	\ee
	If we use a particular $\vev=\frac{1}{\sqrt{n}}\onev$ in \eqref{eq:sv_def}, then we get the following inequality.
	\begin{align*}
	1-\eps \ge & \frac{1}{\sqrt{n}}\onevt\circ\srprod{P}{Q}\circ\frac{1}{\sqrt{n}}\onev = \frac{1}{n}\sum\limits_{i,j} \sqrt{P_{ij}\cdot Q_{ij}} =  1-\hellingersq{\frac{1}{n}P}{\frac{1}{n}Q},
	\end{align*}
	which implies $\hellingersq{\frac{1}{n}P}{\frac{1}{n}Q}\ge\eps$.
\end{prevproof}

