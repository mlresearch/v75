
The core of the proof follows results of \cite{braverman2016communication} and \cite{jayram2009hellinger}. Let $\mathbf{X}=\left(X^{(1)}, \dots, X^{(m)}\right)$ be a random vector distributed $\left(\mu_0^n\right)^m$ where for all $j \in [m]$, $X^{(j)} \in \Omega^n$ is the input of player $j$. Let $\Pi$ be the transcript of a $1/3$-error $(m,n)$ protocol identifying $\mu \in \{\mu_1,\dots,\mu_k\}$, distributed $P_{\Pi \mid \mathbf{X}}$ conditioned on the input of the players being $\mathbf{X}$. Given a vector $\mathbf{a} = (a_1, \dots, a_m) \in \{0,1, \dots, k\}^m$, let $\Pi_{\mathbf{a}}$ be the random variable denoting the transcript $\Pi$ when every player $j \in [m]$ receives an independent input distributed $\mu_{a_j}^n$. Formally, $\Pi_{\mathbf{a}} \sim P_{\Pi \mid \mathbf{X} \sim \left( \mu_{a_1}^n, \dots, \mu_{a_m}^n \right)}$. For any $j\in [m]$ and $i \in [k]$, let $\mathbf{e}_{j,i}$ be the $m$-entry vector that equals $i$ on coordinate $j$ and all other coordinates are zero, and let $\mathbf{i}$ be the all-$i$ vector.

Since $\mathbf{X}\sim \mu_0^{mn}$, for any $j \in [m]$, $P_{\Pi \mid X^{(j)}}$ is the distribution of $\Pi$ conditioned on player $j$ getting the input $X^{(j)} \in \Omega^n$ while all other players get an independent input distributed $\mu_0^n$. Note that for all $j \in [m]$, $\Pi_{\mathbf{0}} \sim P_{\Pi \mid X^{(j)} \sim \mu_0^n}$, and for all $i \in [n]$, $\Pi_{\mathbf{e}_{j,i}} \sim P_{\Pi \mid X^{(j)} \sim \mu_i^n}$. Hence, the conditions of this lemma imply that
\begin{align}
\sum_{j=1}^m \sum_{i=1}^k \helli^2(\Pi_0, \Pi_{\mathbf{e}_{j,i}})
&= \sum_{j=1}^m \sum_{i=1}^k \helli^2(P_{\Pi \mid X^{(j)} \sim \mu_0^n}, P_{\Pi \mid X^{(j)} \sim \mu_i^n})\notag\\
&\le \sum_{j=1}^m \beta (I(\Pi ; X^{(j)}) + 1). \label{eq:bndeji}
\end{align}
In order to bound the last term we present a known inequality in information theory.
\begin{proposition}
If $X^{(1)}, \dots,X^{(m)}$ are independent random variables and $\Pi$ is a random variable then
\[
\sum_{j=1}^m I\left(\Pi; X^{(j)}\right)
\le I\left(\Pi; X^{(1)} \cdots X^{(m)}\right).
\]
\end{proposition}

\begin{proof}
\begin{align*}
I\left(\Pi; X^{(1)} \cdots X^{(m)}\right)
&~=~ \sum_{j=1}^m I\left(\Pi; X^{(j)} \mid X^{(1)} \cdots X^{(j-1)}\right) \\
&~=~ \sum_{j=1}^m H\left(X^{(j)} \mid X^{(1)} \cdots X^{(j-1)}\right) - H\left(X^{(j)}  \mid \Pi X^{(1)} \cdots X^{(j-1)}\right) \\
&~\ge~ \sum_{j=1}^m H\left(X^{(j)}\right) - H\left(X^{(j)}  \middle| \Pi\right) \\
&~=~ \sum_{j=1}^m I\left(\Pi; X^{(j)}\right).
\end{align*}
where the first equation follows from the chain rule for mutual entropy and the first inequality follows from the independence of $X^{(1)}\cdots X^{(m)}$ and the fact that $H(A\mid BC) \le H(A \mid B)$ for any random variables $A,B,C$.
\end{proof}
Hence, \eqref{eq:bndeji} implies that
\begin{equation} \label{eq:eji2}
\sum_{j=1}^m \sum_{i=1}^k \helli^2(\Pi_0, \Pi_{\mathbf{e}_{j,i}})
~\le~ \beta (I(\Pi ;  \mathbf{X}) + m) 
~\le~ \beta (H(\Pi) + m)
~\le~ \beta (\lvert \Pi \rvert + m),
\end{equation}
where $\lvert \Pi \rvert$ is the communication complexity of the protocol. The following Lemma, \citet[Lemma~2]{braverman2016communication} lower bounds $\sum_{j=1}^m \helli^2(\Pi_\mathbf{0}, \Pi_{\mathbf{e}_{j,i}})$.

\begin{lemma}
For any $1 \le i \le m$,
\[
\helli^2(\Pi_{\mathbf{0}}, \Pi_{\mathbf{i}})
\le C \sum_{j=1}^m \helli^2(\Pi_{\mathbf{0}}, \Pi_{\mathbf{e}_{j,i}})
\]
for some numerical constant $C > 0$.
\end{lemma}

This and \eqref{eq:eji2} implies that
\begin{equation} \label{eq:8}
\sum_{i=1}^k \helli^2(\Pi_{\mathbf{0}}, \Pi_{\mathbf{i}})
\le C \beta( \lvert \Pi \rvert + m).
\end{equation}
The next lemma states that for any protocol error $\varepsilon<1/2$, the LHS of \eqref{eq:8} is $\Omega(k)$.

\begin{lemma} \label{lem:err-small}
Assume $\Pi$ is a transcript of a protocol with a worst-case error (over $\mu_i$) of at most $\varepsilon < 1/2$. Then there exists a subset $S \subseteq [k]$ of size $\lvert S \rvert = k-1$ such that for all $i \in S$,
\[
\helli^2(\Pi_{\mathbf{0}}, \Pi_{\mathbf{i}}) \ge \frac{(1-2\varepsilon)^2}{8}.
\]
In particular,
\[
\sum_{i=1}^k \helli^2(\Pi_{\mathbf{0}}, \Pi_{\mathbf{i}}) \ge \frac{(k-1)(1-2\varepsilon)^2}{8}.
\]
\end{lemma}

\begin{proof}
First, note that for any $i\ne i' \in [k]$, $d_{TV}(\Pi_{\mathbf{i}}, \Pi_{\mathbf{i'}}) \ge 1 - 2\varepsilon$. Indeed, fix some $i \ne i'$ and let $\mathcal{A}$ be the set of all values of $\Pi$ such that the protocol outputs $i$ given these values. Since the protocol has $\varepsilon$-error, $\Pr\left[ \Pi_{\mathbf{i}}\in \mathcal{A}\right] \ge 1 - \varepsilon$ and $\Pr\left[\Pi_{\mathbf{i'}}\in \mathcal{A}\right] \le \varepsilon$. Hence, by definition of the total variation distance,
\begin{equation} \label{eq:hellihigh}
d_{TV}(\Pi_{\mathbf{i}}, \Pi_{\mathbf{i'}})
\ge \Pr\left[ \Pi_{\mathbf{i}}\in \mathcal{A}\right] - \Pr\left[\Pi_{\mathbf{i'}}\in \mathcal{A}\right]
\ge 1-2\varepsilon.
\end{equation}

Assume for contradiction that there are $i \ne i' \in [k]$ such that 
\[
\helli^2(\Pi_{\mathbf{0}}, \Pi_{\mathbf{i}}), \helli^2(\Pi_{\mathbf{0}}, \Pi_{\mathbf{i'}}) 
< \frac{(1-2\varepsilon)^2}{8}.
\]
Then, since the Hellinger distance $\helli()$ obeys the triangle inequality and by Proposition~\ref{prop:heltv},
\[
d_{TV}(\Pi_{\mathbf{i}}, \Pi_{\mathbf{i'}})
\le \sqrt{2} \helli(\Pi_{\mathbf{i}}, \Pi_{\mathbf{i'}})
\le \sqrt{2}\helli(\Pi_{\mathbf{0}}, \Pi_{\mathbf{i}}) + \sqrt{2}\helli(\Pi_{\mathbf{0}}, \Pi_{\mathbf{i'}}) 
< 1-2\varepsilon,
\]
in contradiction to \eqref{eq:hellihigh}.
\end{proof}

\lemref{lem:err-small} and \eqref{eq:8} conclude that any $1/3$-error protocol has a communication complexity of at least $Ck/\beta - m$, for some numerical constant $C>0$. We conclude by showing that the communication complexity is at least $Ck/(2\beta)$. Assume for contradiction that the communication complexity is less than $Ck/(2\beta)$. Denote the parties in the protocol by $1,\dots, m$ and assuming without loss of generality that party $1$ is always the first to talk, party $2$ is the first party to talk among parties $2,\dots,m$,  party $3$ talks first among parties $3, \dots, m$ etc.\footnote{The symmetries between the parties imply that if at some point in the protocol a new party is speaking, one can assume that this party has the lowest index among all parties that have not spoken yet.}, then only a subset of parties $1, \dots, \lfloor Ck/(2\beta) \rfloor$ participates in the protocol, hence we can assume that $m \le Ck/(2\beta)$. The communication complexity is at least $Ck/\beta-m \ge Ck/(2\beta)$, which concludes the proof.
