Our results are based on two general theorems, which establish the difficulty of distinguishing generic distributions under communication and memory constraints respectively. These theorems are presented in \subsecref{subsec:general}. We then  apply them to the problem of detecting correlations, for distributions over binary vectors (\subsecref{subsec:binary}) and for Gaussian distributions (\subsecref{subsec:gaussian}).

\subsection{A General Theorem}\label{subsec:general}

Let $\left\{\mu_1,\dots, \mu_k \right\}$ be a $\CD(\rho)$ family of probability distributions centered around $\mu_0$ (namely, $\lvert \mu_i(E)/\mu_0(E)-1 \rvert \le \rho$ for any $i \in [k]$ and any event $E$). The following theorem establishes that under a certain technical condition  (\eqref{eq:corr}), any $(m,n)$ protocol would require a lot of communication to identify the distribution from which the input data is sampled:

\begin{theorem} \label{thm:main}
There exist positive numerical constants $C, C'$ such that the following holds. Let $\{ \mu_1, \dots, \mu_k \}$ be a $\CD(\rho)$ family of distributions centered around $\mu_0$, let $m,n \ge 1$ be integers such that $\rho \le (n \ln k)^{-1/2}/C'$.
If
\begin{equation} \label{eq:corr}
\sum_{S \subseteq [k] \colon \lvert S \rvert \ge 2} n^{-\lvert S \rvert/2} \rho^{-\lvert S \rvert} \left\lvert \mathbb{E}_{A \sim \mu_0} \prod_{i\in S} \left(\frac{\mu_i(A)}{\mu_0(A)} - 1\right) \right\rvert
\le \frac{1}{n}~,
\end{equation}
then any $(m,n)$-protocol identifying $\{\mu_1, \dots, \mu_k\}$ with error $1/3$ has a communication complexity of at least
\[ \frac{k}{C \rho^2 n \log (k /(n\rho^2))}. \]
In particular, \eqref{eq:corr} holds if there exists an integer $\ell \ge 2$ such that all the terms in \eqref{eq:corr} corresponding to $\lvert S \rvert \le \ell$ are zero, and $n \ge k^{2(\ell+1)/(\ell-1)}$.
\end{theorem}

The proof appears in \subsecref{sec:main-pr}, whereas \lemref{lem:main} is its main ingredient.
To explain the intuition, let $B_i$ (for $i\in[k]$) be the random variable $\frac{\mu_i(A)}{\mu_0(A)}$, where $A$ is sampled from $\mu_0$, and note that its expectation is always $1$. \eqref{eq:corr} corresponds to requiring $B_i$ to be approximately uncorrelated when $n$ is large enough, namely
\[
\sum_{S \subseteq [k] \colon \lvert S \rvert \ge 2} n^{-\lvert S \rvert/2} \rho^{-\lvert S \rvert} \left\lvert \mathbb{E}\prod_{i\in S} \left(B_i -\mathbb{E}[B_i]\right)\right\rvert
\le \frac{1}{n}~.
\]
The last part of the theorem simply states that this indeed holds, if the $B_i$ random variables are uncorrelated up to order $\ell$, and $n$ is large enough. In particular, for large $n$, pairwise uncorrelation ($\ell=2$) is sufficient. The theorem implies that if the distributions are ``uncorrelated'' in this sense, then the task of identifying $\mu \in \{ \mu_1, \dots, \mu_k\}$ requires a communication complexity of $\tilde\Omega(k/(n \rho^2))$. Crucially, the required communication scales linearly with the number of distributions $k$, and is no better than what we would need for solving $k$ completely independent problems, each involving distinguishing only two such distributions.

%To simplify the presentation of \thmref{thm:main}, we state a direct corollary which corresponds only to pairwise uncorrelated distributions.
%\begin{corollary}
%	Let $\mu_0,\mu_1,\dots, \mu_k$ be distributions over the same sample space, let $m$ and $n \ge k^6$ be positive integers and let $0 < \rho \le (n \ln k)^{-1/2}/C$, for some absolute constant $C > 0$. Assume that the following holds:
%	\begin{itemize}
%		\item For any event $E$ and any $i \in \{1,\dots,k\}$, $|\mu_i(E)/\mu_0(E)-1| \le \rho$.
%		\item For any $i \ne j \in \{1,\dots, k\}$, $\mathbb{E}_{A \sim \mu_0} \frac{\mu_i(A) \mu_j(A)}{\mu_0(A)\mu_0(A)} = 1$.
%	\end{itemize}
%	Then, any $(m,n)$ protocol which learns a distribution from $\{\mu_1,\dots,\mu_k\}$ with error $1/3$ has a communication of at least $k/(C\rho^2 n \log(k/(n\rho^2)))$.
%\end{corollary}
% As we discuss in \secref{sec:ideas}, this type of argument was used in previous work, but crucially relied on explicit statistical independence assumptions. In contrast, here we only need pairwise uncorrelation assumptions, and this is crucial for our analysis to succeed.

% Intuitively, the implies that the task of identifying distinguishing $\mu_i$ from $\mu_0$, given any data $A$ sampled from 
% implies in particular that 
% We assume that $n$ is polynomially large in $k$ and $\rho = \tilde{O}(n^{-1/2})$. Let $A \sim \mu_0$ and let $B_i$ be the random variable $\frac{\mu_i(A)}{\mu_0(A)}$, for any $i \in [k]$. Then $B_i$ has a mean of $1$. These $B_i$ are pairwise uncorrelated if
% \[
% \mathbb{E}_{A \sim \mu_0} \left( \frac{\mu_i(A)}{\mu_0(A)} - 1 \right)\left( \frac{\mu_j(A)}{\mu_0(A)} - 1 \right) = 0
% \]
% for any $i \ne j \in [k]$, or, equivalently, if
% \[
% \int_\Omega \frac{d \mu_i}{d\mu_0} \frac{d \mu_j}{d \mu_0} d \mu_0 
% = \int_\Omega \frac{d \mu_i}{d \mu_0} d \mu_0 \int_\Omega \frac{d \mu_j}{d \mu_0} d \mu_0
% =1.
% \]
% where $\Omega$ is the joint sample space. We show that if these $B_i$ are pairwise uncorrelated, 



% \note{OS: The following paragraph is a nice observation, but maybe not essential. Since we have space issues, maybe we should drop it?} 
% {commented out}
%As a side note, we believe that if these variables $B_i$ are far from being pairwise independent, then one can save a lot of communication. For instance, consider the case that the distributions $\mu_1, \dots, \mu_k$ can be divided into pairs of distributions, such that the two distributions in each pair are very similar. Then, the following two-step protocol can be applied to find the source of the data: first, find the pair where the data comes from. Then, find which distribution out of the pair generated the data. Note that since the two distributions of any pair are similar, in the first step one does not to communicate separately for each member of the pair and each party can save half of the communication by communicating one bit per pair, instead of one bit per distribution. This enables saving half of the communication. If instead of pairs we can divide the distributions to $k/r$ sets of $r$ similar distributions, one can do with a communication lower by $1/r$ compared to the naive approach. Assuming that the $B_i$ variables are pairwise uncorrelated, one avoids this situation: all distributions are orthogonal to each other in some sense.

% Theorem~\ref{thm:main} shows that the communication complexity for this problem is $\tilde{\Omega}(k/(n\rho^2)-m)$, as opposed to the naive upper bound of $O\left( \frac{k \log k}{n \rho^2}\right)$.
% {correct the upper bound: it should depend on the KL differences between the  distributions}

We now turn from communication complexity to memory complexity. The following theorem establishes a lower bound on the product of the sample size, memory, and number of data passes for any memory-constrained algorithm which identifies $\mu_1,\ldots,\mu_k$: 

\begin{theorem} \label{thm:mem}
There exist positive numerical constants $C^{(2)},C^{(3)}$ such that the following holds. Let $\{ \mu_1, \dots, \mu_k \}$ be a $\CD(\rho)$ family centered around $\mu_0$, and let $t,s,\ell \ge 1$ be integers. Assume that there exists $n \le C^{(2)}/(\rho^2\log k)$ such that the conditions of Theorem~\ref{thm:main} hold, with respect to $k$, $n$ and $\rho$. Then any $(t,s,\ell)$-algorithm identifying $\mu_1, \dots, \mu_k$ with $1/3$ error satisfies
\[
ts\ell \ge \frac{k}{C^{(3)} \rho^2 \log k}.
\]
\end{theorem}
The proof of the theorem is a simple reduction to the communication complexity lower bound of \thmref{thm:main}: Given a $(t,s, \ell)$ algorithm, and any $m,n$ such that $mn\geq t$, one can create an $(m,n)$ protocol which simulates the algorithm in a distributed setting as follows: Fixing some arbitrary order over the parties, each party in turn simulates the $(t,s,\ell)$ algorithm over its data. Once the party exhausts her data, the state of this algorithm (consisting of at most $s$ bits) is transmitted to the next party, which continues to simulate the algorithm, and so on. Once $t$ data points have been processed in this manner, the current party transmits the algorithm's state back to the first party, which starts simulating the next pass of the $(t,s,\ell)$ algorithm. This continues until $\ell$ such passes are done. Then, the output of the protocol is set as the output of the simulated $(t,s,\ell)$ algorithm. The overall communication complexity is at most $ts\ell/n$, so by \thmref{thm:main} (assuming its conditions are fulfilled), we must have
\begin{equation}\label{eq:memcom}
\frac{ts\ell}{n}~\geq~\frac{k}{C\rho^2 n\log(k/(n\rho^2))}.
\end{equation}
In particular, picking $m=k$ and $n=C^{(2)}/(\rho^2 \log k)$ for any constant $C^{(2)}\leq C'^{-2}$ concludes the proof.

%The naive upper bound is $ts\ell = O\left(\frac{k \log^2 k}{\rho^2} \right)$.

% {make sure that this algorithm is formatted correctly inside the paper}
% \begin{wrapfigure}{L}{0.5\textwidth}
% \begin{algorithmic}
% \STATE{\textbf{Input:} Each party $i \in [m]$ receives an input $X^{(i)}=\left(X^{(i)}_1, \dots, X^{(i)}_n\right)$ of $n$ independent samples from an unknown distribution}
% \STATE{The parties initialize the state of a $(t,s,\ell)$ algorithm $A$}
% \FORALL{$j \in [\ell]$}
% 	\FORALL{$i \in [m]$}
% 		\FORALL{$u \in [m]$}
% 			\STATE{Party $i$ feeds algorithm $A$ with example $X^{(i)}_u$ and stores its memory}
% 		\ENDFOR
% 		\STATE{Party $i$ broadcasts the memory of algorithm $A$}
% 	\ENDFOR
% \ENDFOR
% \STATE{\textbf{Output:} the output of algorithm~A}
% \end{algorithmic}
% \caption{From a memory-bounded algorithm to a communication protocol}
% \label{fig:alg-red}
% \end{wrapfigure}

We finish this subsection with two additional remarks:
\begin{remark}[Identification vs. binary decision] \label{remark:id-vs-binary}
In the results of this paper, we focus on the problem of identifying an underlying distribution, under the promise that it belongs to a certain family of distributions $\mu_1,\ldots,\mu_k$ (e.g., which pair of coordinates are correlated). An arguably easier task is to decide whether the underlying distribution is either some fixed $\mu_0$ or one of $\mu_1,\ldots,\mu_k$ (e.g., whether there exists a correlated pair of coordinates or not). However, our lower bounds apply to that task as well, with an almost identical proof. 
\end{remark}%
\begin{remark}[Data access] \label{remark:data-access}
	Our memory-based bounds assume that the algorithm performs one or more 
	passes over the data. An even weaker assumption might be that the algorithm 
	can access the data in an arbitrary order (i.e. has random access). 
	However, proving a super-linear (in dimension) memory lower bound in this 
	setting would imply a super-linear lower bound on the runtime of any 		random-access Turing machine, and unfortunately, this is related to difficult questions in computational complexity (see \citet[Section 1.2]{raz2016fast} for a related discussion).
\end{remark}

\subsection{Binary Vectors}\label{subsec:binary}

Having establishes our main technical results, we now turn to derive concrete bounds in the context of detecting correlations. In this subsection, we begin with the case of distributions over binary vectors, where the goal is to detect some unique (pairwise or higher-order) correlation. Concretely, fix some $0<\rho<1$, and define the sample space as $\Omega = \{-1,1\}^d$ for some $d \ge 2$. Let $\mathcal{I}$ be the set of all nonempty subsets of $\{1,\dots,d\}$. For any $I \in \mathcal{I}$, let $\mu_{I,\rho}$ be the distribution over $\Omega$ defined by 
\[
\mu_{I,\rho}((x_1,\dots,x_d)) = 2^{-d} (1 + \rho \prod_{i\in I} x_i).
\]
Namely, $\mu_{I,\rho}$ samples with probability $\frac{1}{2}(1+\rho)$ an element uniformly from all elements with an even number of $-1$ values in the coordinates corresponding to $I$ and with probability $\frac{1}{2}(1-\rho)$ it samples an element with an odd number of $-1$ values in $I$.
Note that $\mu_{I,\rho}$ encodes a unique correlated subset of indices in the following manner (the proof appears in \subsecref{sec:pr-biased}): 
\begin{lemma} \label{lem:biased-subset}
For any set $I' \in \mathcal{I}$, $I' \ne \emptyset$, it holds that
$
\mathbb{E}_{X\sim \mu_{I,\rho}} \prod_{i\in I'} X_i = \begin{cases}
\rho & I' = I \\
0 & I' \ne I
\end{cases}
$.
\end{lemma}

For any subset $\mathcal{U} \subseteq \mathcal{I}$ and $0 < \rho < 1$, let $\mathcal{P}_{\mathcal{U},\rho} = \{ \mu_{I,\rho} \colon I \in \mathcal{U}\}$.
We apply Theorems~\ref{thm:main} and \ref{thm:mem} on the problem of identifying an underlying distribution $\mu$, promised to belong to the family $\mathcal{P}_{\mathcal{U},\rho}$, to get communication and memory lower bounds (the proof appears in \subsecref{sec:pr-subset}).

\begin{theorem} \label{thm:subset-parity}
Fix some $\mathcal{U} \subseteq \mathcal{I}$ which satisfies $\lvert \mathcal{U} \rvert \ge 2$. Fix integers $m,n\geq 1$ such that $n \ge \lvert \mathcal{U} \rvert^6$, and a positive $\rho \le n^{-1/2} \ln^{-1/2} \lvert \mathcal{U} \rvert/C$, where $C$ is a numerical constant. Then any $(m,n)$ protocol identifying  $\mu \in \mathcal{P}_{\mathcal{U},\rho}$ with $1/3$ error has a communication complexity of at least
\[ \frac{\left\lvert\Ucal\right\rvert}{C \rho^2 n \log (\left\lvert\Ucal\right\rvert^2 /(n\rho^2))}. \]
\end{theorem}

For example, the case of detecting pairwise correlations corresponds to choosing $\mathcal{U} = \{ I \in \mathcal{I} \colon \lvert I \rvert = 2\}$. Since $|\mathcal{U}|=\binom{d}{2}=\Omega(d^2)$, this gives us a lower bound of $\tilde\Omega\left( \frac{d^2}{\rho^2 n}-m \right)$, or $\tilde\Omega\left( \frac{d^2}{\rho^2 n}\right)$. This is optimal up to logarithmic factors, as shown by the upper bound discussed in the introduction. More generally, for order-$r$ correlations (for some constant $r\geq 2$), we simply pick $\mathcal{U} = \{ I \in \mathcal{I} \colon \lvert I \rvert = r\}$, and since $|\mathcal{U}|=\binom{d}{r}=\Omega(d^r)$ in this case, the theorem implies a communication complexity of $\tilde\Omega\left( \frac{d^r}{\rho^2 n}\right)$. Again, this is tight up to logarithmic factors, using a straightforward generalization of the protocol for the pairwise case.

Next, we state the analogue of \thmref{thm:subset-parity} for the memory-constrained setting (derived from \thmref{thm:subset-parity} by the same communication-to-memory reduction discussed earlier):

\begin{theorem} \label{thm:mem-subset}
There exist numerical constants $C,C'>0$ such that the following holds. For any $\mathcal{U} \subseteq \mathcal{I}$ such that $\lvert \mathcal{U} \rvert \ge C'$ , any $\rho$ such that $0\le \rho\le \lvert \mathcal{U} \rvert^{-3} \ln^{-1/2} \lvert \mathcal{U} \rvert C^{-1}$, and any integers $t,s,\ell \ge 1$, it holds that any $(t,s,\ell)$-algorithm identifying $\mu \in \mathcal{P}_{\mathcal{U},\rho}$ with $1/3$ error satisfies
\[
ts\ell \ge \frac{\lvert \mathcal{U} \rvert}{C \ln \lvert\mathcal{U}\rvert \rho^2}.
\]
\end{theorem}

As a special case, the theorem implies that for detecting pairwise correlations, $ts\ell=\Omega(d^2/\rho^2)$, and for order-$r$ correlations, $ts\ell=\Omega(d^r/\rho^2)$. For example, assuming the number of passes $\ell$ is constant, it implies that we cannot successfully detect the correlation, unless either the memory is large (on order $d^r$), or the number of samples used is much larger than what is required without memory constraints (i.e. $\Omega(\log(d)/\rho^2)$) for any constant $r$). 

\begin{remark}[Constraints on problem parameters]\label{remark:rhosmall}
Theorem \ref{thm:mem-subset} requires the correlation $\rho$ to be sufficiently small compared to $|\Ucal|$. Such an assumption is necessary to get a strong lower bound: To see this, consider the case of detecting a pairwise correlation in binary vectors with memory constraints. If we can store $\tilde{O}(d/\rho^2)$ bits in memory, then we can simply collect and store $\tilde{O}(1/\rho^2)$ 
data points, and the empirical correlations in this data will reveal the true 
correlated coordinates with high probability. Thus, to prove an 
$\tilde{\Omega}(d^2)$ memory lower bound (as we do here), the correlation $\rho$ \emph{must} be smaller than $\tilde{O}(d^{-1/2})$.
Similarly, in a communication constrained setting, note that a communication budget of $\tilde{O}(d/\rho^2)$ bits enables the players to exchange $\tilde{O}(1/\rho^2)$ data points and find the correlation. Hence, in order to prove a communication lower bound of $\tilde{\Omega}\left(d^2/\left(n \rho^2\right)\right)$, one has to assume that $n = \tilde\Omega(d)$.
That being said, in the theorems above we require a stronger bounds on $\rho$ and $n$ than what these arguments imply. In Appendix \ref{sec:tuples}, we show that these requirements can be weakened to some extent,  for the case of $\mathcal{U} = \{ I \in \mathcal{I} \colon \lvert I \rvert = r\}$, $r\geq 2$. Precisely characterizing the parameter regimes where non-trivial lower bounds are possible is left to future work. 
\end{remark}


%\begin{remark}[Application to Learning Noisy Parities]
% Another application of theorems~\ref{thm:subset-parity}~and~\ref{thm:mem-subset} are memory/communication lower bounds for the problem of learning parities with noise. In that problem, a learner receives a stream of pairs $(x,y)$, where each $x$ is drawn independently and uniformly from $\{0,1\}^\ell$, and $y = \langle w,x \rangle~\text{mod}~2$ with probability $\frac{1}{2}(1+\rho)$ and $y = 1 - \langle w,x \rangle ~\text{mod}~2$ otherwise (with $w\in \{0,1\}^d$ being unknown). The learner's goal is to determine $w$. Then, if $\rho = \tilde{O}(8^{-\ell})$, one would require $\tilde\Omega(2^\ell/\rho^2)$ samples to find find $w$, and a similar communication complexity result applies. If $w$ is the parity vector, then the data is generated such that the bits of $x$ corresponding to the support of $w$ and the bit of $y$ are all correlated, with a probability of $\frac{1}{2}(1+\rho)$ of being with a parity of $0$. Hence, if we substitute $d = \ell+1$, the data $(x,y)$ is generated from the distribution $\mu_{I_w,\rho}$, where $I_w$ contains the bits of $x$ corresponding to the support of $w$ and the bit of $y$.
% \end{remark}

% \note{Maybe remove remark above? discuss with yuval}

\subsection{Gaussian Distribution}\label{subsec:gaussian}

Having discussed distributions supported on binary vectors, we now turn to prove similar results for another cannonical family of distributions, namely Gaussian distributions on $\reals^d$. In what follows, we focus on pairwise correlations (since a multivariate Gaussian distribution is uniquely determined by its mean and covariance matrix, there is no sense in discussing higher-order correlations as in the binary case). 

Define $\mathcal{I}_2 = \left\{ S \subseteq [d] \colon \lvert S \rvert = 2\right\}$.
Fix some $d \ge 3$ and $0<\sigma<1$. For any set $I \in \mathcal{I}_2$, let $\eta_{I,\sigma}$ denote the zero-mean Gaussian distribution on $\mathbb{R}^d$, with covariance matrix $\Sigma_{I,\sigma}$ defined as follows:
\[
\Sigma_{I,\sigma}(i,j) = \begin{cases}
1 & i=j \\
\sigma & I = \{i,j\} \\
0 & \text{otherwise}.
\end{cases}
\]
In words, each individual coordinate has a variance of $1$, and each pair of distinct coordinates are uncorrelated, except for the pair $(i,j)$ with a correlation $\sigma$. Let $\mathcal{G}_\sigma=\{\eta_{I,\sigma} \colon I \in \mathcal{I}_2 \}$ be the set of all $\binom{d}{2}$ distributions defined this way. The following theorems are analogues of Theorems \ref{thm:subset-parity} and \ref{thm:mem-subset} for the case of pairwise correlations (the proof appears in \subsecref{sec:pr-normal}):

% We show that if $n$ is sufficiently large and $\sigma$ is sufficiently small, one gets a lower bound on the hardness of identifying $\eta \in \mathcal{G}_\sigma$ both in terms of communication complexity, which has to be at least $\tilde\Omega(d^2/\sigma^2 n)$ and in terms of a memory, which satisfies $ts\ell = \tilde\Omega(d^2/\sigma^2)$. As before, the memory complexity theorem is a direct corollary of the communication complexity theorem. 

\begin{theorem} \label{thm:normal}
Fix some $n,m \ge 1$ and $0<\sigma<1$, such that $n \ge Cd^6$ for some numerical constant $C>0$ and $\sigma \le n^{-1/2} \ln^{-1/2} d \ln^{-1}(dnm/\sigma) / C$. Any $(m,n)$-protocol identifying $\eta \in \mathcal{G}_\sigma$ with $1/6$ error has a communication complexity of at least
\[
\frac{d^2}{C \sigma^2 \ln^2 (nmd/\sigma) \ln (d /(n\sigma^2)) n }~.
\]
\end{theorem}

\begin{theorem} \label{thm:normal-mem}
There exist numerical constants $C,C'>0$ such that the following holds. If $d\geq C'$, then for any $\sigma$ such that $ 0 < \sigma \le \left(C d^{3} \ln^{1/2} d \ln (d/\sigma)\right)^{-1}$ and any integers $t,s, \ell \ge 1$, it holds that any $(t,s,\ell)$-algorithm identifying  $\mu \in \mathcal{G}_\sigma$ with $1/6$ error satisfies
\begin{equation*}
ts\ell \ge \frac{d^2}{C \sigma^2 \ln^3 d \ln^2 (1/\sigma)}.
\end{equation*}
\end{theorem}

Whereas for binary vectors, our results are a direct corollary of Theorem~\ref{thm:main}, the proofs in the Gaussian case are more involved,  because no family of distinct Gaussian distributions satisfy the $\CD(\rho)$ property from Definition \ref{def:cd}. Instead, we need to work with truncated Gaussian distributions (which do satisfy this property), with some determinant calculations required to verify the conditions of Theorem~\ref{thm:main}. We then reduce the resulting bound on truncated distributions to non-truncated ones, to get \thmref{thm:normal}. \thmref{thm:normal-mem} is derived from \thmref{thm:normal} by the same communication-to-memory reduction discussed earlier.

% one has to study truncated normal distributions, since non-truncated distributions are not $\CD(\rho)$. Some determinant calculations are required in order to show that these truncated distributions satisfy \eqref{eq:corr}. Then, one can reduce the bound on truncated distributions to non-truncated distributions.
