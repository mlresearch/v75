% !TEX root = paper.tex

We now state a simple, yet powerful result that characterizes when existence of a Burkholder function for a sufficient statistic representation pair $(\suff,V)$ is not only sufficient, but \emph{necessary} to obtain a particular regret bound.
\begin{proposition}
\label{prop:lb}
Let $\delta=(\delta_1,\ldots,\delta_n)$ be a $[-L,L]$-valued martingale difference sequence over filtration $\F_{t-1}=\sigma(\delta_1,\ldots,\delta_{t-1})$ and let $\bz=(\bz_1,\ldots,\bz_n)$ be a sequence of functions $\bz_t: [-L,L]^{t-1} \to \X \times \Y$, each viewed as a predictable process with respect to $\F_{t-1}$. Suppose for every such $(\delta, \bz)$ pair there exists a randomized adversary strategy $(x_t, y_t)$ that guarantees, for every learner strategy $(\yh_t)_{t\leq{}n}$,
\begin{equation}
\label{eq:lb}
\En\sup_{f\in\cF}\brk*{\sum_{t=1}^n\loss(\pred_t,y_t)-\loss(f(x_t),y_t) - \cA(f; x_{1},\ldots,x_{n})}
\geq{} \En\left[  V\left(\sum_{t=1}^n \suff(\bz_t, \delta_t) \right) \right].
\end{equation}
Then, if there exists a strategy $(\yh_t)_{t\leq{}n}$ that achieves the regret bound $\cA(f;\xr[n])$, this implies that
\[
\sup_{\delta, \mb{z}}\En\left[  V\left(\sum_{t=1}^n \suff(\bz_t, \delta_t) \right) \right]\leq{}0.\footnote{In the more general case, if \pref{eq:lb} holds up to additive slack $\Delta$, the corresponding condition is $\sup\En\brk*{V}\leq{}\Delta$.}
\]
Consequently, the regret bound $\cA(f;\xr[n])$ is achievable only if there exists a Burkholder function $\burk:\cT\to\bbR$ that satisfies properties \propone/\proptwo/\propthree of \pref{lem:equivalence_burkholder}. 

When $\alpha \mapsto V(\tau+\suff(z,\alpha))$ is convex for any $z\in\X\times\Y,\tau\in\T$, we only require the preceeding inequalities to hold for $\delta_t=\epsilon_t \cdot L$, $\forall{}t=1,\ldots,n$, where $\epsilon_t$s are independent Rademacher random variables. In this case achievability of the regret bound $\cA(f;\xr[n])$ only implies existence of a Burkholder function $\burk$ satisfying property \propthreep{}, not \propthree{}.
\end{proposition}
\paragraph{Linear Classes}
At first glance the conditions of \pref{prop:lb} may seem fairly restrictive, but it is fairly straightforward to instantiate for all the examples in this paper. Consider the following linear setting: Take $\cX\subseteq{}\bbR^{d}$, $\cY$ arbitrary, and let $\cF$ be a linear class of the form $\crl*{x\mapsto{}\tri*{w,x}\mid{}w\in\cW}$, where $\sup_{x\in\cX,w\in\cW}\tri*{w,x}\leq{}1$ and $\cW$ is symmetric. Pick an arbitrary vector space $\overline{\cT}$, let $\overline{\suff}:\cX\to\overline{\cT}$ be an any featurization of the input space, and let $F:\overline{\cT}\to\bbR$ be an arbitrary function. Our goal will be to achieve a regret bound of the form 
\begin{equation}
\label{eq:suff_example}
\sum_{t=1}^{n}\ls(\yh_t, y_t) - \inf_{f\in\cF}\sum_{t=1}^n \loss(f(x_t),y_t)\leq{} \cA(\xr[n]) \ldef F\prn*{\sum_{t=1}^{n}\overline{\suff}(x_t)}.
\end{equation}
Let us first consider a natural choice of $V$ for the upper bound in this setting. Linearizing and using symmetry of $\cW$, we have
\[
\sum_{t=1}^{n}\ls(\yh_t, y_t) - \inf_{f\in\cF}\sum_{t=1}^n \loss(f(x_t),y_t) - \cA(\xr[n])
\leq{} \sum_{t=1}^{n}\yh_t\cdot\dl_t + \sup_{w\in\cW}\tri*{w,\sum_{t=1}^{n}\dl_{t}x_t} - F\prn*{\sum_{t=1}^{n}\overline{\suff}(x_t)}.
\]
This means that if we choose a sufficient statistic $\suff: (x_t, \yh_t, \dl_{t})\mapsto{} (\yh_t\dl_t, x_t\dl_t, \overline{\suff}(x_t))\in{}\bbR\times{}\bbR^{d}\times{}\overline{\cT}$ and choose $V(a, x, \overline{\tau})=a + \sup_{w\in\cW}\tri*{w,x} - F(\overline{\tau})$, then it holds that
\[
\sum_{t=1}^{n}\ls(\yh_t, y_t) - \inf_{f\in\cF}\sum_{t=1}^n \loss(f(x_t),y_t) - \cA(\xr[n]) \leq{} V\prn*{\sum_{t=1}^{n}\suff(x_t, \yh_t, \dl_t)}.
\]
Noting that $\alpha\mapsto{}V(\tau + \suff(x, \yh, \alpha))$ is convex, \pref{lem:suff_to_martingale} implies that a sufficient condition to achieve the regret bound for any convex $1$-Lipschitz loss is that
\begin{equation}
\label{eq:v_regret}
\sup_{\mb{z}}\En_{\eps}\brk*{V\prn*{\sum_{t=1}^{n}\suff(\mb{z}_t, \eps_t)}}\leq{}0,
\end{equation}
where $\mb{z}$ is any $\cX\times{}\cY$-valued predictable process with respect to the Rademacher sequence $\eps_{1},\ldots,\eps_{n}$. 

By specializing to the absolute loss $\ls(\yh, y)=\abs*{\yh-y}$ and choosing an adversary that plays $y_{t}$ to be Rademacher random variables and $x_{t}$ to be any predictable sequence, it can be shown that \pref{eq:v_regret} is also \emph{necessary}; this is proven formally in the appendix. As a corollary, we derive the following result.
\begin{proposition}
\label{prop:necessary}
There exists a Burkholder function $\burk$ for the pair $(\suff, V)$ \emph{if and only if} the regret bound \pref{eq:suff_example} is achievable.
\end{proposition}
 Consider the matrix prediction setting of \pref{sec:matrix} for the special case of $L=1$ and $r=1$. This setting fits into the linear class framework above by taking $\cW$ to be the nuclear norm ball in $\bbR^{d_1\times{}d_2}$ and setting $\overline{\suff}(X)=\cM(X)$ for any matrix $X\in\bbR^{d_1\times{}d_2}$. For this setting \pref{prop:necessary} implies the following equivalence.
\begin{example}[Matrix Prediction]
The following are equivalent:
\begin{enumerate}
\item The regret bound
\[
\sum_{t=1}^{n}\ls(\yh_t, y_t) - \inf_{W\;:\;\nrm*{W}_{\Sigma}\leq{}1}\sum_{t=1}^n \loss(\tri*{W,X_t},y_t) \leq{} \frac{\eta{}}{2}\nrm*{\sum_{t=1}^{n}\cM(X_t)}_{\sigma} + \frac{c}{\eta}
\]
is achievable.
\item The martingale inequality
\[
\En_{\eps}\nrm*{\sum_{t=1}^{n}\eps_t\mb{X}_{t}(\eps)}_{\sigma} \leq{} \frac{\eta{}}{2}\En_{\eps}\nrm*{\sum_{t=1}^{n}\cM(\mb{X}_t(\eps))}_{\sigma} + \frac{c}{\eta}
\]
holds for all $\bbR^{d_1\times{}d_2}$-valued predictable processes $\mb{X}$.
\item There exists a Burkholder function for the sufficient statistic pair $(\suff, V)$ in \pref{eq:v_regret}.
\end{enumerate}

\end{example}

