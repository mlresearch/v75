%\documentclass[11pt]{article}
\documentclass[final,12pt]{colt2018}

\usepackage[T1]{fontenc}

\input{colt_style}
\input{dylan}
\input{macros}

%\usepackage[suppress]{color-edits}
%\usepackage{color-edits}
%\addauthor{df}{red}
%\addauthor{ks}{blue}
%\usepackage{showlabels}

%%%% Misc macros

% \newcommand{\pt}{\partial_{t}}
%\newcommand{\dl}{\partial}
\newcommand{\reg}{\mathrm{Reg}_n}
\renewcommand{\trn}{\top}
\newcommand{\dl}{\delta}
\newcommand{\sym}{\mathbb{S}}
\newcommand{\Bfun}{Burkholder }

\newcommand{\predt}{\wt{y}}

\newcommand{\propone}{$1^{o}$}
\newcommand{\proptwo}{$2^{o}$}
\newcommand{\propthree}{$3^{o}$}
\newcommand{\propthreep}{$3'$}

%%%% document info %%%%%%%

\title[Online Learning: Sufficient Statistics and the Burkholder Method]{Online Learning: Sufficient Statistics and the Burkholder Method}
\usepackage{times}

\date{}

%\author{
%  Dylan J. Foster\thanks{Cornell University}
%  \and
%  Alexander Rakhlin\thanks{MIT}
%  \and
%  Karthik Sridharan\footnotemark[1]
%  }
  \coltauthor{\Name{Dylan J. Foster}\Email{djfoster@cs.cornell.edu}\\
\addr Cornell University
\AND
\Name{Alexander Rakhlin}\Email{rakhlin@mit.edu}\\
\addr Massachusetts Institute of Technology
\AND
\Name{Karthik Sridharan}\Email{sridharan@cs.cornell.edu}\\
\addr Cornell University
  }

\begin{document}

\maketitle

\begin{abstract}
We uncover a fairly general principle in online learning: If a regret inequality can be (approximately) expressed as a function of certain ``sufficient statistics'' for the data sequence, then there exists a special \emph{Burkholder function} that 1) can be used algorithmically to achieve the regret bound and 2) only depends on these sufficient statistics, not the entire data sequence,  so that the online strategy is only required to keep the sufficient statistics in memory. This characterization is achieved by bringing the full power of the \emph{Burkholder Method}---originally developed for certifying probabilistic martingale inequalities---to bear on the online learning setting.

To demonstrate the scope and effectiveness of the Burkholder method, we develop a novel online strategy for matrix prediction that attains a regret bound corresponding to the variance term in matrix concentration inequalities. We also present a linear-time/space prediction strategy for parameter-free supervised learning with linear classes and general smooth norms. 

\end{abstract}

\section{Introduction}
\input{intro}



\section{Example: Matrix Prediction}
\label{sec:matrix}
\input{section_matrix}

\section{Further Examples}
\label{sec:further}
\input{section_further}


\section*{Discussion}
Due to space constraints the following additional results have been deferred to the appendix: Discussion of further directions (\pref{app:discussion}), algorithms for parameter-free online learning in Banach spaces (\pref{app:linear_loss}), and necessary conditions for existence of Burkholder functions (\pref{app:necessary}).

\section*{Acknowledgements}
We thank Adam Os\k{e}kowski  for helpful discussions and for suggesting the example in \pref{sec:adagrad}.  DF is supported in part by the NDSEG fellowship. Research was supported in part by the NSF under grants no. CDS\&E-MSS 1521529 and 1521544. KS additionally acknowledges support from NSF CAREER Award 1750575 and a Sloan Research Fellowship.

\bibliography{refs}

\appendix

\section{Further Directions}
\label{app:discussion}
\input{section_discussion}

\section{Fast and Easy Parameter-Free Online Learning}
\label{app:linear_loss}
\input{section_linear_loss}

\section{Necessary Conditions}
\label{app:necessary}
\input{section_necessary}


\section{Proofs}
\label{app:proofs}
\input{appendix}
\input{appendix_proofs}


\section{Burkholder Algorithm Implementation}
\label{app:efficient}

\input{appendix_efficient}

\section{Algebra of \Bfun Functions}
\label{app:algebra}
\input{appendix_algebra}


  
\end{document}
