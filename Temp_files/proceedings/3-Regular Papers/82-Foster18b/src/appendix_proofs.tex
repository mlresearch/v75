% !TEX root = paper.tex


\subsection{Proofs from \pref{sec:matrix}}



\begin{proof}[\pfref{prop:matrix_sufficient}]

  Recall that $\cA_{\eta}(X_{1},\ldots,X_{n}) = \frac{\eta{}rL^{2}}{2}\nrm*{\sum_{t=1}^{n}\cM(X_t)}_{\sigma} + \frac{c}{\eta}$. Linearizing the loss with the adaptive bound as in \pref{eq:phi_comp_adap}, 
\begin{align*}
	&\sum_{t=1}^{n}\ls(\pred_t, y_t) - \inf_{W\in\cW}\ls(\tri*{W,X_t}, y_t) - \cA_{\eta}(X_{1},\ldots,X_{n}) \\
    &\leq \sup_{W\in\cW}\crl*{
       \sum_{t=1}^{n}\partial\ls(\pred_t, y_t)(\pred_t - \tri*{W,X_t}) - \cA_{\eta}(X_{1},\ldots,X_{n})
      } \\
    &=
      \sum_{t=1}^{n}\partial\ls(\pred_t, y_t)\pred_t  + r\nrm*{\sum_{t=1}^{n}\partial\ls(\pred_t, y_t)X_t}_{\sigma}- \cA_{\eta}(X_{1},\ldots,X_{n}) .
  \end{align*}
	We now abbreviate $\partial\ls(\pred_t, y_t) = \dl_{t}$ and expand out $\cA_{\eta}$, yielding
\begin{align*}
        &\sum_{t=1}^{n}\dl_t\cdot\pred_t  + r\nrm*{\sum_{t=1}^{n}\dl_tX_t}_{\sigma} - \frac{\eta{}rL^{2}}{2}\nrm*{\sum_{t=1}^{n}\cM(X_t)}_{\sigma} - \frac{c}{\eta}.
\end{align*}
Using the fact that $\lambda_{1}(\cH(X)) = \nrm*{X}_{\sigma}$, linearity of $\cH$, and that $\cM(X_t)$ is positive semidefinite, we write this as
\begin{align*}
      &\sum_{t=1}^{n}\dl_t\cdot\pred_t  + r\lambda_{1}\prn*{\sum_{t=1}^{n}\dl_t\cH(X_t)} - r\lambda_{1}\prn*{\frac{\eta{}L^2}{2}\sum_{t=1}^{n}\cM(X_t)} - \frac{c}{\eta}
\end{align*}
Sub-additivity of $\lambda_{1}$ gives a further upper bound of
\begin{align*}    
        \sum_{t=1}^{n}\dl_t\cdot\pred_t  + r\lambda_{1}\prn*{\sum_{t=1}^{n}\dl_t\cH(X_t)-\frac{\eta{}L^{2}}{2}\sum_{t=1}^{n}\cM(X_t)} - \frac{c}{\eta}
\end{align*}
Then $\suff(X_t,\pred_t,\delta_t) = \left( \delta_t\cdot\pred_t, \delta_t\cdot \cH(X_t), \cM(X_t) \right)\in\reals\times \sym^{d_1+d_2}\times \sym^{d_1+d_2}_{+}$ is a sufficient statistic. Namely, writing
\[
V(a, H, M) = a + r\lambda_1\prn*{H -\frac{\eta{L^{2}}}{2}M} - \frac{c}{\eta},
\]
our calculation shows that
\[
\sup_{W\in\cW}\crl*{\reg(W) - \cA(X_{1},\ldots,X_{n})
      }
      \leq{} V\prn*{
      \sum_{t=1}^{n}\suff(X_t,\pred_t,\delta_t)
      }.
\]
  \end{proof}

  \begin{proof}[\pfref{thm:matrix_burkholder}]

    Recall that
    \[
      \burk(a, H, M) = a+ \frac{r}{\eta}\log\,\Tr\,\exp\prn*{\eta{}H - \frac{\eta^{2}L^{2}}{2}M} - \frac{c}{\eta}
    \]
We will show that $\burk$ satisfies the three properties of \pref{lem:equivalence_burkholder}. For property \propone{}, we have
\[
\burk(0) = \frac{r}{\eta}\log\prn*{\Tr\prn*{\exp\prn*{0}}} - \frac{c}{\eta} = \frac{r\log(d_1 + d_2)}{\eta} - \frac{c}{\eta}.
\]
Thus, $\burk(0)\leq{}0$ as soon as $c\geq{}r\log(d_1+d_2)$.

For property \proptwo{}, it suffices to show that $\lambda_{1}(H-\frac{\eta{}L^{2}}{2}M) \leq{} \frac{1}{\eta}\log\,\Tr\,\exp\prn*{\eta{}H - \frac{\eta^{2}L^{2}}{2}M}$. To this end, we have
\begin{align*}
  \lambda_{1}\left(H-\frac{\eta{}L^{2}}{2}M \right)  =   \frac{1}{\eta}\log \lambda_{1}\prn*{\exp\prn*{\eta{}H-\frac{\eta^2L^{2}}{2}M}} \leq{} \frac{1}{\eta}\log\,\Tr\,\exp\prn*{\eta{}H-\frac{\eta^2L^{2}}{2}M
  },
\end{align*}
where the equality is well-defined because the matrix under consideration is symmetric and the inequality follows because $e^{A}$ is positive semidefinite for any symmetric matrix $A$.

For the third property, observe that the mapping $\alpha\mapsto{}V(\tau + \suff(z,\alpha))$ is convex (e.g. \citep{lewis1996convex}). Consequently, by \pref{lem:equivalence_burkholder}, it suffices only to prove property $3'$, i.e. that the restricted concavity condition holds only for Rademacher random variables.

Fix $\tau\in\cT$ and $z=(X, \pred)\in\cX\times{}\cY$, and let $\eps$ be a Rademacher random variable. Writing
\[
\tau = (\tau_{1}, \tau_{2}, \tau_{3})\in\reals\times \sym^{d_1+d_2}\times \sym^{d_1+d_2}_{+},
\]
we have
\begin{align*}
 &\En_{\eps}\left[\burk(\tau + \suff(z,\eps{}L))\right] \\
 &= \En_{\eps}\brk*{
   \tau_{1} + \pred\eps{}L + \frac{r}{\eta}\log\,\Tr\,\exp\prn*{\eta{}\tau_{2} - \frac{\eta^{2}L^{2}}{2}\tau_{3} + \eta\eps{}L\cH(X) - \frac{\eta^{2}L^{2}}{2}\cM(X)}
   } - \frac{c}{\eta}. \\
   &= \frac{r}{\eta}\En_{\eps}\brk*{
     \log\,\Tr\,\exp\prn*{\eta{}\tau_{2} - \frac{\eta^{2}L^{2}}{2}\tau_{3} + \eta\eps{}L\cH(X) - \frac{\eta^{2}L^{2}}{2}\cM(X)}
   } +      \tau_{1} - \frac{c}{\eta}.
\end{align*}
Focusing on the log-trace-exponential term, observe that
\begin{align*}
  &  \En_{\eps}\brk*{
  \log\,\Tr\,\exp\prn*{\eta{}\tau_{2} - \frac{\eta^{2}L^{2}}{2}\tau_{3} + \eta\eps{}L\cH(X) - \frac{\eta^{2}L^{2}}{2}\cM(X)}
} \\
&=  \En_{\eps}\brk*{
       \log\,\Tr\,\exp\prn*{\eta{}\tau_{2} - \frac{\eta^{2}L^{2}}{2}\tau_{3} + \log\prn*{\exp\prn*{\eta\eps{}L\cH(X)}} - \frac{\eta^{2}L^{2}}{2}\cM(X)}
       }.
\end{align*}
Since $\exp\prn*{\eta\eps{}L\cH(X)}$ is positive definite and $\eta{}\tau_{2} - \frac{\eta^{2}L^{2}}{2}\tau_{3} -\frac{\eta^{2}L^{2}}{2}\cM(X)$ is symmetric (by assumption), we can apply Lieb's Concavity Theorem to upper bound this by
\begin{align*}	   
  & \log\,\Tr\,\exp\prn*{\eta{}\tau_{2} - \frac{\eta^{2}L^2}{2}\tau_{3} + \log\prn*{\En_{\eps}\exp\prn*{\eta\eps{}L\cH(X)}} - \frac{\eta^{2}L^2}{2}\cM(X)}.
\end{align*}
The Rademacher matrix mgf bound \citep{tropp2012user} now yields
\[
\log\prn*{\En_{\eps}\exp\prn*{\eta\eps{}L\cH(X)}} \preceq \log\prn*{\exp\prn*{\eta^{2}L^{2}\cM(X)}/2} = \eta^{2}L^{2}\cM(X)/2.
\]
Since $A\preceq{}B$ implies $\Tr{}e^{A}\leq{}\Tr{}e^{B}$, this implies that
\[
\En_{\eps}\brk*{
  \log\,\Tr\,\exp\prn*{\eta{}\tau_{2} - \frac{\eta^{2}L^{2}}{2}\tau_{3} + \eta\eps{}L\cH(X) - \frac{\eta^{2}L^{2}}{2}\cM(X)}
}
\leq{}
  \log\,\Tr\,\exp\prn*{\eta{}\tau_{2} - \frac{\eta^{2}L^{2}}{2}\tau_{3}}
\]
Combining everything we proved so far, this implies
\[
  \En_{\eps}\left[U(\tau + \suff(z,\eps{}L))\right]
  \leq{} \tau_{1} + \frac{r}{\eta}  \log\,\Tr\,\exp\prn*{\eta{}\tau_{2} - \frac{\eta^{2}L^2}{2}\tau_{3}}  - \frac{c}{\eta} = U(\tau).
\]

\end{proof}

\begin{proof}[\pfref{corr:matrix_strategy}]
  The Burkholder function $\burk$ satisfies the conditions of \pref{lem:det_strat3}. Direct calculation shows that the strategy in \pref{lem:det_strat3} matches the strategy in the statement of the corollary.
\end{proof}

\begin{proof}[\pfref{corr:matrix_square}]
  We invoke the Burkholder function $\burk$ from \pref{thm:matrix_burkholder} for the special case $r=1$ and $c=\log(d_1 + d_2)$, and $L=1$. In particular, its existence per \pref{lem:equivalence_burkholder} implies (for the corresponding $V$, here denoted $V_{\eta}$ to refer to the $V$ given for a fixed value of $\eta$)
  \[
    \inf_{\eta}\sup_{\mb{z},\mb{p}, n}\En\brk*{V_{\eta}\prn*{\sum_{t=1}^{n}\suff(\mb{z}_t, \dl_{t})}}\leq{}0
  \]
  We use this inequality only for the special case where $\dl_{t}=\eps_{t}$ and $\mb{z}_t=(\mb{X}_{t}(\eps), 0)$. For this special case, the inequality implies
    \[
      \inf_{\eta}\sup_{\mb{X}, n}\En\brk*{\nrm*{\sum_{t=1}^{n}\eps_t\mb{X}_{t}(\eps)} - \frac{\eta}{2}\nrm*{\sum_{t=1}^{n}\cM(\mb{X}_{t}(\eps))} - \frac{\log(d_1 + d_2)}
        {\eta} }\leq{}0.
    \]
    For any fixed martingale $(\mb{X}_{t}(\eps))_{t\leq{}n}$, this implies
    \begin{align*}
      \En\nrm*{\sum_{t=1}^{n}\eps_t\mb{X}_{t}(\eps)} &\leq{} \inf_{\eta>0}\crl*{\frac{\eta}{2}\En\nrm*{\sum_{t=1}^{n}\cM(\mb{X}_{t}(\eps))} + \frac{\log(d_1 + d_2)}{\eta} } \\ &= \sqrt{2\En\nrm*{\sum_{t=1}^{n}\cM(\mb{X}_{t}(\eps))}\log(d_1 + d_2)}.
    \end{align*}
	To conclude, observe that for any sequence $(X_t)$ we have  
	  \[
	    \nrm*{\sum_{t=1}^{n}\cM(X_t)}_{\sigma} \leq{}  \max\crl*{\nrm*{\sum_{t=1}^{n}X_tX_t^{\trn}}_{\sigma}, \nrm*{\sum_{t=1}^{n}X_t^{\trn}X_t}_{\sigma}}.
	  \]
	  Indeed, $\sum_{t=1}^{n}\cM(X_t)  = \left(
	  \begin{array}{ll}
	  \sum_{t=1}^{n}X_tX_t^{\trn}& 0\\
	  0 & \sum_{t=1}^{n}X_t^{\trn}X_t
	  \end{array}
	  \right)$ and the spectral norm of a block-diagonal matrix is always obtained by the spectral norm of one of its blocks.
    


\end{proof}

\subsection{Proofs from \pref{sec:further}}
\label{app:square}

\begin{proof}[\textbf{Sketch of proofs for claims from \pref{sec:adagrad}}]

For the $\ls_{2}$ result we have
\begin{align*}
& \sum_{t=1}^n\loss(\pred_t,y_t) - \min_{\norm{w}_{2}\leq 1}\sum_{t=1}^n \loss(\inner{w,x_t},y_t) - 2L\sqrt{\sum_{t=1}^{n}\nrm*{x_t}^{2}_{2}} \\
& \leq{} \sup_{\norm{w}_{2}\leq 1} \crl*{\sum_{t=1}^n\partial\loss(\pred_t,y_t)(\yh_t, - \tri*{w, x_t}) } - 2L\sqrt{\sum_{t=1}^{n}\nrm*{x_t}^{2}_{2}} \\ 
& = \sum_{t=1}^n\partial\loss(\pred_t,y_t)\yh_t + \nrm*{\sum_{t=1}^{n}\partial\loss(\pred_t,y_t)x_t}_{2} - 2L\sqrt{\sum_{t=1}^{n}\nrm*{x_t}^{2}_{2}} \\
& \leq{} \sum_{t=1}^n\partial\loss(\pred_t,y_t)\yh_t + \burk_{\textrm{square}}\prn*{\sum_{t=1}^{n}\partial\loss(\pred_t,y_t)x_t, L\sqrt{\sum_{t=1}^{n}\nrm*{x_t}^{2}_{2}}}. 
\end{align*}
The path from here to a Burkholder function in the sense of \pref{lem:equivalence_burkholder} is clear given the three properties of $\burk_{\textrm{square}}$ stated in the main body.

For the $\ls_{\infty}$ result, the quantity
\begin{align*}
& \sum_{t=1}^n\loss(\pred_t,y_t) - \min_{\norm{w}_{\infty}\leq 1}\sum_{t=1}^n \loss(\inner{w,x_t},y_t) - 2L\nrm*{\prn*{\sum_{t=1}^{n}x_{t}^{2}}^{1/2}}_{1} 
\end{align*}
can be upper bounded by
\begin{align*}
&\sup_{\norm{w}_{\infty}\leq 1} \crl*{\sum_{t=1}^n\partial\loss(\pred_t,y_t)(\yh_t, - \tri*{w, x_t}) } - 2L\nrm*{\prn*{\sum_{t=1}^{n}x_{t}^{2}}^{1/2}}_{1} \\ 
& = \sum_{t=1}^n\partial\loss(\pred_t,y_t)\yh_t + \nrm*{\sum_{t=1}^{n}\partial\loss(\pred_t,y_t)x_t}_{1} - 2L\nrm*{\prn*{\sum_{t=1}^{n}x_{t}^{2}}^{1/2}}_{1} \\
& \leq{} \sum_{t=1}^n\partial\loss(\pred_t,y_t)\yh_t + \sum_{i=1}^{d}\burk_{\textrm{square}}\prn*{\sum_{t=1}^{n}\partial\loss(\pred_t,y_t)x_t[i], L\sqrt{\sum_{t=1}^{n}\prn*{x_t[i]}^{2}_{2}}},
\end{align*}
where $x_{t}[i]$ refers to the $i$th coordinate of $x_t$. Once again, the three properties of $\burk_{\textrm{square}}$ directly lead to a valid Burkholder function $\burk$.
\end{proof}

\begin{proof}[\pfref{prop:square_loss_sufficient}]  
  Let $A_{n}=\rho\sum_{t=1}^{n}z_tz_t^{\trn}+\lambda{}I$ and $A_{0}=\lambda{}I$. Recall that $\Psi_{A}(w) = \frac{1}{2}\tri*{w,Aw}$. We begin by rewriting the desired regret bound as
  \[
    \cA(w;z_{1},\ldots,z_{n}) = \lambda\Phi((w,1)) + c\log\prn*{\det(A_n)/\det(A_{0})}
  \]
  for a constant $c>0$ to be determined. With this definition, we have
  \begin{align*}
    &\sup_{w\in\bbR^{d}}\crl*{
    \reg(w) - \cA(w;z_{1},\ldots,z_{n})
      } \\
    &=\sup_{w\in\bbR^{d}}\crl*{
      \sum_{t=1}^{n}\ls(\yh_t, y_t) - \sum_{t=1}^{n}\ls(\tri*{w,x_t}, y_t)  - \lambda\Phi((w,1))
      } - c\log\prn*{\det(A_n)/\det(A_{0})}
      \intertext{Using strong convexity of $\ls$:}
    &=\sup_{w\in\bbR^{d}}\crl*{
      \sum_{t=1}^{n}\partial{}\ls(\yh_t, y_t)(\yh_t - \tri*{w,x_t}) - \frac{\rho}{2}\prn*{\yh_t - \tri*{w,x_t}}^{2}  - \lambda\Phi((w,1))
      } - c\log\prn*{\det(A_n)/\det(A_{0})}\\
    &=\sup_{w\in\bbR^{d}}\crl*{
      \sum_{t=1}^{n}\partial{}\ls(\yh_t, y_t)(-\tri*{(w,1),z_t}) - \frac{\rho}{2}\prn*{\tri*{(w,1),z_t}}^{2}  - \lambda\Phi((w,1))
      } - c\log\prn*{\det(A_n)/\det(A_{0})}
      \intertext{We now move to an upper bound by allowing the final coordinate of $(w,1)$ to act as a free parameter.}
    &\leq{}\sup_{w\in\bbR^{d+1}}\crl*{
      \sum_{t=1}^{n}\partial{}\ls(\yh_t, y_t)\tri*{w,z_t} - \frac{\rho}{2}\tri*{w,z_t}^{2}  - \lambda\Phi(w)
      } - c\log\prn*{\det(A_n)/\det(A_{0})}
      \intertext{We can rewrite this as}
    &\leq{}\sup_{w\in\bbR^{d+1}}\crl*{
      \tri*{w,\sum_{t=1}^{n}\partial{}\ls(\yh_t, y_t)z_t} - \Psi_{\rho\Sigma_{n}}(w)  - \lambda\Phi(w)
      } - c\log\prn*{\det(A_n)/\det(A_{0})}\\
     &=\sup_{w\in\bbR^{d+1}}\crl*{
      \tri*{w,\sum_{t=1}^{n}\partial{}\ls(\yh_t, y_t)z_t} - \Psi_{A_{n}}(w)
      } - c\log\prn*{\det(A_n)/\det(A_{0})}\\
    &=\Psi_{A_n}^{\star}\prn*{
      \sum_{t=1}^{n}\partial{}\ls(\yh_t, y_t)z_t}
      - c\log\prn*{\det(A_n)/\det(A_{0})}.
  \end{align*}
  
This establishes that $\suff(x_t,\pred_t,\delta_t) = \left( \delta_{t}z_t, z_tz_t^{\trn} \right)\in\bbR^{d+1}\times{}\sym^{d+1}_{+}$ is a sufficient statistic. This is because we can write
\[
V(x, A) = \Psi^{\star}_{\rho{}A + \lambda{}I}\prn*{x} - c\log\prn*{\det(\rho{}A + \lambda{}I)/\det(A_0)}.
\]
and we just proved that
\[
\sup_{w\in\bbR^{d}}\crl*{\reg(w) - \cA(x_{1},\ldots,x_{n})
      }
      \leq{} V\prn*{
      \sum_{t=1}^{n}\suff(x_t,\pred_t,\delta_t)
      }.
\]

\end{proof}

\begin{proof}[\pfref{thm:square_loss_burkholder}]
Recall that we have defined \[\burk(x, A) = V(x,A) =\Psi^{\star}_{A}\prn*{x} - c\log\prn*{\det(A)/\det(A_0)}.\] We verify the properties from \pref{lem:equivalence_burkholder}. Property \proptwo{} is immediate, and for property \propone{} we have
\[
\burk(0) = \Psi^{\star}_{0 + \lambda{}I}(0) - c\log(\det(A_0)/\det(A_0)) = 0.
\]
We proceed to prove property \propthree{}. Fix $\tau=(\tau_1, \tau_2)\in\cT=\bbR^{d+1}\times{}\sym_{+}^{d+1}$ and a mean-zero distribution $p$ over $\brk*{-L, L}$. Then we have
\begin{align*}
\En_{\alpha\sim{}p}\burk(\tau + \suff(z, \alpha))
 &= \En_{\alpha\sim{}p}\brk*{
 \Psi^{\star}_{\rho(\tau_{2}+zz^{\trn})+\lambda{}I}(\tau_{1} + \alpha{}z) - c\log(\det(\rho(\tau_{2}+zz^{\trn})+\lambda{}I)/\det(A_0))
 } \\
 &= \En_{\alpha\sim{}p}\brk*{
 \Psi^{\star}_{\rho(\tau_{2}+zz^{\trn})+\lambda{}I}(\tau_{1} + \alpha{}z)} - c\log(\det(\rho(\tau_{2}+zz^{\trn})+\lambda{}I)/\det(A_0)).
\end{align*}
Let $A=\rho(\tau_{2}+zz^{\trn})+\lambda{}I$ and $B=\rho\tau_{2}+\lambda{}I$. Then since $\Psi^{\star}$ is a squared Euclidean norm and $\alpha$ is mean-zero:
\[
\En_{\alpha\sim{}p}\brk*{\Psi^{\star}_{A}(\tau_{1} + \alpha{}z)} \leq{} \Psi^{\star}_{A}(\tau_{1}) + \En_{\alpha\sim{}p}\brk*{\alpha^{2}\tri*{z, A^{-1}z}} \leq{} \Psi^{\star}_{A}(\tau_{1}) + L^{2}\brk*{\alpha^{2}\tri*{z, A^{-1}z}}.
\]
Also note that since $B\preceq{}A$, $\Psi^{\star}_{A}(\tau_1) \leq{} \Psi^{\star}_{B}(\tau_1)$.

To conclude, observe that we just established
\begin{align*}
\En_{\alpha\sim{}p}\burk(\tau + \suff(z, \alpha)) &\leq{} \Psi_{B}^{\star}(\tau_1) + L^{2}\tri*{z, A^{-1}z} - c\log(\det(A)/\det(A_0)).
\intertext{Using a standard argument (e.g. from \cite{PLG}) and using that $A=B+\rho{}zz^{\trn}$:}
&\leq{} \Psi_{B}^{\star}(\tau_1) + \frac{L^{2}}{\rho}\log(\det(A)/\det(B)) - c\log(\det(A)/\det(A_0)).
\intertext{For $c\geq{}L^{2}/\rho$, this is bounded by}
&\leq{} \Psi_{B}^{\star}(\tau_1) - c\log(\det(B)/\det(A_0)) \\
&= \burk(\tau).
\end{align*}

\end{proof}

\subsection{Proofs from \pref{app:linear_loss}}

\begin{proof}[\pfref{prop:param_sufficient}]
  We define a potential function that will eventually be used in the construction of the Burkholder function $\burk$ we provide for $V$. As discussed in the main body, a variant of this potential was first introduced by \cite{mcmahan2014unconstrained} for the special case of Hilbert spaces. Let $\Psi(x) = \frac{1}{2}\nrm*{x}^{2}$ (not necessarily a Hilbert space norm) and define
  \[
    F_{n}(x) = \gamma\exp\prn*{\frac{\Psi(x)}{an}}.
  \]
  From \cite[Lemma 14]{mcmahan2014unconstrained}, along with the additional fact that $(f(\nrm*{\cdot}))^{\star} = f^{\star}(\nrm*{\cdot}_{\star})$ for general dual norm pairs, it holds that
  \[
    F_{n}^{\star}(w) \leq{} \nrm*{w}_{\star}\sqrt{2an\log\prn*{\frac{\sqrt{an}\nrm*{w}_{\star}}{\gamma} + 1}}.
  \]
  This is all we need to establish the result. We proceed as follows
\begin{align*}
&\sup_{w\in\bbR^{d}}\crl*{
    \reg(w) - \cA(w)
      } \\
      &=
      \sup_{w\in\bbR^{d}}\crl*{
    \sum_{t=1}^{n}\ls(\pred_t, y_t) - \ls(\tri*{w,x_t}, y_t) - \cA(w)
      } \\
     &\leq{}
      \sup_{w\in\bbR^{d}}\crl*{
    \sum_{t=1}^{n}\partial\ls(\pred_t, y_t)(\pred_t - \tri*{w,x_t}) - \cA(w)
      } \\
     &=
\sum_{t=1}^{n}\partial\ls(\pred_t, y_t)\cdot\pred_t  +  \sup_{w\in\bbR^{d}}\crl*{
       \tri*{w,\sum_{t=1}^{n}\partial\ls(\pred_t, y_t)x_t} - \cA(w)
       }
       \intertext{Using the inequality for the potential $F^{\star}_n$ stated above:}
      &\leq{}
        \sum_{t=1}^{n}\partial\ls(\pred_t, y_t)\cdot\pred_t  +  
        F^{\star}_n\prn*{\sum_{t=1}^{n}\partial\ls(\pred_t, y_t)x_t}
        - c
\end{align*}
It follows that $\suff(x_t,\pred_t,\delta_t) = \left( \delta_t\cdot\pred_t, \delta_t\cdot x_t \right)\in\reals\times \X$ is a sufficient statistic. This is because we can write
\[
V(b, x) = b + F^{\star}_n(x) - c.
\]
and we have just shown that
\[
\sup_{w}\crl*{\reg(w) - \cA(x_{1},\ldots,x_{n})
      }
      \leq{} V\prn*{
      \sum_{t=1}^{n}\suff(x_t,\pred_t,\delta_t)
      }.
\]

  
\end{proof}

\begin{proof}[\pfref{thm:param_free}]
    Since $\burk$ depends on time, we generalize the properties of \pref{lem:equivalence_burkholder} to
  \begin{enumerate}
  \item[$1^o$] $\burk_{0}(0) \le 0$
  \item[$2^o$] For any $\tau \in \T$, $\burk_{n}(\tau) \ge V(\tau)$
  \item[$3^o$] For any $\tau \in \T$, $z \in \X \times \Y$, and any mean-zero distribution $p$ on $[-L,L]$, and any $t\geq{}1$
    \begin{align}
      \En_{\alpha\sim p}\left[\burk_{t}(\tau + \suff(z,\alpha))\right] \le \burk_{t-1}(\tau) 
    \end{align}    
  \item[$3'$] For any $\tau \in \T$, $z \in \X \times \Y$, and any $t\geq{}1$,
    $$
    \forall \tau \in \T, z \in \X \times \Y,~~~ \En_\epsilon \burk_{t}(\tau + \suff(z,\epsilon L))  \le \burk_{t-1}(\tau),
    $$
    where $\epsilon$ is a Rademacher random variable. 
  \end{enumerate}

  Recall that for simplicity we assume $L=1$ and $\X$ is a unit ball: $\nrm*{x}\leq{}1$. Let $\Psi(x) = \frac{1}{2}\nrm*{x}^{2}$, where we have assumed that $\beta$-smoothness of $\Psi$:
  \[
    \Psi(x+y) \leq{} \Psi(x) + \tri*{\grad{}\Psi(x), y} + \frac{\beta}{2}\nrm*{y}^{2}.
  \]
  
  Define a family of potentials
  \[
    F_{t}(x) = \gamma\exp\prn*{\frac{\Psi(x)}{at} + \frac{1}{2}\sum_{s=t+1}^{n}\frac{1}{s}}
  \]
  and $F_{0} = \gamma\exp\prn*{\frac{1}{2}\sum_{t=1}^{n}\frac{1}{t}}$. Note that $F_{n}$ here is the same as in the proof of \pref{prop:param_sufficient}.
  
  Observe that
  \[
    \burk_{t}(b, x) = b + F^{\star}_t(x) - c, 
  \]
  where $F_{t}^{\star}$ is as defined as in the proof of \pref{prop:param_sufficient}. We proceed to establish the three properties of $\burk$ from \pref{lem:equivalence_burkholder}. Property $2^o$ holds since $V=\burk_{n}$. We will show property \propthreep{} first, then conclude with property \propone{}. Note that $\alpha\mapsto{}\burk_{t}(\tau + \suff(z,\alpha))$ is convex with respect to $\alpha$, and so it indeed suffices to show property \propthreep{}.

  Fix an element $\tau=(\tau_1, \tau_2)\in\bbR\times{}\cX=\cT$ of the sufficient statistic space. At time $n$ we have
  \[
    \En_{\eps}\left[\burk_{n}(\tau + \suff(z,\eps))\right] = \En_{\eps}\brk*{\tau_{1} +  \eps\cdot\pred + F_{n}(\tau_{2} + \eps{}x_n)} - c = \tau_{1} + \En_{\eps}\brk*{F_{n}(\tau_{2} + \eps{}x_n)} - c.
  \]
  To handle $F_n$, begin by using smoothness of $\Psi$:
  \begin{align*}
\En_{\eps}\brk*{F_{n}(\tau_{2} + \eps{}x_n)} =  \En_{\eps}\exp\prn*{\frac{\Psi(\tau_{2} + \eps{}x)}{an}} &\leq{}
                                                               \En_{\eps}\exp\prn*{\frac{\Psi(\tau_{2}) + \eps\tri*{\grad\Psi(\tau_2), x} + \frac{\beta}{2}\nrm*{x}^{2}}{an}} 
\end{align*}
Using the standard Rademacher mgf bound, $\En_{\eps}e^{\lambda{}\eps}\leq{}e^{\lambda^{2}/2}$, we upper bound the above quantity by
\begin{align*}
\exp\prn*{\frac{\Psi(\tau_{2}) + \frac{\beta}{2}\nrm*{x}^{2}}{an} + \frac{\tri*{\grad\Psi(\tau_2), x}^{2}}{2(an)^{2}}}\leq{}    \exp\prn*{\frac{\Psi(\tau_{2}) + \frac{\beta}{2}\nrm*{x}^{2}}{an} + \frac{\nrm*{\grad\Psi(\tau_2)}_{\star}^{2}\nrm*{x}^{2}}{2(an)^{2}}}.
\end{align*}
Using the assumption $\nrm*{x}\leq{}1$, we obtain an upper bound of
\begin{align*}
&\exp\prn*{\frac{\Psi(\tau_{2}) + \frac{\beta}{2}}{an} + \frac{\nrm*{\grad\Psi(\tau_2)}_{\star}^{2}}{2(an)^{2}}}.
\end{align*}
We now use a basic fact from convex analysis, namely that any $\beta$-smooth convex function $f$, $\frac{1}{2\beta}\nrm*{\grad{}f(x) - \grad{}f(y)}_{\star}^{2} \leq{} f(x) - f(y) - \tri*{\grad{}f(y), x-y}$ . This yields an upper bound
\begin{align*}
&\exp\prn*{\frac{\Psi(\tau_{2}) + \frac{\beta}{2}}{an} + \frac{\beta\Psi(\tau_2)}{(an)^{2}}}
\end{align*}
Setting $a=\beta$, this is equal to
\begin{align*} 
	\exp\prn*{\frac{1}{\beta}\prn*{\frac{1}{n} + \frac{1}{n^2}}\Psi(\tau_2) + \frac{1}{2n}}.
\end{align*}
  As a last step, observe that $\frac{1}{n} + \frac{1}{n^{2}} \leq{} \frac{1}{n-1}$. Indeed,
  \[
    \frac{1}{n} + \frac{1}{n^{2}} = \frac{1}{n}\prn*{1 + \frac{1}{n}} = \frac{1}{n-1}\frac{n-1}{n}\prn*{1+\frac{1}{n}}
    = \frac{1}{n-1}\prn*{1-\frac{1}{n}}\prn*{1+\frac{1}{n}}
    = \frac{1}{n-1}\prn*{1 - \frac{1}{n^{2}}}\leq{} \frac{1}{n-1}.
  \]
  Therefore, we have established that
  \[
    \En_{\eps}\brk*{F_{n}(\tau_{2} + \eps{}x_n)} \leq{} \exp\prn*{ \frac{\Psi(\tau_2)}{\beta(n-1)} + \frac{1}{2n}} = F_{n-1}(\tau_2),
  \]
  and in particular $\En_{\eps}\burk_{n}(\tau + \suff(z,\eps))\leq{}\burk_{n-1}(\tau)$.
  In fact, by folding the terms $\frac{1}{2}\sum_{s=t+1}^{n}\frac{1}{s}$---which do not depend on data---into a multiplicative constant, this argument yields, for any $t$ and any $\nrm*{x}\leq{}1$,
  \[
    \En_{\eps}\brk*{F_{t}(\tau + \eps{}x)} \leq{} F_{t-1}(\tau).
  \]
  Thus, for each $t\geq{}2$ we have
  \[
    \En_{\eps}\left[\burk_{t}(\tau + \suff(z,\eps))\right] = \En_{\eps}\brk*{\tau_{1} +  \eps\cdot\pred + F_{n}(\tau_{2} + \eps{}x)} - c \leq{} \burk_{t-1}(\tau).
  \]
  
The argument also yields (by removing unnecessary steps):
  \[
    \En_{\eps}\brk*{F_{1}(0 + \eps{}x)} \leq{} \gamma\exp\prn*{\frac{1}{2}\sum_{t=1}^{n}\frac{1}{t}} = F_{0}.
  \]
  This means that
  \[
    \burk_{0}(0) = \gamma\exp\prn*{\frac{1}{2}\sum_{t=1}^{n}\frac{1}{t}} - c \leq{} \gamma\exp\prn*{\log(n)/2} - c.
  \]
  We will set $\gamma=\frac{1}{\sqrt{n}}$ and $c=1$, which yields $\burk_{0}(0) \leq{} 0$.
  
\end{proof}

\subsection{Proofs from \pref{app:necessary}}

\begin{proof}[\pfref{prop:necessary}]
Recall that the regret inequality of interest is
\[
\sum_{t=1}^{n}\ls(\yh_t, y_t) - \inf_{f\in\cF}\sum_{t=1}^n \loss(f(x_t),y_t) - F\prn*{\sum_{t=1}^{n}\overline{\suff}(x_t)} \leq{} 0.
\]
As sketched in the \pref{app:necessary}, \pref{lem:suff_to_martingale} shows that this is implied by
\begin{equation}
\sup_{\mb{z}}\En_{\eps}\brk*{V\prn*{\sum_{t=1}^{n}\suff(\mb{z}_t, \eps_t)}}\leq{}0,
\end{equation}
so the remainder of this proof will focus on the opposite direction. Suppose that $\ls(\yh,y)\ldef\abs*{\yh-y}$ is the absolute loss. We fix a Rademacher sequence $\eps_{1},\ldots,\eps_{n}$ and a tree $\x$ with $\x_{t}(\eps)=\x_{t}(\eps_{1},\ldots,\eps_{t-1})$. As a lower bound, consider a randomized adversary that plays $y_{t}=\eps_{t}$ and $x_{t} = \x_{t}(\eps)$. In this case the expected value of the regret inequality is
\[
\En_{\eps}\brk*{
\sum_{t=1}^{n}\ls(\yh_t, \eps_t) - \inf_{f\in\cF}\sum_{t=1}^n \loss(f(\x_t(\eps)),\eps_t) - F\prn*{\sum_{t=1}^{n}\overline{\suff}(\x_t(\eps))}
}.
\]
Observe that for any $\eps\in\pmo$ we have $\ls(\yh, \eps) = \abs*{1-\yh\eps}\geq{}1-\yh\eps$. Since the range of each $f\in\cF$ lies in $\brk*{-1,1}$, we have $\ls(f(x), \eps)=1-f(x)\eps$ exactly. The expected value of the regret inequality is therefore lower bounded by
\begin{align*}
&\En_{\eps}\brk*{
\sum_{t=1}^{n}(1-\yh_t\eps_t) - \inf_{f\in\cF}\sum_{t=1}^n (1-f(\x_t(\eps))\eps_t) - F\prn*{\sum_{t=1}^{n}\overline{\suff}(\x_t(\eps))}
} \\
&= \En_{\eps}\brk*{
- \inf_{f\in\cF}\sum_{t=1}^n (1-f(\x_t(\eps))\eps_t) - F\prn*{\sum_{t=1}^{n}\overline{\suff}(\x_t(\eps))}
} \\
&= \En_{\eps}\brk*{
\sup_{w\in\cW}\tri*{w, \sum_{t=1}^n\eps_{t}\x_{t}(\eps)} - F\prn*{\sum_{t=1}^{n}\overline{\suff}(\x_t(\eps))}
} \\
&= \En_{\eps}\brk*{
V\prn*{\sum_{t=1}^{n}\suff(\x_{t}(\eps), 0, \eps_t)
}}.
\end{align*}

For the final step, let $\wt{\y}$ be an arbitrary $\cY$-valued tree $\wt{\y}_{t}(\eps) = \wt{\y}_{t}(\eps_1,\ldots,\eps_{t-1})$. Using the explicit form for $V$, we have
\begin{align*}
\En_{\eps}\brk*{
V\prn*{\sum_{t=1}^{n}\suff(\x_{t}(\eps), \wt{\y}_{t}(\eps), \eps_t)
}}
&= \En_{\eps}\brk*{
\sum_{t=1}^{n}\eps_{t}\wt{\y}_{t}(\eps) + 
\sup_{w\in\cW}\tri*{w, \sum_{t=1}^n\eps_{t}\x_{t}(\eps)} - F\prn*{\sum_{t=1}^{n}\overline{\suff}(\x_t(\eps))}
} \\
&= \En_{\eps}\brk*{
0 + 
\sup_{w\in\cW}\tri*{w, \sum_{t=1}^n\eps_{t}\x_{t}(\eps)} - F\prn*{\sum_{t=1}^{n}\overline{\suff}(\x_t(\eps))}
} \\
& = \En_{\eps}\brk*{
V\prn*{\sum_{t=1}^{n}\suff(\x_{t}(\eps), 0, \eps_t)
}}.
\end{align*}

Since the argument above holds for any trees $\x$ and $\wt{\y}$, we conclude that the regret inequality implies that
\[
\sup_{\mb{z}}\En_{\eps}\brk*{V\prn*{\sum_{t=1}^{n}\suff(\mb{z}_t, \eps_t)}}\leq{}0.
\]
for all $\cX\times{}\cY$-valued trees.

\end{proof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:

