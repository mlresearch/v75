%!TEX root = ../main.tex
\section{Introduction}

Recent work on developing {\em robust} learning algorithms--
algorithms that succeed with respect to a data set that has been
corrupted in some arbitrary fashion-- has led to important
achievements in unsupervised learning (robust mean estimation of
Gaussians) and classification (learning halfspaces with respect to
malicious or ``nasty noise.'').  In this paper we give the first
efficient algorithms for performing outlier-robust {\em regression}.  That is,
given a training set drawn from distribution ${\cal D}$ such that some
fraction of its points (both labels and/or locations) have been
arbitrarily corrupted, we can find a linear function (or polynomial in
the case of polynomial regression) whose loss is competitive with the
best fitting linear function for ${\cal D}$.

In this paper we focus on distributions whose marginals on inputs are {\em hypercontractive} (a now well-studied condition on the distribution's moments).  Many natural distributions such as Gaussians, log-concave distributions, uniform distribution on the hypercube, fall into this category.  We give simple examples showing that unlike classical regression, achieving any non-trivial guarantee for robust regression is information-theoretically impossible without making assumptions on the distribution $\cD$. Additionally, we show that for the class of hypercontractive distributions, the bounds on the loss of the linear function output by our algorithm is optimal to within multiplicative constants.

\section{Outlier-Robust Regression}
We formally define the problem next by focusing on linear regression with squared-loss for the introduction. In the following, we will use the following notations for brevity: For a distribution $\cD$ on $\R^d \times \R$ and function $f:\R^d \to \R$, we define $\err_\cD(f) = \E_{(x,y) \sim \cD}[ (f(x) - y)^2]$. For a vector $\ell \in \R^d$, we abuse notation and write $\err_\cD(\ell)$ for $\E_{(x,y) \sim \cD}[ (\iprod{\ell,x} - y)^2]$. Finally, let $\opt(\cD) = \min_{\ell \in \R^d} \err_{\cD}(\ell)$ be the least error achievable. 

Let us briefly recall the setup of the classical least-squares linear regression problem. We are given access to samples from a distribution $\cD$ over $\R^d \times \R$ and our goal is to find a linear function $\ell:\R^d \to \R$ whose squared-error---$\err_{\cD}(\ell)$---is close to the best possible, $\opt_\cD$. Our goal is similar however, we are not given access to samples from the distribution $\cD$ but only samples where a fixed fraction of samples are arbitrarily corrupted. 

\begin{definition}

\end{definition}

Let $\cD$ be a distribution over $\R^d \times \R$ and let $\cD_X$ denote the marginal of $\cD$ on the first $d$ coordinates. 
%Let $D$ be a distribution over $\R^d$. Let $\cD$ be a distribution over $\R^d \times \R$ such that the marginal of $\cD$ on the first $d$ coordinates equals $D$. 

\begin{definition}[$\epsilon$-corruption]
Let $Z = \{(x_i, y_i): 1 \leq i \leq n\}$ be an i.i.d. sample drawn from the distribution $\cD$. An $\epsilon$-corruption of $Z$ is an arbitrary subset $Z' = \{ (x_i',y_i') : 1 \leq i \leq n\} \subseteq \R^d\times \R$ such that $|Z' \cap Z| \geq (1-\epsilon)n.$
\end{definition}

We are now ready to define the robust polynomial regression problem for a distribution $\cD$. 
\begin{problem}
Let $D$ be any distribution on $\R^d \times \R.$ In the degree $t$ outlier-robust polynomial regression problem, we are given an $\epsilon > 0$ and a sample $Z'$ of size $n$ that is produced by an $\epsilon$ corruption of an i.i.d. sample of a distribution $\cD'$ whose marginal on the first $d$ coordinates equals $D.$ The goal is to output a polynomial $p$ such that:
\[
\E_{(x,y) \sim \cD}[ |p(x)-y|^2] < \min_{q: \text{ deg t polynomial } } \E_{(x,y) \sim \cD} [ |q(x)-y|^2] + \delta,
\]
\ignore{
\[
\E_{(x,y) \sim \cD'}[ |p(x)-y|] < \min_{q: \text{ deg t polynomial } } \E_{(x,y) \sim \cD'} [ |q(x)-y|] + \delta,
\]}
for as small a $\delta$ as possible.
\end{problem}



\subsection{Statement of Results}
We obtain several results depending on the type of assumption we take
on the marginal distribution and on the choice of loss function.  We
say that a training set $S$ is $\eta$-corrupted if it is formed in the
following fashion: a set $T$ of i.i.d samples from ${\cal D}$ is generated and then replaced by a set $S$ of equal size with the only caveat that
$\frac{|S \cap T|}{|T|} \geq 1 - \eta$.  Our main results are for
hypercontractive distributions and square loss:

\begin{theorem}
Let ${\cal D}$ be a distribution on $\R^{d} \times [-B,B]$ and let ${\cal D_{X}}$ be its
marginal distribution on $\R^n$.  Let $\opt = \min_{\ell} \E_{(x,y) \sim
  {\cal D}}[(y - \ell(x))^2]$, and $\opt_4 = \min_{\ell} \E_{(x,y) \sim \cD}[(y - \ell(x))^4]^{1/2}$ where $\ell$ is a linear function.
Assume that for all linear functions $\ell$ we have $\E_{x \sim {\cal
    D}_X}[\ell(x)^4] \leq  (C \E_{x \sim {\cal D}_X}[\ell(x)^2])^2$
for some constant $C$.
Then there exists an efficient algorithm ${\cal A}$ such that, given a
polynomial-size, $\eta$-corrupted training set $S$, ${\cal A}$ with high probability outputs a linear function $\ell'$ with 

$$ \E_{(x,y) \sim {\cal D}}[(y - \ell'(x))^2] \leq (1 +
O(\sqrt{\eta}))\mathsf{opt} + O(\sqrt{\eta}). $$
\end{theorem}

The above statement assumes that the marginal distribution is
hypercontractive with respect to its fourth moments.  Our results
improve if we assume hypercontractivity for higher moments. 

We also show (by a simple argument) that the theorem is optimal upto the $O(1)$ factors: it is statistically impossible to do get an error bound of $(1 + o(\sqrt{\eta})) \mathsf{opt} + o(\sqrt{\eta})$ under the assumptions of the theorem. 

\begin{itemize}

\item Do we want to state theorems for absolute loss or lower bounds here? 

\item Application to learning boolean concepts with respect to nasty
  noise?

\end{itemize}

\subsection{Part One of Our Approach: Certifying that a Linear
  Function has Small Loss}

Given an $\eta$-corrupted training set $S$, it is non-trivial to solve
our problem even if we allow the learner unbounded computational
resources.  It is tempting to think that the following naive strategy
will work: brute-force search over all subsets $T$ of $S$ of size
$(1 - \eta) |S|$ and perform least-squares regression to obtain linear
function $\ell_T$ with empirical loss $\epsilon_T$.  Then,
output $\ell_T$ with minimal empirical loss $\epsilon_T$ over all
subsets $T$. 

Although it is true that some subset $T^{*}$ will correspond to the
uncorrupted sample, it may be the case that $\ell_R$ for some other
subset $R$ will have empirical loss that is {\em smaller} than
$\ell_{T^{*}}$.  Further, since $\ell_R$ is derived from a sample that
has been drawn from the original distribution ${\cal D}$ and then
corrupted, it is unclear if the empirical loss of $\ell_R$ will
generalize to ${\cal D}$.

This leads to the following interesting question on generalization
error: given a linear function $\ell$ that has small empirical loss
with respect to an $\eta$-corrupted training set $S$, how can we {\em
  certify} that its {\em true} loss with respect to ${\cal D}$ is also
small?

To answer this question, we give an elementary proof of the following
certification lemma: given distributions ${\cal D}_1$ and ${\cal D}_2$ that
are close in statistical distance, if ${\cal D}_1$ is
hypercontractive, then any linear function with small loss with
respect to ${\cal D}_2$ will also have small loss with respect to
${\cal D}_1$ (the loss with respect to ${\cal D}_1$ increases as a
function of the statistical distance and the degree of hypercontractivity).

Roughly, to apply this statement in our setting, we will let
${\cal D}_1$ be the uniform distribution over an uncorrupted sample
$T$ drawn from ${\cal D}$ and ${\cal D}_2$ will be a {\em
  near-uniform} distribution over $S$, its $eta$-corrupted
counterpart.  Applying our certification lemma, it suffices to find a
linear function with small loss with respect to ${\cal D}_2$.
  
We remark that taking some distributional assumption on ${\cal D}$ is
necessary, as we discuss in Section (LOWER BOUND SECTION).

\subsection{Part Two of Our Approach: Enter SoS}

Given $S$, an $\eta$-corrupted training set, from the above
certification lemma we have reduced the problem to finding a
near-uniform distribution $w$ on $S$ and linear function $\ell$ such
that the loss of $\ell$ with respect to $w$ is small.  Such a $w$
clearly exists: we could give zero weight to the corrupted points in
$S$.  As such, we could write the following optimization progam to
find $w$ and $\ell$:

insert QUAD PROGRAM HERE\\

Unfortunately, it is NP-hard to solve general quadratic programs.  At
a high level, the Sum of Squares (SoS) paradigm tells us that the
semidefinite relaxation of this program will output a suitable
'pseudodistribution' $w$ and linear function $\ell$ provided that our
certification lemma can be proved using only steps from the SoS proof
system.

And that's what happens [Pravesh prob want to write a parapgaph or two here]

\subsection{Related Work}

\begin{itemize}

\item Regression with respect to pure label noise; Lasso; Prateek Jain
  has a recent paper on this. 

\item ``Computationally efficient robust estimation of sparse
  functionals'' -- This paper seems to have results similar to Ilias
  robust mean estimation papers (unsupervised learning).  Then it also
  includes results on sparse linear regression, but the results for
  sparse linear regression don't seem to be robust in any way. Sort of
  weird they include it in the paper.

\item Might want to mention the paper ``Robust Regression and Lasso''
  where they discuss something they call Robust Linear Regression
  (though it seems to not handle label noise). 

\end{itemize}

