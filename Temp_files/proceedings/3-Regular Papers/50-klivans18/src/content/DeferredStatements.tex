%!TEX root = ../main.tex
\section{Outlier-Robust Polynomial Regression}\label{sec:app-moved}

Our arguments also extend straightforwardly to get similar guarantees for polynomial regression. We elaborate on these next.

The following extends the definition of hypercontractivity to polynomials.

\begin{definition}[Certifiable polynomial hypercontractivity]\label{def:hyperconc2}
For a function $C:[k] \to \R_+$, we say a distribution $D$ on $\R^d$ is $k$-cerifiably $(C,t)$-hypercontractive if for every $r$ such that $2r t \leq k$, there is a degree $k$ sum of squares proof of the following inequality in variable $P$ where $p$ stands for $\langle P,x^{\otimes t}\rangle.$
\[
\E_D p(x)^{2r} \leq \Paren{(C(r) \E_{D} p(x)^{2}}^{r}.
\] 
\end{definition}
Many natural distributions satisfy certifiably hypercontractivity \citep{DBLP:conf/soda/KauersOTZ14} for polynomials such as gaussian distributions and the product distributions on the hypercube $\zo^n$ with all coordinate marginals in $(0,1)$. Our results will apply to all such distributions. 

Next, we state an extension of our robust certification lemma for polynomial regression. The proof is essentially the same as that of Lemma \ref{lem:identifiability-least-squares-linear}. 
\begin{lemma}[Robust Generalization for polynomial regression]
Fix $k,t \in \N$ and let $\cD,\cD'$ be distributions on $\R^d \times \R$ such that $\|\cD-\cD'\|_{TV} \leq \epsilon$ and the marginal $\cD_X$ of $\cD$ on $x$ is $k$-certifiably $(C,t)$-hypercontractive for some $C:[k] \to \R_+$ and for some even integer $k \geq 4$.

Then, for any degree at most $t$ polynomials $p,p^*:\R^d \to \R$, and any $\epsilon$ such that $2 C(k/2) \epsilon^{1-2/k} < 0.9$, we have:
\[
\err_{\cD}(p) \leq (1+O(C(k/2)) \epsilon^{1-2/k}) \cdot \err_{\cD'}(p) + O(C(k/2))\epsilon^{1-2/k} \cdot \Paren{\E_\cD (y-p^*(x))^k}^{2/k}\mper\]\label{lem:identifiability-least-squares-polynomial}
\end{lemma}



\begin{theorem} \label{thm:polyregression}
Let $\cD$ be a distribution on $\R^d \times [-M,M]$ for some positive real $M$ such the marginal on $\R^d$ is $(C,k)$-certifiably hypercontractive distribution for degree $t$ polynomials. Let $\opt_B(\cD) = \min_{p} \E_{\cD}[ (y - p(x))^2]$ where the minimum is over all polynomials $p$ of degree $t$ and bit complexity $B$. Let $p^{*}$ be any such minimizer. % of $\E_{(x,y) \sim \cD} (y - \iprod{\ell,x})^2$.

Fix any even $k \geq 4$ and any $\epsilon > 0$. Let $X$ be an i.i.d. sample from $\cD$ of size $n \geq n_0 = \poly(d^k, B, M, 1/\epsilon)$. %for some $n_0 = \tilde{O}(d^{k/2}\log{(d)}^{k/2}) + O(1/\epsilon^2) \cdot (\E_{\cD} (y-\iprod{\ell^{*},x})^k/\eta +M^4 + B^2M^2 \log{(M)})$.
Then, with probability at least $1-\epsilon$ over the draw of the sample $X$, given any $\eta$-corruption $U$ of $X$ and $\eta$ as input, there is a polynomial time algorithm (Algorithm \ref{alg:robust-regression-program}) that outputs a $\ell \in \R^d$ such that for $C = C(k/2)$, 
\[
\err_{\cD}(p_M) < (1 + O(C)\eta^{1-2/k}) \opt_B(\cD) + O(C) \eta^{1-2/k} \Paren{\E_{\cD} (y-p^{*}(x))^k}^{2/k} + \epsilon
.
%\E_{(x,y) \sim \cD} (y- f(x))^2 < (1 + O(C)\eta^{1-2/k}) \opt(\cD) + O(C) \eta^{1-2/k} \Paren{\E_{\cD} (y-\iprod{\ell^{*},x})^k}^{2/k} + \epsilon
\] 
\end{theorem}
%\section{Deferred Proofs}\label{app:deferred}
% \begin{proof}[Sketch of Proof of Lemma \ref{lm:lb1}]
% Fix some $\delta = \Theta(\eta)$ to be chosen later. For brevity, let $U$ be the uniform distribution on $[-1,1]$ and let $Z$ be the distribution of the random variable sampled as follows: 1) With probability $1-\delta$, sample $\alpha$ uniformly at random from $[-1,1]$ and output $\eta \cdot \alpha$; 2) With probability $\delta/2$ output $1/\delta^{1/4}$; 3) With probability $\delta/2$ output $-1/\delta^{1/4}$. 

% It follows from standard calculations that $Z$ is $(C_1,4)$-certifiably hypercontractive for a fixed constant $C_1$. Further, if we let $X = (U, Z)$ for $U,Z$ generated independently, then $X$ is also $(C_1,4)$-certifiably hypercontractive; we skip the details from this sketch.

% Now let $\ell = (1,1)$ and $\ell' = (1,-1)$. Let $\cD$ be the distribution of $(X, \iprod{\ell,X} + U)$ for $U$ independent from $X$; similarly, let $\cD'$ be the distribution of $(X, \iprod{\ell',X} + U)$. 

% We first claim that $\|\cD - \cD'\|_{TV}\| = O(\delta)$. To see this, let us consider a coupling between $\cD, \cD'$ obtained by choosing the same $X$ for both. Then, with probability $1-\delta$ over $X$, $|\iprod{\ell,X} - \iprod{\ell',X}| \leq 2 \delta$; further, whenever this happens, the statistical distance between $\iprod{\ell,X} + U$ and $\iprod{\ell',X} + U$ is at most $O(\delta)$. Therefore, $\|\cD - \cD'\|_{TV} = O(\delta)$. 

% Finally, note that 
% $$\E[ (\iprod{\ell,X} - \iprod{\ell',X})^2] = 4 \E[Z^2] \geq 4 \delta (1/\delta^{1/4})^2 = 4 \sqrt{\delta}.$$
% Therefore, for any $w$, 
% $$\E[ (\iprod{w,X} - \iprod{\ell,X})^2] + \E[ (\iprod{w,X} - \iprod{\ell',X})^2] \geq 4 \sqrt{\delta}.$$
% In particular, for any $w$, $\max\left(\E[ (\iprod{w,X} - \iprod{\ell,X})^2], \E[ (\iprod{w,X} - \iprod{\ell',X})^2]\right) \geq 2 \sqrt{\delta}$. Now, note that for any $w$, $\err_\cD(w) = E[U^2] + \E[ (\iprod{w,X} - \iprod{\ell,X})^2]$ and a similar equality holds for $\err_{\cD'}(w)$. 

% Combining the above we get that for any $w$, $\max(\err_\cD(w), \err_{\cD'}(w)) \geq \E[U^2] + 2 \sqrt{\delta}$. Note that $\min_{\ell^*} \err_\cD(\ell^*) = \E[U^2]$ and the same holds for $\cD'$ as well. Finally, as $\|\cD - \cD'\|_{TV} = O(\delta) \leq \eta$, by setting $\delta = \Theta(\eta)$ appropriately. Therefore, $\cD, \cD'$ could be $\eta$-corruptions of each other and no $w$ could get error better than $\E[U^2] + 2 \sqrt{\delta} = \E[U^2] + \Omega(\sqrt{\eta})$ on both of them. The claim now follows.
% \end{proof}

