%!TEX root = ../main.tex
\subsubsection{Bounding the Optimization Error} \label{sec:proofoftheorem}
We now prove Lemma~\ref{lm:opterror}. While the proof can appear technical, it's essentially a line-by-line translation of the robust certifiability Lemma~\ref{lem:identifiability-least-squares-linear}.% which is the most technical part of our analysis.  

\paragraph{Proof Outline} The rough idea is to exploit the following abstract property of pseudo-distributions: If a collection of polynomial inequalities $\cP = \{p_i(z) \geq 0, i \in [r]\}$ SOS-imply another polynomial inequality $q(z) \geq 0$, then any pseudo-distribution $\tmu$ of appropriately high degree (depending on the degree of the SOS proof) that satisfies the inequalities in $\cP$ also satisfies $q$, that is $\pE_{\tmu}[q] \geq 0$. Further, the SoS algorithm allows us to compute pseudo-distributions satisfying a set of polynomial inequalities efficiently. 

Now, let $(w,\ell,X')$ satisfy the inequalities $\cP_{U,\eta}$. Then, by Lemma \ref{lem:identifiability-least-squares-linear}, applied to $\widehat{\cD}$ and the uniform distribution on $X'$, we get 

$$\err_{\widehat{\cD}}(\ell) \leq (1 + c C \eta^{1-2/k}) \left((1/n) \sum_{i=1}^n  (y_i' - \iprod{\ell,x_i'})^2\right) + c C \eta^{1-2/k} \cdot \widehat{\opt}_k,$$
for some universal constant $c > 0$. 

To view the above inequality as a polynomial inequality in variables $w,\ell,X'$, we rephrase it as follows. For brevity, let $\err(w,\ell,X') = (1/n) \sum_{i=1}^n (y_i' - \iprod{\ell,x_i'})^2$. Then, 
$$ \Paren{\err_{\hat{\cD}}(\ell) - \err(w,\ell,X')}^{k/2}  \leq \eta^{k/2-1} \cdot 2^{\Theta(k)} C^{k} \err(w,\ell,X')^{k/2} + \eta^{k/2-1} \cdot 2^{\Theta(k)} C^{k} \cdot \widehat{\opt}_k^{k/2} .$$

We show that the above version of the robust certifiability lemma has a SOS proof; that is, viewing the above inequality as a polynomial inequality in variables $w,\ell,X'$, this inequality has a SOS proof starting from the polynomial inequalities $\cP_{U,\eta}$. Thus, by the property of pseud-densities at the beginning of this sketch, a pseudo-density $\tmu$ as in our algorithm satisfies an analogue of the above inequality which after some elementary simplifications gives us a bound of the form 
$$\pE_{\tmu}[ \err_{\widehat{\cD}}(\ell)] \leq (1 + c C \eta^{1-2/k}) \cdot \widehat{\opt}_{SOS} + c C \eta^{1-2/k} \widehat{\opt}_k.$$

As it stands, the above inequality is not very useful for us as it does not tell us which $\ell$ to choose. However, for any degree at most $k/2$ polynomial $p$, we also have that $(\pE_{\tmu}[ p(w,\ell)])^2 \leq \pE[p(w,\ell)^2] $ (see Fact \ref{fact:pseudo-Holders}). Applying this to each $(y_i - \iprod{\ell,x_i})$, we get that
$$\err_{\widehat{\cD}} (\pE_{\tmu}[\ell]) \leq \pE_{\tmu}[ \err_{\widehat{\cD}}(\ell)]  \leq (1 + c C \eta^{1-2/k}) \cdot \widehat{\opt}_{SOS} + c C \eta^{1-2/k} \widehat{\opt}_k,$$
proving the claim. 

We next formalize the above approach starting with a SOS proof of Lemma \ref{lem:identifiability-least-squares-linear}. We defer the proof of the lemma to Section \ref{sec:soscertification}. 


% \Pnote{push to the intro this commentary}Our analysis of the same algorithm yields all our results depending on the setting we are in. Even though our analysis for every $t$ is the same, we present the case of $t= 1$ separately for the sake of clarity of exposition. Observe that in the ``realizable'' case when the labels are generated by an actual linear function, we can guarantee zero error on the true distribution for a large class of distribution families. In other words, we get a \emph{consistent} regression hypothesis even under adversarial corruption of both examples and labels!

% \paragraph{Algorithmic Version of Lemma \ref{lem:identifiability-least-squares-linear}}


\newcommand{\U}{\mathcal{U}}
\begin{lemma}[SoS Proof of Robust Certifiability of Regression Hypothesis]

Let $X$ be a collection of $n$ labeled examples in $\R^d\times \R$ such that $\widehat{\cD}$, the uniform distribution on  $x_1, x_2, \ldots, x_n$ is $k$-certifiably hypercontractive and all the labels $y_1, y_2, \ldots, y_n$ are bounded in $[-M,M]$. Let $U$ be an $\eta$-corruption of $X$.

Let $(w,\ell,X')$ satisfy the set of system of polynomial equations $\cP_{U,\eta}$. Let $\err_{\hat{\cD}}(\ell)$ be the quadratic polynomial $\E_{(x,y) \sim \hat{\cD}} (y - \iprod{\ell,x})^2$ in vector valued variable $\ell$. Let $\err(w,\ell,X')$ be the polynomial $\frac{1}{n} \sum_{i \leq n} (y'_i - \iprod{\ell, x'_i})^2$ in vector valued variables $w,\ell,x_1',\ldots,x_n'$.

Then, for any $\ell^{*} \in \R^d$ of bit complexity at most $B < \poly(n,d^k)$, $C = C(k/2)$ and any $\eta$ such that $ 100 C \eta^{1-2/k} < 0.9$, 
 
% \[
% \sststile{\ell}{k} \Set{ \Paren{\err_D(\ell) - \err_{D'}(\ell)}^{k'}  \leq \epsilon^{k'-1} \cdot O(C(k/2))^{k} (\err_{D}(\ell))^{k'} + \epsilon^{k'-1} \cdot O(C(k/2))^{k} \Paren{ \frac{1}{n} \sum_{i =1}^n (y_i-\iprod{\ell^{*},x_i})^{k}}} 
% \]

% Using the sum of squares inequality $\delta^k a^k \leq (2\delta)^k (a-b)^k + (2\delta)^k b^k$ for any $a,b$ and even $k$, and applying it with $a = \err_{D}(\ell)$, $b = \err_{D'}(\ell)$ and $\delta = \epsilon^{k'-1} \cdot O(C(k/2))^{k}$ and rearranging, we have:

% \[
% \sststile{\ell}{k} \Set{ (1-\delta) \Paren{\err_D(\ell) - \err_{D'}(\ell)}^{k'}  \leq \epsilon^{k'-1} \cdot O(C(k/2))^{k} (\err_{D'}(\ell))^{k'} + \epsilon^{k'-1} \cdot O(C(k/2))^{k} \Paren{ \frac{1}{n} \sum_{i =1}^n (y_i-\iprod{\ell^{*},x_i})^{k}} } 
% \]

% For $\delta < 0.9$, this implies:


\begin{multline}
\cA_{U,\eta} \sststile{k}{\ell} \Paren{\err_{\hat{\cD}}(\ell) - \err(w,\ell,X')}^{k/2}  \leq \eta^{k/2-1} \cdot 2^{\Theta(k)} C^{k} \err(w,\ell,X')^{k/2} \\+ \eta^{k/2-1} \cdot 2^{\Theta(k)} C^{k} \Paren{ \frac{1}{n} \sum_{i =1}^n (y_i-\iprod{\ell^{*},x_i})^{k}}  \mper
\end{multline}

Moreover, the bit complexity of the proof is polynomial in $n$ and $d^k$.
% Finally, against using the sum of squares inequality $a^k \leq 2^k (a-b)^k + 2^k b^k$ for any even $k$ with $a = \err_{D}(\ell)$ and $b = \err_{D'}(\ell)$, we have:
% \[
% \sststile{\ell}{k} \Set{ \Paren{\err_D(\ell) - \err_{D'}(\ell)}^{k'}  \leq \epsilon^{k'-1} \cdot O(C(k/2))^{k} (\err_{D'}(\ell))^{k'} + \epsilon^{k'-1} \cdot O(C(k/2))^{k} \Paren{ \frac{1}{n} \sum_{i =1}^n (y_i-\iprod{\ell^{*},x_i})^{k}} } 
% \]




% \[
% \sststile{w,\ell}{k} \Set{ \err_{\cD}(\ell)^{k/2} \leq (1 + O(C(k/2))^{k/2} \epsilon^{k/2-1}) \cdot \err_{\cD'}(\ell)^{k/2} + O(C(k/2))^{k/2}\epsilon^{k/2-1} \cdot \Paren{\E_\cD (y-\iprod{\ell^{*},x})^k} }\mper\]
% In the special case when there exists an $\ell' \in \R^d$ such that $y_i = \iprod{\ell',x_i}$ for every $i$, so long as $\epsilon < 1/16C$, we obtain: $\err_D(\ell) = 0$. 
\label{lem:identifiability-least-squares-linear-sos}
\end{lemma}
% Before giving the proof of Lemma \ref{lem:identifiability-least-squares-linear-sos}, we show how we can complete the proof of Theorem \ref{thm:analysis-L2-linear-regression} using it.

We also need the following lemma (that follows from appropriate matrix concentration results) from \cite{DBLP:journals/corr/abs-1711-11581} stating that the uniform distribution on a sufficiently large set of i.i.d samples from a hypercontractive distribution also satisfy hypercontractivity. This allows us to argue that the uncorrupted empirical distribution $\widehat{\cD}$ is also hypercontractive when $\cD$ is. %The lemma follows from using appropriate matrix concentration results (namely, combining convergence guarantees for empirical covariance \cite{MR3127875} and the Matrix Rosenthal inequality \cite{DBLP:journals/ftml/Tropp15}).

%The first statement follows from using appropriate matrix concentration results (namely, combining convergence guarantees for empirical covariance \cite{MR3127875} and the Matrix Rosenthal inequality \cite{DBLP:journals/ftml/Tropp15}). %We point the reader to the standard proof done in \cite{DBLP:journals/corr/abs-1711-11581}. 

% The first statement follows from standard matrix concentration inequalities and is the subject of the following fact (for a proof, see Lemma ?? of \cite{Kothari-Steurer}). 

\begin{lemma}[Lemma 5.5 of \cite{DBLP:journals/corr/abs-1711-11581}]\label{fact:hypercontractivity-preserved-under-sampling}
Let $D$ be a $(C,k)$-certifiably hypercontractive distribution on $\R^d$. Let $X$ be an i.i.d. sample from $D$ of size $n \geq \Omega( (d^{k/2} \log{(d/\delta)})^{k/2})$. Then, with probability at least $1-\delta$ over the draw of $X$, the uniform distribution $D$ over $X$ is $(2C,k)$-certifiably hypercontractive.
\end{lemma}

We can now prove Lemma \ref{lm:opterror}.
\begin{proof}[Proof of Lemma \ref{lm:opterror}]
If $n \geq \tilde{\Theta}(d^{k/2} \log{(d/\epsilon)}^{k/2})$, then by Lemma \ref{fact:hypercontractivity-preserved-under-sampling}, with probability at least $1-\epsilon$, the uniform distribution $\widehat{\cD}$ on $X$ is $(2 C,k)$-hypercontractive. 

Since $\tmu$ is a pseudo-distribution of level $k$, combining Fact \ref{fact:sos-soundness} and Lemma \ref{lem:identifiability-least-squares-linear-sos}, we must have for $C = C(k/2)$,
\begin{equation} \label{eq:proof-to-pseudo-dist}
\pE_{\tmu} \Paren{\err_{\hat{\cD}}(\ell) - \err(w,\ell,X')}^{k/2} \leq O(C^{k/2} \eta^{k/2-1}) \cdot \pE_{\tmu} \err(w,\ell,X')^{k/2} + O(C^{k/2}\eta^{k/2-1}) \cdot \Paren{\E_{\hat{\cD}} (y-\iprod{\ell^{*},x})^k} .
\end{equation}

Taking $2/k$th powers of both sides of the above equation and recalling the definition of $\widehat{\opt}_{SOS}, \widehat{\opt}_k$, we get
\[
(\pE_{\tmu} \err_{\hat{\cD}}(\ell)- \err_{\cD'}(\ell))^{k/2})^{2/k} \leq O(C) \eta^{1-2/k} \cdot \widehat{\opt}_{SOS} + O(C) \eta^{1-2/k} \widehat{\opt}_k^{2/k}\mper
\]

Now, by Fact \ref{fact:pseudo-Holders},
$(\pE_{\tmu} [\err_{\hat{\cD}} (\ell) -\err(w,\ell,X')])^{k/2} \leq \pE_{\tmu}[ (\err_{\hat{\cD}}(\ell) - \err(w,\ell,X'))]^{k/2}$ and thus, 

\[
\pE_{\tmu} \err_{\hat{\cD}} (\ell) \leq (1 + O(C) \eta^{1-2/k}) \cdot \widehat{\opt}_{SOS} + O(C) \eta^{1-2/k} \widehat{\opt}_k \mper
\]

Finally, by another application of Fact \ref{fact:pseudo-Holders}, we have that for every $i$, $(y_i - \iprod{x_i,\pE_{\tmu}[\ell]})^2 \leq \pE_{\tmu}[(y_i \iprod{x_i,\ell})^2]$; in particular,  $\err_{\widehat{\cD}}(\pE_{\tmu}[\ell]) \leq \pE_{\tmu} \err_{\hat{\cD}} (\ell)$. Thus, we have
$$\err_{\widehat{\cD}}(\pE_{\tmu}[\ell]) \leq (1 + O(C) \eta^{1-2/k}) \cdot \widehat{\opt}_{SOS} + O(C) \eta^{1-2/k} \widehat{\opt}_k,$$
proving the lemma. 
\end{proof}

\subsubsection{Proof of Lemma \ref{lem:identifiability-least-squares-linear-sos}}\label{sec:soscertification}
Here we prove Lemma \ref{lem:identifiability-least-squares-linear-sos}. The proof is similar in spirit to that of Lemma \ref{lem:identifiability-least-squares-linear} but we need to adapt the various steps to a form suitable for SOS proof system.
\begin{proof}[Proof of Lemma \ref{lem:identifiability-least-squares-linear-sos}]

For brevity, we write $\err_{\cD}$ for $\err_{\cD}(\ell)$.

Let $w' \in \zo^n$ be given by $w'_i = w_i$ iff $i$th sample is uncorrupted in $U$ and $0$ otherwise.
Then, observe that $\sum_i w_i' = s$ for $s \geq (1-2\eta)n.$ 

Then, 
\[
\sststile{w'}{2} \Set{ \frac{1}{n} \sum_i (1-w'_i)^2 \leq 2 \eta}\mper
\]

Let $\err_{w'}(\ell) = \frac{1}{n} \sum_{i = 1}^n w_i' (v_i - \iprod{\ell,u_i})^2$.
We have:% for $k' = k/2$,
\[
\sststile{w,\ell}{4} \err_{\cD}(\ell) = \frac{1}{n} \sum_{i = 1}^n w'_i (y_i-\iprod{\ell,x_i})^2 + \sum_{i = 1}^n (1-w'_i) \cdot (y_i-\iprod{\ell,x_i})^2
\]

On the other hand, we also have:
\[
\sststile{w,\ell}{4} \frac{1}{n} \sum_{i = 1}^n w'_i (y_i-\iprod{\ell,x_i})^2 \leq \sum_{i =1}^n (y_i' - \iprod{\ell,x'_i})^2 = \err_{\cD'}(\ell)\mper
\]

Combining the above and using the sum-of-squares vesion of the H\"older's inequality, we have:

\begin{align}
\sststile{w,\ell}{k} \Paren{\err_{\cD}(\ell) - \err_{\cD'}(\ell)}^{k/2} &= \Paren{\frac{1}{n} \sum_{i = 1}^n (1-w'_i) \cdot (y_i -\iprod{\ell,x_i})^2}^{k/2}\notag\\
%&= \Paren{\frac{1}{n} \sum_{i = 1}^n (1-w'_i)^{k'-1} \cdot (y_i -\iprod{\ell,x_i})^2}^{k'}\notag\\
&\leq \Paren{\frac{1}{n} \sum_{i = 1}^n (1-w'_i)}^{k/2-1} \Paren{ \frac{1}{n} \sum_{i =1}^n (y_i-\iprod{\ell,x_i})^{k}} \notag\\
&\leq 2^{k/2-1}\eta^{k/2-1} \Paren{ \frac{1}{n} \sum_{i =1}^n (y_i-\iprod{\ell,x_i})^{k}}  \label{eq:CS-bound}\mper
\end{align}

Next, using the sum-of-squares inequality $(a+b)^k \leq 2^k a^k + 2^k b^k$, we have:
\begin{equation}\label{eq:minkowski}
\sststile{\ell}{k} \Set{\Paren{ \frac{1}{n} \sum_{i =1}^n (y_i-\iprod{\ell,x_i})^{k}} \leq 2^{k} \Paren{ \frac{1}{n} \sum_{i =1}^n (y_i-\iprod{\ell^{*},x_i})^{k}} + 2^{k} \Paren{ \frac{1}{n} \sum_{i =1}^n \iprod{\ell-\ell^{*},x_i}^{k}} } 
\end{equation}

By certifiable hypercontractivity of $\cD_x = D$, we have:
\[
\sststile{\ell}{k} \Set{\Paren{ \frac{1}{n} \sum_{i =1}^n \iprod{\ell-\ell^{*},x_i}^{k}} \leq C(k)^{k/2} \Paren{ \frac{1}{n} \sum_{i =1}^n \iprod{\ell-\ell^{*},x_i}^2}^{k/2}} 
\]

Again, by using the sum-of-squares inequality $(a+b)^k \leq 2^k a^k + 2^k b^k$, we have:
\[
\sststile{\ell}{k} \Set{\Paren{ \frac{1}{n} \sum_{i =1}^n \iprod{\ell-\ell^{*},x_i}^2}^{k/2} \leq 2^{k/2}\Paren{ \frac{1}{n} \sum_{i =1}^n (y_i-\iprod{\ell,x_i}^2}^{k/2} + 2^{k/2}\Paren{ \frac{1}{n} \sum_{i =1}^n (y_i-\iprod{\ell^{*},x_i}^2}^{k/2}  } 
\]
Finally, using the sum-of-squares version of H\"older's inequality again, we have:
\[
\sststile{\ell}{k} \Set{\Paren{ \frac{1}{n} \sum_{i =1}^n (y_i-\iprod{\ell^{*},x_i})^2}^{k/2} \leq  \frac{1}{n} \sum_{i =1}^n (y_i-\iprod{\ell^{*},x_i})^k   } 
\]
Combining the above with \eqref{eq:minkowski}, we have:
\begin{multline}
\sststile{\ell}{k}\\ \Set{\Paren{ \frac{1}{n} \sum_{i =1}^n (y_i-\iprod{\ell,x_i})^{k}} \leq O(C(k/2))^{k} \Paren{ \frac{1}{n} \sum_{i =1}^n (y_i-\iprod{\ell^{*},x_i})^{k}} + O(C(k/2))^{k}\Paren{ \frac{1}{n} \sum_{i =1}^n (y-\iprod{\ell,x_i})^2}^{k/2}} 
\end{multline}

Thus, together with \eqref{eq:CS-bound},  we have:
\begin{multline}
\sststile{\ell}{k} \Paren{\err_{\cD}(\ell) - \err_{\cD'}(\ell)}^{k/2}  \leq \eta^{k/2-1} \cdot O(C(k/2))^{k} (\err_{\cD}(\ell))^{k/2} \\+ \eta^{k/2-1} \cdot O(C(k/2))^{k} \Paren{ \frac{1}{n} \sum_{i =1}^n (y_i-\iprod{\ell^{*},x_i})^{k}}  
\end{multline}
Using the sum of squares inequality $\delta^k a^k \leq (2\delta)^k (a-b)^k + (2\delta)^k b^k$ for any $a,b$ and even $k$, and applying it with $a = \err_{\cD}(\ell)$, $b = \err_{\cD'}(\ell)$ and $\delta = \eta^{k/2-1} \cdot O(C(k/2))^{k}$ and rearranging, we have:

\begin{multline}
\sststile{\ell}{k}  (1-\delta) \Paren{\err_{\cD}(\ell) - \err_{\cD'}(\ell)}^{k/2}  \leq \eta^{k/2-1} \cdot O(C(k/2))^{k} (\err_{D'}(\ell))^{k/2} \\+ \eta^{k/2-1} \cdot O(C(k/2))^{k} \Paren{ \frac{1}{n} \sum_{i =1}^n (y_i-\iprod{\ell^{*},x_i})^{k}} 
\end{multline}

For $\delta < 0.9$, this implies:


\begin{multline}
\sststile{\ell}{k}  \Paren{\err_{\cD}(\ell) - \err_{\cD'}(\ell)}^{k/2}  \leq \eta^{k/2-1} \cdot O(C(k/2))^{k} (\err_{\cD'}(\ell))^{k/2} \\+ \eta^{k/2-1} \cdot O(C(k/2))^{k} \Paren{ \frac{1}{n} \sum_{i =1}^n (y_i-\iprod{\ell^{*},x_i})^{k}}  
\end{multline}
This completes the proof.
% Plugging this in to \label{eq:CS-bound} and using that 
% \[
% \sststile{w,\ell}{k} \Set{ \err_D(\ell)^{k'} \leq 2^{k'} \Paren{\err_D(\ell) - \err_{D'}(\ell)}^{k'}+ 2^{k'} \err_ } 
% \]


% Next, using the sum-of-squares inequality $(a-b)^4 \leq 8 a^4 + 8 b^4$, we have:
% \[
% \sststile{\ell}{4} \frac{1}{n}\sum_{i = 1}^n (y_i - \iprod{\ell,x_i})^4 \leq 8 \sum_{i = 1}^n y_i^4 + 8 \sum_{i = 1}^n \iprod{\ell,x_i}^4.
% \]
% Using certifiable subgaussianity of $D$, we have that: 
% \[
% \sststile{\ell}{4} \sum_{i = 1}^n \iprod{\ell,x_i}^4 \leq 2C\Paren{\sum_{i = 1}^n \iprod{\ell,x}^2}^2.
% \]

% Finally, by sum of squares version of the squared triangle inequality for $\ell_2$ norm, we have: 
% \[
% \sststile{\ell}{4} \frac{1}{n} \sum_{i =1}^n \iprod{\ell,x_i}^2 \leq 2\frac{1}{n} \sum_{i = 1}^n  (y_i-\iprod{\ell,x_i})^2 + 2\frac{1}{n} \sum_{i = 1}^n  y_i^2 \leq 2\err_D^2 + \paren{2\frac{1}{n} \sum_{i = 1}^n y_i^2}^2.
% \] 

% Thus, combining the above estimates, we obtain that:
% \[
% \sststile{w,\ell}{4} \frac{1}{n} \sum_{i = 1}^n (y_i-\iprod{\ell,x_i})^4 \leq 8 \frac{1}{n} \sum_{i = 1}^n y_i^4 + 16C (8 \err_{D}^2 + 8(\frac{1}{n} \sum_{i = 1}^n y_i^2)^2 ).
% \] 

% Thus, we obtain:

% \[
% \sststile{w,\ell}{4} \Set{ \Paren{\err_D(\ell) - \err_{w'}(\ell)}^2\leq 16 \epsilon \frac{1}{n} \sum_{i = 1}^n y_i^4 + 32C \epsilon (8 \err_{D}^2 + 8(\frac{1}{n} \sum_{i = 1}^n y_i^2)^2 ) }
% \]

% Apply Lemma \ref{lem:square-subtract-sos} with $A = \err_D(\ell)$ and $B = \err_{'}(\ell)$ and $\delta = 256 C \epsilon$ and $E = 256 C \epsilon \sum_{i = 1}^n y_i^2)^2 ) + 16 \epsilon \frac{1}{n} \sum_{i = 1}^n y_i^4$. 
% Observe that both $A, B$ are SoS polynomials and $\delta, E$ are constants. 

% Then, we conclude that:
% \[
% \Set{(A-B)^2 \leq \delta A^2 + E}
%  \sststile{2}{A,B} \Set{  ((1-\delta) A - \frac{1}{1-\delta} B)^2 \leq (\frac{1}{1-\delta}-1)^2B^2 + E }
% \]

% We then observe that $\cA \sststile{2}{w,\ell} \Set{\err_{w'}(\ell) \leq \err_{w}(\ell) \leq \opt}$. Thus, for the polynomial $B$ above, $B \leq \opt$.

% Thus, 
% \[
% \cA \sststile{2}{w,\ell} \Set{  ((1-\delta) \err_{D}(\ell) - \frac{1}{1-\delta} \err_{w'}(\ell))^2 \leq (4 \delta^2 \opt^2 + E)}
% \]
% We now apply Lemma \ref{lem:deg-reduction-square-sos} to $A = \frac{1}{(4 \delta^2 \opt^2 + E)} ((1-\delta) \err_{D}(\ell) - \frac{1}{1-\delta} \err_{w'}(\ell))$ to conclude that 

% \[
% \cA \sststile{2}{w,\ell} \Set{A \leq 1}\mper
% \]
% Rearranging this yields that 
% \begin{equation}
% \cA \sststile{4}{w,\ell} \Set{ ((1-\delta) \err_{D}(\ell) - \frac{1}{1-\delta} \err_{w'}(\ell))\leq \sqrt{4 \delta^2 \opt^2 + E} }\label{eq:main-step-1}
% \end{equation}

% % Rearranging, this yields:
% % \[
% % \sststile{w,\ell}{4} \Set{ \Paren{\err_D(\ell) - \err_{w'}(\ell)}^2 - 256 C \epsilon \err_{D}^2 \leq 16 \epsilon \frac{1}{n} \sum_{i = 1}^n y_i^4 + 256 C \epsilon 8(\frac{1}{n} \sum_{i = 1}^n y_i^2)^2 ) }
% % \]
% Using that $\cA \sststile{2}{w,\ell} \Set{\err_{w'}(\ell) \leq \opt}$ along with \eqref{eq:main-step-1}, we have:

% \[
% \cA \sststile{2}{w,\ell} \Set{ \err_D(\ell) \leq \frac{1}{(1-\delta)^2}\opt + \frac{1}{1-\delta} \sqrt{ 4 \delta^2 \opt^2 + E} }
% \]
% Plugging in the expressions for $\delta, E$ and using that $\frac{1}{(1-\delta)} \leq 1+ O(\delta)$, we obtain the first claimed bound. 



% Next, consider the second bound for the case when $y_i = \iprod{\ell',x_i}$. Then, 
% \[
% \sststile{2}{w,\ell} \frac{1}{n} \sum_{i = 1}^n w_i'(v_i - \iprod{\ell,u_i})^2 \leq (v_i - \iprod{\ell,u_i})^2 = 0 \mper
% \] 

% Then, using certifiable subgaussianity of $D$, 
% \[
% \frac{1}{n} \sum_{i = 1}^n (y_i-\iprod{\ell,x_i})^4 = \frac{1}{n} \sum_{i = 1}^n \iprod{\ell'-\ell,x_i}^4 \leq 2C(\frac{1}{n} \sum_{i = 1}^n \iprod{\ell-\ell',x_i}^2)^2.
% \] 

% Using the above estimate, rearranging \eqref{eq:CS-bound} with the fact that $\epsilon < 1/16C$ and an application of Lemma \ref{lem:deg-reduction-square-sos} as before implies the second corollary in the lemma. 
\end{proof}


\subsubsection{Bounding the Generalization Error}
In this section we prove Lemma \ref{lm:generror}. The lemma follows from standard concentration inequalities combined with standard generalization bounds for linear regression. 


\begin{proof}[Proof of Lemma \ref{lm:generror}(1)]
Let $\ell^{*}$ be a linear function of bit complexity at most $B$ that achieves the optimum least squares regression error on $\cD$. We will first show that $\widehat{\opt}_{SOS} \leq \err_{\widehat{\cD}}(\ell^*)$ by exhibiting a feasible pseudo-density. To see this, consider the point-mass density, $\tmu$, supported on the following point: $(w,\ell^*,X')$ where $w_i = 1$ if $(x_i,y_i) = (u_i, v_i)$ and $0$ otherwise (i.e., $w_i = 1$ if and only if the $i$'th example was uncorrupted) and $(x_i', y_i') = (x_i,y_i)$ for all $i \in [n]$. Clearly, $\tmu$ is a feasible solution to the optimization progam \ref{alg:robust-regression-program} and $\pE_{\tmu}[\err(w,\ell,X)^{k/2}] = \left((1/n) (\sum_{i=1}^n (y_i - \iprod{\ell^*,x_i})^2)\right)^{k/2} = \err_{\widehat{\cD}}(\ell^*)^{k/2}$. It follows that $\widehat{\opt}_{SOS} \leq  \err_{\widehat{\cD}}(\ell^*)$. 

We next argue that $\err_{\widehat{\cD}}(\ell^*)$ is close to $\err_{\cD}(\ell^*)$ for $n$ sufficiently big. Let $Z$ be the random variable $(y - \iprod{\ell^*,x})^2$ for $(x,y) \sim \cD$. Note that $\err_{\widehat{\cD}}(\ell^*)$ is the average of $n$ independent draws of the random variable $Z$. Also note that $\E[Z] = \opt(\cD)$. We will next bound the variance of $Z$. We have, for $(x,y) \sim \cD$,
\begin{equation*}
\E[Z^2] = \E[(y - \iprod{\ell^*,x})^4] \leq 2 \E[y^4] + 2 \E[\iprod{\ell^*,x}^4] \leq 2 M^4 + 2 C^2 (\E[\iprod{\ell^*,x}^2])^2, 
\end{equation*}
where the last inequality follows by hypercontractivity.
Now, $\E[\iprod{\ell^*,x}^2] \leq 2 \E[(y - \iprod{\ell^*,x})^2] + 2 \E[y^2] \leq 2 \opt(\cD) + 2 M^2 \leq 4 M^2$ as $\opt(\cD) \leq M^2$ (the $0$ function achieves this error). Combining the above we get that $\E[Z^2] = O(M^4)$. 

Thus, for some $n_0 = O(1/ \epsilon^3) (M^4)$, if we take $n \geq n_0$ independent samples $Z_1, Z_2, \ldots,Z_n$ of $Z$, then $\Pr[ |\frac{1}{n} \sum_{i = 1}^n Z_i - \E[Z]| \geq \epsilon] \leq \epsilon$. Thus, with probability at least $1-\epsilon$, $\err_{\widehat{\cD}}(\ell^*) \leq \opt(\cD) + \epsilon$. The claim now follows.
\end{proof}

Part (2) of Lemma \ref{lm:generror} follows from standard generalization arguments such as the following claim applied to $\ell_M$. We omit the details. 

\begin{fact}[Consequence of Theorem 10.1 in \citet{DBLP:books/daglib/0034861}] \label{fact:standard-generalization-bound}
Let $H$ be a class of functions over $\R^d$ such that each $h \in H$ can be described in $B$ bits. Suppose each function in $H$ takes values in $[-M,M]$ for some positive real $M$.  
Let $\cD$ be a distribution on $\R^d \times [-M,M]$ and let $(x_1,y_1),\ldots,(x_n,y_n)$ be $n$ i.i.d samples from $\cD$ for $n > n_0 = O(M^2 B\log{(1/\delta)}/\epsilon^2)$. 

Then, with probability at least $1-\delta$ over the draw of $X$, for every $\ell \in H$, 
\[
\E_{(x,y) \sim D} (y - h(x))^2 \leq (1/n) \sum_{i=1}^n (y_i - h(x_i))^2 + \epsilon \mper
\]

\end{fact}

\ignore{
 The certifiability lemma above immediately implies why the output of the algorithm is a good regression hypothesis over the uncorrupted sample $X$. % Specifically, we will use the following corollary of Lemma \ref{lem:identifiability-least-squares-linear-sos} which shows that the linear function output by the algorithm has low error on the original uncorrupted sample $X$.

\begin{lemma}[Analysis of Algorithm] \label{lem:analysis-full-L2}

Let $X$ be a collection of $n$ labeled examples in $\R^d$ such that $\hat{D}$, the uniform distribution on  $x_1, x_2, \ldots, x_n$ is $(C,k)$-certifiably hypercontractive. 
Let $U$ be an $\eta$-corruption of $X$.
Let $\tmu$ be the pseudo-distribution satisfying $\cA_{U,\eta}$ that minimizes the polynomial $\Paren{\frac{1}{n} \sum_{i = 1}^n (y_i' - \iprod{\ell,x_i'})^2 }^{k/2}$. %produced by the Algorithm \ref{alg:robust-regression-program}.

Then, for any $\ell^{*} \in \R^d$ of bit complexity at most $B = \poly(n,d^k)$, $\hat{\ell} = \pE_{\tmu}\ell$ satisfies: 

\[
\frac{1}{n} \sum_{i = 1}^n (y_i - \iprod{\hat{\ell},x})^2 < (1+O(C)\eta^{1-2/k})  \opt(\hat{\cD}) + O(C) \eta^{1-2/k} \Paren{\frac{1}{n} \sum_{i = 1}^n (y_i - \iprod{\ell^{*},x})^k}^{2/k}  \mper
\]


\end{lemma}


Next, we show that $\opt(\hat{\cD}) \leq \opt(\cD)$. Here, we employ the hypercontractivity of $D$ and the boundedness of the labels coupled with standard concentration arguments. 

\begin{lemma}[Convergence of Empirical Optimum] \label{lem:error-of-output-is-small}
Let $\cD$ be a distribution on $\R^d \times \cY$ such that the marginal $D = \cD_x$ is $(C,k)$-hypercontractive for for some even $k \geq 4$ and $\cY \subseteq [-M,M]$ for some positive real $M$. 
Let $\opt(\cD) = \min_{\ell} \E_{\cD} (y - \iprod{\ell,x})^2$ where the minimum is over all $\ell$ of bit complexity at most $B$. 
Let $X$ be a sample from $\cD$ of size $n \geq n_0$ for $n_0 = O(1/\delta \epsilon^2) \cdot M^4$ and let $\hat{\cD}$ be the uniform distribution on $X$. 
Let $\opt(\hat{\cD}) = \min_{\ell} \E_{\hat{\cD}} (y - \iprod{\ell,x})^2$ where the minimum is over all $\ell$ of bit complexity at most $B$.

Then, with probability at least $1-\delta$ over the draw of $X$,  $\opt(\hat{\cD}) \leq \opt(\cD) + \epsilon$.  

\end{lemma}


Using the above lemmas, it is easy to wrap up the proof of Theorem \ref{thm:analysis-L2-linear-regression}. We need to show three claims 1) if $D = \cD_x$ is $(C,k)$-certifiably hypercontractive, then with high probability over the draw of a large enough sample $X$,  so is $\hat{D}$, 2) when the original sample $X$ is of size large enough, then any regression hypothesis that has low error on uniform distribution $\hat{\cD}$ on $X$ has low error on $\cD$ and 3) for the optimal $\ell^{*}$, $\E_{\hat{\cD}}[ (y - \iprod{\ell^{*},x})^k]$ is close to that of $\E_{\cD} (y - \iprod{\ell^{*},x})^k$. 




The second follows from standard generalization for bounded loss functions. Namely, we will appeal to the following standard generalization result and apply it to the truncation $f = f_{\ell}$ of any linear function $\ell$ defined by:
\[
f (x) = \begin{cases} \iprod{\hat{\ell},x} & \text{ if } |\iprod{\hat{\ell},x}| \leq M \\
       \sign(\iprod{\hat{\ell},x}) \cdot M & \text{ otherwise.}
       \end{cases}
\]

}
%Finally, the last statement can be established based on the following observation:
\ignore{


% \Pnote{Move next two lemmas to SoS toolkit.}
% \begin{lemma}[SoS Fact 1]
% Let $A,B$ be SoS polynomials and $\delta, E$ be constants. Then, 

% \[
% \Set{(A-B)^2 \leq \delta A^2 + E}
%  \sststile{2}{A,B} \Set{  ((1-\delta) A - \frac{1}{1-\delta} B)^2 \leq (\frac{1}{1-\delta}-1)^2B^2 + E }
% \]
% \label{lem:square-subtract-sos}
% \end{lemma}
% \begin{proof}
% Use the identity: $(A-B)^2 - \delta A^2 = ((1-\delta) A - \frac{1}{1-\delta}B)^2 - (\frac{1}{1-\delta}-1)^2B^2$ and rearrange. 


% \end{proof}



% \begin{lemma}[SoS Fact 2]
% \[
% \Set{A^2 \leq 1} \sststile{2}{A} \Set{ A \leq 1}\mper\] \label{lem:deg-reduction-square-sos}
% \end{lemma}
% \begin{proof}
% We use the identity: $2(1-A) + (A^2-1) = (1-A)^2$ or $2(1-A) = (1-A)^2 + 1-A^2$.
% Thus, $\Set{A^2 \leq 1} \sststile{2}{A} \Set{2(1-A) \geq 0}.$
% \end{proof}

% \subsection{Agnostic Robust Linear Regression}
% \begin{lemma}[Identifiability of Robust Linear Regression Hypothesis]
% Let $D,D'$ be distributions on $\R^d \times \R$ such that the marginals $D_s, D'_s$ on $\R^d$ are $4$-certifiably $C$-subgaussian distributions and $\|D-D'\|_{TV} \leq \epsilon.$ 

% Suppose $\E_{D} (y-\iprod{\ell,x})^2 \leq \err$. Then,  \[
% \E_{D'} (y'-\iprod{\ell,x'})^2 \leq \err + 8 \sqrt{C\epsilon} \err + 4 \sqrt{C\epsilon} \E_{D'}y'^4\mper\]
% \end{lemma}

% \begin{proof}
% Let $\err = \E_D ( y-\iprod{\ell,x})^2.$ Then, $\$E_D \1 \Set{ (x,y) = (x',y')}( y-\iprod{\ell,x})^2 \leq \E_D ( y-\iprod{\ell,x})^2 = \err.$

% \begin{align*}
% \E_{G}  (y'-\iprod{\ell,x'})^2 &= \E_D \1 \Set{ (x,y) = (x',y')}( y-\iprod{\ell,x})^2 + \E_{D'} \1 \Set{ (x,y) \neq (x',y')} \cdot (y'-\iprod{\ell,x'})^2\\
% &\leq \err + \sqrt{\E_{D'}\1 \Set{ (x,y) \neq (x',y')}^2} \sqrt{ \E_{D'} (y'-\iprod{\ell,x'})^4}\\
% &\leq \err +  \sqrt{\epsilon} \sqrt{(16 \E_{D'} y'^4 + 16 \E_{D'} \iprod{\ell',x'}^4)}\\
% &\leq \err + 4 \sqrt{\epsilon} \sqrt{C} \E_{D'} \iprod{\ell',x'}^2. + 4\sqrt{C} \E_{D'} y'^4 \sqrt{\epsilon}.
% \end{align*}

% Rearranging and using that $\epsilon < 1/16C$, we obtain that 

% \[
% \E_{D'} (y'-\iprod{\ell,x'})^2 \leq \frac{1}{(1-4\sqrt{C\epsilon})} (\err + \cdot 4 \sqrt{C \epsilon} \E_{D'} y'^4) \leq \err + 8 \sqrt{C\epsilon} \err + 4 \sqrt{C\epsilon} \E_{D'}y'^4.
% \]


% \end{proof}

%   % \begin{algorithm}[Algorithm for moment estimation via sum-of-squares]
  %   %\label[algorithm]{alg:moment-estimation-program}\mbox{}
  %   \begin{description}

  %   \item[Given:]
  %     $\e$-corrupted sample $Y=\set{y_1, \sigma_1 ,\ldots,y_n, \sigma_n}\subseteq \R^d$ where $\sigma_i$s are the labels.
  %   \item[Estimate:]
  %     A sub-sample $X' = \{ (x_i,)} $
  %   \item[Operation:]\mbox{}
  %     \begin{enumerate}
  %     \item 
  %       find a level-$\ell$ pseudo-distribution $D'$ that satisfies $\cA_{Y,\e}$ and $\cB_{C,k,\ell}$
  %     \item
  %       output moment-estimates $\hat M_1,\ldots,\hat M_k$ with $\hat M_r = \pE_{D'(x')} \Brac{\tfrac 1 n \sum_{i=1}^n (x'_i)^{\otimes r}}$
  %     \end{enumerate}
  %   \end{description}    
  % \end{algorithm}
  \begin{proof}[Proof of Lemma \ref{lem:analysis-full-L2}]
Since $\tmu$ is a pseudo-distribution of level $k$, combining Fact \ref{fact:sos-soundness} and Lemma \ref{lem:identifiability-least-squares-linear-sos}, we must have for $C = C(k/2)$,
\begin{equation} \label{eq:proof-to-pseudo-dist}
\pE_{\tmu} \Paren{\err_{\hat{\cD}}(\ell) - \err_{\cD'}(\ell)}^{k/2} \leq O(C^{k/2} \eta^{k/2-1}) \cdot \pE_{\tmu} \err_{\cD'}(\ell)^{k/2} + O(C^{k/2}\eta^{k/2-1}) \cdot \Paren{\E_{\hat{\cD}} (y-\iprod{\ell^{*},x})^k} 
\end{equation}

Since $\tmu$ is of degree $k$ and $\err_{\cD'}$ is a SoS polynomial, $\pE_{\tmu} \err_{\cD'}(\ell) \geq 0.$ Thus, let $\opt(\hat{\cD})$ be a positive real such that: $\opt(\hat{\cD})^k = \pE_{\tmu} \err_{\cD'}(\ell)^k$.

Then, taking $2/k$th powers of both sides of \eqref{eq:proof-to-pseudo-dist}, we thus have:
\[
(\pE_{\tmu} \err_{\hat{\cD}}(\ell)- \err_{\cD'}(\ell))^{k/2})^{2/k} \leq O(C) \eta^{1-2/k} \opt(\hat{\cD}) + O(C) \eta^{1-2/k} \Paren{\E_\cD (y-\iprod{\ell^{*},x})^k}^{2/k}\mper
\]

Using H\"older's inequality for pseudo-distributions, we have:
$(\pE_{\tmu} \err_{\hat{\cD}} (\ell) - \err_{\cD'}(\ell))^{k/2} \leq \pE_{\tmu} (\err_{\hat{\cD}}(\ell) - \err_{\cD'}(\ell))^{k/2}$ and thus, 

\[
|\pE_{\tmu} \err_{\hat{\cD}} (\ell) - \err_{D'}(\ell)| \leq O(C) \eta^{1-2/k} \opt + O(C) \eta^{1-2/k} \Paren{\E_{\hat{\cD}} (y-\iprod{\ell^{*},x})^k}^{2/k}\mper
\]

By another application of H\"older's inequality for pseudo-distributions, we have: $\Paren{\pE_{\tmu} \err_{\hat{\cD}}(\ell)}^{k/2} \leq \pE_{\tmu} \err_{\hat{\cD}} (\ell)^{k/2}$. Thus, we have:
\[
\pE_{\tmu} \err_{\hat{\cD}} (\ell)  \leq (1+O(C \eta^{1-2/k})) \opt(\hat{\cD}) + O(C) \eta^{1-2/k} \Paren{\E_{\hat{\cD}} (y-\iprod{\ell^{*},x})^k}^{2/k}\mper
\]

Using H\"older's inequality for pseudo-distributions again, we have: 
$\err_{\hat{\cD}}(\pE_{\tmu}(\ell)) \leq \pE_{\tmu} \err_{\hat{\cD}} (\ell) .$ 

Plugging this in the above completes the proof. 



% Finally, by another application of H\"older's inequality for pseudo-distributions, we must have:
% $\pE_{\tmu} \E_D (y-\iprod{\ell,x})^2 \geq \E_D (y - \pE_{\tmu}\iprod{\ell,x})^2 = \E_D (y - \iprod{\ell',x})^2$. 


\end{proof}


\begin{proof}[Proof of Lemma \ref{lem:error-of-output-is-small}]
Let $\ell^{*}$ be a linear function of bit complexity at most $B$ that achieves the optimum least squares regression error on $\cD$. Then, $\E_{\cD} \iprod{\ell^{*},x}^2 \leq 2\E_{\cD} (y - \iprod{\ell^{*},x})^2 + 2 \E_{\cD} y^2 \leq 2 \opt(\cD) + 2M^2$. Further, $\E_{\cD} (y - \iprod{\ell^{*},x})^4 \leq 8 \E_{\cD} y^4 + 8 \E_{\cD} \iprod{\ell^{*},x}^4$. By $(C,4)$-hypercontractivity of $\cD$,  $\E_{\cD} \iprod{\ell^{*},x}^4 \leq \Paren{\E_{\cD} \iprod{\ell^{*},x}^2}^2 \leq 4 \opt(\cD)^2 + 4 M^4$. Since $\opt(\cD) \leq M^2$ (the $0$ function achieves this error), this is at most $O(M^4)$.  

Next, let $X$ be an i.i.d. sample from $\cD$ and let $\hat{\cD}$ be the uniform distribution on $X$. We will show that the error of $\ell^{*}$ on $\hat{\cD}$ is at most an $\epsilon$ larger than the error of $\ell^{*}$ on $\cD$ with probability at least $1-\delta$. This will establish the claimed upper bound on $\opt(\hat{\cD})$.

Observe that the error of $\ell^{*}$ on $\hat{\cD}$: $\E_{\hat{\cD}} (y - \iprod{\ell^{*},x})^2$ is the average of independent draws of the random variable $Z= (y - \iprod{\ell^{*},x})^2$. 

From the above calculation, $\E Z^2 \leq O(1) \cdot (\opt(\cD)^4 + M^4)$. Thus, for some $n_0 = O(1/\delta \epsilon^2) (M^4)$, if we take $n \geq n_0$ independent samples $Z_1, Z_2, \ldots,Z_n$ of $Z$, then $\Pr[ |\frac{1}{n} \sum_{i = 1}^n Z_i - \E[Z]| \geq \epsilon] \leq \delta$. 

This finishes the proof. 


\end{proof}
}
% \ignore{
\subsection{Robust L1 Regression} \label{sec:robust-L1-regression-algo}
In this section, we present our robust L1 regression algorithm. Our main goal is the following theorem. 

\begin{theorem} \label{thm:L1-regression-analysis}
Let $\cD$ be an arbitrary distribution on $\R^d \times \cY$ for $\cY \subseteq [-M,M]$ for a positive real $M$. Let $\kappa$ be the ratio of the maximum to the minimum eigenvalue of the covariance matrix of $D$, the marginal of $\cD$ on $x$. Let $\opt(\cD)$ be the minimum of $\E_{\cD} \abs{y-\iprod{\ell,x}}$ over all $\ell$ that have bit complexity bounded above by $B$. Let $\ell^{*}$ be any such minimizer and $\eta > 0$ be an upper bound on the fraction of corruptions. 

For any $\epsilon > 0$, let $X$ be an i.i.d. sample from $\cD$ of size $n \geq n_0$ for some $n_0 = O(1/\epsilon^2) \cdot (M^2\|\ell^{*}\|_2^4 + d \log{(d)} \|\Sigma\|/\eta)$.

Then, with probability at least $1-\epsilon$ over the draw of the sample $X$, given any $\eta$-corruption $U$ of $X$ and $\eta$ as input, there's a polynomial time algorithm (Algorithm \ref{alg:robust-regression-program-L1}) that outputs a function $f:\R^d \times \R$ such that:
\[
\E_{(x,y) \sim \cD} \abs{y- f(x)} < \opt(\cD) + O(\sqrt{\kappa \eta}) (\sqrt{\E_{\cD} y^2} + \sqrt{\Paren{\E_{\cD} (y-\iprod{\ell^{*},x})^2}}) + \epsilon \mper
\] 

\end{theorem}
\begin{remark}
The lower bound example in Lemma \ref{lm:lb1} also shows that the above bound is tight in the dependence on $\eta$ and $\kappa$.
\end{remark}




As in the previous section, our algorithm will find pseudo-distributions satisfying a set of polynomial inequalities that encode the hypotheses of the robust certifiability lemma and the ``error'' polynomial. 

Let $\cA_{U,\eta,Q}$ be the following system of polynomial equations:

\begin{equation}
  \cA_{U,\eta,Q}\colon
  \left \{
    \begin{aligned}
      &&
      \textstyle\sum_{i=1}^n w_i
      &= (1-\eta) \cdot n\\
      &\forall i\in [n].
      & w_i^2
      & =w_i \\
      &\forall i\in [n].
      & w_i \cdot (u_i - x'_i)
      & = 0\\
      &\forall i\in [n].
      & w_i \cdot (v_i - y'_i)
      & = 0\\
      & &\|\ell\|_2^2 &\leq Q^2\\
      &\forall i \in [n] & \tau_i' \geq (y_i' - \iprod{\ell,x'_i})\\
      &\forall i \in [n] & \tau_i' \geq -(y_i' - \iprod{\ell,x'_i})
     % & &\frac{1}{n} \sum_i w_i \tau_i' &\leq \opt
    \end{aligned}
  \right \} \label{eq:polynomial-constraints-w}
\end{equation}

This system of equations takes as parameters the input sample $U$ and a bound on the fraction of outliers $\eta$. 
%It also assumed that $\opt$ (the L1 error of the best regression hypothesis) and $B$, a upper bound on the squared L2 norm of the optimal regressor is known. This assumption can be easily removed by the standard ``doubling trick''. 

We can now describe our algorithm for robust L1 regression. 

\begin{mdframed}
  \begin{algorithms}[Algorithm for Robust L1 Linear Regression via Sum-of-Squares]
   %\label[algorithm]{alg:moment-estimation-program}\mbox{}
     \label{alg:robust-regression-program-L1}\mbox{}
    \begin{description}
    \item[Given:]
     An $\eta$-corruption $U$ of a labeled sample $X$ of size $n$ from an arbitrary distribution $\cD$. 
     %A bound $\opt$ on the error of the best fitting L1 regression hypothesis for $D_0$.
    The Euclidean norm of the best fitting L1 regression hypothesis for $\cD$, $Q$.
    %\item[Estimate:]
     % $\ell \in \arg \min \E_{(x,y) \sim \cD} |y-\iprod{\ell,x}|$.
    \item[Operation:]\mbox{}
      \begin{enumerate}
      \item 
        find a level-$4$ pseudo-distribution $\tmu$ that satisfies $\cA_{U,\eta,Q}$ and minimizes $\Paren{\frac{1}{n}\sum_{i = 1}^n \tau_i}^2$.
      \item
        Return $\hat{\ell} = \pE_{\tmu}\ell$.% Output $f = f_{\hat{\ell}}$ defined by 
        % \[
        % f(x) = \begin{cases} \iprod{\hat{\ell},x} & \text{ if} |\iprod{\hat{\ell},x}| \leq M\\
        %           \sign( \iprod{\hat{\ell},x}) \cdot M & \text{ otherwise.}
        %         \end{cases}
        % \]
      \end{enumerate}
    \end{description}    
  \end{algorithms}
\end{mdframed}



\paragraph{Analysis of Algorithm}
The plan of this subsection and the proofs are essentially analogous to the ones presented in the previous subsection. We will split the analysis into bounding the optimization and generalization errors as before. Let $\opt_{SoS}$ be the L1 error of $\hat{\ell}$ output by Algorithm \ref{alg:robust-regression-program-L1} and let $\ell^{*}$ be the optimal hypothesis for $\cD$. 


\begin{lemma}[Bounding the Optimization Error] \label{lem:opt-error-L1}
Under the assumptions of Theorem \ref{thm:L1-regression-analysis} (and following the above notations), 
$$\err_{\widehat{\cD}}(\widehat{\ell}) \leq \widehat{\opt}_{SOS} + 2 \kappa^{1/2} \eta^{1/2} \sqrt{\sum_{i =1}^n y_i^2} + 2 \kappa^{1/2} \eta^{1/2} \err_{\cD}(\ell^{*})\mper$$ 

% \[
% \frac{1}{n} \sum_{i = 1}^n\abs{y_i - \iprod{\hat{\ell},x_i}} \leq \opt(\hat{\cD}) + 2\kappa^{1/2} \eta^{1/2} \sqrt{ \frac{1}{n} \sum_{i =1}^n y_i^2} + 2 \kappa^{1/2}\eta^{1/2} \cdot\sqrt{  \frac{1}{n}\sum_{ i = 1}^n (y_i - \iprod{\ell^{*},x})^2} \mper
% \]

\end{lemma}



\begin{lemma}[Bounding the Generalization Error]
Under the assumptions of Theorem \ref{thm:L1-regression-analysis}, with probability at least $1-\epsilon$, 
\begin{enumerate}
\item $\widehat{\opt}_{SOS}  \leq \opt(\cD) + \epsilon$.

\item $\err_{\cD}(\widehat{\ell}_M)) \leq \err_{\widehat{\cD}}(\widehat{\ell}) + \epsilon$.

\item $\frac{1}{n} \sum_{i = 1}^n y_i^2 \leq \E_{\cD} y^2$.
\end{enumerate}
\end{lemma}

The proofs of the above two lemmas are entirely analogous to the ones presented in the previous section. The main technical ingredient as before is a SoS version of the robust certifiability result. Since this is the only technical novelty in this subsection, we present the statement and proof of this result below and omit the other proofs. 


 % As in the previous section, the key component of our analysis is a SoS version of the robust certifiability lemma which we present next. We will skip the proof which is, as in the previous section, a line-by-line translation of the proof of robust certifiability presented in Section \ref{sec:robust-certifiability}.

\begin{lemma}[SoS Proof of Robust Certifiability for L1 Regression] \label{lem:robust-certifiability-L1-SoS}

Let $X$ be a collection of $n$ labeled examples in $\R^d$ such that $D$, the uniform distribution on  $x_1, x_2, \ldots, x_n$ has 2nd moment matrix with all eigenvalues within a factor $\kappa$ of each other. Let $U$ be an $\eta$-corruption of $X$.

Let $w,\ell,X', \tau'$ satisfy the set of system of polynomial equations $\cA_{U,\eta,Q}$. Let $\tau_i$ satisfy $\tau_i^2 = (y_i - \iprod{\ell,x_i})^2$ and $\tau_i \geq 0$ for every $i$. Then, for any $\ell^{*} \in \R^d$ such that $\|\ell^{*}\|_2 \leq Q$,

% Then, 
% \[
% \E_{D} \abs{\iprod{\ell,x} - y} \leq  \E_{D'} \abs{\iprod{\ell,x} - y} + \epsilon^{1/2} \cdot \sqrt{\E_{D} (\iprod{\ell,x} - y)^2}.
% \]

% If, in addition, $D$ has 2nd moment with condition number $\kappa$ and $\ell$ has Euclidean norm at most that of an optimal hypothesis with minimum norm, then, 

% \[
% \E_{D} \abs{\iprod{\ell,x} - y} \leq \Paren{1+ O(\kappa^{1/2} \epsilon^{1/2})}\err + O(\kappa^{1/2} \epsilon^{1/2}) \sqrt{ \E_D y^2}\mper
% \]
\[
\cA_{U,\eta,Q} \sststile{4}{w,\tau',X'} \frac{1}{n} \sum_{i = 1}^n \tau_i \leq \frac{1}{n} \sum_{i = 1}^n \tau_i' + 2\kappa^{1/2} \eta^{1/2} \sqrt{\frac{1}{n} \sum_{i = 1}^n y_i^2} + 2 \kappa^{1/2} \eta^{1/2} \cdot \sqrt{ \frac{1}{n} \sum_{i = 1}^n (y_i - \iprod{\ell^{*},x_i})^2}\mper
\]
% \end{lemma}

% As before, we can now take any pseudo-distribution output by the algorithm and obtain guarantees on it:

% \begin{lemma}[Analysis of the Algorithm]
% Let $X$ be a collection of $n$ labeled examples in $\R^d$ such that $D$, the uniform distribution on  $x_1, x_2, \ldots, x_n$ has 2nd moment matrix with all eigenvalues within a factor $\kappa$ of each other. Let $U$ be an $\eta$-corruption of $X$. Let $\tmu$ be any pseudo-distribution of degree $4$ satisfying the set of system of polynomial equations $\cA_{U,\eta,Q}$. 

% Let $\ell^{*} \in \arg \min_{\ell} \frac{1}{n} |y_i - \iprod{\ell,x}|$ and $\min_{\ell} \frac{1}{n} |y_i - \iprod{\ell,x}| = \opt$ where the minimum is over all $\ell$ with bit complexity $B$ and Euclidean norm $Q$. Then, $\hat{\ell} = \pE_{\tmu} \ell$ satisfies:

% \[
% \frac{1}{n} \sum_{i = 1}^n\abs{y_i - \iprod{\hat{\ell},x_i}} \leq \opt(\hat{\cD}) + 2\kappa^{1/2} \eta^{1/2} \sqrt{ \frac{1}{n} \sum_{i =1}^n y_i^2} + 2 \kappa^{1/2}\eta^{1/2} \cdot\sqrt{  \frac{1}{n}\sum_{ i = 1}^n (y_i - \iprod{\ell^{*},x})^2} \mper
% \]

\end{lemma}
% \begin{proof}[Proof Sketch]
% The proof is entirely analogous to the corresponding lemma in the previous subsection where the idea is to observe that any pseudo-distribution that satisfies $\cA_{U,\eta,Q}$ must also satisfy the polynomial inequality in the conclusion of Lemma \ref{lem:robust-certifiability-L1-SoS}. In particular, observe that $\pE_{\tmu} \tau_i' \geq \pE_{\tmu} (y'_i - \iprod{\ell,x'_i})$ and $\pE_{\tmu} \tau_i' \geq \pE_{\tmu} (-y_i' + \iprod{\ell,x_i'})$. Using that $\pE_{\tmu} \ell = \hat{\ell}$, this implies that for every $i$, $\pE_{\tmu} \tau_i' \geq \abs{y_i' - \iprod{\ell,x_i'}}$. With this, we are ready to apply Lemma \ref{lem:robust-certifiability-L1-SoS} and finish the proof as in the proof of Lemma \ref{lem:analysis-full-L2}.
% \end{proof}


%Next, as before, we argue that $\opt(\hat{\cD}) \approx \opt(\cD)$. 
% \begin{lemma}[Empirical Optimum Converges] \label{lem:error-converges-L1}
% Let $\cD$ be a distribution on $\R^d \times \cY$ with covariance $\Sigma$ such that $\cY \subseteq [-M,M]$ for some positive real $M$. Let $\kappa$ be the ratio of the largest to the smallest eigenvalue of $\Sigma$.

% Let $\opt(\cD) = \min_{\ell} \E_{\cD} \abs{y - \iprod{\ell,x}}$ where the minimum is over all $\ell$ of bit complexity at most $B$. Let $\ell^{*}$ be the minimizer.

% Let $X$ be a sample from $\cD$ of size $n \geq n_0$ for $n_0 = O(1/\delta \epsilon^2) \cdot (\|\ell^{*}\|_2^4M^2/\eta)$ and let $\hat{\cD}$ be the uniform distribution on $X$. 
% Let $\opt(\hat{\cD}) = \min_{\ell} \E_{\hat{\cD}} (y - \iprod{\ell,x})^2$ where the minimum is over all $\ell$ of bit complexity at most $B$ and L2 squared norm at most $Q = \|\ell^{*}\|_2^2$.

% Then, with probability at least $1-\delta$ over the draw of $X$,  $\opt(\hat{\cD}) \leq \opt(\cD) + \epsilon$.  
% \end{lemma}

% As before, the final component is the following lemma analogous to Lemma \ref{lem:little-boundedness-fact} above. 

% \begin{lemma} \label{lem:boundedness-fact-L1}
% Let $\cD$ be an arbitrary distribution on $\R^d \times \R$ with covariance matrix of $D$ given by $\Sigma$. Then, there's a distribution that is $\eta$ close to $\cD$ in total variation and satisfies: $\|x\|_2^2 \leq \|\Sigma\|/\eta$ where $\|\Sigma\|$ is the spectral norm of $\Sigma$.
% \end{lemma}

% We can now complete the proof of Theorem \ref{thm:L1-regression-analysis}. 

% \begin{proof}[Proof Sketch of Theorem \ref{thm:L1-regression-analysis}]
% As before, we can assume that $D$ has every point in its support of L2 squared norm at most $\|\Sigma\|/\eta$ via Lemma \ref{lem:boundedness-fact-L1}. Then, by taking $O(1/\epsilon^2)(d \log{(d)} \|\Sigma\|/\eta)$ samples, the covariance converges to within $1 \pm \epsilon$ via the Matrix Chernoff inequality. 

% Next, for an appropriately large enough sample size given by Lemma \ref{lem:error-converges-L1}, $\opt(\hat{\cD}) \leq \opt(\cD) + \epsilon$. This establishes that the empirical error of $\hat{\ell} = \pE_{\tmu} \ell$


% \end{proof}
% \paragraph{Proof of Lemma \ref{lem:robust-certifiability-L1-SoS}}


\begin{proof}[Proof of Lemma \ref{lem:robust-certifiability-L1-SoS}]
%The proof is analogous to that of Lemma \ref{lem:identifiability-least-squares-linear-sos}. 

For every $i \in [n]$, define $w'_i = w_i$ iff $(x_i,y_i)$ is uncorrupted in $U$.
Then, observe that $\sum_i w_i' = s$ for $s \geq (1-2\epsilon)n$ and that $\sststile{w}{2} \Set{w_i^2 - w_i = 0}$. 

Then, 
\[
\sststile{w'}{2} \Set{ \frac{1}{n} \sum_i (1-w'_i)^2 \leq 2 \epsilon}\mper
\]

Thus, we have:% for $k' = k/2$,
\[
\sststile{w,\ell, \tau'}{4} \frac{1}{n} \sum_{i = 1}^n \tau_i = \frac{1}{n} \sum_{i = 1}^n w'_i \tau_i + \sum_{i = 1}^n (1-w'_i) \cdot \tau_i
\]

Further, it's easy to verify by direct expansion that:
\[
\Set{w_i(x_i -x_i') = 0, w_i (y_i - y_i') = 0 \mid \forall i} \sststile{w'}{4} \Set{ w'_i (\tau_i -\tau'_i) = 0 \mid \forall i}
\]

As a result, we have:
\[
\sststile{w,\ell, \tau'}{4} \frac{1}{n} \sum_{i = 1}^n \tau_i = \frac{1}{n} \sum_{i = 1}^n w'_i \tau_i' + \sum_{i = 1}^n (1-w'_i) \cdot \tau_i
\]

For brevity, let's write $\err_{D}(\ell) = \sum_{i =1}^n \sum_{i = 1}^n \tau_i$ and $\err_{D'} (\ell) = \sum_{i =1}^n \sum_{i = 1}^n \tau_i'$. Then, we have:


Using the sum-of-squares vesion of the Cauchy-Shwarz inequality, we have:

\begin{align}
\sststile{w,\ell, \tau'}{4} \Paren{\err_D(\ell) - \err_{D'}(\ell)}^{2} &= \Paren{\frac{1}{n} \sum_{i = 1}^n (1-w'_i) \cdot \tau_i}^2\notag\\
%&= \Paren{\frac{1}{n} \sum_{i = 1}^n (1-w'_i)^{k'-1} \cdot (y_i -\iprod{\ell,x_i})^2}^{k'}\notag\\
&\leq \Paren{\frac{1}{n} \sum_{i = 1}^n (1-w'_i)}^2 \frac{1}{n} \sum_{i =1}^n (y_i - \iprod{\ell,x_i})^2 \notag\\
&\leq \epsilon \frac{1}{n} \sum_{i =1}^n (y_i - \iprod{\ell,x_i})^2  \label{eq:CS-bound-L1} \mper
\end{align}

Next, we have:
\[
\sststile{\ell}{4} \Set{ \frac{1}{n} \sum_{i =1}^n (y_i - \iprod{\ell,x_i})^2 \leq 2\frac{1}{n} \sum_{i =1}^n (y_i - \iprod{\ell^{*},x_i})^2 + 2\frac{1}{n} \sum_{i =1}^n \iprod{\ell-\ell^{*},x_i})^2} \mper
\]

Further, we also have:
\[
\sststile{\ell}{4} \Set{ \frac{1}{n} \sum_{i =1}^n \iprod{\ell-\ell^{*},x_i})^2 \leq 2\frac{1}{n} \sum_{i =1}^n \iprod{\ell,x_i})^2 + 2 \frac{1}{n} \sum_{i =1}^n \iprod{\ell^{*},x_i})^2 }
\]

Using that for any PSD matrix $A$, we have the SoS inequality $\|x\|_2^2 \|A\|_{min} \leq x^{\top} A x \leq \|x\|_2^2 \|A\|_{max}$ where $\|A\|_{max}$ and $\|A\|_{min}$ are the largest and smallest singular values of $A$, respectively, we have:

\[
\sststile{\ell}{4} \Set{ \frac{1}{n} \sum_{i =1}^n \iprod{\ell,x_i})^2 \leq \kappa \frac{1}{n} \sum_{i =1}^n \iprod{\ell^{*},x_i})^2  }
\]

Finally, we also have:
\[
\sststile{\ell}{4} \Set{\frac{1}{n} \sum_{i =1}^n \iprod{\ell^{*},x_i})^2 \leq \frac{1}{n} \sum_{i =1}^n (y_i - \iprod{\ell^{*},x_i})^2  + 2\frac{1}{n} \sum_{i =1}^n y_i^2 }
\]

Combining the above inequalities with \eqref{eq:CS-bound-L1} yields the lemma.

\end{proof}







