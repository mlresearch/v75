\section{Missing Proofs from Section~\ref{sec:partial-validity}}
\label{sec:partial-proofs}

\subsection{Proof of Lemma~\ref{lem:partial-validity-inner-loop}}
To bound the number of iterations, we will show that if no distribution is output, $|D|$ shrinks by a factor $1-\frac  {\eps_2}  {5}$. 
As we start with at most $|Q|$ candidate distributions, this  implies the required bound.

We note that we have a multiplicative term $\log \left(\frac{M \log |Q|}{\eps_1 \eps_2}\right)$ in the expression for $n_2$.
This corresponds to certain estimates being accurate for the first $\poly(M, \log |Q|, \eps_1^{-1}, \eps_2^{-1})$ times they are required by a union bound argument.
As this proof will justify, each line in the algorithm is run at most $\frac{M \log |Q|}{\eps_1\eps_2}$ times.
Thus, for ease of exposition, we simply will state that estimates are accurate for every time the line is run.

We thus need to count how many candidate distributions in $D$ are eliminated in every round given that the empirical invalidity of $\mu'_D$ is at least $\alpha + \frac {4 \eps_2}{5}$, i.e.
$$\frac 1 N \sum_{i=1}^N \Inv(x_i) \Pr_{q \sim \Unif(D)}[q(x_i) {\eps_1} < 3 \mu_D(x_i)  {M} ] > \alpha + \frac {4 \eps_2}{5}.$$
This implies that the true invalidity of $\mu'_D$ is at least $\alpha + \frac {3 \eps_2}{5}$:
since $n_2 = \Omega\left(\frac{1}{\eps_2^2} \cdot \log\left(\frac{M \log |Q|}{\eps_1\eps_2}\right)\right)$, we have that $\overline \Inv(\mu'_D) = \Inv(\mu'_D) \pm \frac {\eps_2}{5}$ each time this line is run, with probability $29/30$.

Similarly, for every $q$ we have that the estimator $\frac{1}{n_2}  \sum_{i=1}^{n_2} \Inv(x_i) \frac {q(x_i)} {\mu_D(x_i)} \Ind[q(x_i) {\eps_1} < 3 \mu_D(x_i) {M} ]$ is an accurate estimator for the validity of $q'$ which is the distribution that generates a sample $x$ from $q$ and returns $x$ if $q(x)  {\eps_1} \le 3 \mu_D(x)  {M}$ and $x^*$ otherwise. This is because, since $\Inv(x^*)=0$, we have
$$\begin{aligned}
\Exp_{x \sim \mu_D} \left[ \Inv(x) \frac {q(x)} {\mu_D(x)} \Ind[q(x) {\eps_1} < 3 \mu_D(x) {M} ] \right] &= \Exp_{x \sim q} \left[ \Inv(x) \Ind[q(x) {\eps_1} < 3 \mu_D(x) {M} ] \right]\\
&= \Exp_{x \sim q'} \left[ \Inv(x) \right] = \Inv(q').
\end{aligned}$$

Note that our estimate $\overline \Inv(q')$ is the empirical value
$$\frac{1}{n_2} \sum_{i=1}^{n_2} \Inv(x_i) \frac {q(x_i)} {\mu_D(x_i)} \Ind[q(x_i) {\eps_1} < 3 \mu_D(x_i) {M} ],$$
where
$$\frac{q(x_i)}{\mu_D(x_i)} \Ind[q(x_i) {\eps_1} < 3 \mu_D(x_i) {M} ] \leq \frac{3M}{\eps_1}.$$
Since we are estimating the expectation of a function upper bounded by $O(M/\eps_1)$ and there are at most $|Q|$ distributions $q'$ at each iterations, $n_2 = \Omega\left(\frac {M^2} {\eps_1^2 \eps_2^2} \log |Q| \log \left(\frac{M \log |Q|}{\eps_1 \eps_2}\right)\right)$ samples are sufficient to have that the empirical estimator $\overline \Inv(q') = \Inv(q') \pm \frac {\eps_2}{5}$ for all distributions $q'$ considered and all times this line is run, with probability $29/30$. 
Thus, it is sufficient to count how many $q \in D$ exist with $\Inv(q') > \alpha + \frac {3 \eps_2}{5}$.

To do this, we notice that $\Exp_{q \in \Unif(D)} [\Inv(q')] = \Inv(\mu'_D) > \alpha + \frac {3 \eps_2}{5}$. Then, as $\Inv(q') \le 1$, we have that $\Pr_{q\sim \Unif(D)}[\Inv(q') > \alpha + \frac {2 \eps_2}{5}] \ge \frac {\eps_2}{5}$. This yields the required shrinkage of the set $D$.


\subsection{Proof of Lemma~\ref{lem:partial-validity-invalid}}
The estimator $\frac{1}{n_2} \sum_{i=1}^{n_2} \Inv(x_i) \Pr_{q \sim \Unif(D)}[q(x_i) {\eps_1} < 2 \mu_D(x_i) {M} ]$ estimates the empirical fraction of samples that are invalid for distribution $\mu'_D$. 
Since $n_2 = \Omega\left( \frac {1} {\eps_2^2} \log \left(\frac{M \log |Q|}{\eps_1 \eps_2}\right) \right)$, and by Lemma~\ref{lem:partial-validity-inner-loop} each line is run at most $O\left(\frac{M \log |Q|}{\eps_1\eps_2}\right)$ times, the empirical estimate of $\overline \Inv(\mu'_D) = \Inv(\mu'_D) \pm \frac {\eps_2} 5$ for all iterations, with probability at least $14/15$.
The statement holds as $\mu'_D$ is only returned if the estimate for the invalidity of $\mu'_D$ is at most $\alpha + \frac {4 \eps_2}{5}$.


\subsection{Proof of Lemma~\ref{lem:partial-validity-loss}}
For any $q \in D$ denote by $q'$ the distribution that generates a sample $x$ from $q$ and returns $x$ if $q(x)  {\eps_1} \le 3 \mu_D(x)  {M}$ and $x^*$ otherwise.
Notice that $\mu'_D(x) = \Exp_{q \sim \Unif(D)} [ q'(x) ]$.
We have that
$$\begin{aligned}
\Loss(\mu'_D) &= \Exp_{x \sim p} [ L(\mu'_D(x)) ] \le \Exp_{x \sim p} [ \Exp_{q \sim \Unif(D)} [ L(q'(x)) ] ] \\
&\le \Exp_{x \sim p \atop q \sim \Unif(D)} \left[ L(q(x)) + M \cdot \Ind[q(x)  {\eps_1} > 3 \mu_D(x)  {M}]  \right]\\
&\le \sup_{q \in D} \Loss(q) + M \cdot \Pr_{x \sim p \atop q \sim \Unif(D)}[q(x)  {\eps_1} > 3 \mu_D(x)  {M}]\\
\end{aligned}$$
The equality is the definition of $\Loss$, the first inequality uses convexity of $L$ and Jensen's inequality, and the second inequality uses the fact that $L(\cdot) \leq M$.

However, for any given $x$, we have that $\Exp_{q \sim \Unif(D)}[ q(x) ] =  \mu_D(x)$ and thus by Markov's inequality we obtain that for all $x$ $$\Pr_{q \sim \Unif(D)}[q(x)  {\eps_1} > 3 \mu_D(x)  {M}] \le \frac {\eps_1} {3 M}.$$
This implies that $M \cdot \Pr_{x \sim p \atop q \sim \Unif(D)}[q(x)  {\eps_1} > 3 \mu_D(x)  {M}]$ is at most $\frac {\eps_1} {3}$.
To complete the proof we note that $\sup_{q \in D} \Loss(q)$ is at most $\ell + \frac {\eps_1} {3}$: 
since we are estimating the mean of $L(\cdot)$ which is bounded by $M$, there are $|Q|$ distributions $q$ which are considered, and $n_1 = \Omega\left(\frac{M^2}{\eps_1^2}\log |Q|\right)$, the statement holds for all $q$ simultaneously with probability at least $14/15$.
