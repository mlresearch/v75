% !TeX root = main.tex
\section{Improper Learning}
\label{sec:improper}

In this section, we show that if we are allowed to output a distribution that is not in the original family $Q$, we can efficiently identify a distribution that achieves close to optimal loss and almost-full validity using only few samples and invalidity queries to $p$.

\subsection{Algorithm}

We provide an algorithm, Algorithm \ref{alg:full-validity}, that can solve the task computationally efficiently assuming access to an optimization oracle $\text{Oracle}(X_P,X_N)$. $\text{Oracle}(X_P,X_N)$ takes as input sets $X_P$ and $X_N$ of positive and negative (invalid) points and outputs a distribution $q$ from the family of distributions $Q$ that minimizes the empirical loss with respect to $X_P$ such that $\Supp(q) \cap X_N = \emptyset$, i.e. no negative point in $X_N$ is in the support of $q$.

\begin{algorithm}[ht]
   \caption{Improperly learning a distribution}
   \label{alg:full-validity}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} Distribution family $Q$, sample and invalidity access to $p$, and parameters $\ve_1, \ve_2 > 0$.
   \STATE Draw a set $X_P$ of $P$ samples from $p$.
   \STATE Set $X_N \leftarrow \emptyset$
   \FOR{$i=1,...,R$}
     \STATE Let $q^i \leftarrow \text{Oracle}(X_P,X_N)$.
     \STATE Generate $T$ samples from $q^i$ and query the invalidity of each of them.
     \STATE Let $x^-_1,...,x^-_k$ be the invalid samples.
     \IF{there are no invalid samples, i.e. $k=0$}
        \RETURN $q^i$
     \ELSE
       \STATE Set $X_N \leftarrow X_N \cup \{x^-_1,...,x^-_k\}$
     \ENDIF
   \ENDFOR
   \STATE Sample $i \sim \Unif(\{1,...,R\})$
   \STATE Let $A \leftarrow {\{x: \exists j \neq i \text{ with } q^j_x \in [\frac12 q^i_x,2 q^i_x]\}}$
   \RETURN the distribution that samples $x \sim q^i$ and outputs $x$ if $x \in A$ and any valid point $x^*$ o/w
\end{algorithmic}
\end{algorithm}

The algorithm repeatedly finds the distribution with minimum loss that doesn't contain any of the invalid points seen so far and tests whether it achieves almost full-validity. If it does, then it outputs that distribution. Otherwise it tries again using the new set of invalid points. However, this process could repeat for a very long time without finding a distribution. To avoid this, after running for a few rounds, if it has failed to output a distribution, the algorithm is able to generate an improper distribution that provides the required guarantee to solve the task. This meta-distribution is obtained by randomly picking one of the candidate distributions examined so far and filtering out points that no other distributions agrees on.

\subsection{Analysis}

We show that this Algorithm~\ref{alg:full-validity} outputs with high probability a distribution $\mu$ that has $\Loss(\mu) \le \Loss(q^*) + \eps_1$ and $\Inv(\mu) \le \eps_2$. 

\begin{theorem}\label{thm:full-validity}
  Let $2^{-m}$ be a bound on the minimum non-zero probability for all $q \in Q$. 
  Setting
  \begin{equation}
    P=\Theta\left(\frac{M^2}{\ve_1^2} \log |Q| \right), \quad R = 32 \frac { M m } {\eps_1}, \quad  T = \Theta\left(\frac{R}{\ve_2} \log |Q| \right)
    \end{equation}
  gives that Algorithm~\ref{alg:full-validity} outputs w.p. $3/4$ a distribution with $\Loss(\mu) \le \Loss(q^*) + \eps_1$ and $\Inv(\mu) \le \eps_2$ using $\Theta\left(\frac{M^2}{\ve_1^2} \log |Q| \right)$ samples from $p$ and $\Theta\left(\frac{M m}{ \ve_1 \ve_2} \log |Q|\right)$ invalidity queries.
\end{theorem}


Theorem~\ref{thm:full-validity} requires that the minimum non-zero probability value is bounded by $2^{-m}$. We note that if this value is small or unbounded in the class $Q$, one can instead consider only values $q_x$ in the effective support with $L(q_x) \le M - \eps_1$. This is because zero-ing out all those probabilities affects the loss only by $\eps_1$. Therefore, Theorem~\ref{thm:full-validity} still holds if we set $m = -\log L^{-1}(M-\eps_1)$.

We prove Theorem~\ref{thm:full-validity} by showing two lemmas, Lemma~\ref{lem:full-validity-inv} and Lemma~\ref{lem:full-validity-loss}.

\begin{lemma}\label{lem:full-validity-inv}
The returned distribution $\mu$ by Algorithm~\ref{alg:full-validity} satisfies $\Inv(\mu) \le \eps_2$ w.p. $7/8$.
\end{lemma}

\begin{proof}
  
  Consider $q^i$ for some $i$ and any distribution $q \in Q$. If $q^i( \Supp(q) \cap \text{Invalid} ) \ge \frac { \eps_2 } { 2 R }$, then with probability at least $\frac { \eps_2 } { 2 R }$ a sample generated from $q^i$ lies in $\Supp(q) \cap \text{Invalid}$. Thus, with $T = \Theta( \frac  { R } { \eps_2 } \log |Q| )$ samples at least one lies in $\Supp(q) \cap \text{Invalid}$ w.p. $1 - \frac 1 {8 |Q| R}$. By a union bound for all $i$ and $q \in Q$, we get that with probability $7/8$ for all $q_i$ and all distributions $q \in Q$, if $q^i( \Supp(q) \cap \text{Invalid} ) \ge \frac { \eps_2 } { 2 R }$ then at least one of the $T$ samples drawn from $q^i$ lies in $\Supp(q) \cap \text{Invalid}$.
  We therefore assume that this holds.
  
  Then, if the returned distribution $\mu = q^i$ for some $i$, we get $\Inv(q^i) < \frac { \eps_2 } { 2 R } \le \eps_2 $ as required.
  To complete the proof we show the required property when returned distribution $\mu$ is the improper meta-distribution.
  
  We have that for all $j > i$, $q^i( \Supp(q^j) \cap \text{Invalid} ) < \frac { \eps_2 } { 2 R }$ since any $q \in Q$ with $q^i( \Supp(q) \cap \text{Invalid} ) \ge \frac { \eps_2 } { 2 R }$ will not be considered after round $i$.
  
  In addition, for all $j > i$, we also have that 
  \begin{align*}
    q^j(\{x: x \in \text{Invalid} \wedge q^j_x \in [\frac12 q^i_x,2 q^i_x]\}) &\le 2 q^i(\{x: x \in \text{Invalid} \wedge q^i_x \in [\frac12 q^j_x,2 q^j_x]\}) \\ &\le 2 q^i( \Supp(q^j) \cap \text{Invalid} ) < \frac { \eps_2 } { R }
  \end{align*}
  Therefore, we have that 
  \begin{align*}
    \Inv(\mu) 
&= \Exp_{x\sim\mu}\left[ \Inv(x) \right] \\
&= \Exp_{x\sim q^i}\left[ \Inv(x) \cdot \Ind\left[\exists j \neq i: q^j_x \in [\frac12 q^i_x,2 q^i_x] \right] \right] \\
&\le  \sum_{j=1}^R \Exp_{x\sim q^i}\left[ \Inv(x) \cdot \Ind\left[q^j_x \in [\frac12 q^i_x,2 q^i_x] \right] \right] \le  \sum_{j=1}^R \frac { \eps_2 } { R } <  \eps_2\\
  \end{align*}
\end{proof}


\begin{lemma}\label{lem:full-validity-loss}
The returned distribution $\mu$ by Algorithm~\ref{alg:full-validity} satisfies $\Loss(\mu) \le \Loss(q^*) + \eps_1$ w.p. $7/8$.
\end{lemma}
\begin{proof}
  Since we draw $P=\Theta\left(\frac{M^2}{\ve_1^2} \log |Q| \right)$ samples from $p$, we have that the empirical loss $\overline \Loss(q) \in \Loss(q) \pm \frac {\eps_1} 4$ for all $q \in Q$ with probability $1-1/16$. We thus assume from here on that this is true.
  
  In that case, must be that $\overline \Loss(q^i) \le \overline \Loss(q^*)$. This is because the algorithm terminates if $q^i = q^*$ since $q^*$ generates no invalid samples and no $q^i$ with $\overline \Loss(q^i) > \overline \Loss(q^*)$ will be considered before examining $q^*$.
  
  This implies that at any point, we have that $\Loss(q^i) \le \overline \Loss(q^i) + \frac {\eps_1} 4 \le \overline \Loss(q^*) + \frac {\eps_1} 4 \le \Loss(q^*) + \frac {\eps_1} 2$. 
  
  Therefore, in the case that the distribution that is output is $\mu = q^i$ it will satisfy the given condition.
  To complete the proof we show the required property when returned distribution $\mu$ is the improper meta-distribution.
  
  In that case, we have that:
    \begin{align*}
      \Loss(\mu) 
  &\le \Exp_{x\sim p}\left[ L\left(q_x^i \Ind\left[\exists j \neq i: q^j_x \in [\frac12 q^i_x,2 q^i_x] \right] \right) \right] \\
  &\le \Loss(q^i) + M \cdot \Pr_{x\sim p}\left[ \nexists j \neq i: q^j_x \in [\frac12 q^i_x,2 q^i_x] \right] \\
  &\le \Loss(q^*) + \frac {\eps_1} 2 + M \cdot \Pr_{x\sim p}\left[ \nexists j \neq i: q^j_x \in [\frac12 q^i_x,2 q^i_x] \right]
    \end{align*}
However, since a random index $i \sim \Unif(\{1,...,R\})$ is chosen, we have that in expectation over this random choice
    \begin{align*}
\Exp_i\left[ \Pr_{x\sim p}\left[ \nexists j \neq i: q^j_x \in [\frac12 q^i_x,2 q^i_x] \right] \right] 
&\le \frac 1 R \sum_{i=1}^R \Pr_{x\sim p}\left[ \nexists j \neq i: q^j_x \in [\frac12 q^i_x,2 q^i_x] \right] \\
&\le \frac 1 R \Exp_{x\sim p} \left[ \sum_{i=1}^R \Ind\left[ \nexists j \neq i: q^j_x \in [\frac12 q^i_x,2 q^i_x] \right] \right] 
\le \frac m R
   \end{align*}
  where the last inequality follows since $\sum_{i=1}^R \Ind\left[ \nexists j \neq i: q^j_x \in [\frac12 q^i_x,2 q^i_x] \right] \le m $ as there can be at most $m$ values $q^i_x \in [2^{-m},1]$ such that for all $j \neq i$, $q^j_x \notin [\frac12 q^i_x,2 q^i_x]$.

By Markov's inequality, we have that with probability $1-1/16$, a random $i$ will have $$\Pr_{x\sim p}\left[ \nexists j \neq i: q^j_x \in [\frac12 q^i_x,2 q^i_x] \right] \le 16 \frac m R.$$

Therefore, by the choice of $R$ we get that $\Loss(\mu) \le \Loss(q^*) + {\eps_1}$ with probability $7/8$.

\end{proof}