

%\appendix

\section{Proofs for Infinite Families of Distributions}
\label{sec:vc-classes-proofs}

The proofs of the results on handling infinite $Q$ sets follow analogously 
to the original proofs for finite $|Q|$, but with a few modifications to make 
use of results from the learning theory literature on infinite function classes.
For completeness, we include the full details of these proofs here.

\subsection{Proof of Theorem~\ref{thm:vc-full-validity}}

We begin with the proof of Theorem~\ref{thm:vc-full-validity}.
As above, we consider two key lemmas.

\begin{lemma}
\label{lem:vc-full-validity-inv}
For $P$, $R$, and $T$ as in Theorem~\ref{thm:vc-full-validity}, 
the distribution returns by Algorithm~\ref{alg:full-validity} satisfies $\Inv(\hat{q}) \leq \eps_{2}$ with probability at least $7/8$.
\end{lemma}
\begin{proof}
Following the original proof above, 
let $\text{Invalid} = \{x : \Inv(x) = 1 \}$ be the set of invalid points.
Consider $q^i$ for some $i$ and any distribution $q \in Q$. 
If $q^i( \Supp(q) \cap \text{Invalid} ) \ge \frac { \eps_2 } { R }$, 
then with probability at least $\frac { \eps_2 } { R }$ a sample generated from $q^i$ lies in $\Supp(q) \cap \text{Invalid}$. 
Furthermore, we note that the VC dimension of the collection of sets $\{ \Supp(q) \cap \text{Invalid} : q \in Q \}$ is at most $\vc$.
Thus, with $T = \Theta( \frac  { R \vc } { \eps_2 } \log \frac{1}{\eps_2} )$ samples from $q^i$, 
the classic sample complexity result from PAC learning \cite{VapnikC74,BlumerEHW89} implies % number of random samples to get an eps-net.
that with probability at least $1 - \frac{1}{8 R}$, 
every $q \in Q$ with $q^i( \Supp(q) \cap \text{Invalid} ) \geq \frac{\eps_{2}}{R}$ 
has at least one of the $T$ samples in $\Supp(q) \cap \text{Invalid}$. 
By a union bound, this holds for all $i$ in the algorithm.  Suppose this event holds.

In particular, this implies that if the algorithm returns in Step 9, so that the returned distribution $\hat{q} = q^i$ for some $i$, 
then $\Inv(q^i) = q^i( \Supp(q^i) \cap \text{Invalid} ) < \frac { \eps_2 } { R } \le \eps_2 $ as required.
Furthermore, if the algorithm returns in Step 16 instead, then the above event implies that for every $i,j$ with $i < j$, 
$q^i( \Supp(q^i) \cap \text{Invalid} ) < \frac { \eps_2 } { R }$.
Therefore, if we fix the value of $i$ selected in Step 14, we have that 
 \begin{align*}
    \Inv(\hat q) 
&= \Exp_{x\sim\hat q}\left[ \Inv(x) \right] \\
&= \Exp_{x\sim q^i}\left[ \Inv(x) \cdot \Ind\left[\exists j > i: x \in \Supp(q^j) \right]\right] \\
&\le  \sum_{j=i+1}^R \Exp_{x\sim q^i}\left[ \Inv(x) \cdot \Ind\left[ x \in \Supp(q^j) \right] \right] \\  
&= \sum_{j=i+1}^R q^i( \Supp(q^j) \cap \text{Invalid} ) \le  \sum_{j=i+1}^R \frac { \eps_2 } { R } <  \eps_2.
  \end{align*}
\end{proof}

\begin{lemma}
\label{lem:vc-full-validity-loss}
For $P$, $R$, and $T$ as in Theorem~\ref{thm:vc-full-validity}, 
the distribution $\hat{q}$ returned by Algorithm~\ref{alg:full-validity} satisfies $\Loss(\hat q) \le \Loss(q^*) + \eps_1$ with probability at least $7/8$.
\end{lemma}
\begin{proof}
Combining Corollary 2 of \cite{Haussler92} with Theorem 1 of \cite{MendelsonV03}, 
we conclude that for $P=\Theta\left(\frac{\fat(c\ve_{1}/M) M^2}{\ve_1^2} \log \frac{M}{\ve_{1}} \right)$ samples from $p$, 
we have that the empirical loss $\overline \Loss(q) \in \Loss(q) \pm \frac {\eps_1} 4$ simultaneously for all $q \in Q$ with probability at least $15/16$. 
From here on, let us suppose this event occurs.

In that case, it must be that $\overline \Loss(q^i) \le \overline \Loss(q^*)$. 
This is because the algorithm terminates if ever $q^i = q^*$ since $q^*$ generates no invalid samples,
and yet no $q^i$ with $\overline \Loss(q^i) > \overline \Loss(q^*)$ will be considered before examining $q^*$.

This implies that at any point, we have that $\Loss(q^i) \le \overline \Loss(q^i) + \frac {\eps_1} 4 \le \overline \Loss(q^*) + \frac {\eps_1} 4 \le \Loss(q^*) + \frac {\eps_1} 2$. 

Therefore, in the case that the distribution that is output is $\hat q = q^i$ it will satisfy the given condition.
To complete the proof we show the required property when returned distribution $\hat q$ is the improper meta-distribution.
  
In that case, we have that:
    \begin{align*}
      \Loss(\hat q) 
  &\le \Exp_{x\sim p}\left[ L\left(q_x^i \cdot \Ind\left[\exists j > i: x \in \Supp(q^j) \right] \right) \right] \\
  &\le \Loss(q^i) + M \cdot \Pr_{x\sim p}\left[ x \in \Supp(q^i) \wedge \forall j > i: x \notin \Supp(q^j) \right] \\
  &\le \Loss(q^*) + \frac {\eps_1} 2 + M \cdot \Pr_{x\sim p}\left[ x \in \Supp(q^i) \wedge \forall j > i: x \notin \Supp(q^j) \right]
    \end{align*}
However, since a random index $i \sim \Unif(\{1,...,R\})$ is chosen, we have that in expectation over this random choice
    \begin{align*}
\Exp_i&\left[ \Pr_{x\sim p}\left[ x \in \Supp(q^i) \wedge \forall j > i: x \notin \Supp(q^j) \right] \right] \\
&= \frac 1 R \sum_{i=1}^R \Pr_{x\sim p}\left[ x \in \Supp(q^i) \wedge \forall j > i: x \notin \Supp(q^j) \right] \\
&= \frac 1 R \Exp_{x\sim p} \left[ \sum_{i=1}^R \Ind\left[ x \in \Supp(q^i) \wedge \forall j > i: x \notin \Supp(q^j) \right] \right] 
\le \frac 1 R
   \end{align*}
  where the last inequality follows since $\sum_{i=1}^R \Ind\left[ x \in \Supp(q^i) \wedge \forall j > i: x \notin \Supp(q^j) \right] \le 1 $ as only the largest $i$ with $x \in \Supp(q^i)$ has that for all $j > i$, $x \notin \Supp(q^j)$.

By Markov's inequality, we have that with probability at least $15/16$, 
a random $i$ will have $$\Pr_{x\sim p}\left[ x \in \Supp(q^i) \wedge \forall j > i: x \notin \Supp(q^j) \right] \le \frac {16} R.$$

Therefore, the choice of $R = 32 \frac M {\ve_1} = \Theta \left( \frac { M } {\eps_1} \right)$ guarantees that $\Loss(\hat q) \le \Loss(q^*) + {\eps_1}$.
The overall failure probability is at most $1/16+ 1/16 = 1/8$.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:vc-full-validity}]
Theorem~\ref{thm:vc-full-validity} follows immediately from the above two lemmas by a union bound.
\end{proof}


\subsection{Proof of Theorem~\ref{thm:vc-partial-validity}}

Next, the proof of Theorem~\ref{thm:vc-partial-validity} follows similarly to the original proof 
of Theorem~\ref{thm:partial-validity}, with a few important adjustments.
As in the statement of the theorem, we consider running Algorithm~\ref{alg:partial-validity}$^{\prime}$ 
with parameters $\eps_{1}$, $\eps_{2}$, and $\alpha+\eps_{2}$.
As in the proof of Theorem~\ref{thm:partial-validity}, we proceed by establishing three key lemmas.
As much of this proof essentially follows by \emph{plugging in} the altered set $D$ (from the new Step 4) 
to the arguments of the original proofs above, in the proofs of these lemmas we only highlight the reasons 
for which this substitution remains valid and yields the stated result.

\begin{lemma}
\label{lem:vc-partial-validity-inner-loop}
With probability at least $14/15$, the loop at Line~\ref{ln:partial-validity-inner-loop} of Algorithm~\ref{alg:partial-validity}$^{\prime}$ 
requires at most $O\left(\frac{{\rm fat}_{c\eps_2}(Q)}{\eps_2} \log\!\left( \frac{1}{\eps_{2}} \right) \right)$ iterations for each $\ell$.
\end{lemma}
\begin{proof}
We invoke the original argument from the proof of Lemma~\ref{lem:partial-validity-inner-loop} verbatim, 
except that rather than bounding the initial size $|D|$ in Step 4 by $|Q|$, 
we use the fact that Step 4 in Algorithm~\ref{alg:partial-validity}$^{\prime}$ initializes $|D|$ 
to the minimal size of an $\eps_{2}$-cover of $\{q \in Q | \overline\Loss(q) \leq \ell \}$, 
which is at most the size of a minimal $\eps_{2}$-cover of $Q$ (under the $L_{1}(\mu_{0})$ pseudo-metric).
Thus, Theorem 1 of \cite{MendelsonV03} implies that, for every $\ell$, this initial set $D$ satisfies 
\begin{equation}
\label{eqn:mv-cover-bound}
\log(|D|) = O\!\left( {\rm fat}_{c\eps_{2}}(Q) \log\!\left( \frac{1}{\eps_{2}} \right) \right).
\end{equation}
The lemma then follows from the same argument as in the proof of Lemma~\ref{lem:partial-validity-inner-loop}.
\end{proof}

\begin{lemma}
\label{lem:vc-partial-validity-invalid}
With probability at least $14/15$, if at any step a distribution $\mu'_D$ is output, $\Inv(\mu'_D) \le \alpha + 2\eps_2$.
\end{lemma}
\begin{proof}
The argument remains identical to the proof of Lemma~\ref{lem:partial-validity-invalid}, 
except again substituting for $\log|Q|$ the quantity on the right hand side of \eqref{eqn:mv-cover-bound}, 
and substituting $\alpha+\eps_{2}$ for $\alpha$.
\end{proof}

\begin{lemma}
\label{lem:vc-partial-validity-loss}
With probability at least $14/15$, if at any step a distribution $\mu'_D$ is output, $\Loss(\mu'_D) \le \ell + 2\eps_1/3$, where $\ell$ is the step at which the distribution was output.
\end{lemma}
\begin{proof}
Combining Corollary 2 of \cite{Haussler92} with Theorem 1 of \cite{MendelsonV03} implies that 
the choice $n_{1} = \Theta\left(\frac{\fat(c\ve_{1}/M) M^{2}}{\ve_1^2} \log \!\left(\frac{M}{\ve_{1}}\right)  \right)$ 
suffices to guarantee every $q \in Q$ has $\overline\Loss(q)$ within $\pm \eps_{1}/3$ of $\Loss(q)$.
Substituting this argument for the final step in the proof of Lemma~\ref{lem:partial-validity-loss}, 
and leaving the rest of that proof intact, this result follows.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:vc-partial-validity}]
The proof of Theorem~\ref{thm:vc-partial-validity} concludes by observing that, upon reaching $\ell$ within $\eps_{1}/3$ of $\Loss(q^*)$ (where $q^{*}$ is the optimal distribution), 
the closest (in $L_{1}(\mu_{0})$) element $q$ of the corresponding $D$ set will have $\Inv(q) \leq \Inv(q^{*})+\eps_{2} \leq \alpha+\eps_{2}$, and (by definition of $D$) $\Loss(q) \leq \Loss(q^{*})+\eps_{1}/3$.
Thus, this $q$ will never be eliminated (assuming all estimates involving its loss and validity are accurate, 
which happens with probability at least $19/20$). % and that the loop in line~\ref{ln:partial-validity-outer-loop} steps by increments of $\eps_1/3$. 
Combining this with Lemma~\ref{lem:vc-partial-validity-loss}, if we output $\hat q$, then $\Loss(\hat q) \leq \Loss(q^*) + \eps_1$.
\end{proof}
