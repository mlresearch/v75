
@article{ValkoFiniteTimeAnalysisKernelised2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1309.6869},
  primaryClass = {cs, stat},
  title = {Finite-{{Time Analysis}} of {{Kernelised Contextual Bandits}}},
  abstract = {We tackle the problem of online reward maximisation over a large finite set of actions described by their contexts. We focus on the case when the number of actions is too big to sample all of them even once. However we assume that we have access to the similarities between actions' contexts and that the expected reward is an arbitrary linear function of the contexts' images in the related reproducing kernel Hilbert space (RKHS). We propose KernelUCB, a kernelised UCB algorithm, and give a cumulative regret bound through a frequentist analysis. For contextual bandits, the related algorithm GP-UCB turns out to be a special case of our algorithm, and our finite-time analysis improves the regret bound of GP-UCB for the agnostic case, both in the terms of the kernel-dependent quantity and the RKHS norm of the reward function. Moreover, for the linear kernel, our regret bound matches the lower bound for contextual linear bandits.},
  journal = {arXiv:1309.6869 [cs, stat]},
  author = {Valko, Michal and Korda, Nathaniel and Munos, Remi and Flaounas, Ilias and Cristianini, Nelo},
  month = sep,
  year = {2013},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {/home/johannes/Documents/PhD/zotero/storage/4U2HG8FF/Valko et al. - 2013 - Finite-Time Analysis of Kernelised Contextual Band.pdf;/home/johannes/Documents/PhD/zotero/storage/3K3AMQC6/1309.html}
}

@article{ThompsonLikelihoodthatOne1933,
  title = {On the {{Likelihood}} That {{One Unknown Probability Exceeds Another}} in {{View}} of the {{Evidence}} of {{Two Samples}}},
  volume = {25},
  number = {3/4},
  journal = {Biometrika},
  author = {Thompson, William R.},
  year = {1933},
  pages = {285--294},
  file = {/home/johannes/Documents/PhD/zotero/storage/Q862JPFZ/2332286.pdf}
}

@inproceedings{AgrawalThompsonSamplingContextual2013,
  title = {Thompson {{Sampling}} for {{Contextual Bandits}} with {{Linear Payoffs}}},
  abstract = {Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state-of-the-art methods. However, many questions regarding its theoretical performance remained open. In this paper, we design and analyze a generalization of Thompson Sampling algorithm for the stochastic contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary. This is among the most important and widely studied versions of the contextual bandits problem. We provide the first theoretical guarantees for the contextual version of Thompson Sampling. We prove a high probability regret bound of \$$\backslash$tilde\{O\}(d\^\{3/2\}$\backslash$sqrt\{T\})\$ (or \$$\backslash$tilde\{O\}(d$\backslash$sqrt\{T $\backslash$log(N)\})\$), which is the best regret bound achieved by any computationally efficient algorithm available for this problem in the current literature, and is within a factor of \$$\backslash$sqrt\{d\}\$ (or \$$\backslash$sqrt\{$\backslash$log(N)\}\$) of the information-theoretic lower bound for this problem.},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Agrawal, Shipra and Goyal, Navin},
  year = {2013},
  keywords = {68W40; 68Q25,Computer Science - Data Structures and Algorithms,Computer Science - Learning,F.2.0,Statistics - Machine Learning},
  pages = {127--135},
  file = {/home/johannes/Documents/PhD/zotero/storage/77NGHAIE/Agrawal and Goyal - 2012 - Thompson Sampling for Contextual Bandits with Line.pdf;/home/johannes/Documents/PhD/zotero/storage/GGE9GKQT/1209.html}
}

@article{LaiAsymptoticallyefficientadaptive1985,
  title = {Asymptotically Efficient Adaptive Allocation Rules},
  volume = {6},
  number = {1},
  journal = {Advances in applied mathematics},
  author = {Lai, Tze Leung and Robbins, Herbert},
  year = {1985},
  pages = {4--22},
  file = {/home/johannes/Documents/PhD/zotero/storage/G7VH42GB/1-s2.0-0196885885900028-main.pdf}
}

@article{AuerFinitetimeanalysismultiarmed2002,
  title = {Finite-Time Analysis of the Multiarmed Bandit Problem},
  volume = {47},
  number = {2-3},
  journal = {Machine learning},
  author = {Auer, Peter and Cesa-Bianchi, Nicolo and Fischer, Paul},
  year = {2002},
  pages = {235--256},
  file = {/home/johannes/Documents/PhD/zotero/storage/447D53SJ/ml-02.pdf}
}

@article{ZhouSurveyContextualMultiarmed2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1508.03326},
  primaryClass = {cs},
  title = {A {{Survey}} on {{Contextual Multi}}-Armed {{Bandits}}},
  abstract = {In this survey we cover a few stochastic and adversarial contextual bandit algorithms. We analyze each algorithm's assumption and regret bound.},
  journal = {arXiv:1508.03326 [cs]},
  author = {Zhou, Li},
  month = aug,
  year = {2015},
  keywords = {Computer Science - Learning,read},
  file = {/home/johannes/Documents/PhD/zotero/storage/TZFCNXG9/Zhou - 2015 - A Survey on Contextual Multi-armed Bandits.pdf;/home/johannes/Documents/PhD/zotero/storage/IWAWGV7N/1508.html}
}

@incollection{Abbasi-YadkoriImprovedAlgorithmsLinear2011,
  title = {Improved {{Algorithms}} for {{Linear Stochastic Bandits}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 24},
  author = {Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
  year = {2011},
  pages = {2312--2320},
  file = {/home/johannes/Documents/PhD/zotero/storage/ASE5T4B3/linear-bandits-NIPS2011.pdf}
}

@inproceedings{AbeilleLinearThompsonSampling2017,
  title = {Linear {{Thompson Sampling Revisited}}},
  booktitle = {{{AISTATS}} 2017-20th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Abeille, Marc and Lazaric, Alessandro},
  year = {2017},
  file = {/home/johannes/Documents/PhD/zotero/storage/9FWBMBB9/Abeille and Lazaric - 2016 - Linear Thompson Sampling Revisited.pdf}
}

@article{NguyenColdstartProblemsRecommendation2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1405.7544},
  primaryClass = {cs},
  title = {Cold-Start {{Problems}} in {{Recommendation Systems}} via {{Contextual}}-Bandit {{Algorithms}}},
  abstract = {In this paper, we study a cold-start problem in recommendation systems where we have completely new users entered the systems. There is not any interaction or feedback of the new users with the systems previoustly, thus no ratings are available. Trivial approaches are to select ramdom items or the most popular ones to recommend to the new users. However, these methods perform poorly in many case. In this research, we provide a new look of this cold-start problem in recommendation systems. In fact, we cast this cold-start problem as a contextual-bandit problem. No additional information on new users and new items is needed. We consider all the past ratings of previous users as contextual information to be integrated into the recommendation framework. To solve this type of the cold-start problems, we propose a new efficient method which is based on the LinUCB algorithm for contextual-bandit problems. The experiments were conducted on three different publicly-available data sets, namely Movielens, Netflix and Yahoo!Music. The new proposed methods were also compared with other state-of-the-art techniques. Experiments showed that our new method significantly improves upon all these methods.},
  journal = {arXiv:1405.7544 [cs]},
  author = {Nguyen, Hai Thanh and Mary, J{\'e}r{\'e}mie and Preux, Philippe},
  month = may,
  year = {2014},
  keywords = {Computer Science - Information Retrieval},
  file = {/home/johannes/Documents/PhD/zotero/storage/QK3MAT9V/Nguyen et al. - 2014 - Cold-start Problems in Recommendation Systems via .pdf;/home/johannes/Documents/PhD/zotero/storage/H3RKASSE/1405.html}
}

@article{AitkenIVleastsquares1936,
  title = {{{IV}}. On Least Squares and Linear Combination of Observations},
  volume = {55},
  journal = {Proceedings of the Royal Society of Edinburgh},
  author = {Aitken, Alexander C},
  year = {1936},
  pages = {42--48}
}

@article{AzumaWeightedsumscertain1967,
  title = {Weighted Sums of Certain Dependent Random Variables},
  volume = {19},
  number = {3},
  journal = {Tohoku Mathematical Journal},
  author = {Azuma, Kazuoki},
  year = {1967},
  pages = {357--367}
}

@incollection{BercuConcentrationinequalitiesmartingales2015,
  address = {Cham},
  title = {Concentration Inequalities for Martingales},
  isbn = {978-3-319-22099-4},
  abstract = {This chapter is devoted to concentration inequalities for martingales such as Azuma-Hoeffding, Freedman, and De la Pena inequalities. Several extensions will also be provided. In particular, we will focus our attention on improved versions of Azuma-Hoeffding and Freedman's type inequalities.},
  booktitle = {Concentration {{Inequalities}} for {{Sums}} and {{Martingales}}},
  publisher = {{Springer International Publishing}},
  author = {Bercu, Bernard and Delyon, Bernard and Rio, Emmanuel},
  year = {2015},
  pages = {61--98}
}

@article{MarsdenSequentialMatrixCompletion2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.08045},
  primaryClass = {cs, stat},
  title = {Sequential {{Matrix Completion}}},
  abstract = {We propose a novel algorithm for sequential matrix completion in a recommender system setting, where the \$(i,j)\$th entry of the matrix corresponds to a user \$i\$'s rating of product \$j\$. The objective of the algorithm is to provide a sequential policy for user-product pair recommendation which will yield the highest possible ratings after a finite time horizon. The algorithm uses a Gamma process factor model with two posterior-focused bandit policies, Thompson Sampling and Information-Directed Sampling. While Thompson Sampling shows competitive performance in simulations, state-of-the-art performance is obtained from Information-Directed Sampling, which makes its recommendations based off a ratio between the expected reward and a measure of information gain. To our knowledge, this is the first implementation of Information Directed Sampling on large real datasets. This approach contributes to a recent line of research on bandit approaches to collaborative filtering including Kawale et al. (2015), Li et al. (2010), Bresler et al. (2014), Li et al. (2016), Deshpande \& Montanari (2012), and Zhao et al. (2013). The setting of this paper, as has been noted in Kawale et al. (2015) and Zhao et al. (2013), presents significant challenges to bounding regret after finite horizons. We discuss these challenges in relation to simpler models for bandits with side information, such as linear or gaussian process bandits, and hope the experiments presented here motivate further research toward theoretical guarantees.},
  journal = {arXiv:1710.08045 [cs, stat]},
  author = {Marsden, Annie and Bacallado, Sergio},
  month = oct,
  year = {2017},
  keywords = {Computer Science - Information Retrieval,Computer Science - Learning,Statistics - Machine Learning},
  file = {/home/johannes/Documents/PhD/zotero/storage/5I26F9BD/Marsden and Bacallado - 2017 - Sequential Matrix Completion.pdf;/home/johannes/Documents/PhD/zotero/storage/T24VUQTG/1710.html}
}

@incollection{LattimoreLinearMultiResourceAllocation2015,
  title = {Linear {{Multi}}-{{Resource Allocation}} with {{Semi}}-{{Bandit Feedback}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  publisher = {{Curran Associates, Inc.}},
  author = {Lattimore, Tor and Crammer, Koby and Szepesvari, Csaba},
  year = {2015},
  pages = {964--972},
  file = {/home/johannes/Documents/PhD/zotero/storage/SHU3XWU8/Lattimore et al. - 2015 - Linear Multi-Resource Allocation with Semi-Bandit .pdf;/home/johannes/Documents/PhD/zotero/storage/F4D7XI9H/5931-linear-multi-resource-allocation-with-semi-bandit-feedback.html}
}

@article{AudibertExplorationexploitationtradeoffusing2009,
  title = {Exploration-Exploitation Tradeoff Using Variance Estimates in Multi-Armed Bandits},
  volume = {410},
  abstract = {Algorithms based on upper confidence bounds for balancing exploration and exploitation are gaining popularity since they are easy to implement, efficient and effective. This paper considers a variant of the basic algorithm for the stochastic, multi-armed bandit problem that takes into account the empirical variance of the different arms. In earlier experimental works, Such algorithms were found to outperform the competing algorithms. We provide the first analysis of the expected regret for such algorithms. As expected, our results show that the algorithm that uses the variance estimates has a major advantage over its alternatives that do not use Such estimates provided that the variances of the payoffs of the suboptimal arms are low. We also prove that the regret concentrates only at a polynomial rate. This holds for all the upper confidence bound based algorithms and for all bandit problems except those special ones where with probability one the payoff obtained by pulling the optimal arm is larger than the expected payoff for the second best arm. Hence, although upper confidence bound bandit algorithms achieve logarithmic expected regret rates, they might not be Suitable for a risk-averse decision maker. We illustrate some of the results by Computer simulations. (C) 2009 Elsevier B.V. All rights reserved.},
  language = {en},
  number = {19},
  journal = {Theoretical Computer Science},
  author = {Audibert, Jean-Yves and Munos, Remi and Szepesvari, Csaba},
  year = {2009},
  pages = {1876--1902},
  file = {/home/johannes/Documents/PhD/zotero/storage/E4JXIBJ6/hal-00711069.html}
}

@article{Bhatiabetterboundvariance2000,
  title = {A Better Bound on the Variance},
  volume = {107},
  number = {4},
  journal = {The American Mathematical Monthly},
  author = {Bhatia, Rajendra and Davis, Chandler},
  year = {2000},
  pages = {353--357}
}

@article{ProkhorovConvergenceRandomProcesses1956,
  title = {Convergence of {{Random Processes}} and {{Limit Theorems}} in {{Probability Theory}}},
  volume = {1},
  abstract = {The convergence of stochastic processes is defined in terms of the so-called ``weak convergence'' (w. c.) of probability measures in appropriate functional spaces (c. s. m. s.).Chapter 1. Let \$$\backslash$Re \$ be the c.s.m.s. and v a set of all finite measures on \$$\backslash$Re \$. The distance \$L($\backslash$mu \_1 ,$\backslash$mu \_2 )\$ (that is analogous to the L{\'e}vy distance) is introduced, and equivalence of L-convergence and w. c. is proved. It is shown that \$V$\backslash$Re  = (v,L)\$ is c. s. m. s. Then, the necessary and sufficient conditions for compactness in \$V$\backslash$Re \$ are given.In section 1.6 the concept of ``characteristic functionals'' is applied to the study of w. cc of measures in Hilbert space.Chapter 2. On the basis of the above results the necessary and sufficient compactness conditions for families of probability measures in spaces \$C[0,1]\$ and \$D[0,1]\$ (space of functions that are continuous in \$[0,1]\$ except for jumps) are formulated.Chapter 3. The general form of the ``invariance principle'' for the sums of independent random variables is developed.Chapter 4. An estimate of the remainder term in the well-known Kolmogorov theorem is given (cf. [3.1]).},
  number = {2},
  journal = {Theory of Probability \& Its Applications},
  author = {Prokhorov, Y.},
  month = jan,
  year = {1956},
  pages = {157--214},
  file = {/home/johannes/Documents/PhD/Library/papers/1956/Prokhorov_1956_Convergence of Random Processes and Limit Theorems in Probability Theory.pdf;/home/johannes/Documents/PhD/zotero/storage/UE8W737U/1101016.html}
}

@inproceedings{KerstingMostLikelyHeteroscedastic2007,
  series = {ICML '07},
  title = {Most {{Likely Heteroscedastic Gaussian Process Regression}}},
  abstract = {This paper presents a novel Gaussian process (GP) approach to regression with input-dependent noise rates. We follow Goldberg et al.'s approach and model the noise variance using a second GP in addition to the GP governing the noise-free output value. In contrast to Goldberg et al., however, we do not use a Markov chain Monte Carlo method to approximate the posterior noise variance but a most likely noise approach. The resulting model is easy to implement and can directly be used in combination with various existing extensions of the standard GPs such as sparse approximations. Extensive experiments on both synthetic and real-world data, including a challenging perception problem in robotics, show the effectiveness of most likely heteroscedastic GP regression.},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Machine Learning}}},
  publisher = {{ACM}},
  author = {Kersting, Kristian and Plagemann, Christian and Pfaff, Patrick and Burgard, Wolfram},
  year = {2007},
  pages = {393--400},
  file = {/home/johannes/Documents/PhD/Library/papers/2007/Kersting et al_2007_Most Likely Heteroscedastic Gaussian Process Regression.pdf}
}

@book{GoldbergRegressionInputdependentNoise1998,
  title = {Regression with {{Input}}-Dependent {{Noise}}: {{A Gaussian Process Treatment}}},
  shorttitle = {Regression with {{Input}}-Dependent {{Noise}}},
  abstract = {Gaussian processes provide natural non-parametric prior distributions over regression functions. In this paper we consider regression problems where there is noise on the output, and the variance of the noise depends on the inputs. If we assume that the noise is a smooth function of the inputs, then it is natural to model the noise variance using a second Gaussian process, in addition to the Gaussian process governing the noise-free output value. We show that the posterior distribution of the noise rate can be sampled using Gibbs sampling. Our results on a synthetic data set give a posterior variance that well-approximates the true variance. We also show that the predictive likelihood of a test data set approximates the true likelihood better under this model than under a uniform noise model. 1 Background and Motivation  A very natural approach to regression problems is to place a prior on the kinds of function that we expect, and then after observing the data to obtain a posterior. Th...},
  author = {Goldberg, Paul and Williams, Christopher K. I. and Bishop, Christopher M.},
  year = {1998},
  file = {/home/johannes/Documents/PhD/Library/papers/1998/Goldberg et al_1998_Regression with Input-dependent Noise2.pdf;/home/johannes/Documents/PhD/zotero/storage/HG95SMU4/summary.html}
}

@phdthesis{Abbasi-YadkoriOnlinelearninglinearly2012,
  title = {Online Learning for Linearly Parametrized Control Problems},
  author = {Abbasi-Yadkori, Yasin},
  year = {2012}
}

@book{DurrettProbabilitytheoryexamples2010,
  title = {Probability: Theory and Examples},
  publisher = {{Cambridge university press}},
  author = {Durrett, Rick},
  year = {2010}
}

@book{RasmussenGaussianprocessesmachine2006,
  title = {Gaussian Processes for Machine Learning},
  volume = {1},
  author = {Rasmussen, Carl Edward and Williams, Christopher KI},
  year = {2006}
}

@book{BoydConvexoptimization2004,
  title = {Convex Optimization},
  publisher = {{Cambridge university press}},
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  year = {2004}
}

@article{CowanNormalBanditsUnknown2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1504.05823},
  primaryClass = {cs, stat},
  title = {Normal {{Bandits}} of {{Unknown Means}} and {{Variances}}: {{Asymptotic Optimality}}, {{Finite Horizon Regret Bounds}}, and a {{Solution}} to an {{Open Problem}}},
  shorttitle = {Normal {{Bandits}} of {{Unknown Means}} and {{Variances}}},
  abstract = {Consider the problem of sampling sequentially from a finite number of \$N $\backslash$geq 2\$ populations, specified by random variables \$X\^i\_k\$, \$ i = 1,$\backslash$ldots , N,\$ and \$k = 1, 2, $\backslash$ldots\$; where \$X\^i\_k\$ denotes the outcome from population \$i\$ the \$k\^\{th\}\$ time it is sampled. It is assumed that for each fixed \$i\$, \$$\backslash$\{ X\^i\_k $\backslash$\}\_\{k $\backslash$geq 1\}\$ is a sequence of i.i.d. normal random variables, with unknown mean \$$\backslash$mu\_i\$ and unknown variance \$$\backslash$sigma\_i\^2\$. The objective is to have a policy \$$\backslash$pi\$ for deciding from which of the \$N\$ populations to sample form at any time \$n=1,2,$\backslash$ldots\$ so as to maximize the expected sum of outcomes of \$n\$ samples or equivalently to minimize the regret due to lack on information of the parameters \$$\backslash$mu\_i\$ and \$$\backslash$sigma\_i\^2\$. In this paper, we present a simple inflated sample mean (ISM) index policy that is asymptotically optimal in the sense of Theorem 4 below. This resolves a standing open problem from Burnetas and Katehakis (1996). Additionally, finite horizon regret bounds are given.},
  journal = {arXiv:1504.05823 [cs, stat]},
  author = {Cowan, Wesley and Honda, Junya and Katehakis, Michael N.},
  month = apr,
  year = {2015},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {/home/johannes/Documents/PhD/Library/papers/2015/Cowan et al_2015_Normal Bandits of Unknown Means and Variances.pdf;/home/johannes/Documents/PhD/zotero/storage/Z3SC6XK4/1504.html}
}

@book{CoverElementsinformationtheory2012,
  title = {Elements of Information Theory},
  publisher = {{John Wiley \& Sons}},
  author = {Cover, Thomas M and Thomas, Joy A},
  year = {2012}
}

@article{BurnetasOptimaladaptivepolicies1996,
  title = {Optimal Adaptive Policies for Sequential Allocation Problems},
  volume = {17},
  number = {2},
  journal = {Advances in Applied Mathematics},
  author = {Burnetas, Apostolos N and Katehakis, Michael N},
  year = {1996},
  pages = {122--142}
}

@inproceedings{WangMaxvalueEntropySearch2017,
  series = {Proceedings of Machine Learning Research},
  title = {Max-Value {{Entropy Search}} for {{Efficient Bayesian Optimization}}},
  volume = {70},
  abstract = {Entropy Search (ES) and Predictive Entropy Search (PES) are popular and empirically successful Bayesian Optimization techniques. Both rely on a compelling information-theoretic motivation, and maximize the information gained about the  of the unknown function; yet, both are plagued by the expensive computation for estimating entropies. We propose a new criterion, Max-value Entropy Search (MES), that instead uses the information about the maximum function value. We show relations of MES to other Bayesian optimization methods, and establish a regret bound. We observe that MES maintains or improves the good empirical performance of ES/PES, while tremendously lightening the computational burden. In particular, MES is much more robust to the number of samples used for computing the entropy, and hence more efficient for higher dimensional problems.},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  publisher = {{PMLR}},
  author = {Wang, Zi and Jegelka, Stefanie},
  month = aug,
  year = {2017},
  keywords = {Statistics - Machine Learning},
  pages = {3627--3635},
  file = {/home/johannes/Documents/PhD/Library/papers/2017/Wang_Jegelka_2017_Max-value Entropy Search for Efficient Bayesian Optimization.pdf;/home/johannes/Documents/PhD/zotero/storage/E4R8L24U/1703.html}
}

@inproceedings{ScarlettLowerBoundsRegret2017,
  title = {Lower {{Bounds}} on {{Regret}} for {{Noisy Gaussian Process Bandit Optimization}}},
  booktitle = {Conference on {{Learning Theory}} ({{COLT}})},
  author = {Scarlett, Jonathan and Bogunovic, Ilija and Cevher, Volkan},
  year = {2017},
  keywords = {Computer Science - Information Theory,Computer Science - Learning,Statistics - Machine Learning},
  file = {/home/johannes/Documents/PhD/zotero/storage/KA6D6BX5/Scarlett et al. - 2017 - Lower Bounds on Regret for Noisy Gaussian Process .pdf;/home/johannes/Documents/PhD/zotero/storage/SGKTBADV/1706.html}
}

@incollection{LattimoreScaleFreeAlgorithm2017,
  title = {A {{Scale Free Algorithm}} for {{Stochastic Bandits}} with {{Bounded Kurtosis}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Lattimore, Tor},
  year = {2017},
  keywords = {Statistics - Machine Learning},
  pages = {1583--1592}
}

@inproceedings{ChowdhuryKernelizedMultiarmedBandits2017,
  series = {Proceedings of Machine Learning Research},
  title = {On {{Kernelized Multi}}-Armed {{Bandits}}},
  volume = {70},
  abstract = {We consider the stochastic bandit problem with a continuous set of arms, with the expected reward function over the arms assumed to be fixed but unknown. We provide two new Gaussian process-based algorithms for continuous bandit optimization \textendash{} Improved GP-UCB (IGP-UCB) and GP-Thomson sampling (GP-TS), and derive corresponding regret bounds. Specifically, the bounds hold when the expected reward function belongs to the reproducing kernel Hilbert space (RKHS) that naturally corresponds to a Gaussian process kernel used as input by the algorithms. Along the way, we derive a new self-normalized concentration inequality for vector-valued martingales of arbitrary, possibly infinite, dimension. Finally, experimental evaluation and comparisons to existing algorithms on synthetic and real-world environments are carried out that highlight the favourable gains of the proposed strategies in many cases.},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  publisher = {{PMLR}},
  author = {Chowdhury, Sayak Ray and Gopalan, Aditya},
  month = aug,
  year = {2017},
  keywords = {Computer Science - Learning},
  pages = {844--853},
  file = {/home/johannes/Documents/PhD/zotero/storage/ZUUT6GDN/Chowdhury and Gopalan - 2017 - On Kernelized Multi-armed Bandits.pdf;/home/johannes/Documents/PhD/zotero/storage/WQKT5JIC/1704.html}
}

@inproceedings{LattimoreEndOptimismAsymptotic2017,
  title = {The {{End}} of {{Optimism}}? {{An Asymptotic Analysis}} of {{Finite}}-{{Armed Linear Bandits}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Lattimore, Tor and Szepesvari, Csaba},
  year = {2017},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  pages = {728--737},
  file = {/home/johannes/Documents/PhD/zotero/storage/622PINZU/Lattimore and Szepesvari - 2016 - The End of Optimism An Asymptotic Analysis of Fin.pdf;/home/johannes/Documents/PhD/zotero/storage/P3UTD6IG/1610.html}
}

@incollection{RussoLearningOptimizeInformationDirected2014,
  title = {Learning to {{Optimize}} via {{Information}}-{{Directed Sampling}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  author = {Russo, Daniel and Van Roy, Benjamin},
  year = {2014},
  keywords = {_tablet,Computer Science - Learning},
  pages = {1583--1591},
  file = {/home/johannes/Documents/PhD/zotero/storage/7HMU53CU/Russo_Van Roy_2014_Learning to Optimize via Information-Directed Sampling.pdf;/home/johannes/Documents/PhD/zotero/storage/7BSS9NJH/1403.html}
}

@article{RussoinformationtheoreticanalysisThompson2016,
  title = {An Information-Theoretic Analysis of {{Thompson}} Sampling},
  volume = {17},
  number = {1},
  journal = {The Journal of Machine Learning Research},
  author = {Russo, Daniel and Van Roy, Benjamin},
  year = {2016},
  keywords = {_tablet,1,2,3,Computer Science - Learning},
  pages = {2442--2471},
  file = {/home/johannes/Documents/PhD/zotero/storage/RJZRA9ZE/Russo_Van Roy_2014_An Information-Theoretic Analysis of Thompson Sampling.pdf;/home/johannes/Documents/PhD/zotero/storage/55NF8248/1403.html}
}

@article{FanExponentialinequalitiesmartingales2015,
  title = {Exponential Inequalities for Martingales with Applications},
  volume = {20},
  journal = {Electronic Journal of Probability},
  author = {Fan, Xiequan and Grama, Ion and Liu, Quansheng},
  year = {2015},
  keywords = {60E15; 60G42,Mathematics - Probability},
  file = {/home/johannes/Documents/PhD/Library/papers/2013/Fan et al_2013_Exponential inequalities for martingales with applications.pdf;/home/johannes/Documents/PhD/zotero/storage/H9852NJ3/1311.html}
}

@article{BubeckRegretanalysisstochastic2012,
  title = {Regret Analysis of Stochastic and Nonstochastic Multi-Armed Bandit Problems},
  volume = {5},
  number = {1},
  journal = {Foundations and Trends in Machine Learning},
  author = {Bubeck, S{\'e}bastien and Cesa-Bianchi, Nicolo},
  year = {2012},
  keywords = {bandits,favourite},
  pages = {1--122},
  file = {/home/johannes/Documents/PhD/zotero/storage/9QWDT8ZQ/Bubeck and Cesa-Bianchi - 2012 - Regret Analysis of Stochastic and Nonstochastic Mu.pdf;/home/johannes/Documents/PhD/zotero/storage/H243MQUE/1204.html}
}

@inproceedings{ChaudhuriActiveHeteroscedasticRegression2017,
  series = {Proceedings of Machine Learning Research},
  title = {Active {{Heteroscedastic Regression}}},
  volume = {70},
  abstract = {An active learner is given a model class $\Theta$, a large sample of unlabeled data drawn from an underlying distribution and access to a labeling oracle that can provide a label for any of the unlabeled instances. The goal of the learner is to find a model  $\in\Theta$ that fits the data to a given accuracy while making as few label queries to the oracle as possible. In this work, we consider a theoretical analysis of the label requirement of active learning for regression under a heteroscedastic noise model, where the noise depends on the instance. We provide bounds on the convergence rates of active and passive learning for heteroscedastic regression. Our results illustrate that just like in binary classification, some partial knowledge of the nature of the noise can lead to significant gains in the label requirement of active learning.},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  publisher = {{PMLR}},
  author = {Chaudhuri, Kamalika and Jain, Prateek and Natarajan, Nagarajan},
  month = aug,
  year = {2017},
  pages = {694--702}
}

@article{AntosActivelearningheteroscedastic2010,
  title = {Active Learning in Heteroscedastic Noise},
  volume = {411},
  number = {29},
  journal = {Theoretical Computer Science},
  author = {Antos, Andr{\'a}s and Grover, Varun and Szepesv{\'a}ri, Csaba},
  year = {2010},
  keywords = {Active learning,Heteroscedastic noise,Regression,Sequential allocation,Sequential analysis},
  pages = {2712--2728}
}

@inproceedings{SrinivasGaussianProcessOptimization2010,
  title = {Gaussian {{Process Optimization}} in the {{Bandit Setting}}: {{No Regret}} and {{Experimental Design}}},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Machine Learning}}},
  author = {Srinivas, Niranjan and Krause, Andreas and Seeger, Matthias and Kakade, Sham M},
  year = {2010},
  pages = {1015--1022},
  file = {/home/johannes/Documents/PhD/zotero/storage/T382R3JG/srinivas10gaussian-long.pdf}
}

@incollection{KakadeGeneralizationAbilityOnline2009,
  title = {On the {{Generalization Ability}} of {{Online Strongly Convex Programming Algorithms}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 21},
  author = {Kakade, Sham M and Tewari, Ambuj},
  year = {2009},
  pages = {801--808}
}

@inproceedings{DaniStochasticLinearOptimization2008,
  title = {Stochastic {{Linear Optimization}} under {{Bandit Feedback}}},
  booktitle = {{{COLT}}},
  publisher = {{Omnipress}},
  author = {Dani, Varsha and Hayes, Thomas P. and Kakade, Sham M.},
  year = {2008},
  keywords = {bandit},
  pages = {355--366},
  file = {/home/johannes/Documents/PhD/zotero/storage/FF8Z3RQQ/Dani et al. - 2008 - Stochastic linear optimization under bandit feedba.pdf;/home/johannes/Documents/PhD/zotero/storage/HB4CNMHV/summary.html}
}

@article{Freedmantailprobabilitiesmartingales1975,
  title = {On Tail Probabilities for Martingales},
  journal = {the Annals of Probability},
  author = {Freedman, David A},
  year = {1975},
  pages = {100--118}
}

@article{AssaelHeteroscedastictreedbayesian2014,
	title = {Heteroscedastic Treed Bayesian Optimisation},
	journal = {arXiv preprint arXiv:1410.7172},
	author = {Assael, John-Alexander M and Wang, Ziyu and Shahriari, Bobak and {de Freitas}, Nando},
	year = {2014}
}


