
%early stopping

@article{YRC07,
  title={On early stopping in gradient descent learning},
  author={Yao, Yuan and Rosasco, Lorenzo and Caponnetto, Andrea},
  journal={Constructive Approximation},
  volume={26},
  number={2},
  pages={289--315},
  year={2007},
  publisher={Springer}
}

@article{Fle90,
  title={Equivalence of regularization and truncated iteration in the solution of ill-posed image reconstruction problems},
  author={Fleming, Henry E.},
  journal={Linear Algebra and its applications},
  volume={130},
  pages={133--150},
  year={1990},
  publisher={Elsevier}
}



%RL

@article{S10,
  title={Algorithms for reinforcement learning},
  author={Szepesv{\'a}ri, Csaba},
  journal={Synthesis lectures on artificial intelligence and machine learning},
  volume={4},
  number={1},
  pages={1--103},
  year={2010},
  publisher={Morgan \& Claypool Publishers}
}

@book{SB98,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S. and Barto, Andrew G.},
  volume={1},
    year={1998},
  number={1}
}

@phdthesis{F11,
  author       = {Farahmand, Amir-massoud}, 
  title        = {Regularization in reinforcement learning},
  school       = {University of Alberta},
  year         = 2011
}

%ML

@book{HTF01,
  title={The elements of statistical learning},
  author={Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  volume={1},
  year={2001},
  publisher={Springer series in statistics New York}
}


@article{DCR05,
  title={Model selection for regularized least-squares algorithm in learning theory},
  author={De Vito, Ernesto and Caponnetto, Andrea and Rosasco, Lorenzo},
  journal={Foundations of Computational Mathematics},
  volume={5},
  number={1},
  pages={59--85},
  year={2005},
  publisher={Springer}
}

@article{CD07,
  title={Optimal rates for the regularized least-squares algorithm},
  author={Caponnetto, Andrea and De Vito, Ernesto},
  journal={Foundations of Computational Mathematics},
  volume={7},
  number={3},
  pages={331--368},
  year={2007},
  publisher={Springer}
}


% minmax
@book{T08,
 author = {Tsybakov, Alexandre B.},
 title = {Introduction to Nonparametric Estimation},
 year = {2008},
 isbn = {0387790519, 9780387790510},
 edition = {1st},
 publisher = {Springer Publishing Company, Incorporated},
} 

% older 
@article{RM51,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

% classic
@article{NJLS09,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}

@book{K03,
  title={Stochastic approximation and recursive algorithms and applications},
  author={Kushner, Harold J. and Yin, G. George},
  volume={35},
  year={2003},
  publisher={Springer Science \& Business Media}
}
% sgd vs incremental
@article{B97,
  title={A new class of incremental gradient methods for least squares problems},
  author={Bertsekas, Dimitri P},
  journal={SIAM Journal on Optimization},
  volume={7},
  number={4},
  pages={913--926},
  year={1997},
  publisher={SIAM}
}

% polyak related

@article{P91,
  title={New stochastic approximation type procedures},
  author={Polyak, Boris},
  journal={ Autom. i Telemekh., 7:98Ð107, 1990},
  volume={7},
  number={7},
  pages={98--107},
  year={1990},
  publisher={In Russian}
}

@techreport{R88,
  title={Efficient estimations from a slowly convergent {Robbins--Monro} process},
  author={Ruppert, David},
  year={1988},
  institution={Cornell University Operations Research and Industrial Engineering}
}

@article{PJ92,
  title={Acceleration of stochastic approximation by averaging},
  author={Polyak, Boris T. and Juditsky, Anatoli B.},
  journal={SIAM Journal on Control and Optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992},
  publisher={SIAM}
}

@article{GyW96,
  title={On the averaged stochastic approximation for linear regression},
  author={Gy{\"o}rfi, L{\'a}szl{\'o} and Walk, Harro},
  journal={SIAM Journal on Control and Optimization},
  volume={34},
  number={1},
  pages={31--61},
  year={1996},
  publisher={SIAM}
}


% older 
@article{YS06,
  title={Online learning algorithms},
  author={Smale, Steve and Yao, Yuan},
  journal={Foundations of computational mathematics},
  volume={6},
  number={2},
  pages={145--170},
  year={2006},
  publisher={Springer}
}

@article{YP08,
  title={Online gradient descent learning algorithms},
  author={Ying, Yiming and Pontil, Massimiliano},
  journal={Foundations of Computational Mathematics},
  volume={8},
  number={5},
  pages={561--596},
  year={2008},
  publisher={Springer}
}

@article{TY14,
  title={Online learning as stochastic approximation of regularization paths: Optimality and almost-sure convergence},
  author={Tarres, Pierre and Yao, Yuan},
  journal={IEEE Transactions on Information Theory},
  volume={60},
  number={9},
  pages={5716--5735},
  year={2014},
  publisher={IEEE}
}


%% multipass

@inproceedings{RV15,
  title={Learning with incremental iterative regularization},
  author={Rosasco, Lorenzo and Villa, Silvia},
  booktitle={Advances in  Neural Information Processing Systems Proceedings 28 (NIPS)},
  pages={1630--1638},
  year={2015},
  organization={MIT Press}
}

@article{LR17,
  title={Optimal rates for multi-pass stochastic gradient methods},
  author={Lin, Junhong and Rosasco, Lorenzo},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={97},
  pages={1--47},
  year={2017}
}

@inproceedings{RSS12,
  title={Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization},
  author={Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  year={2012},	
  Booktitle = {Proceedings of the 29th International Conference on Machine Learning (ICML)},
  pages={1571-1578},
}


@article{AW01,
	Author = {Azoury, Katy S. and Warmuth, Manfred K.},
	Journal = {Machine Learning Journal},
	Number = 3,
	Pages = {211--246},
	Title = {Relative Loss Bounds for On-Line Density Estimation with the Exponential Family of Distributions},
	Volume = 43,
	Year = 2001}


@article{Vov01,
	Author = {Vovk, Vladimir},
	Journal = {International Statistical Review},
	Pages = {213--248},
	Title = {Competitive On-line Statistics},
	Volume = 69,
	Year = 2001}

@article{HK14,
  title={Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization},
  author={Hazan, Elad and Kale, Satyen},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={2489--2512},
  year={2014}
}

@article{Haz16,
  title={Introduction to online convex optimization},
  author={Hazan, Elad},
  journal={Foundations and Trends in Optimization},
  volume={2},
  number={3-4},
  pages={157--325},
  year={2016},
  publisher={Now Publishers, Inc.}
}

@article{HAK07,
 author = {Elad Hazan and Amit Agarwal and Satyen Kale},
 title = {Logarithmic regret algorithms for online convex optimization},
 journal = {Machine Learning},
 volume = {69},
 year = {2007},
 pages = {169--192},
} 

@article{SS12,
year = {2012},
volume = {4},
journal = {Foundations and Trends in Machine Learning},
title = {Online Learning and Online Convex Optimization},
number = {2},
pages = {107-194},
author = {Shai Shalev-Shwartz}
}


@inproceedings{LS18,
  author    = {Chandrashekar Lakshminarayanan and
               Csaba Szepesv{\'{a}}ri},
  title     = {Linear Stochastic Approximation: How Far Does Constant Step-Size and
               Iterate Averaging Go?},
  booktitle = {Proceedings of the 28th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages     = {1347--1355},
  year      = {2018}
}

@inproceedings{SZ13,
  title={Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes},
  author={Shamir, Ohad and Zhang, Tong},
  booktitle={Proceedings of the 30th International Conference on Machine Learning (ICML)},
  pages={71--79},
  year={2013}
}


@article{JKKNS16,
  title={Parallelizing stochastic approximation through mini-batching and tail-averaging},
  author={Jain, Prateek and Kakade, Sham M. and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
  journal={arXiv preprint arXiv:1610.03774},
  year={2016}
}

@article{JKKNPS17,
  title={A {Markov} Chain Theory Approach to Characterizing the Minimax Optimality of Stochastic Gradient Descent (for Least Squares)},
  author={Jain, Prateek and Kakade, Sham M. and Kidambi, Rahul and Netrapalli, Praneeth and Pillutla, Venkata Krishna and Sidford, Aaron},
  journal={arXiv preprint arXiv:1710.09430},
  year={2017}
}

@article{DFB16,
  author  = {Aymeric Dieuleveut and Nicolas Flammarion and Francis Bach},
  title   = {Harder, Better, Faster, Stronger Convergence Rates for Least-Squares Regression},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {101},
  pages   = {1-51}
}

@article{DB16,
  title={Nonparametric stochastic approximation with large step-sizes},
  author={Dieuleveut, Aymeric and Bach, Francis},
  journal={The Annals of Statistics},
  volume={44},
  number={4},
  pages={1363--1399},
  year={2016},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{DB15,
  title={Constant Step Size Least-Mean-Square: Bias-Variance Trade-offs and Optimal Sampling Distributions},
  author={D{\'e}fossez, Alexandre and Bach, Francis},
  booktitle={Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages={205--213},
  year={2015}
}


@inproceedings{BM13,
  title={Non-strongly-convex smooth stochastic approximation with convergence rate {$O(1/n)$}},
  author={Bach, Francis and Moulines, Eric},
  booktitle = {Advances in Neural Information Processing Systems 26 (NIPS)},
  pages={773--781},
  year={2013}
}


