\BOOKMARK [1][-]{section.0.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.0.2}{Problem Set-up}{}% 2
\BOOKMARK [2][-]{subsection.0.2.1}{Generic assumptions}{section.0.2}% 3
\BOOKMARK [2][-]{subsection.0.2.2}{Ridge regression}{section.0.2}% 4
\BOOKMARK [2][-]{subsection.0.2.3}{From testing losses to testing error}{section.0.2}% 5
\BOOKMARK [1][-]{section.0.3}{Concrete Examples and Related Work}{}% 6
\BOOKMARK [1][-]{section.0.4}{Stochastic Gradient descent}{}% 7
\BOOKMARK [2][-]{subsection.0.4.1}{Reformulation as noisy recursion}{section.0.4}% 8
\BOOKMARK [2][-]{subsection.0.4.2}{SGD for general Least-Square problems}{section.0.4}% 9
\BOOKMARK [2][-]{subsection.0.4.3}{Non-averaged SGD}{section.0.4}% 10
\BOOKMARK [2][-]{subsection.0.4.4}{Averaged and Tail-averaged SGD with constant step-size}{section.0.4}% 11
\BOOKMARK [1][-]{section.0.5}{Exponentially Convergent SGD for Classification error}{}% 12
\BOOKMARK [2][-]{subsection.0.5.1}{SGD with decreasing step-size}{section.0.5}% 13
\BOOKMARK [2][-]{subsection.0.5.2}{Tail averaged SGD with constant step-size}{section.0.5}% 14
\BOOKMARK [1][-]{section.0.6}{Conclusion}{}% 15
\BOOKMARK [1][-]{section.0.A}{Experiments}{}% 16
\BOOKMARK [1][-]{section.0.B}{Probabilistic lemmas}{}% 17
\BOOKMARK [1][-]{section.0.C}{From H to 0-1 loss}{}% 18
\BOOKMARK [1][-]{section.0.D}{Exponential rates for Kernel Ridge Regression}{}% 19
\BOOKMARK [2][-]{subsection.0.D.1}{Results}{section.0.D}% 20
\BOOKMARK [2][-]{subsection.0.D.2}{Proofs}{section.0.D}% 21
\BOOKMARK [1][-]{section.0.E}{Proofs and additional results about concrete examples}{}% 22
\BOOKMARK [2][-]{subsection.0.E.1}{From gstar to \(A4\)}{section.0.E}% 23
\BOOKMARK [2][-]{subsection.0.E.2}{Examples}{section.0.E}% 24
\BOOKMARK [1][-]{section.0.F}{Preliminaries for Stochastic Gradient Descent}{}% 25
\BOOKMARK [2][-]{subsection.0.F.1}{Proof of the optimality condition on g*}{section.0.F}% 26
\BOOKMARK [2][-]{subsection.0.F.2}{Proof of Lemma 4: reformulation of SGD as noisy recursion}{section.0.F}% 27
\BOOKMARK [1][-]{section.0.G}{Proof of stochastic gradient descent results}{}% 28
\BOOKMARK [2][-]{subsection.0.G.1}{Non-averaged SGD - Proof of Theorem 5}{section.0.G}% 29
\BOOKMARK [3][-]{subsubsection.0.G.1.1}{General result for all gamma}{subsection.0.G.1}% 30
\BOOKMARK [3][-]{subsubsection.0.G.1.2}{Result for gamma }{subsection.0.G.1}% 31
\BOOKMARK [2][-]{subsection.0.G.2}{Averaged SGD for the variance term \040- Proof of Theorem 6}{section.0.G}% 32
\BOOKMARK [2][-]{subsection.0.G.3}{Tail-averaged SGD - Proof of Corollary 7}{section.0.G}% 33
\BOOKMARK [1][-]{section.0.H}{Exponentially convergent SGD for classification error}{}% 34
\BOOKMARK [2][-]{subsection.0.H.1}{SGD with decreasing step-size: proof of Theorem 8}{section.0.H}% 35
\BOOKMARK [2][-]{subsection.0.H.2}{Tail averaged SGD with constant step-size: proof of Theorem 9 }{section.0.H}% 36
\BOOKMARK [1][-]{section.0.I}{Extension of Corollary 7 and Theorem 9 for the full averaged case.}{}% 37
\BOOKMARK [2][-]{subsection.0.I.1}{Extension of Corollary 7 for the full averaged case.}{section.0.I}% 38
\BOOKMARK [2][-]{subsection.0.I.2}{Extension of Theorem 9 for the full averaged case.}{section.0.I}% 39
\BOOKMARK [1][-]{section.0.J}{Convergence rate under weaker margin assumption}{}% 40
