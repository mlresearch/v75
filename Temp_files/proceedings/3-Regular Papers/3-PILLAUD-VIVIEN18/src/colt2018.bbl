\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adams and Fournier(2003)]{adams2003sobolev}
Robert~A. Adams and John~J.F. Fournier.
\newblock \emph{Sobolev spaces}, volume 140.
\newblock Academic Press, 2003.

\bibitem[Audibert and Tsybakov(2007)]{audibert2007fast}
Jean-Yves Audibert and Alexandre~B. Tsybakov.
\newblock Fast learning rates for plug-in classifiers.
\newblock \emph{The Annals of statistics}, 35\penalty0 (2):\penalty0 608--633,
  2007.

\bibitem[Bach and Moulines(2011)]{gradsto}
F.~Bach and E.~Moulines.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2011.

\bibitem[Bach and Moulines(2013)]{newsto}
F.~Bach and E.~Moulines.
\newblock Non-strongly-convex smooth stochastic approximation with convergence
  rate ${O}(1/n)$.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2013.

\bibitem[Bartlett et~al.(2006)Bartlett, Jordan, and
  McAuliffe]{bartlett2006convexity}
Peter~L. Bartlett, Michael~I. Jordan, and Jon~D. McAuliffe.
\newblock Convexity, classification, and risk bounds.
\newblock \emph{Journal of the American Statistical Association}, 101\penalty0
  (473):\penalty0 138--156, 2006.

\bibitem[Bottou and Le~Cun(2005)]{bottou2005line}
L.~Bottou and Y.~Le~Cun.
\newblock On-line learning for very large data sets.
\newblock \emph{Applied Stochastic Models in Business and Industry},
  21\penalty0 (2):\penalty0 137--151, 2005.

\bibitem[Caponnetto and De~Vito(2007)]{caponnetto2007optimal}
Andrea Caponnetto and Ernesto De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock \emph{Foundations of Computational Mathematics}, 7\penalty0
  (3):\penalty0 331--368, 2007.

\bibitem[Ciliberto et~al.(2016)Ciliberto, Rosasco, and
  Rudi]{ciliberto2016consistent}
Carlo Ciliberto, Lorenzo Rosasco, and Alessandro Rudi.
\newblock A consistent regularization approach for structured prediction.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[De~Vito et~al.(2014)De~Vito, Rosasco, and Toigo]{de2014learning}
Ernesto De~Vito, Lorenzo Rosasco, and Alessandro Toigo.
\newblock Learning sets with separating kernels.
\newblock \emph{Applied and Computational Harmonic Analysis}, 37\penalty0
  (2):\penalty0 185--217, 2014.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock {SAGA}: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2014.

\bibitem[D{\'e}fossez and Bach(2015)]{defossez2014constant}
A.~D{\'e}fossez and F.~Bach.
\newblock Constant step size least-mean-square: Bias-variance trade-offs and
  optimal sampling distributions.
\newblock In \emph{Proc. AISTATS}, 2015.

\bibitem[Devroye et~al.(2013)Devroye, Gy{\"o}rfi, and
  Lugosi]{devroye2013probabilistic}
Luc Devroye, L{\'a}szl{\'o} Gy{\"o}rfi, and G{\'a}bor Lugosi.
\newblock \emph{A Probabilistic Theory of Pattern Recognition}, volume~31.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Dieuleveut and Bach(2016)]{dieuleveut2016nonparametric}
Aymeric Dieuleveut and Francis Bach.
\newblock Nonparametric stochastic approximation with large step-sizes.
\newblock \emph{The Annals of Statistics}, 44\penalty0 (4):\penalty0
  1363--1399, 2016.

\bibitem[Dieuleveut et~al.(2017)Dieuleveut, Flammarion, and Bach]{daft}
Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach.
\newblock Harder, better, faster, stronger convergence rates for least-squares
  regression.
\newblock \emph{Journal of Machine Learning Research}, pages 1--51, 2017.

\bibitem[Fukumizu et~al.(2004)Fukumizu, Bach, and
  Jordan]{fukumizu2004dimensionality}
Kenji Fukumizu, Francis Bach, and Michael~I. Jordan.
\newblock Dimensionality reduction for supervised learning with reproducing
  kernel {H}ilbert spaces.
\newblock \emph{Journal of Machine Learning Research}, 5\penalty0
  (Jan):\penalty0 73--99, 2004.

\bibitem[Jain et~al.(2016)Jain, Kakade, Kidambi, Netrapalli, and
  Sidford]{jain2016parallelizing}
Prateek Jain, Sham~M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron
  Sidford.
\newblock Parallelizing stochastic approximation through mini-batching and
  tail-averaging.
\newblock Technical Report 1610.03774, arXiv, 2016.

\bibitem[Johnson and Zhang(2013)]{johnson2013accelerating}
R.~Johnson and T.~Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2013.

\bibitem[Kakade and Tewari(2009)]{kakade2009generalization}
Sham~M. Kakade and Ambuj Tewari.
\newblock On the generalization ability of online strongly convex programming
  algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2009.

\bibitem[Koltchinskii and Beznosova(2005)]{koltchinskii2005exponential}
Vladimir Koltchinskii and Olexandra Beznosova.
\newblock Exponential convergence rates in classification.
\newblock In \emph{International Conference on Computational Learning Theory}.
  Springer, 2005.

\bibitem[{Le Roux} et~al.(2012){Le Roux}, Schmidt, and
  Bach]{roux2012stochastic}
Nicolas {Le Roux}, Mark Schmidt, and Francis Bach.
\newblock A stochastic gradient method with an exponential convergence rate for
  finite training sets.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2012.

\bibitem[Mammen and Tsybakov(1999)]{mammen1999smooth}
Enno Mammen and Alexandre Tsybakov.
\newblock Smooth discrimination analysis.
\newblock \emph{The Annals of Statistics}, 27\penalty0 (6):\penalty0
  1808--1829, 1999.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirovski2009robust}
A.~Nemirovski, A.~Juditsky, G.~Lan, and A.~Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on Optimization}, 19\penalty0 (4):\penalty0
  1574--1609, 2009.

\bibitem[Nemirovski and Yudin(1983)]{nemirovsky1983problem}
A.~S. Nemirovski and D.~B. Yudin.
\newblock \emph{Problem complexity and method efficiency in optimization}.
\newblock John Wiley, 1983.

\bibitem[Nesterov and Vial(2008)]{nesterov2008confidence}
Y.~Nesterov and J.~P. Vial.
\newblock Confidence level solutions for stochastic programming.
\newblock \emph{Automatica}, 44\penalty0 (6):\penalty0 1559--1568, 2008.

\bibitem[Osokin et~al.(2017)Osokin, Bach, and Lacoste-Julien]{NIPS2017_6634}
Anton Osokin, Francis Bach, and Simon Lacoste-Julien.
\newblock On structured prediction theory with calibrated convex surrogate
  losses.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Pinelis(1994)]{pinelis1994optimum}
Iosif Pinelis.
\newblock Optimum bounds for the distributions of martingales in banach spaces.
\newblock \emph{The Annals of Probability}, pages 1679--1706, 1994.

\bibitem[Polyak and Juditsky(1992)]{polyak1992acceleration}
B.~T. Polyak and A.~B. Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock \emph{SIAM Journal on Control and Optimization}, 30\penalty0
  (4):\penalty0 838--855, 1992.

\bibitem[Robbins and Monro(1951)]{robbins:monro:1951}
H.~Robbins and S.~Monro.
\newblock A stochastic approximation method.
\newblock \emph{Ann. Math. Statistics}, 22:\penalty0 400--407, 1951.

\bibitem[Rosasco et~al.(2010)Rosasco, Belkin, and Vito]{rosasco2010learning}
Lorenzo Rosasco, Mikhail Belkin, and Ernesto~De Vito.
\newblock On learning with integral operators.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Feb):\penalty0 905--934, 2010.

\bibitem[Rudi and Rosasco(2017)]{rudi2017generalization}
Alessandro Rudi and Lorenzo Rosasco.
\newblock Generalization properties of learning with random features.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Rudi et~al.(2014)Rudi, Canas, De~Vito, and Rosasco]{rudi2014learning}
Alessandro Rudi, Guillermo~D Canas, Ernesto De~Vito, and Lorenzo Rosasco.
\newblock Learning sets and subspaces.
\newblock \emph{Regularization, Optimization, Kernels, and Support Vector
  Machines}, pages 337--357, 2014.

\bibitem[Rudi et~al.(2017)Rudi, Carratino, and Rosasco]{rudi2017falkon}
Alessandro Rudi, Luigi Carratino, and Lorenzo Rosasco.
\newblock {FALKON}: An optimal large scale kernel method.
\newblock In \emph{Advances in Neural Information Processing Systems 30}. 2017.

\bibitem[Schmidt and {Le Roux}(2013)]{schmidt2013fast}
Mark Schmidt and Nicolas {Le Roux}.
\newblock Fast convergence of stochastic gradient descent under a strong growth
  condition.
\newblock Technical Report 1308.6370, arXiv, 2013.

\bibitem[Sch{\"o}lkopf and Smola(2002)]{smola-book}
B.~Sch{\"o}lkopf and A.~J. Smola.
\newblock \emph{Learning with Kernels}.
\newblock MIT Press, 2002.

\bibitem[Shalev-Shwartz et~al.(2007)Shalev-Shwartz, Singer, and
  Srebro]{shalev2007pegasos}
S.~Shalev-Shwartz, Y.~Singer, and N.~Srebro.
\newblock Pegasos: Primal estimated sub-gradient solver for svm.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, 2007.

\bibitem[Shamir(2011)]{shamir2011SGD}
Ohad Shamir.
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock \emph{CoRR}, abs/1109.5647, 2011.

\bibitem[Shawe-Taylor and Cristianini(2004)]{Cristianini2004}
J.~Shawe-Taylor and N.~Cristianini.
\newblock \emph{Kernel Methods for Pattern Analysis}.
\newblock Cambridge University Press, 2004.

\bibitem[Solodov(1998)]{solodov1998incremental}
Mikhail~V Solodov.
\newblock Incremental gradient algorithms with stepsizes bounded away from
  zero.
\newblock \emph{Computational Optimization and Applications}, 11\penalty0
  (1):\penalty0 23--35, 1998.

\bibitem[Taskar et~al.(2005)Taskar, Chatalbashev, Koller, and
  Guestrin]{taskar2005learning}
B.~Taskar, V.~Chatalbashev, D.~Koller, and C.~Guestrin.
\newblock Learning structured prediction models: A large margin approach.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, 2005.

\bibitem[Xiao(2010)]{xiao2010dual}
L.~Xiao.
\newblock Dual averaging methods for regularized stochastic learning and online
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 9:\penalty0 2543--2596,
  2010.

\bibitem[Yao et~al.(2007)Yao, Rosasco, and Caponnetto]{yao2007early}
Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto.
\newblock On early stopping in gradient descent learning.
\newblock \emph{Constructive Approximation}, 26\penalty0 (2):\penalty0
  289--315, 2007.

\bibitem[Zhang(2004)]{zhang2004statistical}
Tong Zhang.
\newblock Statistical behavior and consistency of classification methods based
  on convex risk minimization.
\newblock \emph{Annals of Statistics}, pages 56--85, 2004.

\end{thebibliography}
