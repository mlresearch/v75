\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{bottou2005line}
\citation{nemirovsky1983problem,polyak1992acceleration}
\citation{robbins:monro:1951}
\citation{polyak1992acceleration,nesterov2008confidence,nemirovski2009robust,shalev2007pegasos,xiao2010dual,gradsto,newsto,daft}
\citation{nemirovsky1983problem}
\citation{roux2012stochastic}
\citation{johnson2013accelerating}
\citation{defazio2014saga}
\jmlr@workshop{31st Annual Conference on Learning Theory}
\jmlr@title{Exponential Convergence of Testing Error for Stochastic Gradient Methods}{Exponential Convergence of Testing Error\\ for Stochastic Gradient Methods}
\jmlr@author{\Name {Loucas Pillaud-Vivien} \Email {loucas.pillaud-vivien@inria.fr}\\ \Name {Alessandro Rudi} \Email {alessandro.rudi@inria.fr}\\ \Name {Francis Bach} \Email {francis.bach@inria.fr}\\ \addr INRIA - D\'epartement d'Informatique de l'ENS \\ Ecole Normale Sup\'erieure, CNRS, INRIA, PSL Research University }{\Name {Loucas Pillaud-Vivien} \Email {loucas.pillaud-vivien@inria.fr}\\ \Name {Alessandro Rudi} \Email {alessandro.rudi@inria.fr}\\ \Name {Francis Bach} \Email {francis.bach@inria.fr}\\ \addr INRIA - D\'epartement d'Informatique de l'ENS \\ Ecole Normale Sup\'erieure, CNRS, INRIA, PSL Research University }
\newlabel{jmlrstart}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.0.1}}
\citation{solodov1998incremental,schmidt2013fast}
\citation{dieuleveut2016nonparametric,daft}
\citation{zhang2004statistical,bartlett2006convexity}
\citation{mammen1999smooth}
\citation{bartlett2006convexity}
\citation{audibert2007fast,koltchinskii2005exponential}
\citation{newsto}
\citation{jain2016parallelizing}
\citation{caponnetto2007optimal,dieuleveut2016nonparametric}
\citation{mammen1999smooth,audibert2007fast,koltchinskii2005exponential}
\@writefile{toc}{\contentsline {paragraph}{Main contributions of the paper.}{3}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem Set-up}{3}{section.0.2}}
\newlabel{sec:setup}{{2}{3}{Problem Set-up}{section.0.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Generic assumptions}{3}{subsection.0.2.1}}
\newlabel{asm:separability}{{1}{3}{Generic assumptions}{assshort.1}{}}
\citation{Cristianini2004,smola-book}
\citation{caponnetto2007optimal}
\newlabel{asm:kernel-bounded}{{2}{4}{Generic assumptions}{assshort.2}{}}
\newlabel{asm:data-iid}{{3}{4}{Generic assumptions}{assshort.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Ridge regression}{4}{subsection.0.2.2}}
\newlabel{asm:flambda-correct-sign}{{4}{4}{Ridge regression}{assshort.4}{}}
\citation{devroye2013probabilistic}
\citation{devroye2013probabilistic,bartlett2006convexity}
\citation{bartlett2006convexity}
\citation{caponnetto2007optimal}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}From testing losses to testing error}{5}{subsection.0.2.3}}
\newlabel{lm:appr-correct-sign-to-01}{{1}{5}{From approximately correct sign to 0-1 error}{theorem.0.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Concrete Examples and Related Work}{5}{section.0.3}}
\newlabel{sec:examples}{{3}{5}{Concrete Examples and Related Work}{section.0.3}{}}
\citation{adams2003sobolev}
\citation{de2014learning,rudi2014learning}
\newlabel{prop:gstar-in-hh-gives-gla-good}{{2}{6}{Concrete Examples and Related Work}{theorem.0.3.2}{}}
\newlabel{asm:margin}{{5}{6}{Concrete Examples and Related Work}{assshort.5}{}}
\newlabel{asm:kernel-rich}{{6}{6}{Concrete Examples and Related Work}{assshort.6}{}}
\newlabel{prop:2g-makes-gstar}{{3}{6}{Concrete Examples and Related Work}{theorem.0.3.3}{}}
\newlabel{ex:independent-noise-on-labels}{{1}{6}{Independent noise on the labels}{example.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Stochastic Gradient descent}{6}{section.0.4}}
\newlabel{sec:sgd}{{4}{6}{Stochastic Gradient descent}{section.0.4}{}}
\citation{fukumizu2004dimensionality}
\citation{caponnetto2007optimal}
\citation{polyak1992acceleration}
\citation{jain2016parallelizing}
\newlabel{eq:firstSGD}{{1}{7}{Stochastic Gradient descent}{equation.0.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Reformulation as noisy recursion}{7}{subsection.0.4.1}}
\newlabel{le:SGDrecursion}{{4}{7}{Reformulation as noisy recursion}{theorem.0.4.4}{}}
\newlabel{eq:SGDrecursion}{{2}{7}{Reformulation as noisy recursion}{equation.0.4.2}{}}
\citation{defossez2014constant}
\citation{gradsto}
\citation{kakade2009generalization}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}SGD for general Least-Square problems}{8}{subsection.0.4.2}}
\newlabel{eq:SGDabstract}{{3}{8}{SGD for general Least-Square problems}{equation.0.4.3}{}}
\newlabel{asm:init}{{1}{8}{SGD for general Least-Square problems}{sgdassshort.1}{}}
\newlabel{asm:noise-iid}{{2}{8}{SGD for general Least-Square problems}{sgdassshort.2}{}}
\newlabel{asm:noise-bound}{{3}{8}{SGD for general Least-Square problems}{sgdassshort.3}{}}
\newlabel{asm:weird-bound}{{4}{8}{SGD for general Least-Square problems}{sgdassshort.4}{}}
\newlabel{asm:commute}{{5}{8}{SGD for general Least-Square problems}{sgdassshort.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Non-averaged SGD}{8}{subsection.0.4.3}}
\newlabel{th:SGDalpha}{{5}{8}{SGD, decreasing step size: $\gamma _n = \gamma /n^\alpha $}{theorem.0.4.5}{}}
\citation{gradsto}
\citation{daft}
\citation{newsto}
\citation{jain2016parallelizing,shamir2011SGD}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Averaged and Tail-averaged SGD with constant step-size}{9}{subsection.0.4.4}}
\newlabel{th:SGDaveraged}{{6}{9}{Convergence of the variance term in averaged SGD}{theorem.0.4.6}{}}
\newlabel{eq:Et}{{5}{9}{Convergence of the variance term in averaged SGD}{equation.0.4.5}{}}
\citation{daft}
\citation{shamir2011SGD}
\citation{caponnetto2007optimal,dieuleveut2016nonparametric}
\newlabel{co:SGDtailaveraged}{{7}{10}{Convergence of tail-averaged SGD}{theorem.0.4.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Exponentially Convergent SGD for Classification error}{10}{section.0.5}}
\newlabel{sec:expo}{{5}{10}{Exponentially Convergent SGD for Classification error}{section.0.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}SGD with decreasing step-size}{10}{subsection.0.5.1}}
\citation{kakade2009generalization}
\citation{bartlett2006convexity}
\newlabel{th:erroralpha}{{8}{11}{SGD with decreasing step-size}{theorem.0.5.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Tail averaged SGD with constant step-size}{11}{subsection.0.5.2}}
\newlabel{th:errortail}{{9}{11}{Tail averaged SGD with constant step-size}{theorem.0.5.9}{}}
\citation{caponnetto2007optimal}
\citation{ciliberto2016consistent,NIPS2017_6634}
\citation{taskar2005learning}
\citation{dieuleveut2016nonparametric}
\citation{rudi2017falkon,rudi2017generalization}
\bibdata{exp_rates}
\bibcite{adams2003sobolev}{{1}{2003}{{Adams and Fournier}}{{}}}
\bibcite{audibert2007fast}{{2}{2007}{{Audibert and Tsybakov}}{{}}}
\bibcite{gradsto}{{3}{2011}{{Bach and Moulines}}{{}}}
\bibcite{newsto}{{4}{2013}{{Bach and Moulines}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{12}{section.0.6}}
\newlabel{sec:conc}{{6}{12}{Conclusion}{section.0.6}{}}
\bibcite{bartlett2006convexity}{{5}{2006}{{Bartlett et~al.}}{{Bartlett, Jordan, and McAuliffe}}}
\bibcite{bottou2005line}{{6}{2005}{{Bottou and Le~Cun}}{{}}}
\bibcite{caponnetto2007optimal}{{7}{2007}{{Caponnetto and De~Vito}}{{}}}
\bibcite{ciliberto2016consistent}{{8}{2016}{{Ciliberto et~al.}}{{Ciliberto, Rosasco, and Rudi}}}
\bibcite{de2014learning}{{9}{2014}{{De~Vito et~al.}}{{De~Vito, Rosasco, and Toigo}}}
\bibcite{defazio2014saga}{{10}{2014}{{Defazio et~al.}}{{Defazio, Bach, and Lacoste-Julien}}}
\bibcite{defossez2014constant}{{11}{2015}{{D{\'e}fossez and Bach}}{{}}}
\bibcite{devroye2013probabilistic}{{12}{2013}{{Devroye et~al.}}{{Devroye, Gy{\"o}rfi, and Lugosi}}}
\bibcite{dieuleveut2016nonparametric}{{13}{2016}{{Dieuleveut and Bach}}{{}}}
\bibcite{daft}{{14}{2017}{{Dieuleveut et~al.}}{{Dieuleveut, Flammarion, and Bach}}}
\bibcite{fukumizu2004dimensionality}{{15}{2004}{{Fukumizu et~al.}}{{Fukumizu, Bach, and Jordan}}}
\bibcite{jain2016parallelizing}{{16}{2016}{{Jain et~al.}}{{Jain, Kakade, Kidambi, Netrapalli, and Sidford}}}
\bibcite{johnson2013accelerating}{{17}{2013}{{Johnson and Zhang}}{{}}}
\bibcite{kakade2009generalization}{{18}{2009}{{Kakade and Tewari}}{{}}}
\bibcite{koltchinskii2005exponential}{{19}{2005}{{Koltchinskii and Beznosova}}{{}}}
\bibcite{roux2012stochastic}{{20}{2012}{{{Le Roux} et~al.}}{{{Le Roux}, Schmidt, and Bach}}}
\bibcite{mammen1999smooth}{{21}{1999}{{Mammen and Tsybakov}}{{}}}
\bibcite{nemirovski2009robust}{{22}{2009}{{Nemirovski et~al.}}{{Nemirovski, Juditsky, Lan, and Shapiro}}}
\bibcite{nemirovsky1983problem}{{23}{1983}{{Nemirovski and Yudin}}{{}}}
\bibcite{nesterov2008confidence}{{24}{2008}{{Nesterov and Vial}}{{}}}
\bibcite{NIPS2017_6634}{{25}{2017}{{Osokin et~al.}}{{Osokin, Bach, and Lacoste-Julien}}}
\bibcite{pinelis1994optimum}{{26}{1994}{{Pinelis}}{{}}}
\bibcite{polyak1992acceleration}{{27}{1992}{{Polyak and Juditsky}}{{}}}
\bibcite{robbins:monro:1951}{{28}{1951}{{Robbins and Monro}}{{}}}
\bibcite{rosasco2010learning}{{29}{2010}{{Rosasco et~al.}}{{Rosasco, Belkin, and Vito}}}
\bibcite{rudi2017generalization}{{30}{2017}{{Rudi and Rosasco}}{{}}}
\bibcite{rudi2014learning}{{31}{2014}{{Rudi et~al.}}{{Rudi, Canas, De~Vito, and Rosasco}}}
\bibcite{rudi2017falkon}{{32}{2017}{{Rudi et~al.}}{{Rudi, Carratino, and Rosasco}}}
\bibcite{schmidt2013fast}{{33}{2013}{{Schmidt and {Le Roux}}}{{}}}
\bibcite{smola-book}{{34}{2002}{{Sch{\"o}lkopf and Smola}}{{}}}
\bibcite{shalev2007pegasos}{{35}{2007}{{Shalev-Shwartz et~al.}}{{Shalev-Shwartz, Singer, and Srebro}}}
\bibcite{shamir2011SGD}{{36}{2011}{{Shamir}}{{}}}
\bibcite{Cristianini2004}{{37}{2004}{{Shawe-Taylor and Cristianini}}{{}}}
\bibcite{solodov1998incremental}{{38}{1998}{{Solodov}}{{}}}
\bibcite{taskar2005learning}{{39}{2005}{{Taskar et~al.}}{{Taskar, Chatalbashev, Koller, and Guestrin}}}
\bibcite{xiao2010dual}{{40}{2010}{{Xiao}}{{}}}
\bibcite{yao2007early}{{41}{2007}{{Yao et~al.}}{{Yao, Rosasco, and Caponnetto}}}
\bibcite{zhang2004statistical}{{42}{2004}{{Zhang}}{{}}}
\citation{adams2003sobolev}
\@writefile{toc}{\contentsline {section}{\numberline {A}Experiments}{16}{section.0.A}}
\newlabel{ap:experiments}{{A}{16}{Experiments}{section.0.A}{}}
\citation{daft}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Representing the $\rho _{\EuScript  X}$ density (uniform with $\varepsilon $-margin), the best estimator, i.e., ${\mathbb  E}(x|y)$ and $g_\lambda $ used for the simulations ($\lambda = 0.01$).}}{17}{figure.1}}
\newlabel{fig:densities}{{1}{17}{Representing the $\rho _\X $ density (uniform with $\varepsilon $-margin), the best estimator, i.e., $\E (x|y)$ and $g_\lambda $ used for the simulations ($\lambda = 0.01$)}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Showing linear convergence for the $L^{01}$ errors in the case of margin of width $\varepsilon $. {\bfseries  Left} figure corresponds to the test and training loss in the averaged case whereas the {\bfseries  right} one corresponds to the error in the same setting. Note that the y-axis is the same while the x-axis is different of a factor 10. The fact that the error plot is a line after a certain $n$ matches our theoretical results. We took the following parameters : $\varepsilon = 0.05$, $\gamma = 0.25$, $\lambda = 0.01$.}}{18}{figure.2}}
\newlabel{fig:plots}{{2}{18}{\small Showing linear convergence for the $L^{01}$ errors in the case of margin of width $\varepsilon $. {\bfseries Left} figure corresponds to the test and training loss in the averaged case whereas the {\bfseries right} one corresponds to the error in the same setting. Note that the y-axis is the same while the x-axis is different of a factor 10. The fact that the error plot is a line after a certain $n$ matches our theoretical results. We took the following parameters : $\varepsilon = 0.05$, $\gamma = 0.25$, $\lambda = 0.01$}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\bfseries  Left} plot shows the error in the non-averaged case for $\gamma _n = \gamma / \sqrt  {n}$ and {\bfseries  right} compares the test error between averaged and tail averaged case. We took the following parameters : $\varepsilon = 0.05$, $\gamma = 0.25$, $\lambda = 0.01$.}}{18}{figure.3}}
\newlabel{fig:techplots}{{3}{18}{\small {\bfseries Left} plot shows the error in the non-averaged case for $\gamma _n = \gamma / \sqrt {n}$ and {\bfseries right} compares the test error between averaged and tail averaged case. We took the following parameters : $\varepsilon = 0.05$, $\gamma = 0.25$, $\lambda = 0.01$}{figure.3}{}}
\citation{pinelis1994optimum}
\citation{pinelis1994optimum}
\@writefile{toc}{\contentsline {section}{\numberline {B}Probabilistic lemmas}{19}{section.0.B}}
\newlabel{sec:proba}{{B}{19}{Probabilistic lemmas}{section.0.B}{}}
\newlabel{Probabilisticprop}{{10}{19}{Probabilistic lemmas}{theorem.0.B.10}{}}
\newlabel{Probabilisticcor}{{11}{19}{Probabilistic lemmas}{theorem.0.B.11}{}}
\citation{caponnetto2007optimal}
\citation{fukumizu2004dimensionality}
\@writefile{toc}{\contentsline {section}{\numberline {C}From ${\EuScript  H}$ to 0-1 loss}{20}{section.0.C}}
\newlabel{sect:proof-A5-to-01}{{C}{20}{From \texorpdfstring {$\H $}{H} to 0-1 loss}{section.0.C}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Exponential rates for Kernel Ridge Regression}{20}{section.0.D}}
\newlabel{sect:exp-rates-for-KRR}{{D}{20}{Exponential rates for Kernel Ridge Regression}{section.0.D}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1}Results}{20}{subsection.0.D.1}}
\citation{caponnetto2007optimal}
\citation{caponnetto2007optimal}
\citation{yao2007early}
\citation{koltchinskii2005exponential}
\citation{caponnetto2007optimal}
\citation{caponnetto2007optimal}
\newlabel{lm:krls-analytic-variance}{{12}{21}{Results}{theorem.0.D.12}{}}
\newlabel{thm:exp-class-krls}{{13}{21}{Results}{theorem.0.D.13}{}}
\citation{pinelis1994optimum}
\citation{rosasco2010learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2}Proofs}{22}{subsection.0.D.2}}
\@writefile{toc}{\contentsline {section}{\numberline {E}Proofs and additional results about concrete examples}{23}{section.0.E}}
\newlabel{sect:examples-for-glambda}{{E}{23}{Proofs and additional results about concrete examples}{section.0.E}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.1}From $g_\ast \in \EuScript  H$ to {\textbf  {(A\ref  {asm:flambda-correct-sign})}}}{23}{subsection.0.E.1}}
\newlabel{sect:from-g-in-H-to-A5}{{E.1}{23}{From \texorpdfstring {$g_\ast \in \hh $}{gstar} to \asm {asm:flambda-correct-sign}}{subsection.0.E.1}{}}
\newlabel{lm:Ala-go-to-0}{{14}{23}{From \texorpdfstring {$g_\ast \in \hh $}{gstar} to \asm {asm:flambda-correct-sign}}{theorem.0.E.14}{}}
\newlabel{lm:krls-analytic-bias}{{15}{24}{From \texorpdfstring {$g_\ast \in \hh $}{gstar} to \asm {asm:flambda-correct-sign}}{theorem.0.E.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.2}Examples}{25}{subsection.0.E.2}}
\newlabel{sect:A5-examples}{{E.2}{25}{Examples}{subsection.0.E.2}{}}
\newlabel{prop:exists-gPN}{{16}{25}{Examples}{theorem.0.E.16}{}}
\citation{adams2003sobolev}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Pictorial representation of a model in $1$D satisfying Example\nobreakspace  {}\ref  {ex:independent-noise-on-labels}, ($p = 0.15$). Blue: ${\rho _{{\EuScript  X}}}$, green: $\mathbb  {E}\left ({y}\middle |{x}\right )$, red: $g_\lambda $.}}{26}{figure.4}}
\newlabel{fig:example-A5}{{4}{26}{Pictorial representation of a model in $1$D satisfying Example~\ref {ex:independent-noise-on-labels}, ($p = 0.15$). Blue: $\rhox $, green: $\condexp {y}{x}$, red: $g_\la $}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {F}Preliminaries for Stochastic Gradient Descent}{27}{section.0.F}}
\newlabel{ap:SGDdevelopment}{{F}{27}{Preliminaries for Stochastic Gradient Descent}{section.0.F}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {F.1}Proof of the optimality condition on $g_*$}{27}{subsection.0.F.1}}
\newlabel{ap:optimality}{{F.1}{27}{Proof of the optimality condition on \texorpdfstring {$g_*$}{g*}}{subsection.0.F.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {F.2}Proof of Lemma\nobreakspace  {}\ref  {le:SGDrecursion}: reformulation of SGD as noisy recursion}{28}{subsection.0.F.2}}
\newlabel{ap:SGDreformulation}{{F.2}{28}{Proof of Lemma~\ref {le:SGDrecursion}: reformulation of SGD as noisy recursion}{subsection.0.F.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {G}Proof of stochastic gradient descent results}{28}{section.0.G}}
\newlabel{sec:AppSGD}{{G}{28}{Proof of stochastic gradient descent results}{section.0.G}{}}
\@writefile{toc}{\contentsline {paragraph}{Notations.}{28}{section*.4}}
\newlabel{eq:decomposition}{{11}{28}{Notations}{equation.0.G.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {G.1}Non-averaged SGD - Proof of Theorem \ref  {th:SGDalpha}}{29}{subsection.0.G.1}}
\newlabel{ap:SGDalpha}{{G.1}{29}{Non-averaged SGD - Proof of Theorem \ref {th:SGDalpha}}{subsection.0.G.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {G.1.1}General result for all $(\gamma _n)$}{29}{subsubsection.0.G.1.1}}
\newlabel{le:boundsalpha}{{17}{29}{General result for all \texorpdfstring {$(\gamma _n)$}{gamma}}{theorem.0.G.17}{}}
\citation{gradsto}
\newlabel{prop:alphabetazeta}{{18}{30}{General result for all \texorpdfstring {$(\gamma _n)$}{gamma}}{theorem.0.G.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {G.1.2}Result for $\gamma _n = \gamma /n^\alpha $ }{30}{subsubsection.0.G.1.2}}
\newlabel{le:nalpha}{{19}{30}{Result for \texorpdfstring {$\gamma _n = \gamma /n^\alpha $}{gamma}}{theorem.0.G.19}{}}
\newlabel{prop:fullalpha}{{20}{34}{SGD, decreasing step size: $\gamma _n = \gamma /n^\alpha $}{theorem.0.G.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {G.2}Averaged SGD for the variance term ($\eta _0 = 0$) - Proof of Theorem \ref  {th:SGDaveraged}}{34}{subsection.0.G.2}}
\newlabel{ap:SGDaverage}{{G.2}{34}{Averaged SGD for the variance term \texorpdfstring {($\eta _0 = 0$)}{} - Proof of Theorem \ref {th:SGDaveraged}}{subsection.0.G.2}{}}
\newlabel{le:averagebounds}{{21}{35}{Averaged SGD for the variance term \texorpdfstring {($\eta _0 = 0$)}{} - Proof of Theorem \ref {th:SGDaveraged}}{theorem.0.G.21}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 1:}{35}{section*.5}}
\@writefile{toc}{\contentsline {paragraph}{Step 2: }{37}{section*.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {G.3}Tail-averaged SGD - Proof of Corollary\nobreakspace  {}\ref  {co:SGDtailaveraged}}{38}{subsection.0.G.3}}
\newlabel{ap:SGDcorrolary}{{G.3}{38}{Tail-averaged SGD - Proof of Corollary~\ref {co:SGDtailaveraged}}{subsection.0.G.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {H}Exponentially convergent SGD for classification error}{39}{section.0.H}}
\newlabel{sec:error}{{H}{39}{Exponentially convergent SGD for classification error}{section.0.H}{}}
\newlabel{le:noise}{{22}{40}{Showing \sgdasm {asm:init}, \sgdasm {asm:noise-iid}, \sgdasm {asm:noise-bound},\sgdasm {asm:weird-bound} for SGD recursion}{theorem.0.H.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {H.1}SGD with decreasing step-size: proof of Theorem\nobreakspace  {}\ref  {th:erroralpha}}{41}{subsection.0.H.1}}
\newlabel{ap:EXPalpha}{{H.1}{41}{SGD with decreasing step-size: proof of Theorem~\ref {th:erroralpha}}{subsection.0.H.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {H.2}Tail averaged SGD with constant step-size: proof of Theorem\nobreakspace  {}\ref  {th:errortail} }{42}{subsection.0.H.2}}
\newlabel{ap:EXPaverage}{{H.2}{42}{Tail averaged SGD with constant step-size: proof of Theorem~\ref {th:errortail}}{subsection.0.H.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {I}Extension of Corollary \ref  {co:SGDtailaveraged} and Theorem \ref  {th:errortail} for the full averaged case.}{42}{section.0.I}}
\newlabel{ap:average}{{I}{42}{Extension of Corollary \ref {co:SGDtailaveraged} and Theorem \ref {th:errortail} for the full averaged case}{section.0.I}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {I.1}Extension of Corollary \ref  {co:SGDtailaveraged} for the full averaged case.}{42}{subsection.0.I.1}}
\newlabel{ap:SGDfullaverage}{{I.1}{42}{Extension of Corollary \ref {co:SGDtailaveraged} for the full averaged case}{subsection.0.I.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Notations.}{42}{section*.7}}
\newlabel{eq:auxiliarysequence}{{19}{43}{Notations}{equation.0.I.19}{}}
\newlabel{th:withbias}{{24}{43}{Notations}{theorem.0.I.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {I.2}Extension of Theorem \ref  {th:errortail} for the full averaged case.}{44}{subsection.0.I.2}}
\newlabel{ap:EXPfullaverage}{{I.2}{44}{Extension of Theorem \ref {th:errortail} for the full averaged case}{subsection.0.I.2}{}}
\newlabel{th:expwithbias}{{25}{44}{Extension of Theorem \ref {th:errortail} for the full averaged case}{theorem.0.I.25}{}}
\newlabel{le:noiseauxiliary}{{26}{44}{Showing \sgdasm {asm:init}, \sgdasm {asm:noise-iid}, \sgdasm {asm:noise-bound}, \sgdasm {asm:weird-bound} for the auxiliary recursion}{theorem.0.I.26}{}}
\citation{caponnetto2007optimal}
\citation{audibert2007fast}
\@writefile{toc}{\contentsline {section}{\numberline {J}Convergence rate under weaker margin assumption}{46}{section.0.J}}
\newlabel{ap:weakmargin}{{J}{46}{Convergence rate under weaker margin assumption}{section.0.J}{}}
\newlabel{asm:wseparability}{{7}{46}{Convergence rate under weaker margin assumption}{assshort.7}{}}
\newlabel{asm:sflambda-correct-sign}{{8}{46}{Convergence rate under weaker margin assumption}{assshort.8}{}}
\newlabel{asm:eigenvalues}{{9}{46}{Convergence rate under weaker margin assumption}{assshort.9}{}}
\newlabel{co:weakmargin}{{27}{46}{Explicit onvergence rate under weaker margin condition}{theorem.0.J.27}{}}
\citation{caponnetto2007optimal}
\newlabel{jmlrend}{{J}{47}{end of Exponential Convergence of Testing Error for Stochastic Gradient Methods}{section*.8}{}}
