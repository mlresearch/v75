\section{Soundness Analysis}\label{sec-NO-case}
Given the \LC instance $\mc{L}$, suppose that there is a degree-$d$ polynomial (over $\mc{F}$) $\ol{P}_{\tn{global}}$ such that
the Final PCP Test accepts with probability $1/2 + \xi$. Our goal in the rest of this paper is to show that in this case there exists a labeling that satisfies at least $2^{-c_0R}$-fraction of the edges of $\mc{L}$, for an appropriate choice of constants $R$ and $J$ in Theorem \ref{thm-LC-hardness} and because of its NO Case we would be done.

Let $P_{\tn{global}}$ be the representation of $\ol{P}_{\tn{global}}$  in $\mathbb{R}^{\mc{Y}}$, so that  $\ol{P}_{\tn{global}}(\ol{\bf y}) = P_{\tn{global}}({\bf y})$ where $\ol{\bf y} \in \mc{F}$ is a point generated by the Final PCP Test from a point ${\bf y}$ generated by the Basic PCP Test as given in Appendix \ref{sec-final-pcp-test}. Therefore, $P_{\tn{global}}({\bf y}) = b$ with probability at least $1/2 + \xi$ over the pairs $({\bf y}, b)$ output by the Basic PCP Test. Using this, we focus on analyzing the structure of $P_{\tn{global}}$.

To begin the analysis note that with probability at least $2\xi$ over
the choices of the verifier other than $b$, $P_{\tn{global}}$ flips
its sign on flipping $b$.  Call a choice of $\{v_j\,\mid\, j \in [T]\}$ 
\emph{good} if conditioned on this, the same holds with
probability at least $\xi$ over the rest of the choices (other than $b$) of the
verifier. 
By averaging, with probability at least $\xi$,
the verifier makes a good choice. We now fix such a 
good choice $\{v_j\,\mid\, j \in [T]\}$.


For convenience, we shall use $P$ to denote the restriction of
$P_{\tn{global}}$ to ${\bf Y} := \{Y_{ij}\, \mid\, i\in[k], j\in [T]\}$. 
Let $\mc{D}$ be the distribution on $({\bf Y}, b)$ generated by the
steps of the verifier.
 Our analysis shall first show that  
in terms of this basis $P$ must have a  certain structure which  
will then be used to determine a good labeling for $\mc{L}$.


  
 

\subsection{Basis Transformations}
For the purpose of the analysis, we shall rewrite the variables
${\bf Y}$ in different bases. Before we
do that, we shall isolate the noisy set $\mc{I}$ of the Basic PCP Test.



\subsubsection{Choice of set $\mc{I}$}
The distribution $\mc{D}$ involves choosing the set $\mc{I}$ in which
each $(i,j)$ is added independently at random with probability $\eps$.
Let us call a setting of $\mc{I}$ as \emph{nice} if it satisfies:
\begin{enumerate}
\item For each $j$, $\left|\{i\,\mid\, (i,j)\in\mc{I}\}\right| \leq
k/2$.
\item 
With probability $\xi/2$ over the rest of the choices of the verifier
(except $b$), $P$ flips its sign on flipping $b$. 
\end{enumerate}
By our setting of $\eps$ and $T$, for a large enough value of $k$, and
applying the Chernoff Bound, a union bound and an averaging argument,
we have:
\begin{equation}
\Pr_{\mc{D}}\left[\mc{I} \tn{ is nice}\right] \geq \xi/4.
\label{eqn-nice-I}
\end{equation}

Going forward, we shall fix a nice choice of $\mc{I}$. By relabeling, we
may assume that there exist $k/2 \leq k_j \leq k$ for $j \in [T]$ such that
\begin{equation}
\mc{I} = \bigcup_{j=1}^T \{(i,j) \,\mid\, i = k_j+1,\dots, k\}.
\end{equation}
Based on this  {nice} choice of $\mc{I}$, we now define new bases
for the ${\bf Y}$ variables. Let $\mc{D}_{\mc{I}}$
denote the distribution of the variables after fixing a nice $\mc{I}$.

\subsubsection{Bases {\bf W} and {\bf U}} \label{sec:R=1Basis}
For each $j \in [T]$, we define $(W_{1j}, W_{2j}, \dots, W_{k_jj})$ as a fixed orthogonal transformation of $(Y_{1j}, Y_{2j}, \dots, Y_{k_jj})$ so that
\begin{equation}
W_{1j} = \frac{1}{ k_j} \sum_{i=1}^{k_j} Y_{ij},  \ \ \ \ \tn{and} \ \ \ \ 
W_{ij} = \sum_{\ell \in [k_j]}c_{i\ell}Y_{\ell j}\ \tn{for all $i \in [2, k_j]$}
\end{equation} 
where the vectors $\left\{{\bf c}_i =[c_{i1}, c_{i2}, \ldots ,c_{ik_j}]^{\sf{T}}\right\}_{i=2}^{k_j}$ satisfy 
\begin{itemize}
	\item For all $i,i^\prime \in [k_j] \setminus \{1\}$ we have $\langle {\bf c}_i, {\bf c}_{i^\prime} \rangle = 0$
	\item Each vector ${\bf c}_i$ satisfies $\|{\bf c}_i\|^2 = 1/k_j$ and ${\bf c}_i \perp \mathbbm{1}$ where $\mathbbm{1}$ is the all ones vector in $\mathbb{R}^{k_j}$.
\end{itemize}
We shall also define the vector ${\bf c}_1 = \frac{1}{k_j}\cdot\mathbbm{1}$ where $\mathbbm{1} \in \R^{k_j}$ is the the vector of all ones.
The above along with the distribution of $\{Y_{ij}\,\mid\, i = 1,\dots,k_j\}_{j=1}^T$ in $\mc{D}_{\mc{I}}$ directly implies the following.
\begin{lemma}\label{lem:W-dist}
Under the distribution $\mc{D}_\mc{I}$:
\begin{enumerate*}
\item[(i)]
$W_{1j} = \sqrt{\nfrac{(T-1)}{T}} \cdot \delta_j + b\eta$ 
\item[(ii)]  For $i \neq 1$, $W_{ij} = 0$.
\end{enumerate*}
\end{lemma}
Let $U_1, \dots, U_T$ be a fixed orthonormal transformation of $(W_{11}, \dots, W_{1T})$, where
\begin{equation}
U_1 = \frac{1}{\sqrt{T}} \sum_{j=1}^T W_{1j}, \ \ \ \ \tn{and}\ 
U_t = \sum_{j \in [T]}a_{tj}W_{1j} \ \tn{for all $t \in [2, T]$}
\end{equation}
where vectors ${\bf a}_2,\ldots,{\bf a}_T$ are orthonormal and each vector ${\bf a}_t = [a_{t1}, a_{t2}, \ldots ,a_{tT}]^{\sf{T}}$  satisfies   $\sum_{j \in T}{a}_{tj} =0$ (i.e., they are orthogonal to the all ones vector).
\begin{lemma}\label{lem:U-dist}
Under the distribution $\mc{D}_{\mc{I}}$,
\begin{itemize}
\item[(i)] $U_1 = {b\eta}{\sqrt{T}}$
\item[(ii)] For each $1 < t \leq T$, $U_t \sim N(0,1)$ i.i.d.  
\end{itemize}
\end{lemma}
\begin{proof}
Lemma \ref{lem:W-dist} along with the definition of $U_1$ yields the first part. 
The second part follows from an application of  Lemma \ref{lem-ortho-transform}.
\end{proof}
Before we proceed, we briefly summarize the variables and their
distribution under $\mc{D}_{\mc{I}}$.


\begin{itemize}
	\item {\bf Noisy Indices} For a fixed $j \in [T]$, $[k_j]$ is the set of non-noisy $i$'s where $k_j \geq k/2$.
	\item {\bf The $Y$-variables} . For each $(i,j) \in [k] \times [T] \setminus\mc{I}$, $Y_{ij} =  \sqrt{\nfrac{(T-1)}{T}}\cdot\delta_j +b\eta$. For $(i,j) \in \mc{I}$, $Y_{ij}$'s are independent $N(0,1)$ random variables. 
	\item {\bf The $W$-variables} For a fixed $j$, we define variables $W_{1j},\ldots,W_{k_jj}$ with $W_{1j} =  \sqrt{\nfrac{(T-1)}{T}}\cdot\delta_j + b\eta$ and $W_{2j},\ldots,W_{{k_j}j}$ are $0$. 
	\item {\bf $U$-variables} We define $U_1 = \frac{1}{\sqrt{T}}\sum_{j \in [T]}W_{1j}$ which is $b\eta\sqrt{T}$ and is independent of the 
variables $U_2,\ldots,U_T$ where each $U_t$ is i.i.d. $N(0,1)$ for $t>1$.
\end{itemize}


\subsection{A Hybrid Basis Relative to $j^*$ and $d^*$}\label{sec:hybrid}
Recall that we have fixed a nice $\mc{I}$. In this section, we define a basis for polynomials using a fixed choice of $j^* \in [T]$ and $d^* \in [d]$.  For convenience let $[T_{-j^*}] := [T]\setminus\{j^*\}$.

\begin{definition}\label{def-hermite-basis-partial}
Let $\mcb{H}_{-j^*}$ be the Hermite basis for
all polynomials over the independent Gaussian variables $\{Y_{ij}\,\mid\, i \in [k]\setminus[k_j], j\in[T_{-j^*}]\}$.
In particular, $\E[H^2] = \E[G^2] = 1$ and $\E[HG] = 0$ for each
$H, G \in \mcb{H}_{-j^*}$, $H\neq G$. Let $\mcb{H}_{-j^*d^*}$  be the
set of basis elements of $\mcb{H}_{-j^*}$ of degree exactly $d^*$. 
\end{definition}
\begin{definition}\label{def-monomial-Ws-partial}
Let $\mcb{M}_{-j^*}$ be the \emph{standard monomial basis} for
polynomials over the variables $\{W_{1j}\,\mid\,j\in [T_{-j^*}]\}$. In
particular, each element of $\mcb{M}_{-j^*}$ is of the form
$\prod_{j\in [T_{-j^*}]}W_{1j}^{a_j}$ for some non-negative integers
$a_j$ ($j \in [T_{-j^*}]$).
\end{definition}
\begin{definition}\label{def-combined-basis}
Let $\mcb{B}_{-j^*} := \mcb{H}_{-j^*} \circ \mcb{M}_{-j^*}$ be the
combined basis for polynomials over the variables of 
$\mcb{H}_{-j^*}$ and $\mcb{M}_{-j^*}$, where each element $B$ is of
the form $HM$ for some $H \in \mcb{H}_{-j^*}$ and $M \in
\mcb{M}_{-j^*}$ and $\tn{deg}(B) = \tn{deg}(H) + \tn{deg}(M)$. For
convenience  we also define the subset $\mcb{B}_{-j^*d^*} := 
\mcb{H}_{-j^*d^*} \circ \mcb{M}_{-j^*}$, i.e. each element of 
$\mcb{B}_{-j^*d^*}$ is of the form $HM$ where $H \in
\mcb{H}_{-j^*d^*}$ and $M \in \mcb{M}_{-j^*}$.
\end{definition}
Lastly, let $\mcb{S}_{j^*}$ be the set of all multisets of
$R_{j^*} = \{(i,j^*)\,\mid\, i\in [k]\}$. For an element $S \in
\mcb{S}_{j^*}$, let $S(i,j^*)$ denote the number of occurrences of
$(i,j^*)$ in $S$. Using this, we define $Y_S := \prod_{(i,j^*) \in R_{j^*}} Y_{ij^*}^{S(i,j^*)}$.

\medskip
Writing the polynomial $P$  in the basis given by products of $\mcb{B}_{-j^*}$, $\{W_{ij}: j \in [T_{-j^*}], i \in [k_j]\setminus\{1\}\}$ and $\{Y_{ij^*}: i \in [k]\}$, 
the polynomial $P$ can be  
represented as:
\begin{equation}
P = P_{\tn{omit}} + \sum_{\substack{S\in \mcb{S}_{j^*} \\ B\in \mcb{B}_{-j^*}}}
c_{S, B}  Y_S B, \label{eqn-P-hybrid-rep}
\end{equation}
where $c_{S,B}$ are constants and\footnote{The reason for treating $P_{\tn{omit}}$ separately is that it vanishes under the distribution $\mc{D}_{\mc{I}}$.} $P_{\tn{omit}}$ is the sub-polynomial of $P$ consisting of all monomials containing a variable from $\{W_{ij}: j \in [T_{-j^*}], i \in [k_j]\setminus\{1\}\}$. Of course, since $P$ is of degree at
most $d$, the only terms that occur in the above sum satisfy
$\tn{deg}(B) + |S| \leq d$.

For a fixed $0 \leq d^* \leq d-1$ we will be interested in capturing the
the mass of $P$ linear in $Y_{ij^*}$ and the subset
$\mcb{B}_{-j^*d^*}$. Abusing notation
to let $c_{(i,j^*), B} = c_{(S, B)}$ where $S = \{(i,j^*)\}$ is the
singleton multiset, define
\begin{equation}
c_{i,j^*,d^*} = \sqrt{\sum_{B \in \mcb{B}_{-j^*d^*}} c_{(i,j^*), B}^2} 
\label{eqn-lin-coeff}
\end{equation} 
for each $(i,j^*) \in R_{j^*}$ and $0 \leq d^* \leq d-1$. 

\subsection{Main Structural Lemma}
We are now ready to describe the structure that $P$ must exhibit in order to pass the Basic PCP test. 
Let us first define a {\em distinguished pair} $(j^*, d^*)$ for a fixed setting of $\mc{I}$.

\begin{definition}\label{def-distinguished-jd}
A pair $(j^*, d^*) \in [T]\times\{0,\dots,d-1\}$ is said to be
\emph{distinguished} for $\mc{I}$ if,
\begin{equation}
\sum_{(i,j^*) \in \mc{I}} c_{i,j^*,d^*}^2 \leq
\frac{\eps^4}{4} \cdot \left(\sum_{(i,j^*) \in
([k]\times\{j^*\})\setminus\mc{I}} c_{i,j^*,d^*}^2\right), \label{eqn-main-nice-I-1}
\end{equation}
and,
\begin{equation}
\sum_{(i,j^*) \in
([k]\times\{j^*\})\setminus\mc{I}} c_{i,j^*,d^*}^2 > 0. \label{eqn-main-nice-I-3}
\end{equation}
Here, $\eps$ is the noise parameter used in the PCP test.
\end{definition}

The main lemma that we prove is the following.

\begin{lemma}[Main Structural Lemma]\label{lem-main-nice-I}
For every \emph{nice} choice of $\mc{I}$, there exists $j^* \in [T]$
and $d^* \in \{0,1,\dots, d-1\}$ such that $(j^*, d^*)$ is
distinguished for $\mc{I}$.
\end{lemma}
The proof of the above lemma is given in Appendix \ref{sec:findingj}
building upon analysis in Appendix \ref{sec:anticonc}. Both Appendices
\ref{sec:anticonc} and \ref{sec:findingj} assume a setting of nice $\mc{I}$.

Using \eqref{eqn-nice-I} and a simple averaging, 
the above lemma implies that there exists $(j^*, d^*)$ such that:
\begin{equation}
\Pr_{\mc{I}}\left[(j^*,d^*) \tn{ is distinguished for }\mc{I}\right] \geq
\frac{\xi}{4Td}.
\label{eqn-distinguished-I}
\end{equation}

\subsection{Implications of the Structural Lemma}
We now fix $(j^*, d^*)$ satisfying
\eqref{eqn-distinguished-I}. Let us consider the random choice of $\mc{I}$ as
first picking $\mc{I}_{-j^*} := \mc{I}\cap([k]\times ([T]\setminus
\{j^*\}))$, and then picking $\mc{I}_{j^*} := \mc{I}\cap([k]\times
\{j^*\})$. Note that the choice of $\mc{I}_{j^*}$ is independent of
$\mc{I}_{-j^*}$.
Call a choice of $\mc{I}_{-j^*}$ as \emph{shared-heavy} if,
\begin{equation}
\Pr_{\mc{I}_{j^*}}\left[(j^*,d^*) \tn{ is distinguished for
}\mc{I}_{j^*}\cup\mc{I}_{-j^*}\right] \geq \frac{\xi}{8Td}. 
\label{eqn-shared-heavy-def}
\end{equation}
From \eqref{eqn-distinguished-I} and an averaging argument we
have:
\begin{equation}
\Pr_{\mc{I}_{-j^*}}\left[\mc{I}_{-j^*} \tn{ is shared-heavy}\right]
\geq \frac{\xi}{8Td}.
\label{eqn-shared-heavy-prob}
\end{equation}

Let us fix a shared-heavy $\mc{I}_{-j^*}$. Note that with this fixing,
the bases given in Appendix \ref{sec:hybrid} are well defined, and in
particular $P$ can be represented as in
\eqref{eqn-P-hybrid-rep}. Since there is at least one choice of
$\mc{I}_{j^*}$ such that $(j^*, d^*)$ is distinguished for
$\mc{I}_{j^*}\cup \mc{I}_{-j^*}$, using \eqref{eqn-main-nice-I-3} this implies
\begin{equation}
\sum_{i \in
[k]} c_{i,j^*,d^*}^2 > 0. \label{eqn-main-shared-heavy-I-3}
\end{equation}
Further we have the following lemma. (This is where we are finally randomizing over $\mc{I}_{j^*}$.)
\begin{lemma}\label{lem:Azuma-apply}
There exists $i^* \in [k]$ such that,
$$c_{i^*,j^*,d^*}^2 \geq \nu^2 
\left(\sum_{i \in [k]} c_{i,j^*,d^*}^2\right),$$
for $\nu = \eps^2/2$. 
\end{lemma}
\begin{proof}
Assume that there is no such $i^*$ as in the lemma.
Over the choice of ${\mc{I}_{j^*}}$, consider the random variable 
$\sum_{(i,j^*) \in \mc{I}_{j^*}} c_{i,j^*,d^*}^2$. The
contribution from each $i$ to this sum is independently $0$ with
probability $(1- \eps)$ and
$c_{i,j^*,d^*}^2$ with probability $\eps$.
Thus,
$$\E_{\mc{I}_{j^*}}\left[\sum_{(i,j^*) \in \mc{I}_{j^*}}
c_{i,j^*,d^*}^2\right] = \eps\left(\sum_{i \in [k]}
c_{i,j^*,d^*}^2\right).$$
Now,
{\allowdisplaybreaks
\begin{align*}
&\Pr\left[\sum_{(i,j^*) \in \mc{I}}
c_{i,j^*,d^*}^2 \leq (\eps/2)\left(\sum_{(i,j^*) \in
[k]\times\{j^*\}\setminus \mc{I}}
c_{i,j^*,d^*}^2\right) \right] \nonumber \\
&\leq \Pr\left[\sum_{(i,j^*) \in \mc{I}_{j^*}}
c_{i,j^*,d^*}^2 \leq (\eps/2)\left(\sum_{i \in [k]}
c_{i,j^*,d^*}^2\right) \right] \nonumber \\
&\leq  
\Pr\left[\left|\sum_{(i,j^*) \in \mc{I}_{j^*}}
c_{i,j^*,d^*}^2 - \E\left[\sum_{(i,j^*) \in \mc{I}_{j^*}}
c_{i,j^*,d^*}^2\right]\right| \geq (\eps/2)\left(\sum_{i \in [k]}
c_{i,j^*,d^*}^2\right) \right]\\
&\overset{1}{\leq} 2\cdot\tn{exp}\left(-\frac{2(\eps/2)^2\cdot \left(\sum_{i \in [k]}
c_{i,j^*,d^*}^2\right)^2}{\sum_{i\in[k]} c_{i,j^*,d^*}^4}\right)
\nonumber \\
&\leq  2\cdot \tn{exp}\left(-\frac{(\eps^2/2) \cdot \left(\sum_{i \in [k]}
c_{i,j^*,d^*}^2\right)^2}{\max_{i\in[k]} c_{i,j^*,d^*}^2
\sum_{i\in[k]} c_{i,j^*,d^*}^2}\right)
 \leq   2\cdot \tn{exp}\left(-\eps^2/2\nu^2\right) \leq  \eps,
\end{align*}}
for $\nu^2 = \eps^4/4 \leq \eps^2/(2\log(2/\eps))$. Here, step $1$ follows from the Chernoff-Hoeffding inequality (\cref{thm:chernoff}). Since our choice of $\eps <
\xi/(8Td)$, this yields a contradiction
to our choice of $\mc{I}_{-j^*}$,  \eqref{eqn-main-nice-I-1},
and  \eqref{eqn-shared-heavy-def}.

\end{proof}



\subsection{Decoding a Labeling for $\mc{L}$}

\begin{figure}[t]
\begin{mdframed}
\begin{center} \textbf{Randomized Partial Labeling $\sigma$} \end{center}  
\begin{enumerate*}
\item Choose $j^* \in [T]$ and $d^* \in \{0,\dots, d-1\}$
independently and u.a.r.
\item Choose $v_j \in V$ independently and u.a.r. for each $j \in
[T]\setminus\{j^*\}$.
\item Choose the random subset $\mc{I}_{-j^*}$ of $[k]\times ([T]\setminus
\{j^*\})$ by independently adding each element with probability $\eps$.   
\item For each $v \in V$, \\[-1.5em]
\begin{enumerate*}
\item Set $v_{j^*} = v$.
\item Letting $P$ be the restriction of $P_{\tn{global}}$ to 
${\bf Y} = \{Y_{ij}\,\mid\, i\in[k], j \in [T]\}$, define
the set:
\begin{eqnarray}
\Gamma_0(v) & := & \left\{i' \in [k]\,\mid\, c_{i',j^*,d^*}^2 >
\frac{\nu^2}{4}\left(\sum_{i \in [k]} c_{i,j^*,d^*}^2\right)\right\},
\label{eqn-Gamma-0} 
\end{eqnarray} 
where $\nu = \eps^2/4$ (as in Lemma \ref{lem:Azuma-apply}). 
\item If $\Gamma_0(v)$ is non-empty, assign $v$ a label chosen
uniformly at random from $\Gamma_0(v)$.
\end{enumerate*}
\end{enumerate*}
\end{mdframed}
\caption{Randomized Partial Labeling}
\label{fig:labeling}
\end{figure}
In Figure \ref{fig:labeling} we define a randomized (partial) labeling $\sigma$ 
for the vertices $V$ of $\mc{L}$.
To analyze $\sigma$, we first define the
following random subsets of vertices and edges, where the randomness
is over the choices made in the above procedure of labeling.

\medskip
\noindent
{\bf Vertex subset $V_0 \subseteq V$:} Consists of all $v \in V$ such
that:
\begin{itemize}
\item Setting $v_{j^*} = v$, the choice of $\{v_j \,\mid\, j \in
[T]\}$ is good,
\item The choice of $(j^*, d^*)$ satisfies
\eqref{eqn-distinguished-I} and,
\item The choice of $\mc{I}_{-j^*}$ is shared-heavy.
\end{itemize}
Over the randomness of the labeling procedure and a random choice of
$v$, the above happens with probability at least:
\begin{equation}\label{eqn-Delta0}
\Delta_0 := \xi\cdot\frac{1}{Td}\cdot\frac{\xi}{8Td}.
\end{equation}
Thus, $$\E\left[|V_0|\right] \geq \Delta_0|V|.$$ 
Moreover, by the weak
expansion property in Theorem \ref{thm-LC-hardness},
\begin{equation}\label{eqn-E0}
\E\left[|E(V_0)|\right] \geq 
\E\left[\left(\slfrac{|V_0|}{|V|}\right)^2\right]\cdot (|E|/2)
 \geq   \left(\E\left[\slfrac{|V_0|}{|V|}\right]\right)^2\cdot
(|E|/2) 
 \geq  \left(\Delta_0^2/2\right)|E|.
\end{equation}

\medskip
\noindent
{\bf Edge Set $E' \subseteq E(V_0)$:} 
Let us first define for each $v \in V$
\begin{eqnarray}
\Gamma_1(v) & := & \left\{i' \in [k]\,\mid\, c_{i',j^*,d^*}^2 >
\frac{\nu^2}{100\cdot 4^{2R}}\left(\sum_{i \in [k]}
c_{i,j^*,d^*}^2\right)\right\},
\label{eqn-Gamma-1}
\end{eqnarray}
when $v_{j^*}$ is set to $v$ in Step 4a of Figure \ref{fig:labeling}.
Here,  $R$ is the parameter (to be set) from Theorem
\ref{thm-LC-hardness}.
From \eqref{eqn-Gamma-0} and \eqref{eqn-Gamma-1}, we have $\Gamma_0(v) \subseteq \Gamma_1(v)$ along with
\begin{equation}\label{eqn-Gamma-bd}
\left|\Gamma_0(v)\right| \leq 4/\nu^2, \ \ \ \tn{ and } \ \ \ \left|\Gamma_1(v)\right| \leq (100\cdot4^{2R})/\nu^2.
\end{equation}
The set $E'$ is defined as:
\begin{equation}\label{eqn-edgesEprime}
E' := \big\{e = (u,w) \in E(V_0)\,\mid\, \left|\pi_{e,u}(\Gamma_1(u))\right| = \left|\Gamma_1(u)\right|\tn{ and } 
\left|\pi_{e,w}(\Gamma_1(w))\right| = \left|\Gamma_1(w)\right|
\big\}.
\end{equation}
Since the graph $G$ of the instance $\mc{L}$ is regular, using second bound in \eqref{eqn-Gamma-bd} along with 
the smoothness property  of Theorem \ref{thm-LC-hardness}, the fraction of edges $e = (u,w) \in E$ that do not 
satisfy $$\Big(\left|\pi_{e,u}(\Gamma_1(u))\right| = \left|\Gamma_1(u)\right|\tn{ and } 
\left|\pi_{e,w}(\Gamma_1(w))\right| = \left|\Gamma_1(w)\right|\Big)$$ 
is at most,
$$\Delta_1 := \left(\frac{10^4\cdot 4^{4R}}{\nu^4J}\right).$$
Thus,
\begin{equation}\label{eqn-Eprimebd}
\E\left[\left|E'\right|\right] \geq \left(\Delta_0^2/2 - \Delta_1\right)\left|E\right|.
\end{equation}
The following lemma gives the desired property of edges in $E'$.
\begin{lemma}\label{lem-intersect-proj}
For every edge $e = (u,w) \in E'$, 
\begin{equation}\label{eqn-lem-intesect-proj}
\pi_{e,u}\left(\Gamma_0(u)\right)\cap \pi_{e,w}\left(\Gamma_0(w)\right) \neq \emptyset.
\end{equation}
\end{lemma}
\begin{proof}
Suppose for a contradiction that \eqref{eqn-lem-intesect-proj} does not hold for an edge $e = (u,w) \in E'$, i.e.
\begin{equation}\label{eqn-supposition}
\pi_{e,u}\left(\Gamma_0(u)\right)\cap \pi_{e,w}\left(\Gamma_0(w)\right) = \emptyset.
\end{equation}
Let us now define for $v \in\{u, w\}$, and $i \in [k]$, vector ${\bf C}_{v, i} \in \mathbb{R}^{\mcb{B}_{-j^*d^*}}$ where for
any $B \in \mcb{B}_{-j^*d^*}$
\begin{equation}
{\bf C}_{v, i}(B) = c_{(i,j^*),B} \ \ \ \tn{ when } v_{j^*}\tn{ is set to }v.
\end{equation}
Without loss of generality, we may assume that 
\begin{equation}\label{eqn-WLOG-umore}
\sum_{i\in [k]} \left\|{\bf C}_{u, i}\right\|_2^2 \ \geq \ \sum_{i\in [k]} \left\|{\bf C}_{w, i}\right\|_2^2.
\end{equation}
Since $u \in V_0$, \eqref{eqn-main-shared-heavy-I-3} and Lemma \ref{lem:Azuma-apply} imply that there exists $i_u \in [k]$ such that
\begin{equation}\label{eqn-u-distinguished}
\left\|{\bf C}_{u, i_u}\right\|_2 \geq \nu\left(\sum_{i\in [k]} \left\|{\bf C}_{u, i}\right\|_2^2\right)^{\frac{1}{2}} > 0.
\end{equation}
This implies that $i_u \in \Gamma_0(u)$.
Now, let $\ell^* := \pi_{e,u}(i_u)$. Since $P$ is a restriction of $P_{\tn{global}}$ which is a representation of the folded polynomial $\ol{P}_{\tn{global}}$, Lemma \ref{lem-folding-poly}
along with Remark \ref{rem-folding-poly} (applied to elements $B$ of $\mcb{B}_{-j^*d^*}$) implies
\begin{equation}\label{eqn-apply-folding}
\sum_{i \in \pi_{e,u}^{-1}(\ell^*)}{\bf C}_{u, i} = \sum_{i \in \pi_{e,w}^{-1}(\ell^*)}{\bf C}_{w, i}.
\end{equation}
On the other hand, since $e \in E'$, \eqref{eqn-edgesEprime} along with our supposition \eqref{eqn-supposition} and the construction of $\{\Gamma_r(v)\,\mid\, r\in\{0,1\},\ v\in\{u,w\}\}$  
implies that
\begin{itemize}
\item For all $i \in \pi_{e,u}^{-1}(\ell^*)\setminus\{i_u\}$
\begin{equation}\label{eqn-projbd-1}
\left\|{\bf C}_{u, i}\right\|_2 \leq \frac{\nu}{10\cdot 4^R} \left(\sum_{i\in [k]} \left\|{\bf C}_{u, i}\right\|_2^2\right)^{\frac{1}{2}}.
\end{equation}
\item For all $i \in \pi_{e,w}^{-1}(\ell^*)$
\begin{equation}\label{eqn-projbd-2}
\left\|{\bf C}_{w, i}\right\|_2 \leq \frac{\nu}{2} \left(\sum_{i\in [k]} \left\|{\bf C}_{w, i}\right\|_2^2\right)^{\frac{1}{2}}.
\end{equation}
\item There exists at most one $i' \in [k]$ such that,
\begin{equation}\label{eqn-projbd-3}
\left\|{\bf C}_{w, i}\right\|_2 > \frac{\nu}{10\cdot 4^R} \left(\sum_{i\in [k]} \left\|{\bf C}_{w, i}\right\|_2^2\right)^{\frac{1}{2}}.
\end{equation}
\end{itemize}
The above implications along with \eqref{eqn-apply-folding} and \eqref{eqn-WLOG-umore} yields
\begin{eqnarray}\label{eqn-mass-contra}
\left\|{\bf C}_{u, i_u}\right\|_2 & \leq &  \sum_{\substack{i \in \pi_{e,u}^{-1}(\ell^*)\\ i \neq i_u}} \left\|{\bf C}_{u, i}\right\|_2 + \sum_{i \in \pi_{e,w}^{-1}(\ell^*)}\left\|{\bf C}_{w, i}\right\|_2 \nonumber \\
& \leq & \frac{\nu \left|\pi_{e,u}^{-1}(\ell^*)\right|}{10\cdot 4^R} \left(\sum_{i\in [k]} \left\|{\bf C}_{u, i}\right\|_2^2\right)^{\frac{1}{2}} + 
 \left(\frac{\nu}{2} + \frac{\nu \left|\pi_{e,w}^{-1}(\ell^*)\right|}{10\cdot 4^R}\right) \left(\sum_{i\in [k]} \left\|{\bf C}_{w, i}\right\|_2^2\right)^{\frac{1}{2}} \nonumber \\
& \leq & \frac{\nu}{10}\left(\sum_{i\in [k]} \left\|{\bf C}_{u, i}\right\|_2^2\right)^{\frac{1}{2}} + \left(\frac{\nu}{2} + \frac{\nu}{10}\right) \left(\sum_{i\in [k]} \left\|{\bf C}_{w, i}\right\|_2^2\right)^{\frac{1}{2}} \nonumber \\
& \leq & \frac{7\nu}{10}\left(\sum_{i\in [k]} \left\|{\bf C}_{u, i}\right\|_2^2\right)^{\frac{1}{2}},
\end{eqnarray}
where we used the property (from Theorem \ref{thm-LC-hardness}) that $\left|\pi_{e,u}^{-1}(\ell^*)\right|, \left|\pi_{e,w}^{-1}(\ell^*)\right| \leq 4^R$. Clearly, \eqref{eqn-mass-contra}
 is a contradiction to \eqref{eqn-u-distinguished} which completes the proof of the lemma.
\end{proof}
Note that the set $E'$ is determined by Step 3 of the randomized labeling procedure. Lemma \ref{lem-intersect-proj} implies that in the subsequent steps of the procedure, each edge $e = (u,w) \in E'$ is satisfied with probability at least
$$\frac{1}{\left|\Gamma_0(u)\right|\left|\Gamma_0(w)\right|} \geq \frac{\nu^4}{16},$$
using the first bound in \eqref{eqn-Gamma-bd}. The above along with \eqref{eqn-Eprimebd} lower bounds the expected fraction of edges $\sigma$ satisfies by
$$\Delta_2 := \left(\Delta_0^2/2 - \Delta_1\right)\left(\frac{\nu^4}{16}\right).$$
Choosing $R$ to be large enough and $J \gg 4^{4R}$ we can ensure that $\Delta_2 > 2^{-c_0R}$ which yields a contradiction to the soundness of Theorem \ref{thm-LC-hardness}, completing the NO case analysis.


\subsection{Loose Ends}\label{sec:ends}

\paragraph{Discretization of the Basic PCP Test Distribution.}
Let $\mc{H}_N$ be the distribution of $\left(\sum_{i=1}^N B_i\right)/\sqrt{N}$
where each $B_i$ is an independent $\{-1,1\}$-valued balanced
Bernoulli random variable. The following theorem was proved in \cite{DOSW11}.

\begin{theorem} Fix any constant $D \geq 1$, and let $f(x_1,\dots,
x_m)$ be any degree-$D$ polynomial over $\mathbb{R}^m$. Let $(y, z)
\in \mathbb{R}^m\times \mathbb{R}^m$ be generated by sampling each
$(y_i,z_i)$ from $(N(0,1), \mc{H}_N)$ where $N = m^{24D^2}$. Then,
$$\Pr\left[\tn{sign}(f(y)) \neq \tn{sign}(f(z))\right] \leq O(1/m).$$
\end{theorem}

In our Basic PCP Test distribution (for a fixed choice of the vertices of
the \LC instance) we have $m = \Theta(kT)$ Gaussian random
variables. Choosing $D = d$ and $N = m^{24D^2}$, we can completely
discretize the test distribution using $\tn{exp}((kT)^{O(d^2)})$ points. Note
that this also incorporates the possible $2^{O(kT)}$ choices of the
noise set $\mc{I}$. From the above theorem, this discretization
results in an at most $O(1/kT)$ loss in the acceptance probability of
the test. This discretization is done for all
possible choices by the test of the vertices of the instance. 

\paragraph{Ruling out functions of constantly many degree-$d$ PTFs.}
Analogous to the argument in \cite{KS11}, consider any function $\ol{h}$ of
$K$ degree-$d$ PTFs (over $\mc{F}$) that passes the Final PCP test with probability $1/2 +
\xi$. Let $h$ be the function $\ol{h}$ with the PTFs represented over $\mathbb{R}^{\mc{Y}}$. 
By averaging, $h$ flips its sign with respect to flipping $b$ for at
least $\xi$ fraction of the rest of the choices made by the Basic PCP Test.
Again by averaging, there must be a degree-$d$ PTF $\tn{sign}\left(P'_{\tn{global}}\right)$ satisfying the
same for at least $\xi/K$ fraction of the choices. The entire
analysis can then be repeated using $P'_{\tn{global}}$.
