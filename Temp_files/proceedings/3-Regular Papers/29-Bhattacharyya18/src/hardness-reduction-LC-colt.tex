\section{Hardness Reduction}\label{sec:reduction}



The following reduction from \LC  directly implies our main theorem.
\begin{theorem}\label{thm-hardness-redn}
For any $\xi > 0$ and $d \in \mathbb{Z}^+$, there exists a choice of $R$ and $J$ in Theorem \ref{thm-LC-hardness} and
a  polynomial-time reduction from the corresponding \LC instance $\mc{L}$ to a set of point-sign pairs $\mc{Q} \subseteq \mathbb{R}^N\times \{-1,1\}$ such that:
\begin{itemize}
\item \textbf{YES Case.} If $\mc{L}$ is a YES instance, then there exists
a linear form $L$ satisfying
$$\Pr_{({\bf x}, s) \in \mc{Q}} \left[\tn{sign}\left(L({\bf x})\right) = s\right]
\geq 1 - \xi.$$

\item \textbf{NO Case.} If $\mc{L}$ is a NO instance, then for any degree-$d$
polynomial $P$
$$\Pr_{({\bf x}, s) \in \mc{Q}} \left[\tn{sign}\left(P({\bf x})\right) = s\right]
\leq \frac{1}{2} + \xi.$$
\end{itemize}

\end{theorem} 


The last sentence of \cref{thm:main} is justified in Appendix \ref{sec:ends}.

%\acks{We thank a bunch of people.}
\bibliography{Refs-LC-colt}

\appendix

\section{Appendix Preliminaries}


\subsection{Hermite Bases for Multivariate Polynomials}
For integer $d \geq 0$, the {\em Hermite polynomials} $H_d(x)$ are degree-$d$ univariate polynomials such that \\ $\E_{X \sim N(0,1)}[H_d(X)^2]=1$ and $\E_{X \sim N(0,1)}[H_d(X) H_{d'}(X)]=0$ for any $d \neq d'$. For example, $H_0(x) = 1$, $H_1(x) = x$, $H_2(x) = \frac{1}{\sqrt{2}}(x^2 - 1)$, and $H_3(x) = \frac{1}{\sqrt{6}}(x^3 - x)$. 

For $\mbf{d} \in \N^n$, we define $H_{\mbf{d}}(x_1, \dots, x_n) = \prod_{i \in [n]} H_{d_i}(x_i)$. For $D \geq 0$, let $\mc{H}_D = \{H_{\mbf{d}} : \mbf{d} \in \N^n, \sum_{i \in [n]} d_i \leq D\}$ denote the {\em Hermite basis for degree-$D$ polynomials}. The following is immediate.

\begin{fact}
	The set $\mc{H}_D$ forms an orthonormal basis for $n$-variate degree-$D$ polynomials whose inputs are drawn from $N(0,1)^n$. In particular, for any $P: \R^n \to \R$ of degree $\leq D$, we can write:
	$$P(x) = \sum_{\mbf{d} \in \N^n: \sum_i d_i \leq D} \hat{f}(\mbf{d}) \cdot H_{\mbf{d}}(x)$$ 
	and moreover, $\E_{x} P(x) = \hat{f}(\mbf{0})$ and $\E_x[P(x)^2] = \sum_{\mbf{d}} \hat{f}^2(\mbf{d})$.
\end{fact} 
\subsection{Concentration and Anti-Concentration}\label{sec-conc-anticonc}
The magnitude of polynomials in our analysis is controlled using the following standard bound.

\medskip
\noindent
\textbf{Chebyshev's Inequality.} For any random variable $X$ and $t > 0$, $\Pr\left[|X| > t\right] \leq \slfrac{\E[X^2]}{t^2}$.

\medskip
\noindent
The above is used in conjunction with \cite{CW01}'s powerful anti-concentration bound for polynomials over independent Gaussian variables.
\begin{theorem}	\label{thm:carbery-wright-prelim}
	\tn{(\cite{CW01})} Suppose $P: \R^\ell \to \R$ is a degree-$d$ polynomial over independent $N(0,1)$ random variables. Then,
	$$\Pr\left[|P| \leq \eps \|P\|_2\right] = O(d\eps^{1/d}).$$
\end{theorem}
In addition, we also use following Chernoff-Hoeffding bound.
\begin{theorem}[Chernoff-Hoeffding]\label{thm:chernoff}
	Let $X_1,\dots, X_n$ be independent random variables, each bounded as $a_i \leq X_i \leq b_i$ with $\Delta_i = b_i - a_i$ for $i = 1,\dots, n$. Then, for any $t > 0$,
	$$\Pr\left[\left|\sum_{i=1}^n X_i - \sum_{i=1}^n\E[X_i]\right| > t\right] \leq 2\cdot\tn{exp}\left(-\frac{2t^2}{\sum_{i=1}^n\Delta_i^2}\right).$$
\end{theorem}


\section{Hardness Reduction contd.}\label{sec:reduction-contd}
\subsection{The Basic PCP Test}\label{sec-basic-pcp-test}

\begin{figure}[t]
\begin{mdframed}
\begin{center}\textbf{The Basic PCP Test given instance $\mc{L}$ of \LC}\end{center}
\begin{enumerate*}

\item For each $j \in [T]$, the test chooses $T$ random vertices $v_1,v_2,\ldots,v_T \overset{u.a.r.}{\sim} V$.
Let $Y_{ij} := Y^{v_j}_i$.

\item Sample $\{\delta_{j}\, \mid\, j\in[T]\}$ from the joint Gaussian
	distribution where the marginals are $N(0,1)$,
	$\E[\delta_j\delta_{j'}] = -1/(T-1)$ for all $j\neq j'$, 
	and $\sum_{j=1}^T \delta_{j} = 0$. 


\item Sample $b \in \{-1,1\}$ uniformly at random.

\item Sample $\mc{I} \subseteq [k]\times[T]$ to be a random subset
where each $(i,j) \in [k]\times[T]$ is added to $\mc{I}$
independently with probability $\eps$.

\item For each $(i,j) \in ([k]\times[T])\setminus \mc{I}$, set $Y_{ij} := \sqrt{{(T-1)}/{T}}\cdot \delta_{j} +
b\eta.$

\item Independently for each $(i,j) \in \mc{I}$,
sample  $Y_{ij}$ from $N(0,{1})$. 

\item Set the variables of all other vertices (except $\{v_j
\mid\, j \in [T]\}$) to be $0$. Let this setting of the variables be the point ${\bf y} \in \mathbb{R}^{\mc{Y}}$.

\item Output the point-sign pair $({\bf y},b)$.

\end{enumerate*}
\end{mdframed}
\caption{Basic PCP Test}
\label{fig:pcptest}
\end{figure}



We begin with a Basic PCP Test given an instance
 $\mc{L}(G(V, E),k,L, \{\pi_{e,v}\}_{e \in E,v \in e})$ of \LC.  
For each vertex $v
\in V$, there is a set of variables $\{Y^v_i\}_{i=1}^k$, and the set
of all the variables $\mc{Y}$ is a union over all vertices $v \in V$ of these variable sets. 
The test is described by  
the sampling procedure in Figure \ref{fig:pcptest}, and yields a distribution over point-sign pairs which is independent of the constraints in $\mc{L}$. It uses some additional parameters set as follows: $T \coloneqq 10d$, $\eps \coloneqq (\xi/32Td)$, $\eta \coloneqq \Big(\frac{\eps\xi}{20kdT}\Big)^{d6^{3d}}$, where $d$ is from the statement of Theorem \ref{thm-hardness-redn}. 


\subsubsection{Folding over constraints of $\mc{L}$} \label{sec-folding}
To ensure consistency across the edges of $\mc{L}$, the points generated by the Basic PCP Test are \emph{folded} over a specific subspace.  The points generated by the Basic PCP Test reside in the space $\mathbb{R}^{\mc{Y}}$. 
Now, for a fixed $e=(u,w) \in E$ and $j \in [L]$, we define the vector ${\bf h}^{e}_j \in \mathbb{R}^{\mc{Y}}$ as 
\begin{align}
	{\bf h}^e_j(Y^v_i) &= \begin{cases}
		1 & \mbox{ if } v = u\tn{ and } i \in \left(\pi_{e,u}\right)^{-1}(j), \\
		-1 &\mbox{ if } v = w\tn{ and } i \in \left(\pi_{e,w}\right)^{-1}(j), \\
		0 & \mbox{otherwise.}
		\end{cases}
\end{align}
Let $\mc{H} \subseteq \mathbb{R}^{\mc{Y}}$ be the subspace formed by the linear span of the vectors $\{{\bf h}^e_{j}\}_{e \in E, j \in [L]}$, and let $\mc{F}$ be the orthogonal complement of $\mc{H}$ in $\mathbb{R}^{\mc{Y}}$, i.e. $\mathbb{R}^{\mc{Y}} = \mc{H}\oplus \mc{F}$ and $\mc{H}\perp \mc{F}$. For each point-sign pair $({\bf y},b)$ generated by the Basic PCP Test, construct $(\ol{{\bf y}},b)$ where $\ol{\bf y}$ is the projection of ${\bf y}$ onto the subspace $\mc{F}$, represented in some (fixed) orthogonal basis for $\mc{F}$. 

Conversely, for any vector $\ol{\bf z} \in \mc{F}$, let ${\bf z}$ be its representation in $\mathbb{R}^{\mc{Y}}$. It is easy to see that such a ${\bf z}$ satisfies: for every $e=(u,w) \in E$ and  $j \in [L]$, $\langle {\bf z}, {\bf h}^{e}_j \rangle = 0$ which is equivalent to
\begin{equation}					\label{eq:folding_constraint}
\tn{\it Constraint }\ \ {\mc{C}_{e,j}:} \qquad\qquad \sum_{i \in \left(\pi_{e,u}\right)^{-1}(j)} {\bf z}(Y^u_i)  = \sum_{i \in \left(\pi_{e,w}\right)^{-1}(j)} {\bf z}(Y^w_i). 
\end{equation}
For our purpose we shall extend the above constraint to polynomials as well. Consider a polynomial $Q$ in $\mathbb{R}^{\mc{Y}}$. For any monomial $M$ over the variables $\mc{Y}$, let $c_{Q,M}$ be its coefficient in $Q$. Fix an edge $e = (u,w)$ and $j \in [L]$, and a monomial $M$ such that $M$ does not contain any variable from the set $\{Y^u_i\, \mid\, i\in \left(\pi_{e,u}\right)^{-1}(j)\}\cup \{Y^w_i\, \mid\, i\in \left(\pi_{e,w}\right)^{-1}(j)\}$. For such a choice of $e, j,$ and $M$ we say that $\mc{C}_{e,j,M}$ is a \emph{valid} constraint where:
\begin{equation}					\label{eq:folding_constraint_poly}
\tn{\it Constraint }\ \ {\mc{C}_{e,j,M}:} \qquad\qquad \sum_{i \in \left(\pi_{e,u}\right)^{-1}(j)} c_{Q,M\cdot Y^u_i}  = \sum_{i \in \left(\pi_{e,w}\right)^{-1}(j)} c_{Q,M\cdot Y^w_i}. 
\end{equation}
We have the following lemma.
\begin{lemma}\label{lem-folding-poly}
Let $\ol{Q}$ be a polynomial that resides in $\mc{F}$, i.e. is represented in an orthogonal basis\footnote{A polynomial $\ol{Q}$ being represented in an orthogonal basis for a subspace $\mc{F}$ means $\ol{Q}$ can be written as a polynomial over the linear forms corresponding to an orthogonal basis for $\mc{F}$.} for $\mc{F}$, and let $Q$ be its representation in $\mathbb{R}^{\mc{Y}}$. Then, $Q$ satisfies all valid constraints $\mc{C}_{e,j,M}$.
\end{lemma}
\begin{proof}
Suppose for a contradiction $Q$ does not satisfy a valid constraint $\mc{C}_{e, j, M}$. Consider the vector ${\bf r}$ where,
$${\bf r}(Y^v_i) = \begin{cases}
c_{Q,M\cdot Y^u_i} & \tn{ if } v=u, i \in \left(\pi_{e,u}\right)^{-1}(j)\\
c_{Q,M\cdot Y^w_i} & \tn{ if } v=w, i \in \left(\pi_{e,w}\right)^{-1}(j)\\
0 & \tn{ otherwise.}
\end{cases}$$
Since Equation \eqref{eq:folding_constraint_poly} is not satisfied, it is easy to see that $\langle {\bf r}, {\bf h}^e_j\rangle \neq 0$, and thus ${\bf r} = {\bf r}_0 + {\bf r}_1$ where  ${\bf r}_0 \in \mc{F}$ and ${\bf r}_1 \in \mc{H}$. On the other hand, consider an orthogonal basis $\mc{B}$ for $\mathbb{R}^{\mc{Y}}$ that is an extension of $\{{\bf r}_1\}$, i.e. ${\bf r}_1$ is an element of $\mc{B}$. $P$ can now be represented as:
$$P \equiv  {\bf r}_1[\mc{Y}]\cdot P_1 + P_0,$$
where $P_1$ is a polynomial represented in $\mc{B}$, $P_0$ is represented in $\mc{B}\setminus\{{\bf r}_1\}$, and ${\bf r}_1[\mc{Y}]$ is the $\mc{Y}$-linear form $\sum_{Y \in \mc{Y}} {\bf r}_1(Y)\cdot Y$. Note that $P_1$ is not identically zero, in particular it contains the monomial $M$.
This implies that $P$ cannot be represented over any basis for $\mc{F}$, which is a contradiction.  
\end{proof}
\begin{remark}\label{rem-folding-poly}
Instead of monomials $M$, the constraints in \eqref{eq:folding_constraint_poly} analogously hold for elements $B$ of a basis $\mcb{B}$ for polynomials over any set of variables not containing $\{Y^u_i\, \mid\, i\in \left(\pi_{e,u}\right)^{-1}(j)\}\cup \{Y^w_i\, \mid\, i\in \left(\pi_{e,w}\right)^{-1}(j)\}$.
\end{remark}


  

\subsection{ The Final PCP Test}\label{sec-final-pcp-test}
Given a degree-$d$ polynomial $\ol{P}_{\tn{global}}$ over the space $\mc{F}$, the test samples $({\bf y},b)$ from the Basic PCP Test (as described in Figure \ref{fig:pcptest}), and constructs $(\ol{\bf y},b)$ as described in Appendix \ref{sec-folding}. The test accepts {\it iff} ${\rm sign}\left(\ol{P}_{\tn{global}}(\ol{\bf y})\right)=b$.

		
\begin{remark}The Basic PCP Test generates a distribution over $\mathbb{R}^{\mc{Y}}\times \{-1,1\}$ using
various independently Gaussian random variables. Therefore, the support set of this distribution is not finite. In Appendix \ref{sec:ends}, using techniques from \cite{DOSW11}, we discretize the Basic PCP Test. 
Building upon the discretized Basic PCP Test, the Final PCP Test yields the desired finite subset $\mc{Q}$ in polynomial time. 
\end{remark}






