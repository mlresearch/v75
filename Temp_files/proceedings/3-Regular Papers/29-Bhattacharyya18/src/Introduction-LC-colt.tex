\section{Introduction}

Given a distribution $\mc{D}$ over $\{-1,1\}$-labeled points in
$\mathbb{R}^n$, the accuracy of a classifier function $f : \mathbb{R}^n \to
\{-1,1\}$ is the probability that $f(x) = \ell$ for a random point-label pair $(x, \ell)$
sampled from $\mc{D}$. 
A concept class $\mc{C}$ is said to be \emph{learnable}
by hypothesis class $\mc{H}$ if there is an efficient procedure which,
given access to samples from any distribution $\mc{D}$ consistent with some $f \in
\mc{C}$, generates with high probability a classifier $h \in \mc{H}$ of
accuracy approaching that of $f$ for $\mc{D}$. When $\mc{H}$ can be
taken as $\mc{C}$ itself, the latter is said to be \emph{properly} learnable. The
focus of this work is one
of the simplest and most well-studied concept classes: the \emph{halfspace} which
maps $x \in \mathbb{R}^n$ to $\tn{sign}(\langle v, x\rangle - c)$ for
some $v \in \mathbb{R}^n$ and $c \in \mathbb{R}$. The study of
halfspaces goes back several decades to the development of various
algorithms in artificial intelligence and machine learning such as the
Perceptron~\citep{Rosenblatt62,MP69} and  SVM~\citep{CV95}.  Since then,
halfspace-based classification has found applications in many other
areas, such as computer vision~\citep{OJM90} and
data-mining~\citep{RRK04}.





It is known that a halfspace can be properly learnt by using linear
programming along with a polynomial number of samples to compute a
separating hyperplane~\citep{BEHW89}. In noisy data however, it is not
always possible to find a hyperplane separating the differently
labeled points. Indeed, in the presence of (adversarial) noise, 
i.e.  the \emph{agnostic} setting, proper learning of a halfspace to
optimal accuracy with no distributional assumptions  
was shown to be NP-hard by \cite{JP78}.  Subsequent results showed the hardness of
approximating the accuracy of properly learning a noisy halfspace to
constant factors: $\tfrac{262}{261}-\eps$ by \cite{AK98}, $\tfrac{418}{415}-\eps$ by \cite{BDEL00}, and $\tfrac{85}{84}-\eps$ by \cite{BB06}. These results were considerably strengthened 
independently by \cite{FGKP09} and by \cite{GR09}\footnote{The reduction of \cite{GR09}
works even for the special case when the points are over the boolean hypercube.} 
who proved  hardness of even \emph{weakly}
proper learning a noisy halfspace, i.e. 
to an accuracy 
beyond the random threshold of $1/2$. This implies an optimal
$(2-\eps)$-inapproximability in terms of the learning accuracy. 
Building upon these works \cite{FGRW12} showed that the same hardness holds 
for learning noisy monomials (OR functions over the boolean hypercube) using
halfspaces. 





At this point, it is  natural to ask whether the halfspace learning problem remains hard
if the classifier is allowed to be from a larger class of functions, i.e., {\em non-proper} learning.
In particular, consider the class of degree-$d$ \emph{polynomial
threshold functions} (PTF) which are given by mapping $x \in
\mathbb{R}^n$ to $\tn{sign}(P(x))$ where $P$ is a degree-$d$ polynomial.
They generalize halfspaces a.k.a.~\emph{linear threshold functions}
(LTFs) which are degree-$1$ PTFs and are very common hypotheses in machine learning
because they are output by kernelized models (e.g., perceptrons, SVM's, kernel k-means, kernel PCA, etc.) when instantiated with the polynomial kernel. From a complexity viewpoint,
PTFs were studied by  \cite{DOSW11} who showed the hardness of weakly  proper 
learning a noisy degree-$d$ PTF for any constant $d \in
\mathbb{Z}^+$, assuming Khot's Unique Games Conjecture
(UGC)~\citep{Khot02}. On the other hand, proving the hardness of
weakly learning noisy halfspaces using degree-$d$ PTFs has turned out
to be quite challenging. Indeed, the only such result is by
\cite{DOSW11} who showed the corresponding hardness 
of learning using a degree-$2$ PTF. With no further
progress till now, the situation remained unsatisfactory. 

In this work, we significantly advance our understanding  by proving  the hardness 
of weakly learning an $\eps$-noisy halfspace by a degree-$d$ PTF \emph{for any
constant} $d \in \mathbb{Z}^+$. Our main
result is formally stated as follows.

\begin{theorem}\tn{(This work)}\label{thm:main} For any constants $\delta > 0$, and $d
\in \mathbb{Z}^+$, it is \NP-hard to decide whether a given
set of $\{-1,1\}$-labeled points in $\mathbb{R}^n$ satisfies:

\smallskip
\noindent
~~~~~~~\tn{\textbf{YES Case.}} There exists a halfspace that correctly classifies
$(1-\delta)$-fraction of the points, or

\smallskip
\noindent
~~~~~~~\tn{\textbf{NO Case.}} Any degree-$d$ PTF
classifies at most $(1/2+\delta)$-fraction of the points
correctly.

\smallskip
\noindent
The \tn{NO} case
 can be strengthened to rule out any function of constantly many
degree-$d$ PTFs.
\end{theorem}

To place our results in context, we note that algorithmic results for
learning noisy halfspaces are known under assumptions on the
distribution of the noise or the pointset.
In the presence of \emph{random
classification noise}, \cite{BFKV96}
gave an efficient learning algorithm approaching optimal accuracy, 
which was improved by \cite{Cohen97}
who showed that in this case the halfspace can in fact 
be properly learnt. For
certain well behaved distributions, \cite{KKMS05} showed that halfspaces can be learnt even in
the presence of adversarial noise. Subsequent works by 
\cite{KLS09}, and \cite{ABL17} improved the noise
tolerance and introduced new algorithmic techniques. Building upon
them, \cite{Daniely15} recently obtained a PTAS for minimizing
the hypothesis error with respect to the uniform distribution over a sphere.
Several of these learning algorithms
use halfspaces and low degree PTFs 
(or simple combinations thereof) as their hypotheses, 
and one could conceivably apply their techniques to the setting 
 without any distributional
assumptions. Our work provides evidence to the contrary.


\subsection{Previous related work}


Hypothesis-independent intractability results for 
learning for halfspaces are also known, but they make average-case or
cryptographic hardness assumptions which seem considerably stronger than
\Pclass$\neq$\NP.  Specifically, for exactly learning noisy halfspaces,
such results have been shown in the works of \cite{FGKP09}, \cite{KKMS05}, \cite{KK14}, and \cite{DS16}. In a
recent interesting work, \cite{Daniely16} rules out weakly learning
noisy halfspaces by \emph{efficient algorithms} assuming the intractability of strongly refuting random
$K$-XOR formulas. In particular, their reduction ensures that random instances of $K$-XOR map to random instances of learning halfspaces. Since the number of different hypotheses that an efficient algorithm can output can be upper bounded as a function of its running time, by a union bound argument, it follows that no hypothesis in the hypothesis class can give non-trivial guarantees under the random distribution. The hardness is based on a non-standard average-case assumption that for some particular clause density, the problem of distinguishing between nearly satisfiable $K$-XOR formulas and uniformly random ones is computationally hard. As a complexity theoretic hardness result, Theorem \ref{thm:main} has arguably broader consequences by showing that a polynomial time algorithm for weakly learning noisy halfspaces using constant degree PTFs yields polynomial time algorithms %for \textsc{SAT} and (consequently) 
\emph{for all problems in} NP, rather than only for refuting random $K$-XOR formulas as implied by \cite{Daniely16}'s result (though the latter is applicable to learning algorithms with unrestricted hypotheses). 


%On the other hand, \cite{Daniely16}'s result implies that existence of polynomial time algorithm for non-proper hypothesis independent learning of halfspaces would imply polynomial algorithm for refuting random $K$-XOR formulas. At present, such an algorithm does not necessarily lead to polynomial time algorithms for other problems in NP.    

On the other hand, \cite{ABX08} have shown that hypothesis-independent
hardness results under standard complexity assumptions would imply a
major leap in our current understanding of complexity theory and are
unlikely to be obtained for the time being. Therefore, any 
study (such as ours) of the standard complexity-theoretic hardness of learning
halfspaces would probably need to constrain the hypothesis. 


A natural generalization of the learning halfspaces problem is that of
learning intersections of two or more halfspaces.  Observe that unlike
the single halfspace, properly learning the intersection of two
halfspaces without noise does not in general admit a
separating hyperplane based solution.  Indeed, this problem was shown
to be NP-hard by \cite{BR93}, later strengthened by
\cite{ABFKP08} to
rule out intersections of constantly many halfspaces as hypotheses.
The corresponding hardness of even weak learning was established by
\cite{KS11}, while \cite{KS09}
proved under a cryptographic hardness assumption the intractability of
learning the intersection of $n^\eps$ halfspaces. Algorithms
for learning intersections of constantly many halfspaces have been
given in the works of \cite{BK97} and
\cite{Vempala97} for the uniform distribution over the unit
ball, ~\cite{KOS04} for the uniform
distribution over the boolean hypercube, and by \cite{AV06} and \cite{KlivansS08} for instances
with good \emph{margin}, i.e. the points being well separated from the
hyperplanes.


As was the case for learning a single noisy halfspace, there is
no known NP-hardness for learning intersections of two halfspaces
using (intersections of) degree-$d$ PTFs. This cannot, however, be
said of the finite field analog of learning halfspaces, i.e. the
problem of learning noisy parities over $\Ft$. While
H\r{a}stad's seminal work \cite{Hastad} itself rules out
weakly proper learning a noisy parity over $\Ft$, later work of
\cite{GKS10} showed the hardness of
learning an $\eps$-noisy parity by a degree-$d$ PTF to
within $(1 - 1/2^d + \eps)$-accuracy  -- which, however, 
is not optimal for $d > 1$.
Shortly thereafter, \cite{Khot-personal} 
observed\ignore{\footnote{Khot's observations
remained unpublished for while, before they were included with his
permission by \cite{BGGS16} in their paper which
made a similar use of \cite{Viola09}'s pseudo-random generator.}}
that Viola's pseudo-random generator~\cite{Viola09} fooling degree-$d$
PTFs can be combined with coding-theoretic inapproximability 
results to yield
optimal lower bounds for all constant degrees $d$. From the
algorithmic perspective, one can learn an $\eps$-noisy parity over the
uniform distribution in $2^{O(n/\log n)}$-time as shown by \cite{FGKP09} and \cite{BKW03}. For general
distributions, \cite{KMV08} gave a
non-proper $2^{O(n/\log n)}$-time algorithm achieving an accuracy
close to optimal. 

Several of the inapproximability results mentioned above, e.g.~those of \cite{GR09}, \cite{GKS10}, \cite{KS11}, \cite{FGRW12} and \cite{DOSW11},
follow the \emph{probabilistically checkable proof
(PCP) test} based approach for
their hardness reductions. While our result builds upon these methods,
in the remainder of this section, we give an overview of our techniques
and describe the key enhancements which allow us to overcome some of
the technical limitations of previous hardness reductions.

\subsection{Overview of Techniques}

For hardness reductions, due to the uniform convergence results of \cite{Hau92, KSS94}, it is sufficient to take the optimization 
version of the learning halfspaces problem which consists of a set of
coordinates and a finite set of labeled points, the latter 
replacing a random distribution. A typical reduction (including ours)
given a hard instance of a constraint satisfaction problem (CSP) $\mc{L}$ over
vertex set $V$ and label set $[k]$, defines $\mc{C} := V\times [k]$
to be the set of coordinates over $\mathbb{R}$.
We let the formal variables
$Y_{(w,i)}$ be associated with the coordinate $(w,i) \in \mc{C}$.
The hypothesis $H$ (the
\emph{proof} in PCP terminology) is defined
over these variables. In our case, the proof will be a degree-$d$ PTF. The PCP test chooses
randomly a small set of vertices $S$
of $\mc{L}$, and runs a \emph{dictatorship} test on $S$: it tests $H$ on a set
of labeled points $P_S \subseteq \mathbb{R}^{\mc{C}}$ generated by the
dictatorship test. We desire the following two properties from the test: 
\begin{itemize}
\item \textbf{(completeness)} if $H$
``encodes'' a
good labeling for $S$, then it is a good classifier for  $P_S$,
\item \textbf{(soundness)} a good classifier $H$ for
$P_S$ can be ``decoded'' into a good labeling for $S$. 
\end{itemize}
 The soundness
property is leveraged to show that if $H$ classifies
$P_S$ for a significant fraction of the choices $S$, it can be used to
define a good global labeling for $\mc{L}$.
The CSP of choice in the above template is usually the Label Cover or
the Unique Games problem. While the NP-hardness of Label Cover is
unconditional, its projective constraints seem to present 
technical roadblocks --
also faced by \cite{DOSW11} -- in analyzing 
learnability by  degree-$d$  ($d > 2$) PTFs.

Our work overcomes these issues and gives a hardness reduction from
Label Cover. The key ingredient to incorporate the Label Cover
projective constraints is a \emph{folding} over an appropriate
subspace defined by them. This amounts to restricting the entire instance
to the corresponding orthogonal subspace.  Similar folding for analyzing linear forms
has been used earlier in the works of \cite{KS11},
\cite{FGRW12}, and \cite{GRSW}. We are able to extend it over
degree-$d$ polynomials leveraging the linear-like structure decoded by
an appropriate dictatorship test. This uses a \emph{smoothness} property of 
the constraints (analogous to \cite{KS11,FGRW12,GRSW})  
of the Label Cover instance which is combined with the 
dictatorship test -- along with folding --
to yield the PCP test. 






In the rest of this section, we informally describe our dictatorship
test, the motivation behind its design and the key ingredients
involved in its analysis. To begin, we present a simple preliminary
dictatorship test $\mathfrak{P}_0$ over $\mathbb{R}^{k}$ which works for linear thresholds. 
Of course, the NP-hardness of properly learning noisy halfspaces is
already known~\citep{FGKP09,GR09}, so this test does not yield anything
new.
Our purpose is illustrative and we include a sketch
of the arguments of its analysis. 
Taking $\eps > 0$ as a
small constant and  $\eta > 0$ a small parameter (to be defined
later), the description of $\mathfrak{P}_0$ is given in Figure \ref{fig:pcptest-0-intro}.
\begin{figure}
\begin{mdframed}
\begin{center} $\mathfrak{P}_0(\mathbb{R}^k, \eta, \eps)$ tests halfspace
$\tn{sign}(f(Y))$. \end{center}
\begin{enumerate*}

\item Sample $b \in \{-1,1\}$ uniformly at random.

\item Choose a random ``noise'' subset $\mc{I} \subseteq [k]$ by
including each $i$ independently with probability $\eps$.

\item For $i \in [k]\setminus\mc{I}$,  
set
$y_i = b\eta$,

\item For $i \in \mc{I}$, sample $y_i$ 
i.i.d. at random from $N(0,1)$.


\item Accept iff $\tn{sign}(f(y)) =  b$.
\end{enumerate*}
\end{mdframed}
\caption{Dictatorship Test $\mathfrak{P}_0$}
\label{fig:pcptest-0-intro}
\end{figure}

  
Observe that the linear threshold $\tn{sign}(Y_i)$ for each $i \in
[k]$ correctly classifies $(y,b)$ with probability $(1-\eps)$. 
In other words, every \emph{dictator} corresponds to a good
solution.


\subsubsection{Soundness analysis of
$\mathfrak{P}_0$}\label{sec-intro-soundness-P0}
Suppose there exists a linear form $f =  
\sum_{i \in [k]}\wh{f}_iY_i$ (assuming for
simplicity $f$ has no
constant term) such that
$\tn{sign}(f)$ passes $\mathfrak{P}_0$
 with probability $1/2 +
2\xi$ for
some $\xi = \Omega(1)$. Using (by now) standard analytical
arguments, we show that there exists $i^* \in [k]$ such that
\begin{equation}
\wh{f}_{i^*}^2 \geq
\Omega(1)\cdot\sum_{i\in[k]}\wh{f}_{i}^2 > 0. 
\label{eqn-into-example-bound}
\end{equation}
In other words, every good solution $f$ can be decoded into a 
dictator.

\medskip
It is not particularly challenging to obtain \eqref{eqn-into-example-bound}.
However, we sketch a systematic proof which shall be useful
when analyzing a more complicated dictatorship test for PTFs.  

Call a setting of $\mc{I}$ \emph{good} if $\tn{sign}(f)$ passes the test
conditioned on $\mc{I}$ with probability $1/2 + \xi$. By averaging,
it is easy to see that $\Pr_{\mc{I}}\left[\mc{I}\tn{ is good}\right] \geq
\xi/2$.  Let us fix such a good
$\mc{I}$. Without loss of generality, we may assume that $\mc{I} = \{k^* + 1, \dots, k\}$ and
further that $k^* \geq k/2$ by the Chernoff bound. We now define 
 $\{W_1, \dots, W_{k^*}\}$ as a basis for $\{Y_{i}\,\mid\,i \in [k^*]\}$
where $W_1 := (1/k^*)\sum_{i\in[k^*]} Y_{i}$, 
such that $\{W_1, \dots, W_{k^*}\}$ is an orthogonal
transformation of $\{Y_{i}\,\mid\,i \in [k^*]\}$ of the same $1/\sqrt{k^*}$ 
norm. 
Thus, we may rewrite $f$ as:
\begin{equation}\label{eqn-fYW-intro}
f = \sum_{i \in
[k]\setminus[k^*]}\tilde{f}_{i}Y_{i} + \sum_{\ell \in [k^*]}
\ol{f}_{\ell}W_\ell.
\end{equation} The variables in the first sum in the RHS of the
above are all i.i.d. $N(0,1)$. Further, it can be seen that under the
test distribution, $W_1 = b\eta$, and $W_\ell = 0$ ($\ell
= 2,\dots, k^*$). Therefore, we may assume that,
\begin{equation}
\ol{f}_1^2 > 0.\label{eqn-mass-nonzero-intro}
\end{equation}
Since the sign of $f$ must flip with that of $b$
with probability $\Omega(\xi) = \Omega(1)$, one can apply
Carbery-Wright's Gaussian
anti-concentration theorem to show that,
\begin{equation}\label{eqn-apply-anti-conc-intro}
\sum_{i \in
[k]\setminus[k^*]}\tilde{f}_{i}^2 \leq O(\eta^2)\ol{f}^2_{1}, 
\end{equation}
since otherwise, contributions from the first sum of \eqref{eqn-fYW-intro}  will 
overwhelm the contribution of $W_1$ to $f$.
Further, from the definition of $\{W_\ell\}_{\ell=1}^{k^*}$, we obtain
\begin{equation}
\sum_{i \in [k^*]}\tilde{f}_{i}^2 = \frac{1}{k^*}\sum_{\ell \in [k^*]}
\ol{f}_{\ell}^2 \geq  \ol{f}_1^2/k^*. \label{eqn-unwinding-intro}
\end{equation}
Let us now revert to the notation with $\mc{I} = [k]\setminus[k^*]$. 
Using   \eqref{eqn-unwinding-intro} along with   \eqref{eqn-apply-anti-conc-intro},
and taking $\eta = o(\eps^3/\sqrt{k})$ one can ensure that,
\begin{equation}
\sum_{i \in \mc{I}}\tilde{f}_{i}^2 \leq
\frac{\eps}{10}\sum_{i \in [k]}\tilde{f}_{i}^2,
\label{eqn-1-bound-intro}
\end{equation}
and from   \eqref{eqn-mass-nonzero-intro} we obtain
\begin{equation}
\sum_{i \in [k]}\tilde{f}_{i}^2 > 0. 
\label{eqn-3-bound-intro}
\end{equation}
Note that   \eqref{eqn-1-bound-intro} holds for every good
$\mc{I}$ which is at least $\xi/2$ fraction of the choices of
$\mc{I}$. Randomizing over $\mc{I}$, an application of the Chernoff-Hoeffding
bound shows that \eqref{eqn-1-bound-intro} holds only with substantially smaller
probability unless there exists $i^* \in [k]$ such that:
\begin{equation}
\tilde{f}_{i^*}^2 \geq \frac{\eps^3}{8}\sum_{i \in
[k]}\tilde{f}_{i}^2. \label{eqn-heavy-i-intro}
\end{equation}
The desired bound in   \eqref{eqn-into-example-bound} now easily follow
from \eqref{eqn-3-bound-intro} and
\eqref{eqn-heavy-i-intro}.
The details are omitted.

The main idea of the above 
methodical analysis is a natural definition of the $W$
variables using which we isolate the sign-perturbation $b\eta$ into a
single variable $W_1$! Gaussian anti-concentration directly lower bounds 
the squared mass corresponding to $W_1$. Moreover, when transforming back 
to the squared mass of $Y_{i}$ ($i \in [k]\setminus\mc{I}$), the presence
of the heretofore ignored $W_{\ell}$ ($\ell > 1$) terms can only increase this quantity, as shown
in \eqref{eqn-unwinding-intro}. Lastly, the 
the ``decoding list size''  does not depend on the
sign-perturbation parameter $\eta$ which can be taken to
be small enough to makes sure that this size is a constant
depending only on the noise parameter $\eps$ and
the marginal acceptance probability $\xi$ of the test.



\subsubsection{Enhancing the Dictatorship Test for degree-$d$ PTFs}


Our goal is a reduction proving the hardness of weakly
learning noisy halfspaces using degree-$d$ PTFs. One could hope
to utilize the dictatorship test $\mathfrak{P}_0$ itself for this purpose.
Unfortunately, this presents problems even for $d = 5$. To see this
consider  
the
degree-$5$ polynomial,
$$f(Y) = Y_{i^*}^3\left(\sum_{i\in[k]\setminus\{i^*\}}Y^2_i\right),$$
for some distinguished $i^* \in [k]$.
It is easy to see that $\tn{sign}(f)$ passes the test with probability close to
$1$. However, the distinguished variable $Y_{i^*}$ appears with a cubic
power in $f$, whereas the folding approach works well only when $Y_{i^*}$
occurs as a linear factor of some sub-polynomial. This is due to the inherently
linear nature of the folding constraints. Consequently, 
when $\mathfrak{P}_0$ is combined with a Label Cover
instance the analysis becomes infeasible. 

Our approach to overcome this bottleneck is for the PCP to test
several independently and randomly chosen vertices. For this, the
dictatorship test would be on the domain
$\mathbb{R}^{[k]\times[T]}$ where
$T$ is chosen much larger than the degree $d$ of the PTF to be tested.
The space $\mathbb{R}^{[k]\times[T]}$ is thought of as real space
spanned by $T$ blocks of $k$ dimensions each. 
In this case, if the test passes with probability $ > 1/2$, then there
is a way to decode a good label to at least one out of the $T$
blocks. 
A key step in our analysis crucially leverages the choice of $T$ to extract
out a specific sub-polynomial which is linear in the variables of one of the
$T$ blocks. This
is done via an application of the following lemma which is
proved in Appendix \ref{sec:struct}.
\begin{lemma}\label{lem-approx-structural-intro}
Given a degree-$d$ polynomial of the form $(Y_1 + \dots +
Y_T) \cdot S(Y_1,\dots, Y_T)$, where $T  > 2d$ and $S$ is a degree-$(d-1)$ polynomial, there exist at least $T/2$ indices $j \in [T]$
such that: for each such $j$, the sum of squares of the coefficients corresponding to the
terms (in the monomial representation) linear in $Y_j$ is
at least $c$ times the sum of squares of coefficients of $S$,
where $c := c(T,d) > 0$.  
\end{lemma}

In Figure \ref{fig:pcptest-1-intro}, 
we give a formal description of the Dictatorship test $\mathfrak{P}_1$
employed by our reduction. Its analysis builds upon that of
$\mathfrak{P}_0$ above, so we provide a short sketch.
Let $T = 10d$ and $\eps > 0$ be a constant, and $\eta > 0$ be
parameter to be defined later.
\begin{figure}
\begin{mdframed}
\begin{center}PCP Test $\mathfrak{P}_1(\mathbb{R}^{[k]\times[T]}, \eta, \eps)$ tests
degree-$d$ PTF
$\tn{sign}(P(Y))$ \end{center}

\begin{enumerate*}




 
\item Sample $\{\delta_{j}\, \mid\, j\in[T]\}$ from the joint Gaussian
	distribution where the marginals are $N(0,1)$,
	$\E[\delta_j\delta_{j'}] = -1/(T-1)$ for all $j\neq j'$, 
	and $\sum_{j=1}^T \delta_{j} = 0$. 





\item Sample $b \in \{-1,1\}$ uniformly at random.

\item Sample $\mc{I} \subseteq [k]\times[T]$ to be a random subset
where each $(i,j) \in [k]\times[T]$ is added to $\mc{I}$
independently with probability $\eps$.

\item For each $(i,j) \in ([k]\times[T])\setminus \mc{I}$, set
$y_{ij} = (\sqrt{(T-1)/T})\delta_{j} +
b\eta$.

\item Independently for each $(i,j) \in \mc{I}$,
sample  $y_{ij} \sim N(0,{1})$. 


\item Accept iff $\tn{sign}(P(y)) = b$.

\end{enumerate*}
\end{mdframed}
\caption{Dictatorship Test $\mathfrak{P}_1$}
\label{fig:pcptest-1-intro}
\end{figure}
Consider the linear
threshold given by,
$$\tn{sign}\left(\sum_{j=1}^TY_{i_jj}\right),$$
for any $i_j \in [k]$ ($1\leq j \leq T$). It is easy to see that this
passes the test with probability at least $(1 - \eps T)$. Thus,
choosing a dictator for each block yields a good solution for the
test.

For the soundness analysis, as in Section
\ref{sec-intro-soundness-P0} we fix a good noise set $\mc{I}$ 
conditioned on which the test accepts $P$ with probability at least
$1/2 + \xi$, and $\Pr[\mc{I} \tn{ is good }] \geq \xi/2.$ 
Further, without loss of generality, we assume that $\mc{I} = \cup_{j=1}^T \left(\{k_j + 1,\dots
k\}\times\{j\}\right)$, where (by Chernoff bound) $k_j \geq k/2$ for
$1\leq j \leq T$. For each $j$, $\{W_{1j},\dots, W_{k_jj}\}$ is
defined to be an orthogonal transformation of $\{Y_{1j},\dots,
Y_{k_jj}\}$ of the same $1/\sqrt{k_j}$ norm, where $W_{1j} =
(1/k_j)\sum_{i=1}^{k_j}Y_{ij}$. It is easy to see that $W_{1j} =  
(\sqrt{(T-1)/T})\delta_{j} + b\eta$, while $W_{\ell j} = 0$ under the
test distribution for $\ell > 1$.

Additionally, we also define $\{U_1,\dots, U_T\}$ to be an orthonormal
transformation of $\{W_{11},\dots, W_{1T}\}$ where $U_1 =
(1/\sqrt{T})\sum_{j=1}W_{1j}$. Again, it can observed that $U_1 =
(\sqrt{T})b\eta$ and $U_2,\dots, U_T$ are independent $N(0,1)$. Using
this we write  the polynomial $P = P' + Q_0 + U_1 Q_1$, where $P'$
consists of all the terms which have any $W_{\ell j}$, $\ell > 1$ as a
factor. Further, $Q_0$ is independent of $U_1$. Since $P' = 0$ under
the distribution we ignore it for now, noting that $\|Q_1\|_2^2 =
\E[Q_1^2] > 0$, since the test accepts with probability $ > 1/2$. 
The first step is to show, via Gaussian
anti-concentration on $Q_0$ and Chebyshev's inequality on $Q_1$, that
\begin{equation}\label{eqn:relative}
\|Q_0\|_2^2 \leq O(\eta^2)\|Q_1\|_2^2.
\end{equation}
Let us write $Q_1 = \sum_{H\in \mcb{H}} H\cdot Q_{1,H}(U_1,\dots, U_T)$, where the
sum is over the set $\mcb{H}$ of normalized Hermite monomials\footnote{By Hermite \emph{monomials},
we mean elements of the polynomial Hermite basis over the corresponding variables.} 
over the independent $N(0,1)$
variables $\cup_{j=1}^T\{Y_{ij}\}_{i=k_j+
1}^k$. Moreover, let  $Q^{(D)}_1 = \sum_{H \in \mcb{H}_D} 
H\cdot Q_{1,H}(U_1,\dots, U_T)$ for $0\leq D \leq
d-1 \geq \tn{deg}(Q_1)$, where $\mcb{H}_D$ is the subset of $\mcb{H}$
of degree exactly $D$.
Thus,  $\|Q_1\|_2^2 = \sum_{H\in \mcb{H}} \|Q_{1,H}\|_2^2$. Writing
$Q_{1,H} = Q_{1,H}(W_{11}, \dots, W_{1T})$ we also
define $\|Q_{1,H}\|_{\tn{mon}}^2$ as sum of squares of the
coefficients in 
the standard monomial basis $\mcb{M}$ of $\{W_{11}, \dots, W_{1T}\}$. A
straightforward calculation shows that:
\begin{equation}\label{eqn:relative2}
\|Q_{1,H}\|_2^2  \leq O(1)
\|Q_{1,H}\|_{\tn{mon}}^2,
\end{equation}
where the constants depending on $T$ and $d$ are absorbed in 
the $O(1)$ notation.	
On the other hand, since $Q_0$ is independent of $U_1$, using similar
definition of $Q_{0,H}$, we can
establish the reverse bound for it:
\begin{equation}\label{eqn:relative3}
\|Q_{0,H}\|_{\tn{mon}}^2  \leq O(1)\|Q_{0,H}\|_2^2.
\end{equation}

The rest of the arguments significantly build upon those in Section
\ref{sec-intro-soundness-P0}. We present a semi-formal description,
omitting much of the technical details. For
 reasons made clear later, we first 
carefully select $d^* \in \{0,\dots, d-1\}$ to be
the largest $D \in \{0,\dots, d-1\}$ such that $\|Q_1^{(D)}\|_2^2 \geq
\frac14\rho^D\|Q_1\|_2^2$ for a small enough constant depending on $k,T,
d,$ and $\eps$. It is easily observed that such a $d^*$ must exist
satisfying the properties: (i) $\|Q_1^{(d^*+1)}\|_2^2 \leq \frac14\rho^{d^* + 1} 
\|Q_1\|_2^2$, and (ii) $\|Q_1^{(d^*)}\|_2^2 \geq \frac14\rho^{d^*}\|Q_1\|_2^2$. 

Now we focus our attention on $U_1Q_1^{(d^*)}$ writing it as
\begin{equation}\label{eqn-U1Q1-intro}
U_1Q_1^{(d^*)} = \sum_{H \in \mcb{H}_{d^*}} HU_1Q_{1,H}(W_{11},\dots,
W_{1T}) = \sum_{H\in \mcb{H}_{d^*}}\sum_{M \in \mcb{M}}c_{H,M}HM.
\end{equation}
Let $\mcb{H}_{-j^*D}\subseteq \mcb{H}_{D}$ (resp.
$\mcb{M}_{-j^*} \subseteq \mcb{M}$) be the subset of basis elements
not containing any
variable from the $j^*$th block, i.e. $\{Y_{ij^*}\}_{k_{j^*}<i\leq k}$
(resp. $W_{1j^*}$). 
Now with $U_1 = (1/\sqrt{T})\sum_{j=1}W_{1j}$, we 
apply Lemma  \ref{lem-approx-structural-intro}
to each $U_1Q_{1,H}(W_{11},\dots, W_{1T})$ in the first expansion of
\eqref{eqn-U1Q1-intro}.
Using the fact that each $H$ has at most $d$
variables along with our choice of $T = 10d$ yields a $j^* \in [T]$
such that 
\begin{eqnarray}
\sum_{H \in \mcb{H}_{-j^*d^*}}\sum_{M \in \mcb{M}_{-j^*}}
c^2_{H,MW_{1j^*}} & \geq & \Omega(1)\bigg(\sum_{H \in \mcb{H}_{d^*}}\sum_{M
\in \mcb{M}} c^2_{H,M}\bigg) \label{eqn-lemma-apply-intro-1}\\
& \geq & \Omega(1)\|Q_1^{(d^*)}\|_2^2\ \geq\ \Omega(1)\rho^{d^*} \|Q_1\|_2^2
\label{eqn-lemma-apply-intro-2}
\end{eqnarray}
where the last two inequalities use
\eqref{eqn:relative2} along
with property (ii) above.

The next component of the analysis is to relate the bounds above with
the coefficients of
a suitable sub-polynomial of $P$ which is linear in the
variables $Y_{ij^*}$, $1 \leq i \leq k_{j^*}$. For this, let us first define
$\tilde{Q}$ to be exactly the sub-polynomial of $P$ 
 which does not contain any term with $W_{ij}$ where $i \neq
1$ and $j \neq j^*$. Rewriting the variables $\{W_{ij^*}\,\mid\, i \in
[k_{j^*}]\}$ in terms of $\{Y_{ij^*}\,\mid\, i \in [k_{j^*}]\}$, we
consider the sub-polynomial $\tilde{Q}_{\tn{lin}}$ (of $\tilde{Q}$) which is linear in the
variables $\{Y_{ij^*}\,\mid\, 1\leq i \leq k\}$. Note that
$\left(\cup_{D=0}^{d-1}\mcb{H}_{-j^*D}\right)\circ \mcb{M}_{-j^*}\circ\{Y_{ij^*}\}_{i=1}^k$ is a
basis in which $\tilde{Q}_{\tn{lin}}$ can be written with 
coefficients $\tilde{c}_{H, M, i}$ corresponding to the basis element
$HMY_{ij^*}$. Using the orthogonal transformation between
$\{W_{ij^*}\}_{i\in [k_{j^*}]}$ and $\{Y_{ij^*}\}_{i=1}^{k_{j^*}}$ we obtain
\begin{equation} \label{eqn-Y-W-intro}
\sum_{H\in \mcb{H}_{-j^*d^*}}\sum_{M \in \mcb{M}_{-j^*}}\sum_{i \in
[k_{j^*}]} \tilde{c}_{H,M,i}^2 \geq \frac{1}{2k_{j^*}} \left(
\sum_{H \in \mcb{H}_{-j^*d^*}}\sum_{M \in \mcb{M}_{-j^*}}
c^2_{H,MW_{1j^*}} \right),
\end{equation}
neglecting any contribution to the LHS of the above from $Q_0$ by our
a small enough choice of $\eta \ll \rho$ along with
\eqref{eqn:relative} and \eqref{eqn:relative3}.
The loss of $k_{j^*}$ factor in \eqref{eqn-Y-W-intro} is
compensated by the dependence of $\rho$ on $k$ as we shall see later. 
Combining
\eqref{eqn-Y-W-intro} with
\eqref{eqn-lemma-apply-intro-1}-\eqref{eqn-lemma-apply-intro-2} yields
\begin{equation}\label{eqn-non-noise-bd-intro}
\sum_{H\in \mcb{H}_{-j^*d^*}}\sum_{M \in \mcb{M}_{-j^*}}\sum_{i \in
[k_{j^*}]} \tilde{c}_{H,M,i}^2 \geq \Omega\left(1/k_{j^*}\right)
\rho^{d^*} \|Q_1\|_2^2.
\end{equation}
Consider now the sum 
$$\sum_{H\in \mcb{H}_{-j^*d^*}}\sum_{M \in
\mcb{M}_{-j^*}}\sum_{k_{j^*} < i \leq k} \tilde{c}_{H,M,i}^2.$$ 
Contribution
to the above can be from $Q_0$ or from $U_1Q_1^{(d^*+1)}$ -- the latter
due to the presence of $Y_{ij^*}$ ($k_{j^*} < i \leq k$) which
increases
the degree of $H \in \mcb{H}_{-j^*d^*}$ to $(d^* + 1)$ in the
representation of $Q_1$ over the basis $\mcb{H}\circ\mcb{M}$. 
Property (i) from our careful selection of $d^*$ is leveraged along with
our small enough choice of $\eta$ in \eqref{eqn:relative} along with
\eqref{eqn:relative3} to yield
\begin{equation}\label{eqn-noise-bd-intro}
\sum_{H\in \mcb{H}_{-j^*d^*}}\sum_{M \in
\mcb{M}_{-j^*}}\sum_{k_{j^*} < i \leq k} \tilde{c}_{H,M,i}^2 \leq 
O(1)
\rho^{d^*+1} \|Q_1\|_2^2.
\end{equation}
Using a choice $\rho \ll \eps/k$ we can combine the above with 
\eqref{eqn-non-noise-bd-intro} to obtain the following analog of
\eqref{eqn-1-bound-intro}:
\begin{equation}\label{eqn-second-1-bound-intro}
\sum_{H\in \mcb{H}_{-j^*d^*}}\sum_{M \in
\mcb{M}_{-j^*}}\sum_{i \in \mc{I}_{j^*}} \tilde{c}_{H,M,i}^2 \leq
\frac{\eps}{10} 
\sum_{H\in \mcb{H}_{-j^*d^*}}\sum_{M \in
\mcb{M}_{-j^*}}\sum_{i \in [k]} \tilde{c}_{H,M,i}^2,
\end{equation}
where $\mc{I}_{j^*} := \mc{I}\cap ([k]\times\{j^*\})$. 
Of course, since $\|Q_1\|_2 > 0$, we also obtain 
\begin{equation}\label{eqn-second-3-bound-intro}
\sum_{H\in \mcb{H}_{-j^*d^*}}\sum_{M \in
\mcb{M}_{-j^*}}\sum_{i \in [k]} \tilde{c}_{H,M,i}^2 > 0.
\end{equation}
The analysis above shows that for every good
choice of $\mc{I}$ there exist $(d^*, j^*)$ satisfying
\eqref{eqn-second-1-bound-intro}-\eqref{eqn-second-3-bound-intro}.
What remains is a probabilistic
concentration argument. 
Since $\Pr\left[\mc{I} \tn{ is good}\right] \geq \xi/2$, by averaging
we get that there exist $(d^*,j^*)$ and a fixing of
$\mc{I}\setminus\mc{I}_{j^*}$ such that with probability at least
$\xi/4Td$ over
the choice $\mc{I}_{j^*}$,
\eqref{eqn-second-1-bound-intro}-\eqref{eqn-second-3-bound-intro}
hold. Since each $i$ is added to $\mc{I}_{j^*}$ independently with
probability $\eps$, an application of Chernoff-Hoeffding shows that
the large deviation observed in 
\eqref{eqn-second-1-bound-intro} cannot occur with probability
$\xi/4Td$ (which is significant) unless the squared mass on the LHS of 
\eqref{eqn-second-3-bound-intro} is concentrated on a small number of
$i \in [k]$. This yields the desired decoding completing our sketch of
the analysis. The formal proof appearing in this work -- while
following the approach given above --  employs additional
notation and definitions for handling a few technicalities and ease of
presentation.


\medskip
\noindent
{\bf Combining $\mathfrak{P}_1$ with Label Cover and Folding.} We now describe how to combine our dictatorship test with Label Cover instances. Consider an instance of \textsc{Smooth Label Cover} instance $\mc{L} = \mc{L}(G(V,E),k,L,\{\pi_{e,u}:[k] \mapsto [L]\}_{e \in E,u \in e})$ (see Section \ref{subsec:label_cover} for a formal definition). For every vertex $v \in V$, we introduce a set of $k$ variables $\mathcal{Y}_v = \{Y^v_i : i \in [k]\}$. The output instance of the reduction will take as input polynomials over variables $\mathcal{Y} = \cup_{v \in V} \mc{Y}_v$ of degree at most $d$. Then, the test $\mathfrak{P}_1$ is executed on the $T$ blocks of coordinates corresponding to $T$ randomly chosen vertices of $V$ (as used in \cite{GRSW}). The resulting instance is then folded, i.e. 
the distribution on
the point-label pairs is projected onto a subspace $\mc{F}$ orthogonal to the
span of all the linear constraints implied by the edges of the Label
Cover. Formally,  the subspace $\mc{F}$ is constructed as follows. Consider an edge $e = (u,v) \in E$  and let $\pi_{e,u},\pi_{e,v}:[k] \mapsto [L]$ be the corresponding projection functions. Then for every $l \in [L]$, and for every monomial $M$ disjoint from $\{Y^u_i : i \in (\pi_{e,u})^{-1}(l)\} \cup \{Y^v_i : i \in (\pi_{e,v})^{-1}(l) \}$, 
we add the following linear homogeneous constraint on the coefficients of polynomial:
\begin{equation}
\mathcal{C}_{e,l,M}: \qquad\qquad \sum_{i \in (\pi_{e,u})^{-1}(l)} c_{Q,M\cdot Y^u_i} = \sum_{i \in (\pi_{e,v})^{-1}(l)} c_{Q,M\cdot Y^v_i}  
\end{equation}

Let $\mc{F}$ be the linear subspace resulting from the above set of constraints. We say that a polynomial $Q$ is folded over $\mc{F}$ if the coefficients satisfy all of the above constraints. In particular, these linear constraints ensure that any polynomial folded over $\mc{F}$ has
equal mass sum in the coordinates of the two pre-images of a label given by an edge's projections.  

\paragraph{Randomized Partial Decoding.}

Given a polynomial $P \in \R[\mc{Y}]$ folded over $\mc{F}$ which passes the test with probability at least $\frac12 + \xi$, we give a decoding algorithm which recovers a good labeling for $\mc{L}$ from $P$. Our decoding strategy will give us a randomized partial labeling, which can then be extended to a full labeling by assigning arbitrary labels to unlabeled vertices. Informally, the decoding algorithm proceeds by randomly guessing the following: (i) a degree-index pair $(d^*,j^* )\in \{0,1,\ldots,d-1\} \times [T]$, which is intended to satisfy \eqref{eqn-second-1-bound-intro}-\eqref{eqn-second-3-bound-intro} (ii) a set of noisy indices $\mc{I}_{-j^*}$ (excluding the set of indices corresponding to $j^*$), which is intended to be a good set of noisy indices  (iii) a set of vertices $\mc{V}_{-j^*} = \{v_j | j \in [T]\setminus \{j^*\}\}$, such that the polynomial restricted to vertices $\mc{V}_{-j^*}$ along with a random choice of vertex $v_{j^*} \in V$ (to be fixed later) passes the test with probability at least $\frac12 + \frac{\xi}{2}$. 

%Then, the decoding algorithm iterates over all possible choices of $v_{j^*} = v \in V$; for each such choice of $\{v_{j^*}\} \cup \mc{V}_{-j^*}$, the decoder attempts to decode a label for $v = v_{j^*}$. 
From the soundness analysis it follows that all of the intended conditions for (i)-(iii) hold with probability (including over the random choice of $v_{j^*}$) at least $\Delta_0 = \xi\cdot\frac{1}{Td}\cdot\frac{\xi}{8Td}$ . Fixing the choices in (i)-(iii), let $V' \subset V$ be the set of vertices $v_{j^*} = v$ for which the condition from (iii) holds. In expectation over the choices in (i)-(iii) we have $|V'| \ge \Delta_0|V|$ which we fix for now. Furthermore, our Chernoff-Hoeffding based argument mentioned earlier in this section shows that for all $v \in V'$ there exists at least one label which contributes at least $\epsilon^4$-fraction of the LHS of \eqref{eqn-second-3-bound-intro}. For all $v \in V'$, let $\Gamma(v) \subset [k]$ denote the set of such labels, so that $1 \leq  |\Gamma(v)| \le 1/\epsilon^4$. For each $v \in V'$,  assign a label by sampling uniformly at random from $\Gamma(v)$. The following observations imply that this randomized partial labeling is indeed a good one. 
\begin{itemize}
	\item[1.] The induced subgraph on $V'$ contains a significant fraction ($\gtrsim \Delta^2_0/2$) of edges, which follows from the \emph{Expansion} property of Smooth Label Cover.
	\item[2.] Furthermore, using the \emph{smoothness} property of \textsc{Smooth Label Cover}, it can be shown that for all but a small fraction of the above edges $(u,v)$, the projection functions $\pi_{e,u}, \pi_{e,v}$ are bijections, when restricted to the sets $\Gamma(u),\Gamma(v)$ respectively. For every such edge $(u,v)$ we will show that $\pi_{e,u}(\Gamma(u)) \cap \pi_{e,v}(\Gamma(v)) \neq \phi$, which follows roughly using the following argument. From the folding constraints we have
	\begin{equation}    
	\sum_{i \in (\pi_{e,u})^{-1}(l)}\tilde{c}_{H,M,i,u} = \sum_{i \in
		(\pi_{e,v})^{-1}(l)}\tilde{c}_{H,M,i,v}.\label{eqn-folding-intro}
	\end{equation}
	\noindent which as discussed before, are intended to balance the sum of coefficients corresponding to the pre-images of any $l \in [L]$. In particular, if $(\pi_{e,u})(\Gamma(u)) \cap (\pi_{e,v})(\Gamma(v)) = \phi$, it follows that for any $l^* \in \pi_{e,u}(\Gamma(u))$, we have $l^* \notin \pi_{e,v}(\Gamma(v))$. Since $\pi_{e,u}$ restricted to the set $\Gamma(u)$ is a bijection, the LHS in the above equation (instantiated with $l = l^*$) has exactly one large term, and other small terms, which overall leads to a large term.  On the other hand, since $(\pi_{e,v})^{-1}({l^*} )\cap \Gamma(v) = \phi$, 	by a stronger application of the smoothness property it can be deduced that all the terms in the RHS are small enough for it to be  much smaller that the LHS in magnitude, contradicting \eqref{eqn-folding-intro}. Thus, this edge is satisfied with probability at least $1/(|\Gamma(u)||\Gamma(v)|) \simeq \epsilon^8$.
\end{itemize}
Putting all of the above arguments together, it follows that in expectation over the choices (i)-(iii), using the above partial labeling at least $\Omega(\epsilon^8\Delta^2_0) = \Omega(1)$-fraction of edges will be satisfied. 
 
\ignore{ 
$\mathfrak{P}_1$ which is via a sub-polynomial
$\tilde{Q}_{\tn{lin}}$ linear in the variables
$\{Y_{ij^*}\}_{i=1}^k$ of the $j^*$th block.

More specifically, we may
fix the vertices corresponding to all the blocks except the $j^*$th
and also the restriction of $\mc{I}$ to all the blocks except the
$j^*$th. This fixes $\mcb{H}_{-j^*d^*}\circ M_{-j^*}$ used
in \eqref{eqn-second-1-bound-intro}-\eqref{eqn-second-3-bound-intro}.
For a vertex $v$ let
$\tilde{c}_{H,M,i,v} = \tilde{c}_{H,M,i}$ when $v$ is chosen as the
$j^*$th vertex. Suppose for an edge between $u$ and $v$ (not among
the fixed vertices) the respective
pre-images of a common label are $A$ and $B$. Then, the folding
constraints imply
\begin{equation}
\sum_{i \in A}\tilde{c}_{H,M,i,u} = \sum_{i \in
B}\tilde{c}_{H,M,i,v}.\label{eqn-folding-intro}
\end{equation}
We combine the above with the decoding obtained from the analysis of
$\mathfrak{P}_1$ using appropriately set smoothness parameters 
to prevent masses in the pre-images containing
the decoded coordinates from cancelling out. 
The constraints  \eqref{eqn-folding-intro} then imply that the
decoded labels
define a labeling satisfying a significant fraction of edges of the Label Cover instance.
\vspace{-1em}
}
%\paragraph{Organization.}
%Section \ref{sec:prelim} presents some preliminaries. Section \ref{sec:reduction} describes
%the reduction from Label Cover in the form of a PCP test. Section \ref{sec-folding} gives the 
%constraints implied by folding extended to polynomials. 
% In Section \ref{sec-NO-case}, we show the soundness
%of the reduction assuming a lemma (essentially restating \eqref{eqn-second-1-bound-intro}-\eqref{eqn-second-3-bound-intro}) 
%about the structure of polynomials passing the test. The rest of the paper is
%devoted to proving this lemma. In Section \ref{sec:anticonc}, we apply Gaussian anti-concentration to prove the analog 
%of \eqref{eqn:relative}.
%In Section \ref{sec:findingj}, we prove the structural lemma using Lemma \ref{lem-approx-structural-intro} as a key ingredient. Lemma 
%\ref{lem-approx-structural-intro} is proved in Section \ref{sec:struct}.


