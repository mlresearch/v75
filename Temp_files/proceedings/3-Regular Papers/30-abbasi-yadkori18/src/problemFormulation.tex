% !TEX root = Main.tex 
\section{More on the problem formulation in a general 
	adversarial and stochastic settings}
\label{s:set}
%
%We complete the description of the adversarial and stochastic 
%problems initiated in the introduction.    
Let $[a:b]=\{a,a+1,\ldots,b\}$ with
$a,b\in\Integer$, $a\leq b$, and $[a]=[1:a]$.
%For a vector $\w\in \Real^n$, $n\in\Integer$,  $\maxnorm{\w}= \max_{k\in[n]} |\w_k|$. 
A vector indexed by both round $t$ and a specific element
index $k$ is $\w_{k,t}$. We detail the general game protocol in Figure~\ref{fig:protocol}.
%
%
\begin{figure}[H]
\centering
\framebox{
	\begin{minipage}{.95\textwidth}
		\textbf{Input:} the time horizon $\timeHorizon$ and the number of arms $\nArms$\\
		\textbf{For} $t=1,2, \ldots, \timeHorizon$, 
		
$\qquad$\raisebox{.00cm}{\textcolor{bull}{$\blacktriangleright$}}~ simultaneously, the learner chooses arm $\pulledArm_t\in [\nArms]$, and

$\qquad$\raisebox{.00cm}{\textcolor{bull}{$\blacktriangleright$}}~  the adversary picks a gain vector. 
$\gainVector_t\in [0,1]^\nArms$.

$\qquad$\raisebox{.04cm}{\textcolor{bull}{$\blacktriangleright$}}~ Then, the learner 
observes $\gainVector_{t,\pulledArm_t}$.

		After round $\timeHorizon$ is over, the learner 
		recommends the arm  $\recomArm_{\timeHorizon}$ with the objective that $\recomArm_{\timeHorizon} = \bestArm_{\gainVector}$.
	%	\todov{it should be $\recomArm_{n+1}$ ot at least dont say the game stop when $t=n$ be careful with conventions! (starting to change now!)}
	\end{minipage}
}
\caption{General protocol of the adversarial setting. The adversary 
	is oblivious.}\label{fig:protocol}
\end{figure}
%\todomi{may be confusing that adversary observes $\gainVector_{t,\pulledArm_t}$ in General Protocol}
%
\paragraph{Adversarial case} 
      The adversary is denoted as \advPro{}. It is the process that 
     generates $\gainVector$.
     %
   %  In hindsight, at time $t$, %at the end of the game when 
  %   $t=\timeHorizon$, 
    % we can order the arm with respect to their 
     %cumulative gains $\cumulGain_{k}= \sum^{\timeHorizon}_{t=1} \gainVector_{k,t}$. %\todo{X or g?} 
%At the end of the game, $t=\timeHorizon$. 
 Let $(m)$ denote 
the index of the $m$-th best arm in $\setArms$
 and $\cumulGain_{(m)}$ its corresponding 
 cumulative gain
 so that $\cumulGain_{(1)}> \cumulGain_{(2)} \geq \ldots 
 \geq \cumulGain_{(\nArms)}$. Dually to $(\cdot)$, $\langle k \rangle$~denotes 
 the rank of arm $k$ after sorting according to~$\cumulGain_{(\cdot)}$ so that $\langle (k) 
 \rangle=(\langle k \rangle)=k,~ \forall k\in \setArms$.
%\todov{using accolade for the rank is too confusing}
Without loss of generality, note that we assumed there 
exists
 a unique best arm $\bestArm_{\gainVector}=(1)$. %\todov{really no loss of generality?}
%
%\todov{think of how to define here or not \advPro{} }
%
For each arm $k\in \setArms$, we define the gap 
$\gap^\gainVector_k$ as
%
\begin{align*} 
\timeHorizon\gap^\gainVector_{k} \triangleq \begin{cases}  
\cumulGain_{(1)}-\cumulGain_{k} & \text{if } k \neq \bestArm_{\gainVector}, \\
\cumulGain_{(1)}-\cumulGain_{(2)}       & \text{if } k =\bestArm_{\gainVector}.
\end{cases}%\cdot
\end{align*}
%
%\todov{choose how much we want to put that $\gainVector$ as an index everywhere}
The gap can also be written as 
$\timeHorizon\gap^\gainVector_{k} = \big|\max\limits_{i \neq k} 
\cumulGain_{i} - \cumulGain_{k}\big|$. 
%
%
%\todov{call him learner or forecaster?}
%\todov{discuss adaptive or oblivious adversary and think about
% what is the difference for us for the results}
% \todomi{indeed, I think P. Auer makes a distinctions about these}
 $\recomArm_{\timeHorizon} \in \setArms$ is the  arm 
returned by the learner at the end of the exploration phase.  
Given a budget
  $\timeHorizon$ and a fixed adversarial set of rewards 
  $\gainVector$ designed by an  adversary \advPro{}, the
   performance of the learner is measured by the 
   probability
    $\ProErr_{\advPro(\gainVector)}(\timeHorizon)$ 
    of not identifying the best 
    arm, i.e.,~$\ProErr_{\advPro(\gainVector)}(\timeHorizon)
     \triangleq \Pro\left(\recomArm_{\timeHorizon} \neq \bestArm_{\gainVector}\right).$
     The smaller $\ProErr_{\advPro(\gainVector)}(\timeHorizon)$, 
    the better the learner. The probability 
    is taken over the randomness of the learner as $\gainVector$ is fixed.
    An alternative definition of  the best arm, i.e., with
     highest $G_{k}$ in expectation over adversarially 
     sampled $\gainVector$, would lead to an impossible problem. 
     Indeed, it could happen that the best arm in expectation is actually 
     always clearly suboptimal on any realization $\gainVector$.
%
%\todomi{remark: It is somehow uncommon to have $\gainVector$, \emph{sampled} from the adversary..., so  we should comment that it makes sense here.}
%
%
%
%The learner will often chose the arm from a distribution
% at time $t$, $\learnerDist_t$, where $\learnerDist_{k,t}=\Pro(\pulledArm_t=k)$.
%
We define the random variables $\estGainVector_{k,t}$ as
 %
\begin{equation*}
\estGainVector_{k,t} 
\triangleq
 \frac{\gainVector_{k,t}\indicator{\pulledArm_t = k}}
 {\learnerDist_{k,t}}\CommaBin
\end{equation*}
%
for 
arm $k$ at round $t$ and for which  $\Exp_{\pulledArm_t\sim\learnerDist_{t}}
\left[\estGainVector_{k,t}\right]=\gainVector_{k,t}.$
We similarly define $\estCumulGainVector_{k,t} \triangleq \sum_{t'=1}^{t} \estGainVector_{k,t'}.
$
%we have for all $k\in\setArms$,  $\Exp_{\pulledArm_{[t]}\sim\learnerDist_{[t]}}\left[\estCumulGainVector_{k,t}\right]=\cumulGain_{k,t}$.
%\todov{improve the simulation under the expectation}
%
%We say that the adversary is oblivious if $\gainVector_{k,t}$ is independent of $\pulledArm_1,\ldots,\pulledArm_{t-1}$ for all $k\in\setArms$ and $t\in[\timeHorizon]$. Otherwise the adversary is said to be adaptive. 
 %
 %
\paragraph{Stochastic case}
 In stochastic bandits~\citep{Audibert10BA}, that
  we denote \stoPro{}, the distribution~$\nu_{k}$ of arm $k$ is bounded in $[0,1]$ with mean $\mu_{k}$. % and variance $\sigma^2_{k}$. 
 The ordering $(\cdot)$ is such that $\mean_{(1)}>\mean_{(2)}
 \geq\ldots\geq\mean_{(\nArms)}$, as we assume  the uniqueness of the best arm without loss 
 of generality.
 The distributions $\{\nu_{k}\}$ are unknown to the learner.
 %At each round $t$, $\gainVector_{k,t}$ is an independent sample drawn from the 
 %distribution $\nu_{k}$.
% Let $\pullsNumber_{k}(t)$ be the number of times that arm 
% $k$ has been pulled by the end of round $t$.
% %
% Usually the forecaster estimates the expected value of each arm by computing the
% average of the samples observed over time $\widehat{\mu}_{k}(t) = \frac{1}{T_{k}(t)}\sum_{t=1}^{\timeHorizon}\indicator{\pulledArm_t=k}\gainVector_{k,t}$.
 %
 The best arm to be identified is  $\bestArmSto= \argmax_{k\in\setArms} \mean_k$.
 % involved
 % Indeed $\bestArmSto$ is not define for any $\gainVector$ but in expectation with the means. With very high probability (smaller than the probability of errors of interest) the best arms are the same anyway
 %
 Similar to the adversarial case, the gaps are 
 $\gap_{k} \triangleq |\max_{i \neq k} \mean_{i} - \mean_{k}|$, ranked as
$\gap_{(1)} \triangleq \gap_{(2)}\leq\ldots\leq\gap_{(\nArms)}$, 
 and the error 
 %\todom{here the definition of the error included the randomness over rewards as well, right? so maybe not so similar\\
% Victor:
 %That is what i try to explain in the next sentence, that the diff is very small, it is not clear?}
 of the learner is $\ProErr_{\stoPro}(\timeHorizon) 
 \triangleq \Pro\left(\recomArm_n \neq \bestArmSto\right)$.  
 However, unlike in the adversarial case, this definition of the 
 error includes the randomness of rewards.
 Nonetheless, it is only with a probability upper-bounded 
 by $\rho=\cO(\nArms\exp(-\timeHorizon\gap^2_{(1)}))$
   that a $\gainVector\sim\stoPro$ can verify 
   $\bestArmSto\neq\bestArm_\gainVector$. However, this difference 
   with the adversarial formulation is not significant as the 
   probability $\rho$ is never larger than the probabilities of errors studied in this paper 
   and it is often insignificant with 
   respect to it.
 
\paragraph{Notions of complexity} Given 
gaps $\gap_{k}$ for $k\in\setArms$, we define two notions of 
complexity of the identification task, 
$\complexitySR$ and $\complexityUnif$, that were introduced by~\cite{Audibert10BA}. 
% In the stochastic case, these notions are defined 
In particular,
 %
 \[
 \complexitySR
 \triangleq
 \max_{k\in\setArms}\frac{k}{\gap^2_{(k)}}
 \qquad\text{and}\qquad
 \complexityUnif 
\triangleq
% \nArms\max_{k\in\setArms}\frac{1}{\gap_{(k)}^2}=\frac{\nArms}{\min_{k\in\setArms}\gap_{(k)}^2}
% ~=~
% \nArms\max_{k\in\setArms}\frac{1}{\gap_{(k)}^2}
% =
 \frac{\nArms}{\gap_{(1)}^2}\cdot
 \]
  %\todov{which way to write tit? }
  %
  %\todomi{isn't this a good place to relate to the complexities in the prior papers?\\
 % Victor: These are the complexity of previous paper}
   $\complexitySR$ relates to the complexity of the stochastic 
   case. $\complexityUnif$ will be used both in the 
   adversarial and stochastic cases. 
In the adversarial case, the complexity and the gaps 
are defined with respect to $\gainVector$ and 
but we also sometimes  write the uniform complexity as  $ \complexityUnif (\gainVector)$ for clarity.


\paragraph{Class of problems}  We define a set of classes to group problems with
very similar gap structure and with complexities that are only a 
constant multiplicative factor apart.
For any $0<\gap_1=\gap_2\leq\gap_3\leq \ldots\leq 
\gap_\nArms\leq 1/8$, we define a \textit{problem class} 
$\Gaps_{c}$ with
$c\geq 1$. 
Given these gaps and $c$, in the adversarial case, we say that 
$\gainVector\in\Gaps_{c}$  if  for all $k\in\setArms$, $\gap_k/c
 \leq\gap^{\gainVector}_k\leq c\gap_k$ except for only one 
 arm~$\refarm$  whose gap is related to the smallest gap as $\gap_{1}/c\leq\gap^{\gainVector}_{\refarm}\leq c\gap_{1}$.
In the stochastic case, $\stoPro\in\Gaps_{c}$ under 
the same condition on its gaps defined as $\gap_{k} \triangleq
 |\max_{i \neq k} \mean_{i} - \mean_{k}|$  for $k\in\setArms$. 


%Our results will expressed in a
%context where \advPro{} generates deterministically
%$\gainVector$ before the start of the game and therefore 
%in that case \advPro{} and $\gainVector$ are essentially 
%the same thing.
%However in some proofs we will construct adversaries \advPro{} that generate 
%randomly  $\gainVector$ before the game starts. In those cases we will be more 
%careful to discriminate  between $\gainVector$ and \advPro{} 
%in the notations.


%\section{Notation and technical results for the proofs}
%\todov{unused at the moment}
%Following the notations in\cite{Carpentier16TB}:
%
%For two distributions $\nu$, $\nu'$ defined on $\Reals$ and that are such that $\nu$ is absolutely continuous with
%respect to $\nu'$, we write
%\[
%KL(\nu,\nu')
%~=~
%\int_{\Reals}\log\left(\frac{d\nu(x)}{d\nu'(x)}\right)d\nu(x)
%\]
%for the Kullback Leibler divergence between distribution $\nu$ and $\nu'$.
