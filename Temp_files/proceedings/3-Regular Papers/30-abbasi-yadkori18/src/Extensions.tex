% !TEX root = Main.tex
\section{The simplicity of \Pone{} and its sampling routine have potential extensions}\label{s:exte}
%
%As \Pone{} and its sampling routine are simple and 
%parameter-free, they have the potential to be adapted 
%to variants of the fixed-budget setting of our paper. 
%it could be reused in numerous other variations of the setting of the best arm identification problem where additionally one wants to be robust to potential adversarial chosen rewards. 
%In this section, we discuss some of them.

%\subsection{Fixed Confidence setting}\label{s:exteFC}
%
\paragraph{Fixed-confidence}
%\todov{Well i am not sure anymore about this one!! need to think more! the stoping criterion is of importance?}
%Our results naturally extend to the fixed-confidence 
%setting .
In this i.i.d.\,stochastic setting, (\citealp{Maron93HR}, \citealp{Even-Dar06AE,Mnih08EB,
	Kalyanakrishnan12PA,Kaufmann13IC,Garivier16OB}) the goal is to 
design a learner that stops as soon as possible 
and returns the best arm with a fixed confidence.
Let~$\widetilde{n}$ be the round when the algorithm stops and 
$\recomArm_{\widetilde{n}}$ its returned arm. Given a 
confidence level $\delta$, the learner has to guarantee 
that $\Pro\left(\recomArm_{\widetilde{n}}= 
\bestArmSto\right) \leq \delta$. The performance of 
the learner is then measured by its \textit{sample 
	complexity}, which is the number of rounds $\widetilde{n}$ before stopping, 
either in expectation or in high probability.

Mimicking the Hoeffding and Bernstein Races~\citep{Maron93HR,
	Mnih08EB}, we could design Freedman Races based on \Pone{}. %\todom{FReed\\ Victor, why not ? thats just the name of the algorithm} 
All the arms would be pulled according to 
the parameter-free sampling routine of \Pone{}. No arm could be ever discarded because we could happen to face an adversary.
%\todom{discuss with Victor and rewrite}
The learner would stop using the \Pone{} routine based on having the confidence intervals from the Bernstein concentration 
inequality for martingales and in particular, it would stop when the confidence interval for the empirically best 
arm is separated from the confidence intervals for all the other arms.
For this fixed-confidence variant of  \Pone{}, we could reuse the proof 
techniques developed for the fixed-budget setting and bound the 
accumulation of variance of the estimates. Then, in the stochastic case, we would be 
able to guarantee that the expected 
sample complexity of such  algorithm is $\tcO\left(\complexityBoth\log(1/\delta)\right)$, up to log factors. %\todom{either remove tilde or the log }

For the adversarial case, we can consider
an infinite sequence of rewards $\gainVector$ fixed by 
the adversary for all arms.
%The learner try to stop as earny
%We could probi
%measure of  sample complexity 
% (as small as possible) on the rewards up to round $t$, 
% 
Assume that the sample complexity  on the rewards up to round $t$, 
is bounded by some $\n(\gainVector_{[t]})$, the smaller the better. 
%,  such that he can
We can then guarantee that  %\todom{not too clear}
if at any round $\tilde\timeHorizon$, 
$\gainVector_{[\tilde\timeHorizon]}$  verifies 
$\n(\gainVector_{[\tilde\timeHorizon]}) \leq \tilde\timeHorizon$, 
then with probability $1-\delta$, the learner can both 
stop and recommend the best arm $\bestArm_{\gainVector_{[\tilde\timeHorizon]}}$ 
at round $\tilde\timeHorizon$.
Our Freedman algorithm would be able to satisfy this 
requirement with the complexity of uniform allocation,
$\tcO\left(\complexityUnif(\gainVector_{[\tilde\timeHorizon]})\log(1/\delta)\right)\!.$ %$\todom{either remove tilde or the log }. 
Note that the learner could possibly never stop. 
%It can continuously scan all the rewards and, in case of a change of the best arm, find all the rounds where different best arms can be identified with probability larger than $1-\delta$.
%
%this measure of complexity at a time $t$ then the learner is able to by itself decide to recommend an arm and do so with a probability of error less than the targeted error.
%
% 
%\todov{if we have time we should write it but maybe we can just claim its easy because it is}
%
%\subsection{Stream of data}
%\noindent\textbf{Stream of Rewards, Best Arm in a Window. }
%
%
%\subsection{$m$-set, Threshold Bandit and Active Anomaly Detection}
\paragraph{Streams, windows, thresholds, $m$-set, and active 
	anomaly detection}
\Pone{} can  be used to
recommend  the  best arm in 
the latest time window 
between  $t-W$ and $t$ for each round $t$. Essentially, $W$ would replace $\timeHorizon$ in our bounds if \Pone{} recommends the estimated best arm in that window. Also, \Pone{} could also be adapted to identify $m$ best arms out of $\nArms$ as did~\cite{Bubeck12MI}
The key is to redefine the gap with respect to the $m$-th best arm instead of the best arm.
We could also extend \Pone{} to a setting where the rewards converge~\citep{Li16HA}.
\cite{Locatelli16AO} defined the gaps with respect 
to a given fixed threshold and the objective is to determine which arms have 
a mean higher than the threshold. Again, our approach would apply. Moreover, it could prove to be a good 
robust approach to the problems that are linked to the 
threshold bandit problem as discussed by~\citet[Section~3]{Locatelli16AO}, one of them being the \emph{active anomaly detection} \citep{carpentier2014extreme}. %\todovout{would p1 be applicable to the extreme bandit setting? MV: yes possibly, but here the link is "active anomay detection"}.
Indeed, in adversarial anomaly detection, the learner might be monitoring 
different streams of non-stochastic rewards and could potentially detect an 
anomaly if one of the streams outputs a reward signal that is on average larger
than a given threshold during a time window period $W.$ A variant of \Pone{} would then be a robust anomaly detector and would be also adaptive to easy data.
%
\paragraph{Adaptive adversaries} Our upper bounds results extend 
naturally  to adaptive adversaries given the following condition:  
$\complexityUnif$ is an upper bound on the complexity of 
all $\gainVector$ that the adaptive adversary can possibly generate. The proofs remain the same except that the Bernstein concentration inequality in Theorem~\ref{th:UPARU} is replaced by the Bernstein concentration inequality for martingales.
\vfil
