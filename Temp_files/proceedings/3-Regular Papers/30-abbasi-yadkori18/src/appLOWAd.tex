% !TEX root = Main.tex
\section{Lower bound in the general adversarial setting} %(Theorem~\ref{th:LOWadPRO}) }
\label{app:prooflowadPRO}
%
	\setcounter{scratchcounter}{\value{theorem}}\setcounter{theorem}{\the\numexpr\getrefnumber{th:LOWadPRO}-1}
\toto*
\setcounter{theorem}{\the\numexpr\value{scratchcounter}}
\begin{proof}%[Theorem~\ref{th:LOWadPRO}]
	Without loss of generality, we assume that $n$ is even.
	Let $\pullsNumber_{k}(t)$ be the number of times that arm 
	 $k$ has been pulled by the end of round $t$.
	%
	Let us consider any fixed learner and a base problem, denoted \basePro{}, with $\nu_1,\ldots, \nu_\nArms$, 
	Bernoulli distributions with means $\mean^{\basePro}_1,\ldots, \mean^{\basePro}_\nArms$ 
	such that $\mean^{\basePro}_1\triangleq 1/2$, and for all $k\in[2,\nArms], \mean^{\basePro}_k\triangleq1/2-\gap_k$ 
	(the gaps specified in the claim of the theorem).
		The expectation and probability with respect to the
	learner and the samples of this problem are denoted
	$\Exp_{\basePro}$ and $\Pro_{\basePro}$.% with $\gap_k\geq 0$ for .
	
	We consider  an early  period of the game $t=1, \dots, \timeHorizon/2$,\
	that we denote as $\earlyPhase\triangleq[1: \timeHorizon/2]$. %	
	The proof could work with any set of $\timeHorizon/2$ 
	rounds that are not necessarily the first of the game 
	but we stick to the early ones to ease the notation.
	Now we want to identify an arm that the learner will 
	not pull
	very much in the base problem \basePro{}
	during the period $\earlyPhase$.
	For our learner, during the period~$\earlyPhase$, in 
	any case at least one \emph{fixed} arm  among $\setArmsmo$ is pulled 
	in expectation % \todom{why in expectation? not always?} 
	less or equal to   
	$ \timeHorizon/\nArms $ as otherwise the total 
	number of pulls
	is in expectation strictly larger than $(\nArms-1)\times\timeHorizon/\nArms \geq \timeHorizon/2$. 
	We denote this arm as~$\refarm$. By this construction we have that
	%
	\begin{equation}\label{eq:notSoMuchPullsAD}
	\Exp_{\basePro}\left[\pullsNumber_{\refarm}\left(\frac{n}{2}\right) \right]
%		\pullsNumber_{\refarm}(n_i) 
	\leq  \frac{\timeHorizon}{\nArms}\cdot
	\end{equation}
	%
%	\todomi{what is $i$ in $n_i$?}
	Now we construct the two adversarial problems $\advPro_1$ and  $\advPro_2$.
	These adversarial problems sample  $\gainVector$ randomly.
	 During phase $\earlyPhase$, at rounds $t\in\earlyPhase$, 
	 for any arm $k$, $g_{k,t}$ is sampled from the Bernoulli 
	 distribution with means $\mu_{k}^{p}(t)$, $p\in\{1,2\}$ 
	 specifying the problem at hand.
	The expectation and probability with respect to the
	learner and the samples of this problem $p$ are denoted by
	$\Exp_{p}$ and~$\Pro_{p}$.
	
	
	For problem $\advPro_1$, the distributions of 
	the arms during phase $\earlyPhase$, $t\in\earlyPhase$, 
	are the same as in the \basePro{} 
	$\mu^{1}_{k}(t) \triangleq \mu^{\basePro}_{k} $ 
	for all $k\in \setArms$.
	After  phase $\earlyPhase$, for $t>  \timeHorizon/2$, 
	problem $\advPro_1$ assigns deterministically the following gains,
	 for all $k\neq \refarm$, for $t>  \timeHorizon/2$, $\gainVector_{k,t}=0$ 
	 and $\gainVector_{\refarm,t}=\gap_{\refarm}-\gap_1$.
	Therefore, the final expected cumulative gain of arm 
	 $\refarm$ is  \[\Exp_1\left[\cumulGain_{\refarm,n}\right]
	= \frac{\timeHorizon}{2}\mean^{\basePro}_{\refarm}+
	\frac{\timeHorizon}{2}\left(\gap_{\refarm}-\gap_1\right)=
	\frac{\timeHorizon}{2}\left(\mean^{\basePro}_1-\gap_1\right)\] and $\Exp_1\left[\cumulGain_{k,n}\right]= 
	\timeHorizon\mean^{\basePro}_k/2$ for all $k\neq\refarm$.
	The problem $\advPro_2$ only differs from the 
	previous one in its stochastic first part 
	(the deterministic second part is the same) and only for arm $\refarm$ where, 
	for $t\leq  \timeHorizon/2$, where $\mu_{\refarm}^{2}(t)\triangleq\mu_{\refarm}^{\basePro}+2\gap_1$.
	Therefore, the final expected cumulative gain of arm $\refarm$  
	is  \[\Exp_2\left[\cumulGain_{\refarm,n}\right]
	= \frac{\timeHorizon}{2}\left(\mean^{\basePro}_1+\gap_1\right).\] 
	
	
%	We denote the expected cumulative gain in problem $p$ of each arm $k\in\setArms$ as $\expectCumul_{k}=\sum_{t=1}^{\timeHorizon} \mean^{\advPro}_{k}(t)$. \todo{do something with notation here, maybe define outside!}
	
	\noindent
	%\todomi{$p$ is free variable here! for all $p$ or for one fixed or what?}
	Let us consider the events
	%
	\begin{align*}
	\eventc_1 &\triangleq \left\{ \forall k\in\setArmsmo,~  \cumulGain_{k}-
	\Exp_1\left[\cumulGain_{k}\right] \leq  \frac{\timeHorizon \gap_1}{8} \right\} 
	\cup \left\{  \Exp_1\left[\cumulGain_{1}\right]-\cumulGain_{1} 
	\leq  \frac{\timeHorizon \gap_1}{8} \right\} \text{\ and}\\
%	\]
	%
%	
	% 
%		\[
	\eventc_2 &\triangleq \left\{ \forall k\in\setArms, k\neq \refarm,~   
	\cumulGain_{k}-\Exp_2\left[\cumulGain_{k}\right] \leq  \frac{\timeHorizon \gap_1 }{8}
	\right\} \cup \left\{  \Exp_2\left[
	\cumulGain_{\refarm}\right]-\cumulGain_{\refarm} \leq  \frac{\timeHorizon \gap_1 }{8}\right\}\!\cdot
	\end{align*}
	% 
	Using a standard Hoeffding argument with a 
	union bound we have that 
	%\todomi{here we implicitly assume that both the games are run in parallel}
	%
	\[
	\Pro_1\left(\eventc_1\right) \geq 1-\nArms\exp\left(
	-2\left(\frac{\gap_1}{8}\right)^2\frac{\timeHorizon}{2}\right)
	\] %\todom{we don't need the last $1/2$, right? the r.v. are Bernoullis}
	and the same result holds for $\Pro_2\left(\eventc_2\right)$.
	%
	Also note that  in the  problem $p$, for 
	any $\gainVector$ that is compatible with 
	$\eventc_p$, the gaps associated to $\gainVector$ verify
	$\gap_k/2\leq\gap^{\gainVector}_k\leq 3 \gap_k$ for all $k\neq \refarm$
	and $\gap_1/2\leq\gap^{\gainVector}_{\refarm}\leq2\gap_1$
which gives $4\complexityUnif\geq\complexityUnif(\gainVector)\geq
 \complexityUnif/9$. Also on $\gainVector\in\eventc_1$, 
 we have $\bestArm_\gainVector=1$ and  on $\gainVector\in\eventc_2$, we have $\bestArm_\gainVector=\refarm$.
	
	
	Now we prove the following relation 
	between the probability of error 
	in the  problem $\advPro_2$, $\ProErr_{2}(\timeHorizon)$, and the
	probability of successful identification in the  problem $\advPro_1$,
	$1-\ProErr_{1}(\timeHorizon)$,
	where in this case, for problem $p$, $\ProErr_{p}(\timeHorizon)$ 
	%\todomi{here the "adversarial sampling" notation makes a cameo}
	is defined here as $\ProErr_{p}(\timeHorizon) \triangleq
	 \Pro_{\gainVector\sim \advPro_p}\left(\recomArm_n \neq \bestArm_{\gainVector}\right)$,
	% 
		\begin{equation}\label{eq:mainlowGenAd}
	\ProErr_{2}(\timeHorizon)
	+\nArms\exp\left(-\frac{\gap^2_1\timeHorizon}{64}\right)
	\geq
	\frac{1}{8}\left(\frac{1}{4}-\ProErr_{1}(\timeHorizon) \right)
	\exp\left(-\frac{32\gap^2_{1}\timeHorizon}{\nArms}\right)\!\cdot
	\end{equation}
	%
	We have that
	%
	\begin{align*}
	\ProErr_{2}(\timeHorizon)
	+\nArms\exp\left(-\frac{\gap^2_1\timeHorizon}{64}\right)
	&\stackrel{\textbf{(a)}}{\geq}
	\Pro_{2}\left(\recomArm_\timeHorizon\neq \refarm\right)\\
	&\stackrel{\textbf{(b)}}{\geq}
	\Pro_{2}\left(\recomArm_\timeHorizon=1\right)\\
	&\stackrel{\textcolor{white}{\textbf{(b)}}}{\geq}
	\Pro_{2}\left(\recomArm_\timeHorizon=1 \cap \pullsNumber_{\refarm} \left(\frac{\timeHorizon}{2}\right) 
	\leq \frac{2\timeHorizon}{\nArms} \right)\\
	&\stackrel{\textbf{(c)}}{\geq}
	\Pro_{1}\left(\recomArm_\timeHorizon=1 \cap \pullsNumber_{\refarm} \left(\frac{\timeHorizon}{2}\right) 
	\leq \frac{2\timeHorizon}{\nArms} \right)\frac{1}{8}\exp\left(-\frac{16\gap^2_{1}2\timeHorizon}{\nArms}\right)\\
	&\stackrel{\textbf{(d)}}{\geq}
	\left(\Pro_{1}\left(\recomArm_\timeHorizon=1 \right)-\frac{1}{2}\right)\frac{1}{8}
	\exp\left(-\frac{32\gap^2_{1}\timeHorizon}{\nArms}\right)\\
	&\stackrel{\textbf{(a)}}{\geq}
	\frac{1}{8}\left(\frac{1}{4}-\ProErr_{1}(\timeHorizon) \right)
	\exp\left(-\frac{32\gap^2_{1}\timeHorizon}{\nArms}\right)\!\CommaBin
	\end{align*}
	%
	where 
	\textbf{(a)} %\todo{maybe no need for this depending on the definition of the best arm} 
	is because first, as computed above, 
	for problem $p$,
	\[\Pro_{ p}\left(\bestArm_{\gainVector}=1\right)\geq \Pro(\eventc_p) 
	\geq 1-\nArms\exp\left(-\frac{\gap^2_1\timeHorizon}{64}\right)\!\CommaBin\] 
	since; if we denote $1^\star=1$ (the arm with the highest mean in $p=1$) %\todom{correct?}  
	and $2^\star=\refarm$ (the arm with the highest mean in  $p=2$)
	and in general $p^\star$, we have that
	%
	\begin{align*}	
	\ProErr_{p}(\timeHorizon)
	&=
	1- \Pro_{p}\left(\recomArm_\timeHorizon=\bestArm_{\gainVector}\right)
	=
	1- \Pro_{p}\left(\recomArm_\timeHorizon=\bestArm_{\gainVector} 
	\cap \bestArm_{\gainVector}=p^\star\right)
	- \Pro_{p}\left(\recomArm_\timeHorizon=\bestArm_{\gainVector}
	\cap\bestArm_{\gainVector}\neq p^\star\right)\\
&	\geq
	1-
	\Pro_{p}\left(\recomArm_\timeHorizon=p^\star\right)
	- \Pro_{p}\left(\bestArm_{\gainVector}\neq p^\star\right)
		\geq
	1-
	\Pro_{p}\left(\recomArm_\timeHorizon=p^\star\right)
	- \nArms\exp\left(-\frac{\gap^2_1\timeHorizon}{64}\right)\!\cdot
	\end{align*}
	Moreover, 
	$\nArms\exp\left(-\gap^2_1\timeHorizon/64\right) \leq 1/4$ 
	by an assumption of the theorem.
%	
	Next, \textbf{(b)}  is because we have $1\neq\refarm$ 
	by construction and
	\textbf{(c)} uses the change-of-measure 
	argument from Lemma~\ref{l:changeMeas}. % \todo{do we need somethingabout Jn being a deterministic function so that we can use the lemma on events?} \todov{explained in a separate lemma becuase we might be able to have something that we could reuse for the lower bound of the general case above}
	Finally, \textbf{(d)} is because 
	%
	\begin{align*}
	\Pro_{1}\left(\recomArm_\timeHorizon=1 \right)
	&=
	\Pro_{1}\left(\recomArm_\timeHorizon=1 \cap \pullsNumber_{\refarm} \left(\frac{\timeHorizon}{2}\right) 
	\leq \frac{2\timeHorizon}{\nArms} \right)
	+
	\Pro_{1}\left(\recomArm_\timeHorizon=1 \cap 
	\pullsNumber_{\refarm} \left(\frac{\timeHorizon}{2}\right) 
	> \frac{2\timeHorizon}{\nArms} \right)\\
	&\leq
	\Pro_{1}\left(\recomArm_\timeHorizon=1\cap \pullsNumber_{\refarm} \left(\frac{\timeHorizon}{2}\right) 
	\leq \frac{2\timeHorizon}{\nArms} \right)
	+
	\Pro_{1}\left( \pullsNumber_{\refarm} \left(\frac{\timeHorizon}{2}\right) 
	> \frac{2\timeHorizon}{\nArms} \right)\\
	&\leq
	\Pro_{1}\left(\recomArm_\timeHorizon=1 \cap \pullsNumber_{\refarm} \left(\frac{\timeHorizon}{2}\right) 
	\leq \frac{2\timeHorizon}{\nArms} \right)
	+
	\frac12\CommaBin
	\end{align*}
	%
	where the last inequality combines Equation~\ref{eq:notSoMuchPullsAD} 
	and Markov's inequality.
	
%\smallskip	\noindent
\paragraph{Two cases}
After proving Equation~\ref{eq:mainlowGenAd}, we	finally 
	 distinguish two cases. \\
	%\bigskip 
	\noindent\textbf{Case 1) $\ProErr_{2}(\timeHorizon)>
		\exp\left(-32\gap^2_{1}\timeHorizon/\nArms\right)\!/64$:}
	%
%	We have
%	%
%	\begin{align*}
%	\frac{1}{64}\exp\left(-32\gap^2_{1}\timeHorizon/\nArms\right)
%	&~\stackrel{(a)}{\geq}~
%	4\nArms\exp\left(-\gap^2_{1}\timeHorizon/128\right)\exp\left(-32\gap^2_{1}\timeHorizon/\nArms\right)\\
%		&~\stackrel{(b)}{\geq}~
%4\nArms\exp\left(-\gap^2_{1}\timeHorizon/128\right)\exp\left(-\gap^2_{1}\timeHorizon/128\right)
%	\end{align*}
%	%
%	where \textbf{(a)} is because by assumption we have  $8\nArms\exp\left( -\timeHorizon\gap^2_1 /128\right)\leq 1/16$ and
%	\textbf{(b)} is because by assumption we have  $\nArms>32\times128$.
%	
%	Therefore 
%$\ProErr_{2}(\timeHorizon)>
%\frac{1}{64}\exp(-32\gap^2_{1}\timeHorizon/\nArms)$
%	then 	
%	$\ProErr_{2}(\timeHorizon)>
%	4\nArms\exp\left(-\frac{\gap^2_1\timeHorizon}{64}\right)$.
		We claim  there exists $\gainVector\in\eventc_2$,  
	as discussed above,  that its best arm is  $\refarm$ and it 
	possesses a complexity verifying $4\complexityUnif\geq\complexityUnif(\gainVector)\geq \complexityUnif/9$,  
	such that the lower bound holds. We prove the statement by contradiction.
	Indeed if no $\gainVector\in\eventc_2$ is such that 
$
\ProErr_{\gainVector}(n)
	\geq
	 \exp( -\timeHorizon32\gap^2_1 /\nArms)/128,
$	then we have 
%\todomi{equality in the second line}
%\todomi{$p$ should be 2 below}
	\begin{align*}
	\ProErr_{2}(\timeHorizon)
&=
\Pro_{\gainVector\sim \advPro_2}\left(\recomArm_n \neq \bestArm_{\gainVector}\right)\\
&\leq
\Pro\left(\recomArm_n \neq \bestArm_{\gainVector}\cap\gainVector\in\eventc_2\right)
+
\Pro\left(\recomArm_n \neq \bestArm_{\gainVector}\cap\gainVector\notin\eventc_2\right)\\
&\leq
\Pro\left(\recomArm_n \neq \bestArm_{\gainVector}\cap\gainVector\in\eventc_2\right)
+
\Pro\left(\gainVector\notin\eventc_2\right)\\
&\leq
 \frac{1}{128}\exp\left( -\frac{\timeHorizon32\gap^2_1 }{\nArms}\right)
+
\nArms\exp\left(-\frac{\gap^2_1\timeHorizon}{64}\right)\\
 &=
 \frac{1}{128}\exp\left( -\frac{\timeHorizon32\gap^2_1 }{\nArms}\right)
 +
\nArms\exp\left(-\frac{\gap^2_1\timeHorizon}{128}\right)\exp\left(-\frac{\gap^2_1\timeHorizon}{128}\right)\\
 &\stackrel{\textbf{(a)}}{\leq}
   \frac{1}{128}\exp\left( -\frac{\timeHorizon32\gap^2_1 }{\nArms}\right)
 +
 \frac{1}{128}\exp\left( -\frac{\timeHorizon\gap^2_1}{128}\right)\\
 &\stackrel{\textbf{(b)}}{\leq}
 \frac{1}{128}\exp\left( -\frac{\timeHorizon32\gap^2_1 }{\nArms}\right)
 +
 \frac{1}{128}\exp\left( -\frac{\timeHorizon32\gap^2_1 }{\nArms}\right)\!\CommaBin
\end{align*}
where \textbf{\textbf{(a)}} is because
$\nArms\exp\left( -\timeHorizon\gap^2_1 /128\right)\leq 1/128$ 
 and 
 \textbf{(b)}
because $\nArms>32\times128$
 by assumptions.
	This is a contradiction with the original assumption of that case.
	

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\smallskip	
	\noindent\textbf{Case 2) $\ProErr_{2}(\timeHorizon)\leq
	\exp(-32\gap^2_{1}\timeHorizon/\nArms)/64$:}
We first want to prove that this assumption gives
	%
	\begin{equation}\label{eq:lowbobcase2}
\ProErr_{1}(\timeHorizon)\geq \frac{1}{16}\CommaBin
	\end{equation}
	that we prove by first using Equation~\ref{eq:mainlowGenAd} which gives
	\[
		\frac{1}{64}\exp\left(-\frac{32\gap^2_{1}\timeHorizon}{\nArms}\right)
		+
		\nArms\exp\left(-\frac{\gap^2_1\timeHorizon}{64}\right)
		\geq
		\frac{1}{8}\left(\frac{1}{4}-\ProErr_{1}(\timeHorizon) 
		\right)\exp\left(-\frac{32\gap^2_{1}\timeHorizon}{\nArms}\right)\!\CommaBin
%\ProErr_{2}(\timeHorizon)\geq 1/8 - 64\nArms\exp\left(-\frac{\gap^2_1\timeHorizon}{8}\right) / \exp(-32\gap^2_{1}\timeHorizon/\nArms) 
%=1/8-64\nArms\exp\left( \timeHorizon\gap^2_1 (32/\nArms-1/8)  \right)
%\geq 1/8-64\nArms\exp\left( -\timeHorizon\gap^2_1 /16)  \right)
%\geq 1/16
	\]
and hence
	\[
\frac{1}{8}
+
8\nArms\frac{\exp\left(-\frac{\gap^2_1\timeHorizon}{64}\right)}
{\exp\left(-\frac{32\gap^2_{1}\timeHorizon}{\nArms}\right)}
\geq
\frac{1}{4}-\ProErr_{1}(\timeHorizon). 
\]
Therefore, we have 	
	\begin{align*}
\ProErr_{1}(\timeHorizon) 
&\geq
\frac{1}{8}
-
8\nArms\exp\left(\frac{32\gap^2_{1}\timeHorizon}{\nArms}-\frac{\gap^2_1\timeHorizon}{64}\right)\\
&\stackrel{\textbf{(a)}}{\geq}
\frac{1}{8}
-
8\nArms\exp\left(-\frac{\gap^2_1\timeHorizon}{128}\right)\\
&\stackrel{\textbf{(b)}}{\geq}
\frac{1}{16}\CommaBin
\end{align*}
%
where \textbf{(a)} is 
	because $\nArms>32\times128$ and
	\textbf{{(b)}} is because $\nArms\exp\left( -\timeHorizon\gap^2_1 /128\right)\leq 1/128$ by assumptions.	
	We now claim that there exists at least one~$\gainVector\in\eventc_1$   
	with best arm $1$  such that 
	$\ProErr_{\gainVector}(\timeHorizon)\geq 1/32$. 
	The proof is by contradiction. Let us assume that for all $\gainVector\in\eventc_1$, $\ProErr_{\gainVector}(\timeHorizon)< 1/32$. 
	%
	Then, we have 
	\begin{align*}
	%
\ProErr_{1}(\timeHorizon)
	&=
	\Pro_{\gainVector\sim\advPro_1}\left(\recomArm_n \neq 
	\bestArm_{\gainVector}\right)\\
	&=
	\Pro_{\gainVector\sim\advPro_1}\left(\recomArm_n \neq 
	\bestArm_{\gainVector}|\,\gainVector\in\eventc_1\right)
	P\left(\eventc_1\right)
	+
	\Pro_{\gainVector\sim\advPro_1}\left(\recomArm_n \neq 
	\bestArm_{\gainVector}|\,\gainVector\notin\eventc_1\right)
	P\left(\neg\eventc_1\right)\\
	&\leq
\frac{1}{32}
	\times 1
	+
	1\times
	\nArms\exp\left(-\frac{\gap^2_1\timeHorizon}{64}\right)\\
	&\leq
	\frac{1}{32}
	\times 1
	+
	1\times
	\frac{1}{128}\\
	&<
	\frac{1}{16}\CommaBin
	%
\end{align*} which contradicts Equation~\ref{eq:lowbobcase2}.
%
\end{proof}%