% !TEX root = Main.tex
%

\section{Lower bound in the best of both worlds} %setting (Theorem~\ref{th:lowBOB}) }
\label{app:prooflowBOB}
%
\setcounter{scratchcounter}{\value{theorem}}\setcounter{theorem}{\the\numexpr\getrefnumber{th:lowBOB}-1}
\thi*
\setcounter{theorem}{\the\numexpr\value{scratchcounter}}
\begin{proof}%[Theorem~\ref{th:lowBOB}]
	%    
	Let $\pullsNumber_{k}(t)$ be the number of times that arm 
	$k$ has been pulled by the end of round $t$.
	Let us consider any fixed learner.
	Let us consider a base problem, denoted \basePro{}, with $\nu_1,\ldots, \nu_\nArms$, 
	Bernoulli distributions with mean $\mean^{\basePro}_1,
	\ldots, \mean^{\basePro}_\nArms$ 
	such that $\mean^{\basePro}_1 \triangleq 1/2$, and for all $k\in[2,\nArms], 
	\mean^{\basePro}_k \triangleq  1/2-\gap_k$ 
	(the gaps specified in the claim of the theorem).
	The expectation and probability with respect to the
	learner and the samples of this problem are 
	$\Exp_{\basePro}$ and $\Pro_{\basePro}$.% with $\gap_k\geq 0$ for .
	
	Here, $i\in\setArmsmo$ denotes the rank of a suboptimal arm in the
	base problem. Next, we consider  a constant $\dividefac_i\leq 1$.
	We also consider  an early  period of the game $t=1,\ldots,
	\timeHorizon_i\triangleq\lceil \timeHorizon\dividefac_i\rceil$,\
	that we denote~$\earlyPhase_i$. %
	%    \todov{define with equal triangle}
	The proof could work with any set of $\timeHorizon_i$ rounds 
	that are not necessarily the first of the game but we stick 
	to the early ones to ease the notation.
	Now we want to identify an arm with a small gap that the 
	learner will not pull
	very much in the base problem \basePro{}
	during the period~$\earlyPhase_i$.
	From our learner, during the period $\earlyPhase_i$, 
	in any case at most 
	$i-2$ arms among $\setArmsmo$ %, noted $S$,  
	will, in expectation, be pulled strictly more than 
	$2\timeHorizon_i/i $ as otherwise the total number of pulls
	is strictly larger than $(i-1)\times2\timeHorizon_i/i\geq\timeHorizon_i$.
	Therefore, we have at least $\nArms-1-(i-2)=\nArms-i+1$ arms 
	included in the set $\setArmsmo$, 
	and that form a set
	noted $S$,  that are pulled in expectation less than $2\timeHorizon_i/i $.
	Among these arms, let us consider  arm $\refarm \triangleq \argmax_{k\in S}
	\mean_k$ with highest mean. By construction we have
	%
	\begin{equation}\label{eq:notSoMuchPulls}
	\Exp_{\basePro}\left[\pullsNumber_{\refarm}(n_i) \right]\leq \frac{2n_i}{i}\cdot
	\end{equation}
	%        
	Note that by construction, we have also that  $\gap_{\refarm}\leq\gap_i$, because
	otherwise it would mean that $\mean_{\refarm}<\mean_i$ 
	and so there would exist at most $\nArms-i-1$ arms with lower means than $\refarm$.
	This contradicts the fact that $\refarm$ has the highest 
	mean among $\nArms-i+1$ arms.
	
	
	Now we construct an i.i.d.\,stochastic problem, denoted \stoPro{}, where
	the distribution of the arms are the same as in the \basePro{} 
	problem  except for arm $\refarm$, $\mu^{\stoPro}_{k}\triangleq \mu^{\basePro}_{k} $ 
	for all $k\neq \refarm$.
	%    We distinguish two cases: if the  $\refarm=1$ then
	We set in  \stoPro{},         
	$\mu^{\stoPro}_{\refarm} \triangleq 1/2 + \gap_1/2$.
	This means that 
	the best arm in the \stoPro{} is the arm $\refarm$, $\bestArmSto\triangleq\refarm$.
	Also note that the gaps in the \stoPro{} problem verify
	$\gap_k\leq \gap^{\stoPro}_k=\gap_k+\gap_1/2\leq 2\gap_k$ for 
	all $k\in[2,\refarm-1]\cup[\refarm+1:\nArms]$. Also, $\gap_1= 
	\gap^{\stoPro}_1/2$ and $ \gap^{\stoPro}_{\refarm}=\gap_{1}/2$.
	Therefore,  \[\frac{\complexityBoth^{\basePro}}{2}
	\leq\complexityBoth^{\stoPro}\leq 8\complexityBoth^{\basePro}.\]
	The expectation and probability with respect to the
	learner and the samples of this problem are denoted
	$\Exp_{\stoPro}$ and $\Pro_{\stoPro}$.
	
	
	
	The second bandit problem is the adversarial one, denoted \advPro{}.
	This adversarial problem samples  $\gainVector$ randomly.
	At round $t\in[\timeHorizon]$, for arm $k$, $g_{k,t}$ is sampled 
	from the Bernoulli distribution with mean $\mu_{k}^{\advPro}(t)$.
	For all $t$ and $k\neq\refarm$, \advPro{} follows the \basePro{} 
	problem:  $\mu_{k}^{\advPro}(t)\triangleq\mu_{k}^{\basePro}$.
	For arm $\refarm$,
	until the end of phase $\earlyPhase_i$, for all $t$ with $1\leq 
	t\leq n_i$, $\mu^{\advPro}_{\refarm}(t)\triangleq\mu_{\refarm}^{\basePro}$ 
	and then a switch happens, for $n_i< t\leq n$, arm $\refarm$ 
	possesses  the same distributions as in the \stoPro{} problem,  
	$\mu^{\advPro}_{\refarm}(t)\triangleq \mu_{\refarm}^{\stoPro}$.
	The expectation and probability with respect to the
	learner and the samples of this problem are denoted
	$\Exp_{\advPro}$ and $\Pro_{\advPro}$.
	
	
	Let us now study the identity of the best arm in \advPro{}.
	We want to show that, with high probability, 
	the best arm in the \advPro{} is  arm $1$ if we have 
	$\dividefac_i\geq \gap_1/ \gap_i.$
	We denote the expected cumulative gain in \advPro{} of each 
	arm $k\in\setArms$ as $\expectCumul_{k}\triangleq\sum_{t=1}^{\timeHorizon} \mean^{\advPro}_{k}(t)$.
	%\todov{we need to lower bound the lowest mean by a constant to fix when things and constant gets clear}
	%\todov{check what is happening with the flooring of $\timeHorizon_i$,$\lfloor\rfloor\timeHorizon_i\rfloor$ }    
	For arm $\refarm$, we have 
	\begin{align*}    
	\expectCumul_{\refarm}&=
	\sum_{t=1}^{\timeHorizon_i} \mean^{\advPro}_{\refarm}(t)
	+
	\sum_{t=\timeHorizon_i+1}^{\timeHorizon} \mean^{\advPro}_{\refarm}(t)\\
	&=
	\timeHorizon_i \mean^{\basePro}_{\refarm}
	+
	(\timeHorizon-\timeHorizon_i) \mean^{\stoPro}_{\refarm}  \\
	&=
	\timeHorizon_i \left(\frac12 - \gap_i\right)
	+
	(\timeHorizon-\timeHorizon_i) \left(\frac12 + \frac{\gap_1}{2}\right)\\
	&\leq
	\frac{\timeHorizon}{2}-  \timeHorizon_i \gap_i
	+
	\frac{\timeHorizon \gap_1}{2}\ \\    
	&\leq    
	\frac{\timeHorizon}{2} - \dividefac_i\timeHorizon \gap_i
	+
	\frac{\timeHorizon \gap_1}{2}\\\
	&\leq    
	\frac{\timeHorizon}{2} - \timeHorizon \gap_1
	+
	\frac{\timeHorizon \gap_1}{2}\\\        
	&=    
	\frac{\timeHorizon}{2} - \frac{\timeHorizon \gap_1}{2}\cdot
	\end{align*}    
	%\todov{write this with better notation once they are fixed}
	For all $k\in\setArms$, $k\neq\refarm$ and $k\neq1$, we have $\expectCumul_{k}=     
	\timeHorizon/2 - \timeHorizon \gap_k$. Furthermore, $\expectCumul_{1}=     
	\timeHorizon/2$. 
	%    
	Let us consider the event
	$\eventc =\left\{ \forall k\in\setArms,  |\cumulGain_k-\expectCumul_{k}| \leq  \timeHorizon \gap_1/8 \right\}
	$.
	Using a standard Hoeffding argument with a union bound we have 
	that $\Pro(\eventc) \geq 1-\nArms\exp\left(-\gap^2_1\timeHorizon/32\right)$.
	Then, in the \advPro{} problem, for any $\gainVector$ that is 
	compatible with $\eventc$, the gaps associated with $\gainVector$ 
	verify
	$\gap_k/2\leq\gap^{\gainVector}_k\leq2\gap_k$ for all $k\neq 
	\refarm$ and $\gap_1/4\leq\gap^{\gainVector}_{\refarm}\leq\gap_1$.    
	Note that therefore the only difference between the two problems 
	\stoPro{} and \advPro{} is for  arm $\refarm$ during phase $\earlyPhase_i$.
	
	
	If we have $\dividefac_i\geq \gap_1 / \gap_i$,  then with high probability, the respective best arms 
	in problem \stoPro{} and problem \advPro{}
	are % \todom{rephrase} 
	different, i.e., $\bestArmAd\neq\bestArmSto$. That is what we assume for
	the rest of the proof. Indeed, we want to use the fact that 
	the two models are hard to differentiate from the learner point 
	of view with a certain probability and that then the learner has 
	to either choose to recommend $\bestArmAd$ or $\bestArmSto$, which are different, and therefore possibly suffer a mistake.
	
	
	%    \todo{talk about is $J_n$?}
	
	Then we prove the following relation between the probability of error 
	in the stochastic problem $\ProErr_{\stoPro}(\timeHorizon)$ to the
	probability of successful identification in the adversarial problem
	$1-\ProErr_{\advPro}(\timeHorizon)$:
	where $\ProErr_{\advPro}(\timeHorizon)$ is defined here as 
	$\ProErr_{\advPro}(\timeHorizon) \triangleq \Pro_{\gainVector\sim\advPro}\left(\recomArm_n \neq \bestArm_{\gainVector}\right)$,
	%
	\begin{equation}\label{eq:mainlowBOB}
	\ProErr_{\stoPro}(\timeHorizon)
	\geq
	\frac{1}{8}\left(\frac{1}{4}-\ProErr_{\advPro}(\timeHorizon) \right)\exp\left(-\frac{64\gap^2_{i}\timeHorizon_i}{i}\right)\!\cdot
	\end{equation}
	%
	To obtain Equation~\ref{eq:mainlowBOB}, we write
	%
	\begin{align*}
	\ProErr_{\stoPro}(\timeHorizon)
	&=
	\Pro_{\stoPro}\left(\recomArm_\timeHorizon\neq \refarm\right)\\
	&\stackrel{\textbf{(a)}}{\geq}
	\Pro_{\stoPro}\left(\recomArm_\timeHorizon=1\right)\\
	&\stackrel{\textcolor{white}{(b)}}{\geq}
	\Pro_{\stoPro}\left(\recomArm_\timeHorizon=1 
	\cap \pullsNumber_{\refarm} (\timeHorizon_i) \leq  \frac{4\timeHorizon_i}{i}\right)\\
	&\stackrel{\textbf{(b)}}{\geq}
	\Pro_{\advPro}\left(\recomArm_\timeHorizon=1 
	\cap \pullsNumber_{\refarm} (\timeHorizon_i) 
	\leq  \frac{4\timeHorizon_i}{i}\right)\frac{1}{8}\exp\left(-\frac{16\gap^2_{\refarm}4\timeHorizon_i}{i}\right)\\
	&\stackrel{\textbf{(c)}}{\geq}
	\left(\Pro_{\advPro}\left(\recomArm_\timeHorizon=1 \right)
	-\frac{1}{2}\right)\frac{1}{8}\exp\left(-\frac{64\gap^2_{i}\timeHorizon_i}{i}\right)\\
	&\stackrel{\textbf{(d)}}{\geq}
	\frac{1}{8}\left(\frac{1}{4}-\ProErr_{\advPro}(\timeHorizon) \right)
	\exp\left(-\frac{64\gap^2_{i}\timeHorizon_i}{i}\right)\!\CommaBin
	\end{align*}
	%
	where 
	\textbf{(a)}  is because we have $1\neq\refarm$ by construction
	\textbf{(b)} uses the change-of-measure argument from Lemma~\ref{l:changeMeas}. 
	%     \todov{do we need something about Jn being a deterministic function so that we can use the lemma on events?} 
	%    \todov{explained in a separate lemma because we might be able to have something that we could reuse for the lower bound of the general case above}
	Step~\textbf{(c)} is because $\gap_{\refarm}\leq\gap_i$ and 
	%
	\begin{align*}
	\Pro_{\advPro}\left(\recomArm_\timeHorizon=1 \right)
	&=
	\Pro_{\advPro}\left(\recomArm_\timeHorizon=1 \cap 
	\pullsNumber_{\refarm} (\timeHorizon_i) \leq  \frac{4\timeHorizon_i}{i} \right)
	+
	\Pro_{\advPro}\left(\recomArm_\timeHorizon=1 \cap 
	\pullsNumber_{\refarm} (\timeHorizon_i)>  \frac{4\timeHorizon_i}{i} \right)\\
	&\leq
	\Pro_{\advPro}\left(\recomArm_\timeHorizon=1 \cap 
	\pullsNumber_{\refarm} (\timeHorizon_i) \leq  \frac{4\timeHorizon_i}{i}\right)
	+
	\Pro_{\advPro}\left( \pullsNumber_{\refarm} (\timeHorizon_i)> \frac{4\timeHorizon_i}{i} \right)\\
	&\leq
	\Pro_{\advPro}\left(\recomArm_\timeHorizon=1 \cap \pullsNumber_{\refarm} (\timeHorizon_i) \leq \frac{4\timeHorizon_i}{i} \right)
	+
	\frac12\CommaBin
	\end{align*}
	%
	where the last inequality combines 
	Equation~\ref{eq:notSoMuchPulls} and a Markov inequality.
	%
	Step \textbf{(d)} %\todo{maybe no need for this depending on the definition of the best arm} 
	is because first, as computed above,
	$\Pro_{\gainVector\sim\advPro}\left(\bestArm_{\gainVector}=1\right)\geq \Pro(\eventc) \geq 1-\nArms\exp\left(-\gap^2_1\timeHorizon/32\right)$  
	and therefore,
	we have
	%
	\begin{align*}
	\ProErr_{\advPro}(\timeHorizon)
	&    =
	1- \Pro_{\gainVector\sim\advPro}
	\left(\recomArm_\timeHorizon=\bestArm_{\gainVector}\right)\\
	&    =
	1- \Pro_{\advPro}\left(\recomArm_\timeHorizon=\bestArm_{\gainVector}\cap\bestArm_{\gainVector}=1\right)
	- \Pro_{\gainVector\sim\advPro}
	\left(\recomArm_\timeHorizon=\bestArm_{\gainVector}\cap\bestArm_{\gainVector}\neq 1\right)\\
	&    \geq
	1-
	\Pro_{\advPro}\left(\recomArm_\timeHorizon=1\right)
	-      \Pro_{\gainVector\sim\advPro}
	\left(\bestArm_{\gainVector}\neq 1\right)\\
	&    \geq
	1-
	\Pro_{\advPro}\left(\recomArm_\timeHorizon=1\right)
	- \nArms\exp\left(-\frac{\gap^2_1\timeHorizon}{32}
	\right)\\
	&    \geq
	1-
	\Pro_{\advPro}\left(\recomArm_\timeHorizon=1\right)
	- \frac14\CommaBin
	\end{align*}
	%
	where  %\todom{there is also STO here}
	$\nArms\exp\left(-\gap^2_1\timeHorizon/32\right)\leq 1/4$ 
	holds by assumption of the theorem.    
	Having just proved Equation~\ref{eq:mainlowBOB}, we proceed with 
	the rest of the proof.
	In order to maximize the lower bound we maximize $n_i$ by 
	setting $\dividefac_i\triangleq\gap_1/ \gap_i$.
	%\todov{can we have a better constant here?}.
	Then again to maximize the lower bound, we finally 
	choose $i \triangleq \argmax_{k\in\setArms} (k/\gap_k).$
	Using $\complexityBoth^{\basePro}/2\leq\complexityBoth^{\stoPro}\leq8\complexityBoth^{\basePro}$, we have 
	\begin{align*}
	\frac{1}{64}\exp\left(-\frac{2048\timeHorizon}
	{\complexityBoth^{\stoPro}}\right)
	&\leq
	\frac{1}{64}\exp\left(-\frac{256\timeHorizon}{\complexityBoth^{\basePro}}\right)\\
	&= \frac{1}{64}\exp\left(-\frac{256\gap_{i}\gap_1\timeHorizon}{i}\right)\\
	&= \frac{1}{64}\exp\left(-\frac{128\gap^2_{i}}{i}\frac{{2\gap_1}\timeHorizon}{\gap_i}\right)\\
	&\leq \frac{1}{64}\exp\left(-\frac{128\gap^2_{i}}{i}
	\left(\left\lceil\frac{2\gap_1\timeHorizon}{\gap_i}\right\rceil-1\right)\right)\\
	&\leq
	\frac{1}{64}\exp\left(-\frac{64\gap^2_{i}\timeHorizon_i}{i}\right)\!\cdot
	\end{align*}
	%
	Therefore,     
	if $\ProErr_{\stoPro}(\timeHorizon)
	\leq 
	1/64\exp\left(-2048\timeHorizon/\complexityBoth^{\stoPro}\right)$ 
	then using the inequality above, we get that 
	$\ProErr_{\stoPro}(\timeHorizon)\!\leq\!\exp\left(-64\gap^2_{i}\timeHorizon_i/i\right)\!/64.$        
	%
	Finally, using the inequality in Equation~\ref{eq:mainlowBOB},    we have that
	if $\ProErr_{\stoPro}(\timeHorizon)
	\leq 
	\exp\left(-64\gap^2_{i}\timeHorizon_i/i\right)/64$
	then  $\ProErr_{\advPro}(\timeHorizon)\geq 1/8$.
	% \todov{woooo bad bad bad! wrong direction!}
	
	
	
	We now claim that there exists at least one~$\gainVector\in\eventc$  such that 
	$\ProErr_{\gainVector}(\timeHorizon)\geq 1/16$. 
	The proof is by contradiction. Let us assume that for 
	all $\gainVector\in\eventc$,  $\ProErr_{\gainVector}(\timeHorizon)< 1/16$. 
	%
	Then, we have 
	%
	\begin{align*}
	%    
	\ProErr_{\advPro}(\timeHorizon)
	&    =
	\Pro_{\gainVector\sim\advPro}\left(\recomArm_n \neq \bestArm_{\gainVector}\right)\\
	&       =
	\Pro_{\gainVector\sim\advPro}\left(\recomArm_n \neq \bestArm_{\gainVector}|\,\gainVector\in\eventc\right)
	P(\eventc)
	+
	\Pro_{\gainVector\sim\advPro}\left(\recomArm_n \neq \bestArm_{\gainVector}|\,\gainVector\notin\eventc\right)
	P(\neg\eventc)\\
	&    \leq
	\frac{1}{16}
	\times 1
	+
	1\times
	\nArms\exp\left(-\frac{\gap^2_1\timeHorizon}{32}\right)\\
	&    \leq
	\frac{1}{16}
	\times 1
	+
	1\times
	\frac{1}{32}\\
	&    <
	\frac{1}{8}\CommaBin
	%
	\end{align*} which is a contradiction.%
	%
\end{proof}%
