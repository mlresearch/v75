% !TEX root = Main.tex
\section{General adversarial case: An optimal 
	learner can play uniformly at random}
\label{s:ULGenAd}
%
In this section, we define a simple learner  (\RULE{}) and in Theorem~\ref{th:UPARU}, we 
provide an upper bound on its probability of error 
 depending on the gap in hindsight. 
We also give  a matching lower bound for the general 
adversarial best-arm identification (Theorem~\ref{th:LOWadPRO}) 
proving that \RULE{} obtains the optimal gap-dependent rates of error against worst-case adversaries.
%
%\begin{figure}
%    \centering
%    \framebox{
%        \begin{minipage}{.85\textwidth}
%            \textbf{Given:} $K$ \\
%            For $t=1,2, \ldots, \timeHorizon$\\
% \textcolor{white}{erere}Select arm $\pulledArm_t\in [\nArms]$ 
% with probability $\learnerDist_{k,t}=\Pro(\pulledArm_t=k)
% =\frac{1}{\nArms}$ for all $k\in\setArms$.
%
%            \textbf{Returns:} $\recomArm_\timeHorizon=\argmax_{k\in\setArms} \estCumulGainVector_{k,\timeHorizon}$ 
%        \end{minipage}%\todov{inclue pt in this def?}
%    }
%    \caption{The \emph{Random Uniform Learner (\RULE{})} }\label{fig:uniform}
%\end{figure}
%

%The \emph{Random uniform  learner} (\RULE{}) is detailed in 
%Figure~\ref{fig:uniform}. 
%\todov{if in need for space, we can remove the figure to explain a uniform sampling!}
%Select arm $\pulledArm_t\in [\nArms]$ 
% with probability $\learnerDist_{k,t}=\Pro(\pulledArm_t=k)
% =\frac{1}{\nArms}$ for all $k\in\setArms$
At round $t$, the \emph{random uniform  learner} (\RULE{}) 
selects an arm $\pulledArm_t\in [\nArms]$ uniformly at random, i.e., 
with probability $\learnerDist_{k,t}\triangleq\Pro(\pulledArm_t=k)
\triangleq 1/\nArms$ for all $k\in\setArms$.
%he choice of $\pulledArm_t$ is made uniformly  at 
%random from $\setArms$.
At the end of the game, the recommended 
arm $\recomArm_\timeHorizon$ is the one with highest 
estimated  
%\todom{estimator is unbiased but here we have an actual realization that is biased\\
%Victor: What do you mean biased? the realisation is what it is.} 
cumulative gain, 
$\recomArm_\timeHorizon \triangleq \argmax_{k\in\setArms}
\estCumulGainVector_{k,n}$. 
%We first prove an upper bound on the probability of error of ARU.
%
\begin{restatable}[\textcolor{titleTh}{Upper bound for \RULE{} in the adversarial case}]{theorem}{tata}\label{th:UPARU}
	For any horizon $\timeHorizon$, given rewards $\gainVector$ 
	chosen by an oblivious adversary, with $\gainVector_{k,t}\in [0,1]$ 
	for all $t\in[\timeHorizon]$ and for all $k\in\setArms$, \RULE{}  
	outputs an arm $\recomArm_\timeHorizon$
	with the guarantee that its probability of error 
	$\ProErr_{\advPro(\gainVector)}(\timeHorizon)$ verifies    
	%
	%    \todov{is that true against an adaptive adversary be precise}
	\[
	%\mathbb{P}\big[\recomArm_\timeHorizon \neq \bestArmAd\big] 
	\ProErr_{\advPro(\gainVector)}(\timeHorizon)
	\leq
	\nArms\exp\left(-\frac{3\timeHorizon}{28\complexityUnif(\gainVector)}\right)\!\cdot
	\]
	%    
	%Note that the factor $\nArms$ in front of the exponential is easily replaced by a $\log\nArms$ using a decomposition of the error into groups of halves as in \cite{Karnin13AO}. \todov{should we mention that? should actually get the log from the proof?}
\end{restatable}
%\todov{maybe define $\mean_{k,t}$,$\gap_{k,t}$, is this needed?}
%\todov{should we have $\gainVector$ here in the equation?}
%
%\todov{should be able to remove the factor 2 because it appears i think if you use the two-sided Bernstein inequality... and we might only need one side}

\noindent\textbf{Sketch of the proof} \textit{(full proof in 
	Appendix~\ref{app:proofupadPRO})}:
The deviation of 
$\estCumulGainVector_{k,t}$ from $\cumulGain_{k,t}$ is bounded by 
a Bernstein bound which applies  since, given 
$\gainVector$ and the fact that $\learnerDist_t$ is fixed to 
constant values for all the rounds, the $\estGainVector_{k,t}$ 
are independent.  $\estGainVector_{k,t}$ are scaled 
Bernoulli random variables where the use of the Bernstein 
bound leads to a bound that scales with the 
variance of the $\estGainVector_{k,t}$ which is $\nArms$. Using a  Hoeffding bound would lead to  a 
bound that scales with the range of the $\estGainVector_{k,t}$ \emph{squared}
which is $\nArms^2$. %The final bound is obtained by a union argument is over all the different arms.
%
%\todomi{did we check that all these assumptions BELOW are compatible?\\
%Victor: Why would they?
%the gaps are arbitrary: no assumption. Then K is large. then you set n large enough so that the condition is valid \\
%M: is there is at least one combination of parameters when $K\ge 4095$ and ...
%}
\begin{restatable}[\textcolor{titleTh}{Lower bound for the adversarial problem}]{theorem}{toto}\label{th:LOWadPRO}
	Given any problem class $\Gaps_{3}$ with associated complexity
	$\complexityUnif$,    
	for any learner, for any horizon $\timeHorizon$ such that    
	$\nArms\exp\left( -\timeHorizon\gap^2_1 /128\right)\leq 1/128$ and $\nArms\geq 4096$, there exist 
	$\gainVector^1\in\Gaps_{3}$ and $\gainVector^2\in\Gaps_{3}$ so that  
	the  probabilities of error suffered by the learner on 
	$\gainVector^1$ and $\gainVector^2$, denoted $\ProErr_{\gainVector^1}(n)$ 
	and  $\ProErr_{\gainVector^2}(n)$ respectively, verify
	%
	\[
	%
	\max\left(\ProErr_{\gainVector^1}(n),\ProErr_{\gainVector^2}(n)\right)
	%
	\geq
	%
	\min\left(
	\frac{1}{128}\exp\left(-\frac{32\timeHorizon}{\complexityUnif}\right)\CommaBin
	\frac{1}{32}
	\right)\!\cdot
	%
	\]     
	%
\end{restatable}
%
\noindent\textbf{Sketch of the proof} \textit{(full proof given 
	in Section~\ref{app:prooflowadPRO})}:
We construct two similar bandit problems. Between the two 
problems, only one arm differs by a change in the mean of 
order $\gap_{(1)}$ for about $\timeHorizon/(2\nArms)$ 
time steps. Therefore, using a change-of-measure argument, 
with a probability of order $\exp(-\timeHorizon/\complexityUnif)$ these two problems generate each a 
set of rewards,  $\gainVector^1$ and 
$\gainVector^2$ respectively, that the learner is not able to 
discriminate. In this undecidable case,  the learner 
still needs to recommend an estimated best arm.
However, these two problems have different best arms 
$\bestArm_{\gainVector^1}\neq\bestArm_{\gainVector^2}.$ 
Therefore, the learner makes a mistake of order 
$\exp(-\timeHorizon/\complexityUnif)$ on 
at least one of the two problems.

We consider any \emph{fixed} learner and let us have $K$ base Bernoulli distributions with  
means $\mean_1 \triangleq 1/2$ and for all $k\in[2:\nArms], \mean_k=1/2-\gap_k$. 
We consider the first half of the game from round $t=1$ 
to a round 
$\lfloor n/2\rfloor$ as a  set of rounds denoted 
$\earlyPhase$. 
%Because the total number of pulls by the learner is 
%limited by $\lfloor n/2\rfloor$ 
%in $\earlyPhase$, 
By Dirichlet's box principle, 
there exists at least one  arm, denoted $\refarm$,
that is pulled less than $\cO(\timeHorizon/(2\nArms))$ in 
expectation during $\earlyPhase$. This arm, that the learner 
does not explore very much, is then used to 
construct the two bandit problems that look similar to the learner. 
We now describe the two problems in detail.
%and therefore difficult 
% from the original Bernoulli distributions.
%The two bandit problems sample randomly their set of rewards $\gainVector$. 

The first problem  is  following the 
original Bernoulli distributions for all arms in phase~$\earlyPhase$.
Then the second part of the game, $t=\lfloor n/2\rfloor+1,
\ldots,\timeHorizon$, is deterministic.  Almost all the 
rewards of all the arms are $0$, except some rewards of 
$\refarm$ which are set to $1$. This is done so that in 
expectation in this setup, the total reward of $\refarm$ 
is $\timeHorizon(1/2-\gap_1)$ and therefore it becomes the second
best arm. %\todov{make it more readable}

The second problem only differs from the previous one 
in its first, stochastic  part and only affects~$\refarm$ which instead of 
having the Bernoulli mean $1/2-\gap_{\refarm}$,  has now 
a mean 
of $1/2-\gap_{\refarm}+2\gap_{1}$. In this problem, 
the effect of the deterministic part is that in the end, 
when $t=\timeHorizon$, the expected mean of  arm 
$\refarm$ is 
$(1/2+\gap_1)$ instead of $(1/2-\gap_1)$, therefore in this case, 
$\refarm$ becomes the best arm.
%
%The two problems will be hard to discriminate for the learner but he will have to make a different choice in both. the event on which the learner will be confused with have a probability of order $\exp\left(-\frac{\timeHorizon}{\complexityUnif}\right)$ hence the lower bound.
%
% i think the improvement would be from $1/\nArms\exp()$ to $\exp()$.
\begin{remark}
	The assumption $\nArms\exp(-\gap^2_{(1)}
	\timeHorizon/8)\leq 1/32$ is mild. 
	Essentially, it  asks for horizon $\timeHorizon$ to be 
	large 
	enough so that the stochastic problem is learnable within 
	$\timeHorizon$ rounds. The assumption on~$\nArms$ is 
	likely to be an artifact of the proof. Even with this 
	assumption, our main message holds, in general, 
	no learner can perform better than the random uniform leaner, up to constants.
\end{remark}%