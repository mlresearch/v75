





\section{Proof of Theorem \ref{thm:delay-generic}}

\begin{proof}
Let $\scU \subseteq \{1, \ldots, T\}$ be the (random) subset of update
rounds. Let us call for brevity an update round a $u$-round, and
similarly for the other two kinds. First, observe that if $t$ is a $u$-round, 
we have $\avgloss_t \in [0,1]$. This is because, due to~(\ref{eq:compsum}) and the fact that $I_t = I_{t-1} = \ldots = I_{t-2d+1}$,
%
\begin{align*}
	\avgloss_t 
=
	\frac{1}{2d}\sum_{\tau=t-d+1}^t \lcomp_\tau(I_{\tau-d+1},\dots,I_{\tau})
=
	\frac{1}{2d}\sum_{\tau=t-d+1}^t \lcomp_\tau(I_{t},\dots,I_{t})
\le
   \frac{1}{2d}(2d-1) < 1~.
\end{align*}
%
% First, observe that for any $t \in \scU$ we update using $loss_t$,
% where
%\[
% loss_t = \frac{1}{2d}\sum_{\tau=t-d}^t
% \loss_\tau(I_\tau,\dots,I_{\tau-d+1})\in[0,1]
% \]
% For $t\in \scU$ we have $I_t=\cdots=I_{t-2d+1}$, which implies that
% for any $\tau \in [t-2d+1,t]$ we have
% $\loss_\tau(I_{\tau-d+1},\dots,I_{\tau}) =
% \loss_\tau(I_{t-2d+1},\dots,I_{t-2d+1})$. So that
% \[
%     loss_t = \frac{1}{2d}\sum_{\tau=t-d}^t \sum_{s=0}^{d-1} \loss_{\tau-s}^{(s)}(I_{t-2d+1})\in[0,1]~.
% \]
%
Since a $u$-round is followed by an $r$-round, and during the stretch of $s$-rounds between an $r$-round 
and the next $u$-round the action played by Algorithm \ref{a:delayed-app} does not change, the algorithm
behaves exactly as $A(\eta)$ on the steps in $\scU$. Therefore, if we set for brevity
\[
\Delta_t^k = \frac{1}{2d}\sum_{\tau=t-d+1}^t \Big(\lcomp_\tau(I_{\tau-d+1},\dots,I_{\tau-d+1}) - \lcomp_\tau(k,\dots,k)\Big)~,
\qquad\text{for $t \geq 2d-2$},
\]
we have, for any action $k$,
%
\begin{equation}
\label{e:upperupdate}
	\E\left[\sum_{t \in \scU,\, t\geq 2d-2} \Delta_t^k\right]
\le 
	\E\big[R_A(|\scU|,K,\eta)\big] 
\le
	R_A\big(\E[|\scU|],K,\eta\big)
\le
	R_A(T/2d,K,\eta)~,
%    \le \frac{\ln K}{\eta} + \frac{\eta}{2}K\,\E\bigl[|\scU|\bigr] \le \frac{\ln K}{\eta} + \frac{\eta}{4d}KT
\end{equation}
%
where the second-last inequality is due to the concavity of $R_A(\cdot, K,\eta)$, and
the last inequality derives from $|\scU| \leq T/2d$, for there
can be at most one $u$-round every $2d$ rounds.
%({\bf CG:} I believe one can easily show $\E\bigl[|\scU|\bigr] \leq T/(3d)$ when $q=1/d$.)
Now, notice that by definition of a $u$-round we have, for all $t$,
\[
\Ind{t \in \scU} = \Ind{B_t = 1}\,\Ind{\bigwedge_{s=1}^{2d-1}(t-s\not\in \scU)}~.
\]
Moreover,
% if we set for brevity
% \[
% \Delta_t^k=\avgloss_t - \frac{1}{2d}\sum_{\tau=t-d+1}^t \loss_\tau(k,\dots,k)~,
% \]
% for $t \geq 2d-2$, we can write
%
\begin{align}
	\sum_{t=2d-2}^T \Delta_t^k 
&=
	\frac{1}{2d}\sum_{t=2d-2}^T\sum_{\tau=t-d+1}^t \Big( \lcomp_\tau(I_{\tau-d+1},\dots,I_{\tau-d+1}) - \lcomp_\tau(k,\dots,k) \Big)
\nonumber
\\&= 
	\frac{1}{2d}\sum_{t=d-1}^{2d-3} (t-d+2)\Big( \lcomp_t(I_{t-d+1},\dots,I_{t-d+1}) - \lcomp_t(k,\dots,k) \Big)
\nonumber
\\&
	\quad + \frac{d}{2d}\sum_{t=2d-2}^{T-d+1} \Big( \lcomp_t(I_{t-d+1},\dots,I_{t-d+1}) - \lcomp_t(k,\dots,k) \Big)
\nonumber
\\&
	\quad +\frac{1}{2d}\sum_{t=T-d+2}^{T} (T+1-t)\Big( \lcomp_t(I_{t-d+1},\dots,I_{t-d+1}) - \lcomp_t(k,\dots,k) \Big)
\nonumber
\\&\geq 
	\frac{1}{2}\sum_{t=2d-2}^{T-d+1} \Big( \lcomp_t(I_{t-d+1},\dots,I_{t-d+1}) - \lcomp_t(k,\dots,k) \Big)
\nonumber
\\&
	\quad - \frac{1}{2d}\sum_{t=d-1}^{2d-3} (t-d+2)\lcomp_t(k,\dots,k) - \frac{1}{2d}\sum_{t=T-d+2}^{T} (T+1-t)\lcomp_t(k,\dots,k)
\nonumber
\\&\ge
	\frac{1}{2}\sum_{t=2d-2}^{T-d+1} \Big( \lcomp_t(I_{t-d+1},\dots,I_{t-d+1}) - \lcomp_t(k,\dots,k) \Big) - 2(d-1)~,
\label{e:lowerdelta}
\end{align}
where the last inequality holds because, due to~(\ref{eq:compsum}),
\[
	\sum_{t=d-1}^{2d-3} (t-d+2)\lcomp_t(k,\dots,k) \le (d-1)\sum_{t=d-1}^{2d-3} \lcomp_t(k,\dots,k) \le (d-1)(2d-1)
\]
and
\[
	\sum_{t=T-d+2}^{T} (T+1-t)\lcomp_t(k,\dots,k) \le (d-1)\sum_{t=T-d+2}^{T} \lcomp_t(k,\dots,k) \le (d-1)(2d-1)~.
\]
Now, for any action $k$ we have,
\begin{align}
    \E&\left[\sum_{t \in \scU,\, t\geq 2d-2} \Delta_t^k \right]
=
    \E\left[\sum_{t=2d-2}^T \Ind{t \in \scU} \Delta_t^k \right]
\nonumber\\ &=
    \E\left[\sum_{t=2d-2}^T \Ind{B_t = 1}\,\Ind{\bigwedge_{s=1}^{2d-1}(t-s \not\in \scU)} \Delta_t^k \right]
\nonumber\\ &=
    q\,\E\left[\sum_{t=2d-2}^T \E\left[\Ind{\bigwedge_{s=1}^{2d-1}(t-s \not\in \scU)}\Delta_t^k \,\bigg|\, B_0,\dots,B_{t-2d},I_0,\ldots,I_{t-2d+1}
    \right]\right]
\nonumber\\ &=
    q\,\E\left[\sum_{t=2d-2}^T \Delta_t^k\,\E\left[\Ind{\bigwedge_{s=1}^{2d-1}(t-s \not\in \scU)} \,\bigg|\, B_0,\dots,B_{t-2d},I_0,\ldots,I_{t-2d+1}  \right]
    \right]
\nonumber\\ &=
    q\,\E\left[\sum_{t=2d-2}^T \Delta_t^k\,\E\left[\Ind{\bigwedge_{s=1}^{2d-1}(t-s \not\in \scU)} \,\bigg|\, B_0,\dots,B_{t-2d} \right]
     \right]
\nonumber\\ &=
    q\,\E\left[\sum_{t=2d-2}^T \Delta_t^k\,\Pr'\left(\bigwedge_{s=1}^{2d-1}(t-s \not\in \scU) \right)
     \right]\,,\label{e:expecteddelta}
\end{align}
where we set for brevity $\Pr'(\cdot) = \Pr\bigl(\,\cdot \mid
B_0,B_1\dots,B_{t-2d}\bigr)$.
%
We can thus write
%
\begin{align*}
    1 - \Pr'\left(\bigwedge_{s=1}^{2d-1}(t-s \not\in \scU) \right)
&=
    \Pr'\left(\bigvee_{s=1}^{2d-1}(t-s \in \scU) \right)
\\ &\le
    \sum_{s=1}^{2d-1}\Pr'\bigl(t-s \in \scU\bigr)
\\ & \le
    \sum_{s=1}^{2d-1}\Pr'\bigl(B_{t-s} = 1, t-s-1 \notin \scU, \ldots, t-s-2d+1 \notin \scU\bigr)
\\ & \le
    q\,(2d-1)\,.
\end{align*}
%
Hence, substituting into~(\ref{e:expecteddelta}) and combining with~(\ref{e:lowerdelta}), we conclude that
%
\begin{align}
	\E&\left[\sum_{t \in \scU,\,t\geq 2d-2} \Delta_t^k \right] 
\ge
	q(1-q(2d-1))\sum_{t=2d-2}^T \E\big[\Delta_t^k\big]
\nonumber
\\&\ge
    \frac{q}{2}(1-q(2d-1))\left( \sum_{t=2d-2}^{T-d+1} \Big(\E\big[\lcomp_t(I_{t-d+1},\dots,I_{t-d+1})\big] - \lcomp_t(k,\dots,k)\Big) - 2(d-1) \right)~.
     \label{e:updatenoupdate}
\end{align}
%
Next, using $\E_t[\cdot]$ to denote expectation conditioned on all random events at time steps $1,\dots,t-1$, we observe that
%
\begin{align}
\nonumber
    \E&\Bigl[ \lcomp_t(I_{t-d+1},\dots,I_t) - \lcomp_t(I_{t-d+1},\dots,I_{t-d+1}) \Bigr]
\\&=
\nonumber
    \E\left[ \sum_{s=0}^{d-1} \Bigl( \E_{t-s}\big[\loss_{t-s}^{(s)}(I_{t-s})\big] - \E_{t-d+1}\big[\loss_{t-s}^{(s)}(I_{t-d+1})\big] \Bigr)\right]
\\&=
\nonumber
    \E\left[ \sum_{s=0}^{d-1} \sum_{i=1}^K \loss_{t-s}^{(s)}(i) \bigl(p_{t-s}(i) - p_{t-d+1}(i)\bigr) \right]
\\&\le
\label{eq:stability} 
    \E\left[ \sum_{s=0}^{d-1} \sum_{i\,:\,p_{t-s}(i) > p_{t-d+1}(i)} \bigl(p_{t-s}(i) - p_{t-d+1}(i)\bigr) \right]
%\\ &\le
%    \sum_{s=0}^{d-1} \norm{\bp_{t-s} - \bp_{t-d+1}}_1
\le
    \xi~,
\end{align}
%
where the last inequality is because in any block of $2d$ rounds there is
at most one update of distribution $\bp_t$, each loss component $\loss_{t-s}^{(s)}(i)$ is in $[0,1]$, and because $A(\eta)$ is a $\xi$-stable 
Base MAB.
%update of $A$ changes the distribution by at most $2\eta$ in $1$-norm.
Hence we can write
\begin{align*}
	R_T
&\le
	\E\left[\sum_{t=1}^T \lcomp_t(I_{t-d+1},\dots,I_t)\right] - \sum_{t=1}^T \lcomp_t(k,\dots,k)
\\&=
	\E\left[\sum_{t=1}^T \lcomp_t(I_{t-d+1},\dots,I_t) - \sum_{t=1}^T \lcomp_t(I_{t-d+1},\dots,I_{t-d+1})\right]
\\&
	\quad + \E\left[\sum_{t=1}^T \lcomp_t(I_{t-d+1},\dots,I_{t-d+1})\right] - \sum_{t=1}^T \lcomp_t(k,\dots,k)
\\&\le
	T\,\xi + \underbrace{\E\left[\sum_{t=1}^T \lcomp_t(I_{t-d+1},\dots,I_{t-d+1})\right] - \sum_{t=1}^T \lcomp_t(k,\dots,k)}_{(\star)}
	\tag{from~(\ref{eq:stability})}
\end{align*}
%
Furthermore,
%
\begin{align*}
	(\star)
&\leq
	\E\left[\sum_{t=2d-2}^{T-d+1} \lcomp_t(I_{t-d+1},\dots,I_{t-d+1})\right] - \sum_{t=2d-2}^{T-d+1} \lcomp_t(k,\dots,k) + 3d
\\&\leq
	q(d-1) + \frac{2}{q(1-q(2d-1))}\,\E\left[\sum_{t\in \scU,\,t\geq 2d-2} \Delta_t^k \right] + 3d
\tag{from~(\ref{e:updatenoupdate})}
\\&\leq
	q(d-1) + \frac{2}{q(1-q(2d-1))}\,R_A(T/2d,K,\eta) + 3d~.
\tag{from~(\ref{e:upperupdate})}
\end{align*}
%
By picking $q = \tfrac{1}{2(2d-1)}$ so as to maximize the denominator in the second term of the right-most side yields
$
(*) \leq 8(2d-1)\,R_A(T/2d,K,\eta) + \scO(d)
$, 
so that
$
R_T \leq T\,\xi + 8(2d-1)\,R_A(T/2d,K,\eta) + \scO(d),
$
as claimed.
\end{proof}




\section{Proof of Lemma \ref{l:stabilityexp3}}
\begin{proof}
In this case, stability holds pointwise (for all realizations of $I_1,\dots,I_T$) rather that in expectation. From~\cite[Lemma 1]{cgmm16} we have, for any round $t$,
\[
	p_{t+1}(i) - p_t(i) \leq \eta\,p_{t+1}(i) \sum_{j=1}^K p_t(j) \ellhat_t(j)
\]
%
Hence we can write
%
\begin{align*}
\sum_{i\,:\,p_{t+1}(i) > p_t(i)} p_{t+1}(i)-p_t(i)
&\leq
\sum_{i\,:\,p_{t+1}(i) > p_t(i)} \eta\,p_{t+1}(i) \sum_{j=1}^K p_t(j) \ellhat_t(j)\\
&=
\sum_{i\,:\,p_{t+1}(i) > p_t(i)} \eta\,p_{t+1}(i) \ell_t(I_t)\\
&\leq
\eta\,\sum_{i\,:\,p_{t+1}(i) > p_t(i)} p_{t+1}(i)\\
&\leq
\eta
\end{align*}
concluding the proof.
\end{proof}



\section{Proof of Lemma \ref{l:stabExp2}}
%
\begin{proof}
Since $\bq_t$ has exponential form, we can apply again~\cite[Lemma 1]{cgmm16} and obtain
\[
	p_{t+1}(\bv) - p_t(\bv)
=
	(1-\gamma)\big(q_{t+1}(\bv) - q_t(\bv)\big)
\le
	(1-\gamma)\eta\,q_{t+1}(\bv) \sum_{\bv'\in\scK} q_t(\bv')\,\blhat_t^{\top}\bv'~.
\]
%
Hence we can write
%
\begin{align*}
	\E&\left[\sum_{\bv \,:\, p_{t+1}(\bv) > p_t(\bv)} p_{t+1}(\bv) - p_t(\bv) \right]
\\&\le
	(1-\gamma)\eta\,\E\left[\sum_{\bv \,:\, p_{t+1}(\bv) > p_t(\bv)} q_{t+1}(\bv) \sum_{\bv'\in\scK} q_t(\bv') \E_t\Big[\blhat_t^{\top}\bv'\Big] \right]
\\&=
	(1-\gamma)\eta\,\E\left[\sum_{\bv \,:\, p_{t+1}(\bv) > p_t(\bv)} q_{t+1}(\bv) \sum_{\bv'\in\scK} q_t(\bv') \bloss_t^{\top}\bv' \right]
	\tag{because estimates are unbiased}
\\&\le
	(1-\gamma)\eta\,\E\left[\sum_{\bv \,:\, p_{t+1}(\bv) > p_t(\bv)} q_{t+1}(\bv) \right]
	\tag{because $\bloss_t^{\top}\bv \in [0,1]$ for all $t$ and $\bv$}
\\&\le
	(1-\gamma)\eta
\end{align*}
concluding the proof.
\end{proof}



\section{Proof of Lemma \ref{lem:lower}}

\begin{proof}
Fix an algorithm $A_d$ for the $d$-delayed bandit setting. Given an assignment $\bloss_1,\dots,\bloss_{T/d}$ of loss vectors for the linear bandit setting, define, for each $i=1,\dots,K$, the loss components of the $d$-delayed bandit setting:
\[
	\loss_t^{(s)}(i) = \left\{ \begin{array}{cl}
		\loss_{\lceil t/d\rceil}(i) & \text{if $t+s\!\!\!\pmod d=0$,}
	\\[1mm]
		0 & \text{otherwise.}
	\end{array} \right.
\]
These components define the following composite loss for $A_d$ playing actions $I_t$,
\[
	\lcomp_t(I_{t-d+1},\dots,I_t)
=
	\sum_{s=0}^{d-1} \loss_{t-s}^{(s)}(I_{t-s})
=	
	\left\{ \begin{array}{cl}
		d\,\bp_t^{\top}\bloss_{\lceil t/d\rceil} & \text{if $t\!\!\!\pmod d=0$,}
	\\[1mm]
		0 & \text{otherwise}
	\end{array} \right.
\]
where $\bp_t$ is defined from $I_{t-d+1},\dots,I_t \in \{1,\dots,K\}$ as follows
\begin{equation}
\label{eq:p-reduction}
	p_t(j) = \frac{1}{d}\sum_{s=t-d+1}^t \Ind{I_s = j} \qquad j=1,\dots,K.
\end{equation}
Essentially, $A_d$ observes a nonzero composite loss only every $d$ time steps, when $t\pmod d=0$. When this happens, the composite loss is $d$ times the linear loss $\bp_t^{\top}\bloss_{\lceil t/d\rceil}$, where $p_t(i)$ is the fraction of times action $i$ was played by $A_d$ in the last $d$ rounds.
%
Given the algorithm $A_d$, we define the algorithm $A$ as follows. If $t\pmod d\neq 0$, then $A$ skips the round. On the other hand, when $t\pmod d=0$, $A$ performs action $\bp_t$ defined in~(\ref{eq:p-reduction}), observes the loss $\bp_t^{\top}\bloss_{\lceil t/d\rceil}$, and returns to $A_d$ the composite loss $\lcomp_t(I_{t-d+1},\dots,I_t)$.

Note that the loss of $A_d$ is $d$ times the loss of $A$. Also, pick any $\bp\in\Delta_K$ such that each component $p(i)$ has the form $\frac{k}{d}$ for some $k\in\{0,\dots,d\}$. Then, for any $t$ that divides $d$, for any $\bloss_t$, and for any $i_{t-d+1},\dots,i_t\in\{1,\dots,K\}$ such that
\[
	p(j) = \frac{1}{d}\sum_{s=t-d+1}^t \Ind{i_s = j} \qquad j=1,\dots,K
\]
the composite loss $\lcomp_t(i_{t-d+1},\dots,i_t)$ is exactly $d$ times the linear loss $\bp^{\top}\bloss_t$. Finally, noting that $A$ experiences $T/d$ rounds concludes the proof.
\end{proof}





\section{Proof of Proposition \ref{l:propcomposite}}

\begin{proof}
Simply observe that 
%
\begin{align*}
\avglossf_t(\bw) 
&= \frac{1}{2d}\sum_{\tau=t-d+1}^t \sum_{s=0}^{d-1} \alpha_{\tau-s}^{(s)}f_{\tau-s}(\bw)\\
&= \frac{1}{2d}
    \left(
    \sum_{\tau=t-2d+2}^{t-d} \sum_{s=t-d+1}^{\tau+d-1} \alpha_{\tau}^{(s-\tau)}f_{\tau}(\bw)
    + 
    \sum_{\tau=t-d+1}^{t} \sum_{s=\tau}^{\tau+d-1} \alpha_{\tau}^{(s-\tau)}f_{\tau}(\bw)
    \right)\\
&= \frac{1}{2d}
    \left(
    \sum_{\tau=t-2d+2}^{t-d} f_{\tau}(\bw) \left(\sum_{s=t-d+1}^{\tau+d-1} \alpha_{\tau}^{(s-\tau)}\right)
    + 
    \sum_{\tau=t-d+1}^{t} f_{\tau}(\bw)
    \right)~. 
\end{align*}
%
Now, since the first inner sum $\sum_{s=t-d+1}^{\tau+d-1} \alpha_{\tau}^{(s-\tau)}$ is upper bounded by $\sum_{s=0}^{d-1} \alpha_{\tau}^{(s)} = 1$, we
see that $\avglossf_t(\bw)$ is indeed a linear combination of the form
%
%\begin{equation}\label{e:fcomposite}
\[
\avglossf_t(\bw) = \sum_{\tau=t-2d+2}^{t} b_{\tau} f_{\tau}(\bw)~,
\]
%\end{equation}
%
whose coefficients $b_{\tau}$ are nonnegative and sum to a quantity which is less than one. All the three claimed properties then immediately follow.
%
\end{proof}




\section{Proof of Lemma \ref{thm:convex}}

\begin{proof}
The proof is almost the same as the one of Theorem~\ref{thm:delay-generic} (up to a change of notation), with the additional care that has to be taken when dealing with the inheritance of Properties $P$ from $\{f_t\}_{t=1}^T$ to $\{\avglossf_t\}_{t=2d-2}^T$. 
%
In particular, if we define
\[
\Delta_t^{\bw} = \frac{1}{2d}\sum_{\tau=t-d+1}^t \bigl(\fcomp_\tau(\tbw_{\tau-d+1},\dots,\tbw_{\tau-d+1}) - \fcomp_\tau(\bw,\dots,\bw)\bigl),\qquad {\mbox{for $t \geq 2d-2$,}}
\]
we have, for any $\bw \in \Omega$,
\[
\E\left[\sum_{t \in \scU,\, t\geq 2d-2} \Delta_t^{\bw}  \right] \leq R_A(T/2d, n ,\eta)~,
\]
since the average loss function $\avglossf_t(\bw)$ enjoys the same properties as those that allow us to prove the regret bound $R_A(T, n ,\eta)$ for the Base BCO algorithm $A$.
%
Next,
%
\begin{align*}
\E&\left[\sum_{t \in \scU,\,t\geq 2d-2} \Delta_t^{\bw} \right] \\
  &\geq
    \frac{q}{2}(1-q(2d-1))\left(\sum_{t=2d-2}^{T-d+1} \Bigl(\E\left[\fcomp_t(\tbw_{t-d+1},\dots,\tbw_{t-d+1})\right] - \fcomp_t(\bw,\dots,\bw)\Bigr) -2(d-1)\right)
\end{align*}
%
is the counterpart to~(\ref{e:updatenoupdate}), and is proved in exactly the same manner. Then, from the notion of stability given in Definition~\ref{d:stabilitybco}, we can write
%
\begin{align*}
    \E&\Bigl[ \fcomp_t(\tbw_{t-d+1},\dots,\tbw_t) - \fcomp_t(\tbw_{t-d+1},\dots,\tbw_{t-d+1}) \Bigr]
\\&=
    \E\left[ \sum_{s=0}^{d-1} \Bigl( f_{t-s}^{(s)}(\tbw_{t-s}) - f_{t-s}^{(s)}(\tbw_{t-d+1})   \Bigr)\right]\\
&=
    \sum_{s=0}^{d-1}\alpha_{t-s}^{(s)}\,\E\left[ f_{t-s}(\tbw_{t-s}) - f_{t-s}(\tbw_{t-d+1})\right]\\
&\leq
    \sum_{s=0}^{d-1}\alpha_{t-s}^{(s)}\,\Bigl[\E\left[ f_{t-s}(\tbw_{t-s}) - f_{t-s}(\tbw_{t-d+1})\right]\Bigl]_+
\leq
	\xi~,
\end{align*}
since there is at most one update of the underlying state variable $\bw_t$ (which in turn determines the distribution of the corresponding $\tbw_t$)
during the rounds from $t-d+1$ to $t$, the coefficients $\alpha_{t-s}^{(s)}$ are in $[0,1]$ for all $s$ and $t$, and Base BCO is assumed to be $\xi$-stable in 
the sense of Definition~\ref{d:stabilitybco}.
%
Piecing together as in the proof of Theorem~\ref{thm:delay-generic} proves the claim.
\end{proof}





\section{Proof of Lemma \ref{l:stabilityconvexscrible}}

\begin{proof}
First recall the standard fact that if $f\,:\,\Omega \rightarrow [0,1]$ is $\beta$-smooth w.r.t. $||\cdot||$, then
\[
f(\bw') \leq f(\bw) + \nabla f(\bw)^\top(\bw'-\bw) + \frac{\beta}{2}\,||\bw'-\bw||^2~.
\]
Let $\E_t[\cdot]$ denote expectation conditioned on all random events up to time $t-1$. Then, by the convexity of $f_{t+1}$, we have
%
\begin{align*}
\E[f_{t+1}(\tbw_{t})] 
= \E\big[\E_{t}[f_{t+1}(\tbw_{t})]\big]
\geq \E\big[f_{t+1}(\E_{t}[\tbw_{t}])\big]
= \E\big[f_{t+1}(\E_{t}[\bw_{t}])\big]
= \E[f_{t+1}(\bw_{t})]~.
\end{align*}
%
Moreover, by the $\beta$-smoothness of $f_{t+1}$, we can write
%
\begin{align*}
\E[f_{t+1}(\tbw_{t+1})] 
&= \E\big[\E_{t+1}[f_{t+1}(\tbw_{t+1})]\big]\\
&= \E\big[\E_{t+1}[f_{t+1}(\bw_{t+1} + \delta\,H_{t+1}^{-1/2}s_{t+1})]\big]\\
&\leq \E\left[\E_{t+1}\left[f_{t+1}(\bw_{t+1}) + \delta\,\nabla f_{t+1}(\bw_{t+1})^\top H_{t+1}^{-1/2}s_{t+1}  + \frac{\beta\delta^2}{2}s_{t+1}^\top H_{t+1}^{-1}s_{t+1}\right]\right]\\
&= \E\left[f_{t+1}(\bw_{t+1}) + \E_{t+1}\left[\frac{\beta\delta^2}{2}s_{t+1}^\top H_{t+1}^{-1}s_{t+1}\right]\right]\\
&= \E\left[f_{t+1}(\bw_{t+1})\right] + \frac{\beta\delta^2}{2}\,\E\left[||H_{t+1}^{-1/2}s_{t+1}||^2\right]\\
&\leq \E\left[f_{t+1}(\bw_{t+1})\right] + \frac{\beta\delta^2\scD^2}{2}~,
\end{align*}
%
the last inequality following from the properties of the Dikin ellipsoid associated with the self-concordant barrier $\Psi$, ensuring that $\bw_{t+1} + H_{t+1}^{-1/2}s_{t+1}$ belongs to $\Omega$, hence bounding $||H_{t+1}^{-1/2}s_{t+1}||$ by the diameter $\scD$.
%
Putting together, we have so far obtained
%
\begin{align}\label{e:1}
\Bigl[\E\left[f_{t+1}(\tbw_{t+1}) - f_{t+1}(\tbw_{t})\right]\Bigl]_+ 
&\leq 
\left[\E\left[f_{t+1}(\bw_{t+1}) - f_{t+1}(\bw_{t})\right]  + \frac{\beta\delta^2\scD^2}{2}\right]_+ \notag\\
&\leq 
\Bigl[\E\left[f_{t+1}(\bw_{t+1}) - f_{t+1}(\bw_{t})\right]\Bigl]_+ + \frac{\beta\delta^2\scD^2}{2}~,
\end{align}
%
where we have further used the fact that $[a]_+$ is nondecreasing in $a \in \R$, and that $[a+b]_+ \leq [a]_+ + [b]_+$ for all $a,b \in \R$.

\iffalse
***************************************

For similar reasons, recalling the definition of ${\hat f_t}$, the smoothed version of $f_t$, for any fixed $\bw \in \Omega$ we have
%
\begin{align*}
{\hat f_t}(\bw) - f_t(\bw) 
&= \E_{b\sim \B^n} [f_t(\bw+\delta H_t^{-1/2} b) - f_t(\bw)] \\
&\leq \E_{b\sim \B^n} \left[\delta\,\nabla f_{t}(\bw)^\top H_{t}^{-1/2}v  + \frac{\beta\delta^2}{2}v^\top H_{t}^{-1}v\right]\\
&= \frac{\beta\delta^2}{2}\E[v^\top H_{t}^{-1}v]\\
&\leq \frac{\beta\delta^2\scD^2}{2}~.
\end{align*}
%
Hence, we can bound the absolute value of the difference of losses in~(\ref{e:1}) as
%
\begin{align*}
|f_{t+1}(\bw_{t+1}) - f_{t+1}(\bw_{t})| 
&\leq |f_{t+1}(\bw_{t+1}) - {\hat f_{t+1}}(\bw_{t+1})|\\ 
&\ \qquad \ + |{\hat f_{t+1}}(\bw_{t+1}) - {\hat f_{t+1}}(\bw_{t})|\\ 
&\ \qquad \ +  |{\hat f_{t+1}}(\bw_{t}) - f_{t+1}(\bw_{t})|\\ 
&\leq |{\hat f_{t+1}}(\bw_{t+1}) - {\hat f_{t+1}}(\bw_{t})| + \beta\delta^2\scD^2~,
\end{align*}
%
so that, combining with`(\ref{e:1}) this reduces to
%
\begin{equation}
\E\left[f_{t+1}(\tbw_{t+1}) - f_{t+1}(\tbw_{t})\right] \leq \E\left[\bigl|{\hat f_{t+1}}(\bw_{t+1}) - {\hat f_{t+1}}(\bw_{t})\bigl| \right] + \frac{3\beta\delta^2\scD^2}{2}~.
\end{equation}
%
Now, since ${\hat g_t}$ is an unbiased estimator of $\nabla {\hat f_t}(\bw_t)$, . . .
****************************************
\fi

Now, consider the Bregman divergence associated with the (strongly convex) barrier function $\Psi$:
\[
B_{\Psi}(\bw,\bw') = \Psi(\bw)-\Psi(\bw') -\nabla \Psi(\bw')^\top(\bw-\bw')~.
\]
Since the sequence ${\bw_t}_{t=1...T}$ are generated by a Follow The Regularized Leader (FTRL) algorithm, we have ---see, e.g., \citep[Equation~(5.2)]{hazan16}
\[
B_{\Psi}(\bw_t,\bw_{t+1}) \leq \eta{\hat g_t}^\top(\bw_t-\bw_{t+1}) \leq \eta ||{\hat g_t}||^*_t\,||\bw_t-\bw_{t+1}||_t~,
\]
where $||\cdot||_t$ is the local norm induced by the Hessian of $\Psi$ at $\bw_t$, i.e., $||\bw||_t = \big(\bw^\top \nabla^2 \Psi (\bw_t) \bw\big)^{1/2}$, and $||\cdot||^*_t$ is its dual, $||\bw||^*_t = \big(\bw^\top (\nabla^2 \Psi (\bw_t))^{-1} \bw\big)^{1/2}$. By the strong convexity of $\Psi$ w.r.t.\ $||\cdot||$ we have
\[
B_{\Psi}(\bw_t,\bw_{t+1}) \geq \frac{\alpha}{2}\,||\bw_t-\bw_{t+1}||^2~,
\]
for some constant $\alpha > 0$. Moreover, one can show that $||{\hat g_t}||^*_t \leq n/\delta$ \citep{st11} and, provided $\eta \leq \frac{\delta}{16n}$, also that $||\bw_t-\bw_{t+1}||_t \leq 1/2$ \citep{ahr12}, so that
%
\begin{equation}\label{e:2}
||\bw_t-\bw_{t+1}|| = \scO\left(\sqrt{\frac{\eta\,n}{\delta}}\right)~,
\end{equation}
%
where the $\scO$ notation hides here the inverse dependence on $\alpha$.
Finally, since $f_t$ is [0,1]-bounded and $\beta$-smooth on a set of diameter $\scD$, it must be that $f_t$ is also Lipschitz with constant $L \leq \frac{2}{\scD} + \scD\beta$, so that combining with~(\ref{e:1}) and~(\ref{e:2}) yields
%
\begin{align*}
\Bigl[\E\left[f_{t+1}(\tbw_{t+1}) - f_{t+1}(\tbw_{t})\right] \Bigl]_+
&\leq \left(\frac{2}{\scD} + \scD\beta\right)\E\left[||\bw_{t+1} -\bw_{t}||\right]  + \frac{\beta\delta^2\scD^2}{2}\\
&= \scO\left(\left(\frac{1}{\scD} + \scD\beta\right)\sqrt{\frac{\eta\,n}{\delta}}  + \beta\delta^2\scD^2\right)~,
\end{align*}
%
as claimed.
\end{proof}


