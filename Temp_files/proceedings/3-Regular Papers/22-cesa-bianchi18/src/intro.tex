\section{Introduction}\label{s:intro}
%
Multiarmed bandits, originally proposed for managing clinical trials, are now routinely applied to a variety of other tasks, including computational advertising, e-commerce, and beyond. Typical examples of e-commerce applications include content recommendation systems, like the recommendation of products to visitors of merchant websites and social media platforms. A common pattern in these applications is that the response elicited in a user by the recommendation system is typically not instantaneous, and might occur some time in the future, well after the recommendation was issued. This delay, which might depend on several unknown factors, implies that the reward obtained by the recommender at time $t$ can actually be seen as the combined effect of many previous recommendations.

The scenario of bandits with delayed rewards has been investigated in the literature under the assumption that the contributions of past recommendations to the combined reward is individually discernible ---see, e.g., \citep{neu2010online,joulani2013online,cgmm16,VernadeCP17}. In a recent paper, \citet{pike2017bandits} revisited the problem of bandits with delayed feedback under the more realistic assumption that only the combined reward is available to the system, while the individual reward components remain unknown. This model captures a much broader range of practical scenarios where bandits are successfully deployed. Consider for example an advertising campaign which is spread across several channels simultaneously (e.g., radio, tv, web, social media). A well known problem faced by the campaign manager is to disentangle the contribution of individual ads deployed in each channel to the overall change in sales.
%
\citet{pike2017bandits} formalized this harder delayed setting in a bandit framework with stochastic rewards, whereby they introduced the notion of \textsl{delayed anonymous feedback} to emphasize the fact that the reward received at any point in time is the sum of rewards of an unknown subset of past selected actions. More specifically, choosing action $I_t \in \{1,\dots,K\}$ at time $t$ generates a stochastic reward $X_t(I_t) \in [0,1]$ and a stochastic delay $\tau_t(I_t) \in \{0,1,\dots\}$, where, for each $i \in \{1,\dots,K\}$, $X_t(i)$ and $\tau_t$ are drawn i.i.d.\ from fixed distributions $\nu_X(i)$ and $\nu_{\tau}$, respectively. The delayed anonymous feedback assumption entails that the reward observed at time $t$ by the algorithm is the sum of $t$ components of the form $X_s(I_s)\Ind{\tau_s = t-s}$ for $s=1,\dots,t$.
%
The main result in \citep{pike2017bandits} is that, when the expected delay $\mu_{\tau}$ is known, the regret is at most of order of
$
    K\big((\ln T)/\Delta + \mu_{\tau}\big)
$.
This bound is of the same order as the corresponding bound for the setting where the feedback is stochastically delayed, but not anonymous \citep{joulani2013online}, and cannot be improved in general.


In this work we study a bandit setting similar to delayed anonymous feedback, but with two important differences. First, we work in a nonstochastic bandit setting, where rewards (or losses, in our case) are generated by some unspecified deterministic mechanism.
% Second, we relax the assumption that the effect of an action is observed at a single instant in the future. More precisely, we assume that the loss for choosing an action at time $t$ is scattered over at most $d$ time steps in the future, $t,t+1,\dots,t+d-1$.
Second, we relax the assumption that the loss of an action is charged to the player at a single instant in the future. More precisely, we assume that the loss for choosing an action at time $t$ is adversarially spread over at most $d$ consecutive time steps in the future, $t,t+1,\dots,t+d-1$.
Hence, the loss observed by the player at time $t$ is a \textsl{composite loss}, that is, the sum of $d$-many loss components $\loss_{t}^{(0)}(I_{t}),\loss_{t-1}^{(1)}(I_{t-1}),\dots,\loss_{t-d+1}^{(d-1)}(I_{t-d+1})$, where $\loss_{t-s}^{(s)}(I_{t-s})$ defines the $s$-th loss component from the selection of action $I_{t-s}$ at time $t-s$. Note that in the special case when $\loss_t^{(s)}(i) = 0$ for all $s = 0,\dots,d-2$, and $\loss_t^{(d-1)}(i) = \loss_t(i)$, we recover the model of nonstochastic bandits with delayed feedback. Our setting, which we call \textsl{composite anonymous feedback}, can accomodate scenarios where actions have a lasting effect which combines additively over time. Online businesses provide several use cases for this setting. For instance, an impression that results in an immediate clickthrough, later followed by a conversion, or a user that interacts with a recommended item ---such as media content--- multiple times over several days, or the free credit assigned to a user of a gambling platform which might not be used all at once.

Our main contribution is a general reduction technique turning a base nonstochastic bandit algorithm into one operating within the composite anonymous feedback setting. We apply our reduction to the standard nonstochastic bandit setting with no delay to provide an upper bound of order $\sqrt{dKT}$ (ignoring factors logarithmic in $K$) on the regret of nonstochastic bandits with composite anonymous feedback, where $d$ is a known delay parameter and $T$ is the time horizon. We also prove a matching lower bound (up to logarithmic factors), thereby showing that, in the nonstochastic case with delay $d$, anonymous feedback is strictly harder than nonanonymous feedback, whose minimax regret was characterized by \citet{cgmm16}. See the table below for a summary of results for nonstochastic $K$-armed bandits (all rates are optimal ignoring factors logarithmic in $K$).
%
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textsc{no delay} & \textsc{delayed feedback} & \textsc{anonymous composite feedback}
\\ \hline\hline
\rule{0pt}{3ex} $\sqrt{KT}$ & $\sqrt{(d+K)T}$ & $\sqrt{dKT}$
\\
\citep{AuerCeFrSc02} & \citep{cgmm16} & (this paper)
\\ \hline
\end{tabular}
\end{center}
%
Our results can be extended to nonstochastic bandit settings that are more general than the standard $K$-armed bandit problem. In fact, our algorithm applies to any bandit problem for which there exists a suitably stable base algorithm whose regret bound is a sublinear and concave function of time. We give concrete examples %of bounds that can be obtained through this reduction
for the settings of combinatorial linear bandits and online bandit convex optimization.

We now give an idea of the proof techniques, specifically referring to the $K$-armed bandit problem.
%Our proof technique combines three main ideas.
Similarly to \citep{pike2017bandits}, we play the same action for a block of at least $2d$ time steps, hence the feedback we get in the last $d$ steps contains only loss components pertaining to the same action, so that we can estimate in those steps the true loss of that action. Unfortunately, although the original losses are in $[0,1]$, the composite losses can be as large as $d$ (a composite loss sums $d$ loss components, and each component can be as large as $1$). This causes a corresponding scaling in the regret, compromising optimality. However, we observe that the total composite loss over any $d$ consecutive steps can be at most $2d-1$. Hence, we can normalize the total composite loss simply by dividing by $2d$ so as to obtain an average loss in the range $[0,1]$. This gives the right scaling for the regret in the second half of each block, since the bandit regret $\sqrt{KT}$ becomes $d\sqrt{KT/d}=\sqrt{dKT}$. The last problem is how to avoid suffering a big regret in the first $d$ steps of each block, where the composite losses mix loss components belonging to more than one action. We solve this issue by borrowing an idea from \citet{dhk14}, who extend the block size by adding a random number of steps having geometric distribution with expectation $2d$ (for technical reasons, in this paper we us a larger expectation of about $4d$).
% Now the blocks are randomly positioned, and all regrets are magically in control except for the first and the last block, where we pay a constant regret of at most $2d$.
This random positioning of the blocks is the key to preventing the oblivious adversary from causing a large regret in the first half of each block. On the other hand, as we prove in Sections~\ref{s:wrapper} and~\ref{s:bco}, the distribution over actions maintained by the base algorithm is ``backward stable''. This implies that the algorithm is not significantly affected by the uncertainty in the positioning of the blocks.
%The price for randomizing the block sizes is that we no longer know where a block begins. Yet, as we shall see in Sections~\ref{s:wrapper} and~\ref{s:bco}, this can be coped with by requiring that the distribution over actions maintained by the base algorithm be ``backward stable''. Backward stability implies that the algorithm's behavior does not change much even if we pretend we have gotten right the starting point of each block.
%CG: this is too detailed for an intro...
%This allows us to fix arbitrarily the beginning of each block. To see why, note that only a single update takes place in each block. So by fixing the starting point arbitrarily, we get at most one spurious update. However, stability implies that things do not change too much even if we ignore this spurious update and pretend we have gotten right the starting point of the block.

\paragraph{Further related work. }
%
Online learning with delayed feedback was studied in the full
information (non-bandit) setting by
\citet{weinberger2002delayed,mesterharm2005line,langford2009slow,joulani2013online,quanrud2015online,khashabi2016adversarial,joulani2016delay,garrabrant2016asymptotic},
see also \citep{shamir2017online} for an interesting variant. The
bandit setting with delay was investigated in
\citep{neu2010online,joulani2013online,mandel2015queue,cgmm16,VernadeCP17,pike2017bandits}.
Our delayed composite loss function generalizes the composite loss
function setting of \citet{ddkp14} ---see the discussion at the end
of Section~\ref{s:wrapper} for details--- and is also related to the
notion of loss functions with memory. This latter setting has been
investigated, e.g., by \cite{adt12}, who showed how to turn an
online algorithm with regret guarantee of $\scO(T^q)$ into one
attaining $\scO(T^{ 1/(2-q)})$-policy regret, also adopting a
blocking scheme. A more recent paper in this direction is
\citep{ahm15}, where the authors considered a more general loss
framework than ours, though with the benefit of counterfactual
feedback, in that the algorithm is aware of the loss it would incur
had it played any sequence of $d$ decisions in the previous $d$
rounds, thereby making their results incomparable to ours.
