\newcommand{\Rlin}{R^{\mathrm{lin}}}
\section{Lower bound}
\label{s:lower}
In this section we derive a lower bound for bandits with composite anonymous feedback. We do that through a reduction from the setting of linear bandits (in the probability simplex) to our setting. This reduction allows us to upper bound the regret of a linear bandit algorithm in terms of (a suitably scaled version of) the regret of an algorithm in our setting. Since the reduction applies to any instance of a linear bandit problem, we can use a known lower bound for the linear bandit setting to derive a corresponding lower bound for our composite setting.

Let $\Delta_K$ be the probability simplex in $\R^K$. At each round $t$, an algorithm $A$ for linear bandit optimization chooses an action $\bp_t\in\Delta_K$ and suffers loss $\bloss_t^{\top}\bp_t$, where $\bloss_t \in [0,1]^K$ is some unknown loss vector. The feedback observed by the algorithm at the end of round $t$ is the scalar $\bloss_t^{\top}\bp_t$. The regret suffered by algorithm $A$ playing actions $\bp_1,\dots,\bp_T$ is
\begin{equation}
\label{eq:lin-regret}
	\Rlin_T = \sum_{t=1}^T \bloss_t^{\top}\bp_t - \min_{\bp\in\Delta_K} \sum_{t=1}^T \bloss_t^{\top}\bp = \sum_{t=1}^T \bloss_t^{\top}\bp_t - \min_{i=1,\dots,K} \sum_{t=1}^T \loss_t(i)
\end{equation}
where we used the fact that a linear function on the simplex is minimized at one of the corners.
Let $\Rlin_T(A,\Delta_K)$ denote the worst case regret (over the oblivious choice of $\bloss_1,\dots,\bloss_T$) of algorithm $A$. Similarly, let $R_T(A_d,K,d)$ be the worst case regret (over the oblivious choice of loss components $\loss_t^{(s)}(i)$ for all $t$, $s$, and $i$) of algorithm $A_d$ for nonstochastic $K$-armed bandits with $d$-delayed composite anonymous feedback. Our reduction shows the following.
%
\begin{lemma}\label{lem:lower}
For any algorithm $A_d$ for $K$-armed bandits with $d$-delayed composite anonymous feedback, there exists an algorithm $A$ for linear bandits in $\Delta_K$ such that
$
	R_T(A_d,K,d) \ge d\,\Rlin_{T/d}(A,\Delta_K)
$.
\end{lemma}
%
%
Our reduction, described in detail in the proof of the above lemma (see the appendix), essentially builds the probability vectors $\bp_t$ played by $A$ based on the empirical distribution of actions played by $A_d$ during blocks of size $d$. Now, an additional lemma is needed (whose proof is given in the appendix).
\begin{lemma}
\label{l:shamir}
The regret of any algorithm $A$ for linear bandits in the simplex satisfies $\Rlin_T(A,\Delta_K) = \widetilde{\Omega}\big(\sqrt{KT}\big)$.
\end{lemma}
%
Using the above two lemmas we can prove the following theorem.
\begin{theorem}
For any algorithm $A_d$ for $K$-armed bandits with $d$-delayed composite anonymous feedback,
$
R_T(A_d,K,d)=\widetilde{\Omega}\big(\sqrt{dKT}\big)
$.
\end{theorem}
%
\begin{proof}
Fix an algorithm $A_d$. Using the reduction of~Lemma~\ref{lem:lower} gives an algorithm $A$ such that
$
	R_T(A_d,K,d) \ge d\,\Rlin_{T/d}(A,\Delta_K) = \widetilde{\Omega}\big(\sqrt{dKT}\big)
$,
where we used Lemma~\ref{l:shamir} with horizon $T/d$ to prove the $\widetilde{\Omega}$-equality.
\end{proof}
%
Although the loss sequence used to prove the lower bound for linear bandits in the simplex is stochastic i.i.d., the loss sequence achieving the lower bound in our delayed setting is not independent due to the deterministic loss transformation in the proof of Lemma~\ref{lem:lower} (which is defined independent of the algorithm, thus preserving the oblivious nature of the adversary).

