\BOOKMARK [1][-]{section.0.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.0.2}{Sampling as optimization in the space of measures}{}% 2
\BOOKMARK [2][-]{subsection.0.2.1}{The Langevin dynamics as the gradient flow of relative entropy}{section.0.2}% 3
\BOOKMARK [2][-]{subsection.0.2.2}{The unadjusted Langevin algorithm}{section.0.2}% 4
\BOOKMARK [3][-]{subsubsection.0.2.2.1}{Convergence to the biased limit}{subsection.0.2.2}% 5
\BOOKMARK [3][-]{subsubsection.0.2.2.2}{ULA as the Forward-Flow discretization of Langevin dynamics}{subsection.0.2.2}% 6
\BOOKMARK [2][-]{subsection.0.2.3}{The Forward-Backward method for Langevin dynamics}{section.0.2}% 7
\BOOKMARK [2][-]{subsection.0.2.4}{The symmetrized Langevin algorithm}{section.0.2}% 8
\BOOKMARK [3][-]{subsubsection.0.2.4.1}{Convergence to the biased limit}{subsection.0.2.4}% 9
\BOOKMARK [1][-]{section.0.3}{Optimization in the space of measures}{}% 10
\BOOKMARK [2][-]{subsection.0.3.1}{Minimizing relative entropy}{section.0.3}% 11
\BOOKMARK [3][-]{subsubsection.0.3.1.1}{Log-Sobolev inequality as gradient domination of relative entropy}{subsection.0.3.1}% 12
\BOOKMARK [3][-]{subsubsection.0.3.1.2}{Langevin dynamics as gradient flow of relative entropy}{subsection.0.3.1}% 13
\BOOKMARK [3][-]{subsubsection.0.3.1.3}{Forward method for Langevin dynamics}{subsection.0.3.1}% 14
\BOOKMARK [3][-]{subsubsection.0.3.1.4}{Backward method for Langevin dynamics}{subsection.0.3.1}% 15
\BOOKMARK [1][-]{section.0.4}{Langevin dynamics as composite optimization in the space of measures}{}% 16
\BOOKMARK [2][-]{subsection.0.4.1}{Forward-Backward for Langevin dynamics}{section.0.4}% 17
\BOOKMARK [2][-]{subsection.0.4.2}{Backward-Forward for Langevin dynamics}{section.0.4}% 18
\BOOKMARK [1][-]{section.0.5}{Discussion and future work}{}% 19
\BOOKMARK [1][-]{section.0.A}{Details for \2472}{}% 20
\BOOKMARK [2][-]{subsection.0.A.1}{Proof of Lemma 1 \(Contraction of ULA\)}{section.0.A}% 21
\BOOKMARK [2][-]{subsection.0.A.2}{Proof of Lemma 2 \(Bias of ULA\)}{section.0.A}% 22
\BOOKMARK [2][-]{subsection.0.A.3}{Proof of Lemma 3 \(Contraction of SLA\)}{section.0.A}% 23
\BOOKMARK [2][-]{subsection.0.A.4}{SLA and ULA for mixture of Gaussians}{section.0.A}% 24
\BOOKMARK [1][-]{section.0.B}{A review of discretization methods for a flow in space}{}% 25
\BOOKMARK [2][-]{subsection.0.B.1}{Integrator, order, and adjoint}{section.0.B}% 26
\BOOKMARK [2][-]{subsection.0.B.2}{The forward method}{section.0.B}% 27
\BOOKMARK [2][-]{subsection.0.B.3}{The backward method}{section.0.B}% 28
\BOOKMARK [2][-]{subsection.0.B.4}{Symmetrized methods}{section.0.B}% 29
\BOOKMARK [2][-]{subsection.0.B.5}{On order of error and order of bias}{section.0.B}% 30
\BOOKMARK [1][-]{section.0.C}{A review of optimization in Riemannian manifold}{}% 31
\BOOKMARK [2][-]{subsection.0.C.1}{Conditions ensuring exponential convergence}{section.0.C}% 32
\BOOKMARK [2][-]{subsection.0.C.2}{Gradient flow}{section.0.C}% 33
\BOOKMARK [3][-]{subsubsection.0.C.2.1}{Exponential contraction of solutions under strong convexity}{subsection.0.C.2}% 34
\BOOKMARK [3][-]{subsubsection.0.C.2.2}{Exponential convergence of function value under gradient domination}{subsection.0.C.2}% 35
\BOOKMARK [2][-]{subsection.0.C.3}{Gradient descent}{section.0.C}% 36
\BOOKMARK [2][-]{subsection.0.C.4}{Proximal gradient}{section.0.C}% 37
\BOOKMARK [2][-]{subsection.0.C.5}{Symmetrized forward method}{section.0.C}% 38
\BOOKMARK [1][-]{section.0.D}{A brief review of the Wasserstein metric in the space of measures}{}% 39
\BOOKMARK [1][-]{section.0.E}{Details for \2473: Optimization in the space of measures}{}% 40
\BOOKMARK [2][-]{subsection.0.E.1}{Expected value}{section.0.E}% 41
\BOOKMARK [3][-]{subsubsection.0.E.1.1}{Gradient domination and strong convexity}{subsection.0.E.1}% 42
\BOOKMARK [3][-]{subsubsection.0.E.1.2}{Gradient flow}{subsection.0.E.1}% 43
\BOOKMARK [3][-]{subsubsection.0.E.1.3}{Gradient descent}{subsection.0.E.1}% 44
\BOOKMARK [3][-]{subsubsection.0.E.1.4}{Proximal gradient}{subsection.0.E.1}% 45
\BOOKMARK [3][-]{subsubsection.0.E.1.5}{Detail for \247E.1.4: Proximal gradient for expected value}{subsection.0.E.1}% 46
\BOOKMARK [2][-]{subsection.0.E.2}{Negative entropy}{section.0.E}% 47
\BOOKMARK [3][-]{subsubsection.0.E.2.1}{The heat flow as gradient flow of negative entropy}{subsection.0.E.2}% 48
\BOOKMARK [3][-]{subsubsection.0.E.2.2}{Fisher information and convexity of negative entropy}{subsection.0.E.2}% 49
\BOOKMARK [3][-]{subsubsection.0.E.2.3}{Forward method for heat flow}{subsection.0.E.2}% 50
\BOOKMARK [3][-]{subsubsection.0.E.2.4}{Backward method for heat flow}{subsection.0.E.2}% 51
\BOOKMARK [2][-]{subsection.0.E.3}{Gradient flow of variance}{section.0.E}% 52
\BOOKMARK [1][-]{section.0.F}{A review of composite optimization}{}% 53
\BOOKMARK [2][-]{subsection.0.F.1}{Why does Forward-Backward work?}{section.0.F}% 54
\BOOKMARK [3][-]{subsubsection.0.F.1.1}{Example: Quadratic function in R}{subsection.0.F.1}% 55
\BOOKMARK [2][-]{subsection.0.F.2}{Convergence rate of FB under gradient domination in Euclidean space}{section.0.F}% 56
\BOOKMARK [1][-]{section.0.G}{Details for \2474}{}% 57
\BOOKMARK [2][-]{subsection.0.G.1}{Forward-Backward for Langevin dynamics}{section.0.G}% 58
\BOOKMARK [2][-]{subsection.0.G.2}{Backward-Forward for Langevin dynamics}{section.0.G}% 59
