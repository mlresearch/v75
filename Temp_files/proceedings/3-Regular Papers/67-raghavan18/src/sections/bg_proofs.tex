We present the proofs for our results on greedy algorithms in Section~\ref{sec:bayesian_greedy}.%
\footnote{That is, all results in Section~\ref{sec:bayesian_greedy} except the regret bound for LinUCB (Theorem~\ref{thm:main-worst-case}(a)), which is proved in Section~\ref{app:linucb}.} This section is structured as follows. In Section~\ref{app:pf_bg:diversity}, we quantify the diversity of data collected by \GreedyStyle algorithms, assuming perturbed context generation. In Section~\ref{app:pf_bg:simulation}, we show that a sufficiently ``diverse" batch history suffices to simulate the reward for any given context vector, in the sense of Definition~\ref{def:simulation}. Jointly, these two subsections imply  that batch history generated by a \GreedyStyle algorithm can simulate rewards with high probability, as long as the batch size is sufficiently large. Section~\ref{sec:bg-proofs-bg} builds on this foundation to derive regret bounds for \bg. The crux is that the history collected by \bg suffices to simulate a ``slowed-down" run of any other algorithm. This analysis extends to a version of \fg equipped with a Bayesian-greedy prediction rule (and tracks the performance of the prediction rule). Finally, Section~\ref{sec:bg-proofs-fg} derives the regret bounds for \fg, by comparing the prediction-rule version of \fg with \fg itself. To derive the results on group externalities, we present all our analysis in Sections~\ref{sec:bg-proofs-bg} and~\ref{sec:bg-proofs-fg} in a more general framework in which only the minority rounds are counted for regret.

\xhdr{Preliminaries.}
We assume perturbed context generation in this section, without further mention.

Throughout, we will use the following parameters as a shorthand:
\begin{align*}
\delta_R &= T^{-2} \\
\hat R  & = \rho\sqrt{2 \log(2 TKd/\delta_R)} \\
R   &= 1 + \hat{R} \sqrt{d}.
\end{align*}
Recall that $\rho$ denotes perturbation size, and $d$ is the dimension. The meaning of $\hat R$ and $R$ is that they are high-probability upper bounds on the perturbations and the contexts, respectively. More formally, by Lemma~\ref{lem:subgaussian_max} we have:
\begin{align}
\Pr\left[ \|\eps_{a,t}\|_\infty \leq \hat R:\;
    \text{ for all arms $a$ and all rounds $t$ } \right] &\leq \delta_R
    \label{eq:bg-proofs-highprob-R-hat}\\
\Pr\left[ \|x_{a,t}\|_2 \leq R:\;
    \text{ for all arms $a$ and all rounds $t$ } \right] &\leq \delta_R
    \label{eq:bg-proofs-highprob-R}
\end{align}


Let us recap some of the key definitions from Section~\ref{sec:bayesian_greedy-key}. We consider \GreedyStyle algorithms, a template that unifies \BayesGreedy and \FreqGreedy. A bandit algorithm is called \emph{\GreedyStyle} if it divides the timeline in batches of Y consecutive rounds each, in each round $t$ chooses some estimate $\theta_t$ of $\theta$, based only on the data from the previous batches, and then chooses the best action according to this estimate, so that
 $a_t = \argmax_a \theta_t\tran x_{a,t}$.

For a batch $B$ that starts at round $t_0+1$, the \emph{batch history} $h_B$ is the tuple
    $((x_{t_0+\tau},\,r_{t_0+\tau}):\; \tau\in [Y])$,
and the \emph{batch context matrix} $X_B$ is the matrix whose rows are vectors
    $(x_{t_0+\tau}:\; \tau\in [Y])$.
Here and elsewhere, $[Y] = \{1, \cdots, Y\}$. The \emph{batch covariance matrix} is defined as
\begin{align}\label{eq:ZB-defn}
\ZB := X_B\tran\, X_B = \sum_{t=t_0+1}^{t_0+Y} x_t\, x_t\tran.
\end{align}


\subsection{Data diversity under perturbations}
\label{app:pf_bg:diversity}

We are interested in the diversity of data collected by \GreedyStyle algorithms, assuming perturbed context generation. Informally, the observed contexts
    $x_1, x_2,\,\ldots $
should cover all directions in order to enable good estimation of the latent vector $\theta$. Following \citet{kannan2018smoothed}, we quantify data diversity via the minimal eigenvalue of the empirical covariance matrix $Z_t$. More precisely, we are interested in proving that $\lambda_{\min}(Z_t)$ is sufficiently large. We adapt some tools from \citet{kannan2018smoothed}, and then derive some improvements for \GreedyStyle algorithms.


\subsubsection{Tools from~\citet{kannan2018smoothed}}

\citet{kannan2018smoothed} prove that $\lambda_{\min}(Z_t)$ grows linearly in time $t$, assuming $t$ is sufficiently large.


\begin{lemma}[\citet{kannan2018smoothed}]
Fix any \GreedyStyle algorithm. Consider round $t \geq \tau_0$, where
    $\tau_0 =160 \frac{R^2}{\rho^2} \log \frac{2d}{\delta} \cdot \log T$.
Then for any realization of $\theta$, with probability $1-\delta$
  \[
    \lambda_{\min}(Z_t) \ge \frac{\rho^2 t}{32 \log T}.
  \]
  \label{lem:fg_big_cov}
\end{lemma}
\begin{proof}
  The claimed conclusion follows from an argument inside the proof  of
    Lemma B.1 from \citet{kannan2018smoothed},
  plugging in
  $  \lambda_0 = \frac{\rho^2}{2\log T}$.
  This argument applies for any $t\geq \tau'_0$, where
    $\tau'_0 = \max\p{32 \log \frac{2}{\delta}, 160 \frac{R^2}{\rho^2}
  \log \frac{2d}{\delta} \cdot \log T}$.
We observe that $\tau'_0=\tau_0$ since $R \ge \rho$.
\end{proof}

Recall that $Z_t$ is the sum
    $Z_t :=\sum_{\tau=1}^t x_\tau x_\tau\tran$.
A key step in the proof of Lemma~\ref{lem:fg_big_cov} zeroes in on the expected contribution of a single round. We use this tool separately in the proof of Lemma~\ref{lem:min_ev_bg}.

\begin{lemma}[\citet{kannan2018smoothed}]\label{lem:eig_increase}
Fix any \GreedyStyle algorithm, and the latent vector $\theta$. Assume $T \ge 4K$. Condition on the event that all perturbations $\eps_{a,t}$ are upper-bounded by
$\hat R$, denote it with $\mE$.
Then with probability at least $\tfrac14$,
  \[
    \lambda_{\min}\p{\E{x_t x_t\tran \given h_{t-1}, \mE}} \ge \frac{\rho^2}{2\log T}.
  \]
\end{lemma}
\begin{proof}
The proof is easily assembled from several pieces in the analysis in  \citet{kannan2018smoothed}.
Let $\thetahatt$ be the algorithm's estimate for $\theta$ at time
$t$. As in~\citet{kannan2018smoothed}, define
\[
  \hcat = \max_{a' \ne a} \thetahatt\tran x_{a',t},
\]
where $\hcat$ depends on all perturbations other than the perturbation for
$x_{a,t}$. Let us say that $\hcat$ is ``good'' for arm $a$ if
\[
  \hcat \le \thetahatt\tran \mu_{a,t} + \rho \sqrt{2 \log T}
  \|\thetahatt\|_2.
\]

First we argue that
  \begin{align}\label{eq:pf:lem:eig_increase-1}
    \Pr\b{\hcat \text{ is good for } a \given a_t = t, \mE} \ge \tfrac14.
  \end{align}

Indeed, in the proof of their Lemma~3.4, \citet{kannan2018smoothed} show that for
any round, conditioned on $\mE$, if the probability that arm $a$ was chosen over the randomness of the
perturbation is at least $2/T$, then
  the round is good for $a$ with probability at least $\tfrac12$. Let $B_t$ be the set of
  arms at round $t$ with probability at most $2/T$ of being chosen over the
  randomness of the perturbation. Then,
  \[
    \Pr_{\varepsilon \sim \mc N(0, \rho^2 I)} \b{a_t \in B_t} \le \sum_{a \in
    B_T} \Pr_{\varepsilon \sim \mc N(0, \rho^2 I)} \b{a_t = a} \le
    \tfrac{2}{T} |B_t| \le \tfrac{2K}{T} \le \tfrac{1}{2}.
  \]
  Since by assumption $T \ge 4K$, \eqref{eq:pf:lem:eig_increase-1} follows.

Second, we argue that
\begin{align}\label{eq:pf:lem:eig_increase-2}
    \lambda_{\min}\p{\E{x_{a,t}x_{a,t}\tran \given a_t=a, \hcat \text{ is
    good}}} \ge \frac{\rho^2}{2\log T}
\end{align}
This is where we use conditioning on the event $\{ \eps_{a,t} \leq \hat R\}$. We plug in $r = \rho \sqrt{2 \log T}$ and $\lambda_0 = \frac{\rho^2}{2 \log T}$
into Lemma 3.2 of \citet{kannan2018smoothed}. This lemma applies because with these parameters, the perturbed distribution of context arrivals satisfies the
    $(\rho\sqrt{2 \log T}, \rho^2/(2 \log T))$-diversity
condition from \citet{kannan2018smoothed}. The latter is by
    Lemma 3.6 of \citet{kannan2018smoothed}.
This completes the proof of \eqref{eq:pf:lem:eig_increase-2}. The lemma follows from \eqref{eq:pf:lem:eig_increase-1} and \eqref{eq:pf:lem:eig_increase-2}.
\end{proof}

Let $\fmt$ be the \FreqGreedy estimate for $\theta$ at time $t$, as defined in \eqref{eq:FG-est-defn}. We are interested in quantifying how the quality of this estimate improves over time. \citet{kannan2018smoothed} prove, essentially, that the distance between $\fmt$ and $\theta$ scales as
    $\sqrt{t}/\lambda_{\min}(Z_t)$.
\begin{lemma}[\citet{kannan2018smoothed}]
Consider any round $t$ in the execution of \FreqGreedy. Let $t_0$ be the last round of the previous batch. For any $\theta$ and any $\delta>0$, with probability $1-\delta$,
  \[
    \|\theta - \fmt\|_2 \le
    \frac{\sqrt{t_0 \cdot 2dR \log \tfrac{d}{\delta}}}{\lambda_{\min}(Z_{t_0})}.
  \]
  \label{lem:fmt_close}
\end{lemma}

\subsubsection{Some improvements}

We focus on batch covariance matrix $\ZB$ of a given batch in a \GreedyStyle algorithm. We would like to prove that $\lambda_{\min}(\ZB)$ is sufficiently large with high probability, as long as the batch size $Y$ is large enough. The analysis from \citet{kannan2018smoothed} (a version of Lemma~\ref{lem:fg_big_cov}) would apply, but only as long as the batch size is least as large as the $\tau_0$ from the statement of Lemma~\ref{lem:fg_big_cov}. We derive a more efficient version, essentially shaving off a factor of $8$.%
\footnote{Essentially, the factor of $160$ in Lemma~\ref{lem:fg_big_cov} is replaced with factor $\tfrac{8e^2}{(e-1)^2}<20.022$ in \eqref{eq:lem:min_ev_bg-Y}.}

\begin{lemma}
Fix a \GreedyStyle algorithm and any batch $B$ in the execution of this algorithm.  Fix $\delta>0$ and assume that the batch size $Y$ is at least
\begin{align}\label{eq:lem:min_ev_bg-Y}
    Y_0 :=  (\tfrac{R}{\rho})^2 \,
            \tfrac{8e^2}{(e-1)^2}\,
            \p{1 + \log \tfrac{2d}{\delta}}\, \log(T)
    + \tfrac{4e}{e-1} \log \tfrac{2}{\delta}.
\end{align}
Condition on the event that all perturbations in this batch are upper-bounded by $\hat R$, more formally:
\[ \mE_B = \{ \|\eps_{a,t}\|_\infty \leq \hat R:\;
    \text{ for all arms $a$ and all rounds $t$ in $B$} \}. \]
Further, condition on the latent vector $\theta$ and the history $h$ before batch $B$. Then
\begin{align}\label{eq:lem:min_ev_bg}
    \Pr\left[\;  \lambda_{\min}(\ZB) \ge R^2 \given \mE_B,h,\theta \right] \geq 1-\delta.
\end{align}
The probability in \eqref{eq:lem:min_ev_bg} is over the randomness in context arrivals and rewards in batch $B$.
  \label{lem:min_ev_bg}
\end{lemma}

The improvement over Lemma~\ref{lem:fg_big_cov} comes from two sources: we use a tail bound on the sum of
  geometric random variables instead of a Chernoff bound on a binomial random
  variable, and we derive a tighter application of the eigenvalue concentration
inequality of~\citet{tropp2012user}.

\begin{proof}
Let $t_0$ be the last round before batch $B$. Recalling \eqref{eq:ZB-defn}, let
\[ \WB = \sum_{t=t_0+1}^{t_0+Y} \E{x_t x_t\tran \given h_{t-1}} \]
be a similar sum over the expected per-round covariance matrices. Assume $Y\geq Y_0$

The proof proceeds in two steps: first we lower-bound $\lambda_{\min}(\ZB)$, and then we show that it implies \eqref{eq:lem:min_ev_bg}. Denoting
    $ m = R^2\,\tfrac{e}{e-1}\,(1+\log \tfrac{2d}{\delta})$,
we claim that
\begin{align}\label{eq:matrix_conc}
    \Pr\b{\lambda_{\min}(\WB) < m \given \mE_B,h} \le \tfrac{\delta}{2}.
\end{align}

To prove this, observe that $\WB$'s minimum eigenvalue increases by at least $\lambda_0
  = \rho^2/(2\log T)$ with probability at least $1/4$ each round by
  Lemma~\ref{lem:eig_increase}, where the randomness is over the history, \ie the
  sequence of (context, reward) pairs. If we want it to go up to $m$, this
  should take $4m/\lambda_0$ rounds in expectation. However, we need it to go to
  $m$ with high probability. Notice that this is dominated by the sum of
  $m/\lambda_0$ geometric random variables with parameter $\frac{1}{4}$. We'll
  use the following bound from~\citet{janson2017tail}: for $X = \sum_{i=1}^n X_i$
  where $X_i \sim \text{Geom}(p)$ and any $c \ge 1$,
  \[
    \Pr[X \ge c\E{X}] \le \exp\p{-n(c - 1 - \log c)}.
  \]
  Because we want the minimum eigenvalue of $\WB$ to be $m$, we need $n =
  m/\lambda_0$, so $\E{X} = 4m/\lambda_0$. Choose  $c =
  \p{1+\frac{\lambda_0}{m} \log \tfrac{2}{\delta}} \tfrac{e}{e-1}$. By
  Corollary~\ref{cor:e1ex},
  \begin{align*}
    c - 1 - \log c &\ge \tfrac{e-1}{e} \cdot c - 1 = \tfrac{\lambda_0}{m} \log \tfrac{2}{\delta}.
  \end{align*}
  Therefore,
  \[
    \Pr\b{X \ge c\E{X}} \le \exp\p{-n \cdot \tfrac{\lambda_0}{m} \log
    \tfrac{2}{\delta}} = \p{\tfrac{\delta}{2}}^{n \cdot \lambda_0/m} =
    \tfrac{\delta}{2}
  \]
  Thus, with probability $1 - \frac{\delta}{2}$, $\lambda_{\min}(\WB) \ge m$ as long as the batch size $Y$ is at least
  \[
    \frac{e}{e-1} \p{1 + \frac{\lambda_0}{m} \log \frac{2}{\delta}} \cdot
    \E{X} = \frac{4e}{e-1} \p{\frac{m}{\lambda_0} + \log \frac{2}{\delta}} = Y_0.
  \]
  This completes the proof of \eqref{eq:matrix_conc}.

To derive \eqref{eq:lem:min_ev_bg} from \eqref{eq:matrix_conc}, we proceed as follows. Consider the event
\[ \mE = \left\{\;  \lambda_{\min}(\ZB) \le R^2 \text{ and }
             \lambda_{\min}(\WB) \ge m \; \right\}. \]
Letting $\alpha = 1 - R^2/m$ and rewriting $R^2$ as $(1-\alpha)m$, we use a concentration inequality from \citet{tropp2012user} to guarantee that
  \[
    \Pr[\mE \given \mE_B,h]
        \le d \p{e^\alpha (1-\alpha)^{1-\alpha}}^{-m/R^2}.
  \]
  Then, using the fact that $x^x \ge e^{-1/e}$ for all $x > 0$, we
  have
\begin{align*}
\Pr[\mE \given \mE_B,h]
    &\le d\p{e^{1-R^2/m-1/e}}^{-m/R^2}
    = d\, e^{-(m-R^2-m/e)/R^2} \\
    &= d \exp\p{-\frac{\p{\frac{e-1}{e}}m}{R^2} + 1}
    \le \tfrac{\delta}{2},
  \end{align*}
  since $m \ge \frac{e}{e-1} R^2 \p{1+\log \frac{2d}{\delta}}$.
  Finally, observe that, omitting the conditioning on $\mE_B,h$, we have:
  \[
    \Pr\b{\lambda_{\min}(\ZB) \leq R^2 }
        \le \Pr\b{\mE} + \Pr\b{\lambda_{\min}(\WB) < m} \le
    \tfrac{\delta}{2} + \tfrac{\delta}{2} = \delta.
  \]
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%

\subsection{Reward simulation with a diverse batch history}
\label{app:pf_bg:simulation}

We consider reward simulation with a batch history, in the sense of
Definition~\ref{def:simulation}. We show that a sufficiently ``diverse" batch
history suffices to simulate the reward for any given context vector. Coupled
with the results of Section~\ref{app:pf_bg:diversity}, it follows that batch history generated by a \GreedyStyle algorithm can simulate rewards as long as the batch size is sufficiently large.

Let us recap the definition of reward simulation (Definition~\ref{def:simulation}).  Let $\Rew(\cdot)$ be a randomized function that takes a context $x$ and outputs an independent random sample from $\mathcal{N}(\theta\tran x, 1)$. In other words, this is the realized reward for an action with context vector $x$.

\begin{definition}
Consider batch $B$ in the execution of a \GreedyStyle algorithm. Batch history $h_B$ can simulate $\Rew()$ up to radius $R>0$ if there exists a function
    $g: \{\text{context vectors}\}\times \{ \text{batch histories $h_B$}\} \to \R$
such that $g(x,h_B)$ is identically distributed to $\Rew(x)$ conditional on the batch context matrix, for all $\theta$ and all context vectors $x\in \R^d$ with $\|x\|_2\leq R$.
%$x\in B^d_R$.
\label{def:simulation-app}
\end{definition}


Note that we do not require the function $g$ to be efficiently computable. We do not require algorithms to compute $g$; a mere existence of such function suffices for our analysis.

The result in this subsection does not rely on the ``greedy" property. Instead, it applies to all ``batch-style" algorithms, defined as follows: time is divided in batches of $Y$ consecutive rounds each, and the action at each round $t$ only depends on the history up to the previous batch. The data diversity condition is formalized as $\{\lambda_{\min}(Z_B) \ge R^2 \}$; recall that it is a high-probability event, in a precise sense defined in Lemma~\ref{lem:min_ev_bg}. The result is stated as follows:

\begin{lemma}
Fix a batch-style algorithm and any batch $B$ in the execution of this algorithm.
Assume the batch covariance matrix $Z_B$ satisfies $\lambda_{\min}(Z_B) \ge R^2$. Then batch history $h_B$ can simulate $\Rew$ up to radius $R$.
  \label{lem:lin_sim}
\end{lemma}


\begin{proof}
Let us construct a suitable function $g$ for Definition~\ref{def:simulation-app}. Fix a context vector $x\in \R^d$ with $\|x\|_2\leq R$. Let $r_B$ be the vector of realized rewards in batch $B$, \ie
    $r_B = (r_t: \text{rounds $t$ in $B$})\in \R^Y$. Define
\begin{align}\label{eq:lem:lin_sim:defn-g}
     g(x, h_B) = w_B\tran\, r_B + \mc N\left(0,1-\|w_B\|_2\right),
     \text{where $w_B = X_B\, Z_B^{-1}\, x\in \R^Y$}.
\end{align}

Recall that the variance of the reward noise as $1$. (We can also handle a more general version in which the variance of the reward noise is $\sigma^2$. Then the noise variance in \eqref{eq:lem:lin_sim:defn-g} should be $\sigma^2\,(1-\|w_B\|_2)$, with essentially no modifications throughout the rest of the proof.)

Note that $w_B$ is well-defined: indeed, $Z_B$ is invertible since
  $\lambda_{\min}(Z_B) \ge R^2>0$.
In the rest of the proof we show that $g$ is as needed for Definition~\ref{def:simulation-app}.


  First, we will show that for any $x \in \R^d$ such that $\|x\|_2 \le R$, the
  weights $\weights \in \R^t$ as defined above satisfy $X_B\tran \weights = x$
  and $\|\weights\|_2 \le 1$.
  Then, we'll show that if each $r_\tau \sim \mc N(\theta \tran x_\tau, 1)$,
  then $\vrt\tran \weights + \mc N(0, 1 - \|\weights\|_2^2)
  \sim \mc N(\theta\tran x, 1)$.

  Trivially, we have
  \[
    X_B\tran \weights = X_B\tran X_B (X_B\tran X_B)^{-1} x = x
  \]
  as desired. We must now show that $\|\weights\|^2_2 \le 1$. Note that
  \[
    \|\weights\|_2^2 = \weights\tran \weights = \weights\tran X_B Z_B^{-1} x =
    x\tran Z_B^{-1} x = \|x\|_{Z_B^{-1}}^2
  \]
  where $\|v\|_M^2$ simply denotes $v\tran M v$. Thus, it is sufficient to show
  that $\|x\|_{Z_B^{-1}}^2 \le 1$. Since
  $\|x\|_2 \le R$ and $\lambda_{\min}\p{Z_B} \ge R^2$, we have by
  Lemma~\ref{lem:norm_eigen}
  \[
    Z_B \succeq R^2 I \succeq xx\tran.
  \]
  By Lemma~\ref{lem:conj_succ}, we have
  \[
    I \succeq Z_B^{-1/2} xx\tran Z_B^{-1/2}.
  \]
  Let $z = Z_B^{-1/2} x$, so $I \succeq zz\tran$. Again by
  Lemma~\ref{lem:norm_eigen}, $\lambda_{\max}(zz\tran) = z\tran z$. This means
  that
  \[
    1 \ge z\tran z = (Z_B^{-1/2}x)\tran Z_B^{-1/2} x = x\tran Z_B^{-1} x =
    \|x\|_{Z_B^{-1}}^2 = \|\weights\|_2^2
  \]
  as desired.
  Finally, observe that
  \[
    \vrt\tran \weights = (X_B\theta + \eta)\tran \weights = \theta\tran X_B\tran
    \weights + \eta \tran \weights = \theta\tran x + \eta\tran \weights
  \]
  where $\eta \sim \mc N(0, I)$ is the noise vector. Notice that
  $\eta\tran \weights \sim \mc N(0, \|\weights\|_2)$, and therefore,
  $\eta\tran \weights + \mc N(0, 1-\|\weights\|_2^2) \sim \mc N(0,
  1)$. Putting this all together, we have
  \[
    \vrt\tran \weights + \mc N(0, 1-\|\weights\|_2^2) \sim \mc
    N(\theta\tran x, 1)
  \]
  and therefore $D$ can simulate $E$ for any $x$ up to radius $R$.
\end{proof}


\subsection{Regret Bounds for \BayesGreedy}
\label{sec:bg-proofs-bg}

We apply the tools from Sections~\ref{app:pf_bg:diversity} and~\ref{app:pf_bg:simulation} to derive regret bounds for \bg. On a high level, we prove that the history collected by \bg suffices to simulate a ``slowed-down" run of any other algorithm $\ALG_0$. Therefore, when it comes to choosing the next action, \bg has at least as much information as $\ALG_0$, so its Bayesian-greedy choice cannot be worse than the choice made by $\ALG_0$.


Our analysis extends to a more general scenario which is useful for the analysis of \fg. We formulate and prove our results for this scenario directly. We consider an extended bandit model which separates data collection and reward collection. Each round $t$ proceeds as follows: the algorithm observes available actions and the context vectors for these actions, then it chooses \emph{two} actions, $a_t$ and $a'_t$, and observes the reward for the former but not the latter. We refer to $a'_t$ as the ``prediction" at round $t$. We will refer to an algorithm in this model as a bandit algorithm (which chooses actions $a_t$) with ``prediction rule" that chooses the predictions $a'_t$. More specifically, we will be interested in an arbitrary \GreedyStyle algorithm with prediction rule given by \bg, as per
\eqref{eq:BG-est-defn}. We will assume this prediction rule henceforth. We are interested in \emph{prediction regret}: a version of regret from \eqref{eq:regret-def} if actions $a_t$ are replaced with predictions $a'_t$:
\begin{align}\label{eq:pred-regret-def}
\PReg(T) = \textstyle
    \sum_{t=1}^T \theta\tran x_t^* -
\theta\tran x_{a'_t, t}
\end{align}
 where $x^*_{t}$ is the context vector of the best action at round $t$, as in \eqref{eq:regret-def}.
More precisely, we are interested in \emph{Bayesian prediction regret}, the expectation of \eqref{eq:pred-regret-def} over everything: the context vectors, the rewards, the algorithm's random seed, and and the prior over $\theta$.

We use essentially the same analysis to derive implications on group externalities.
For this purpose, we consider a further generalization in which regret is restricted to rounds that correspond to a particular population.
Formally, let $\rounds \subseteq [\infty]$ be a randomly chosen subset of the rounds where
$\Pr[t \in \rounds]$ is a constant and rounds are chosen to be in $\rounds$ independently of
one another. We allow for the possibility that the underlying context
distribution differs for rounds in $\rounds$ compared to rounds in $[T] \backslash \mc
T$. More precisely, we allow the event $\{t\in \mc T\}$ be correlated with context $x_t$, for each round $t$. Similar to the definition of minority regret, we define \emph{$\rounds$-restricted regret} (resp., prediction regret)
in $T$ rounds to be the portion of regret (resp., prediction regret) that corresponds to $\rounds$-rounds:
\begin{align}
  \rReg(T)
    &= \textstyle
    \sum_{t\leq T,\;t\in \rounds} \theta\tran x_t^* - \theta\tran x_{a_t, t}.
        \label{eq:restricted-regret-def} \\
  \rPReg(T) &= \textstyle
    \sum_{t\leq T,\;t\in \rounds} \theta\tran x_t^* -
\theta\tran x_{a'_t, t}.
    \label{eq:restricted-regret-def-pred}
\end{align}
$\rounds$-restricted \emph{Bayesian} (prediction) regret is defined as
an expectation over everything.

Thus, the main theorem of this subsection is formulated as follows:

\begin{theorem}
  Consider perturbed context generation. Let $\ALG$ be an arbitrary \GreedyStyle
  algorithm whose batch size is at least $Y_0$ from \eqref{eq:lem:min_ev_bg-Y}.
Fix any bandit algorithm $\ALG_0$, and let
    $\rReg_0(T)$
be the $\rounds$-restricted regret of this algorithm on a particular problem
instance $\mc I$. Then on the same instance, $\ALG$ has $\rounds$-restricted Bayesian prediction regret
\begin{align}\label{eq:thm:bg}
  \E{\rPReg(T)} \leq Y \cdot \E{\rReg_0(T/Y)} + \tilde O(1/T).
\end{align}
\label{thm:bg}
\end{theorem}

\xhdr{Proof sketch.} We use a $t$-round history of \ALG to simulate a $(t/Y)$-round history of $\ALG_0$. More specifically, we use each batch in the history of \ALG to simulate one round of $\ALG_0$. We prove that the simulated history of $\ALG_0$ has exactly the same distribution as the actual history, for any $\theta$. Since $\ALG$ predicts the Bayesian-optimal action  given the history (up to the previous batch), this action is at least as good (in expectation over the prior) as the one chosen by $\ALG_0$ after $t/Y$ rounds. The detailed proof is deferred to Section~\ref{sec:thm-bg-pf}.

\xhdr{Implications.} As a corollary of this theorem, we obtain regret bounds for \bg in Theorem~\ref{thm:main-greedy} and Theorem~\ref{thm:main-worst-case}. We take $\rounds$ to be the set of all rounds, \ie $\Pr[t \in \rounds] = 1$, and $\ALG$ to be \bg. For Theorem~\ref{thm:main-worst-case}(b), we take $\ALG_0$ to be LinUCB. Thus:

\begin{corollary}
In the setting of Theorem~\ref{thm:bg}, \bg has Bayesian regret at most
    $Y \cdot \E{R_0(T/Y)} + \tilde O(1/T)$
on problem instance $\mc I$. Further, under the assumptions of Theorem~\ref{thm:main-worst-case}, \bg has Bayesian regret at most
    $\tilde O(d^2\,K^{2/3}\;T^{1/3}/\rho^2)$
on all instances.
\end{corollary}

We also obtain a similar regret bound on the Bayesian prediction regret of \fg, which is essential for Section~\ref{sec:bg-proofs-fg}.

\begin{corollary}\label{cor:thm-bg-fg}
In the setting of Theorem~\ref{thm:bg}, \fg has Bayesian prediction regret \eqref{eq:thm:bg}.
\end{corollary}


To derive Theorem~\ref{thm:main-greedy-externalities} for \bg, we take $\rounds$ to be the set of all minority rounds, and apply Theorem~\ref{thm:bg} twice: first when $\ALG_0$ is run over the minority rounds only (and can behave arbitrarily on the rest), and then when $\ALG_0$ is run over full population.

\subsubsection{Proof of Theorem~\ref{thm:bg}}
\label{sec:thm-bg-pf}

We condition on the event that all perturbations are bounded by $\hat{R}$, more precisely, on the event
\begin{align}\label{eq:thm:bg-pf-E1}
\mE_1 = \left\{ \|\eps_{a,t}\|_\infty \leq \hat R:\;
    \text{ for all arms $a$ and all rounds $t$ } \right\}.
\end{align}
Recall that $\mE_1$ is a high-probability event, by \eqref{eq:bg-proofs-highprob-R-hat}.
We also condition on the event
\[
  \mE_2 = \left\{ \lambda_{\min}(Z_B) \ge R^2
  : \; \text{for each batch $B$},\right\}
\]
where $Z_B$ is the batch covariance matrix, as usual. Conditioned on $\mE_1$, this too is a high-probability event by
Lemma~\ref{lem:min_ev_bg} plugging in $\delta/T$ and taking a union bound over
all batches.

We will prove that \ALG satisfies
\begin{align}\label{eq:bg-cond}
\E{\rPReg(T) \given \mE_1, \mE_2}
    \leq Y \cdot \E{\rReg_0(T/Y) \given \mE_1, \mE_2},
\end{align}
where the expectation is taken over everything: the context vectors, the rewards, the algorithm's random seed, and the prior over $\theta$.


\xhdr{Proof of Eq.~\eqref{eq:bg-cond}.}
Fix round $t$. We use a $t$-round history of $\ALG$ to simulate a $\ty$-round run of $\ALG_0$, where $Y$ is the batch size in $\ALG$. Stating this formally requires some notation.  Let $A_t$ be the set of actions available in round $t$, and let
    $\con_t = (x_{a,t}:\, a\in A_t)$
be the corresponding tuple of contexts. Let $\CON$ be the set of all possible context tuples, more precisely, the set of all finite subsets of $\R^d$. Let $h_t$ and $h^0_t$ denote, resp., the $t$-round history of $\ALG$ and $\ALG_0$. Let $\mH_t$  denote the set of all possible $t$-round histories. Note that $h_t$ and $h^0_t$ are random variables which take values on $\mH_t$.  We want to use history $h_t$ to simulate history $h^0_\ty$. Thus, the simulation result is stated as follows:

\begin{lemma}\label{lm:bg-simulation}
Fix round $t$ and let $\sigma = (\con_1 \LDOTS \con_\ty)$ be the sequence of context arrivals up to and including round $\ty$. Then there exists a ``simulation function"
    \[ \simF = \simF_t: \mH_t\times \CON_{\ty} \to \mH_{\ty} \]
such that the simulated history $\simF(h_t,\sigma)$ is distributed identically to $h^0_{\ty}$, conditional on sequence $\sigma$, latent vector $\theta$, and events $\mE_1,\mE_2$.
\end{lemma}

\begin{proof}
Throughout this proof, condition on events $\mE_1$ and $\mE_2$. Generically,
$\simF(h_t,\sigma)$ outputs a sequence of pairs
    $\{(x_\tau, r_\tau)\}_{\tau=1}^{\lfloor t/Y \rfloor}$,
where $x_\tau$ is a context vector and $r_\tau$ is a simulated reward for this
context vector. We define $\simF(h_t,\sigma)$ by induction on $\tau$ with base case $\tau=0$. Throughout, we maintain a run of algorithm $\ALG_0$. For each step $\tau\geq 1$, suppose $\ALG_0$ is simulated up to round $\tau-1$, and the corresponding history is recorded as
    $((x_1,r_1) \LDOTS (x_{\tau-1},r_{\tau-1}))$.
Simulate the next round in the execution of $\ALG_0$ by presenting it with the action set $A_\tau$ and the corresponding context tuple $\con_\tau$. Let $x_\tau$ be the context vector chosen by $\ALG_0$. The corresponding reward $r_\tau$ is constructed using the $\tau$-th batch in $h_t$, denote it with $B$. By Lemmas~\ref{lem:min_ev_bg} and~\ref{lem:lin_sim}, the batch history
$h_B$ can simulate a single reward, in the sense of
Definition~\ref{def:simulation-app}. In particular, there exists a function
$g(x,h_B)$ with the required properties (recall that it is explicitly defined in
\eqref{eq:lem:lin_sim:defn-g}). Thus, we define $r_\tau = g(x_\tau,h_B)$, and
return $r_\tau$ as a reward to $\ALG_0$. This completes the construction of
$\simF(h_t,\sigma)$. The distribution property of $\simF(h_t,\sigma)$ is immediate from the construction.
\end{proof}

Let $t_0 = t_0(B)$ be the last round of some batch $B$. Let $\tau = 1+t_0/Y$, and consider the context vector $x^0_\tau$ chosen by $\ALG_0$ in round $\tau$. This context vector is a randomized function $f$ of the current context tuple $\con_\tau$ and the history $h^0_{\tau-1}$:
    \[ x^0_\tau = f(\con_\tau; h^0_{\tau-1}).\]
By Lemma~\ref{lm:bg-simulation}, letting
    let $\sigma = (\con_1 \LDOTS \con_\ty)$,
it holds that
\begin{align}\label{eq:bg-proof-tau}
 \E{ x^0_\tau \cdot \theta \given \sigma,\theta,\mE_1,\mE_2}
    = \E{ f(\con_\tau;\,\simF(h_{t_0},\sigma)) \cdot \theta \given \sigma,\theta,\mE_1,\mE_2}
\end{align}

Let $t$ be some round in the next batch after $B$, and let
    $x'_t = x_{a'_t,t}$,
be the context vector predicted by $\ALG$ in round $t$. Recall that $x'_t$ is a Bayesian-greedy choice from the context tuple $\con_t$, based on history $h_{t_0}$.
Observe that the Bayesian-greedy action choice from a given context tuple based on history $h_{t_0}$ cannot be worse, in terms of the Bayesian-expected reward, than any other choice from the same context tuple and based on the same history. Using \eqref{eq:bg-proof-tau}, we obtain:
\begin{align}\label{eq:bg-proof-MII-cond}
 \E{ x'_t \cdot \theta \given \con_t = \con,\mE_1,\mE_2  }
    \geq \E{ x^0_\tau \cdot \theta  \given \con_\tau = \con,\mE_1,\mE_2},
 \end{align}
 for any given context tuple $\con\in\CON$ that has a non-zero arrival probability given $\mE_1 \cap\mE_2$.

Observe that $\con_t$ and $\con_\tau$ have the same distribution, even conditioned on event $\mE_1 \cap\mE_2$. (This is by symmetry in the definition of $\mE_1$ and $\mE_2$, and the fact that rounds $t$ and $\tau$ lie in different batches.)
Therefore, we can integrate \eqref{eq:bg-proof-MII-cond} over $\con$:
\begin{align}\label{eq:bg-proof-MII}
 \E{ x'_t \cdot \theta \given t\in\rounds,\mE_1,\mE_2  }
    \geq \E{ x^0_\tau \cdot \theta  \given t\in\rounds,\mE_1,\mE_2},
 \end{align}
Finally, we obtain ~\eqref{eq:bg-cond} by summing up \eqref{eq:bg-proof-MII} over all rounds $t$ in the same batch (which results in the factor of $Y$ on the right-hand side), and over all batches $B$.


\xhdr{Completing the proof of Theorem~\ref{thm:bg} given Eq.~\eqref{eq:bg-cond}.}
We must take care of the low-probability failure events $\overline{\mE}_1$ and
$\overline{\mE}_2$.
Specifically, we need to upper-bound the expression
  \[
    \Exp_{\theta \sim P} \b{\bpreg{T} \given \overline{\mc E}_1 \cup \overline{\mc
    E}_2} \cdot \Pr[\overline{\mc E}_1 \cup \overline{\mc E}_2].
  \]
  For ease of exposition, we focus on the special case  $\Pr\b{t \in \rounds} = 1$; the general case is treated similarly. We know that $\Pr[\overline{\mc E}_1 \cup \overline{\mc E}_2] \le \delta +
  \delta_R$.
  Lemma~\ref{lem:exp_reg_ub_er} with $\ell = \hat R$
  gives us that the instantaneous regret of every round is at most
  \begin{align*}
    2\Exp_{\theta \sim (\prior \given h_{t-1})} & \b{\|\theta\|_2\p{1 + \rho(2 +
      \sqrt{2 \log K}) + \hat R}} \\
    &\le 2\b{\p{\|\pmt\|_2 + \sqrt{d\lambda_{\max}(\pvt)}}\p{1 + \rho(2 +
      \sqrt{2 \log K}) + \hat R}}
  \end{align*}
  by Lemma~\ref{lem:gaus_norm}. Letting $\delta = \delta_R = \frac{1}{T^2}$, we
  verify that our definition of $Y$ means that Lemma~\ref{lem:min_ev_bg} indeed
  holds with probability at least $1-T^{-2}$.
  Using~\eqref{eq:bg-cond}, the Bayesian prediction regret of $\ALG$ is
  \begin{align*}
    \Exp_{\theta \sim \prior} &\b{\bpreg{T}} \\
    &\le Y \Exp_{\theta \sim \prior} \b{\basereg{\tfrac{T}{Y}}} 
    + 2\,T(\delta + \delta_R)\b{\p{\|\pmt\|_2 + \sqrt{d\lambda_{\max}(\pvt)}}\p{1
      + \rho(2 + \sqrt{2 \log K}) + \hat R}} \\
    &\le Y \Exp_{\theta \sim \prior} \b{\basereg{\tfrac{T}{Y}}} + \tilde
    O\p{\tfrac{1}{T}}.
  \end{align*}
This completes the proof of Theorem~\ref{thm:bg}.

\subsection{Regret Bounds for \FreqGreedy}
\label{sec:bg-proofs-fg}

To analyze \fg, we show that its Bayesian regret is not too different from its Bayesian prediction regret, and use Corollary~\ref{cor:thm-bg-fg} to bound the latter. As in the previous subsection, we state this result in more generality for the sake of group externality implications: we consider $\rounds$-restricted (prediction) regret, exactly as before.

\begin{theorem}
  Assuming perturbed context generation, \fg satisfies
  \[  \left|\; \E{\rReg(T) - \rPReg(T)} \; \right| \leq
    \tilde O\p{\frac{\sqrt{d}}{\rho^2}} \p{\sqrt{\lambda_{\max}(\pvt)} +
    \frac{1}{\sqrt{\lambda_{\min}(\pvt)}}},
  \]
  where $\pvt$ is the covariance matrix of the prior and $\rho$ is the perturbation size.
  \label{thm:bg_fg}
\end{theorem}

Taking $\rounds$ to be the set of all contexts, and using Corollary~\ref{cor:thm-bg-fg}, we obtain Bayesian regret bounds for \fg in Theorem~\ref{thm:main-greedy} and Theorem~\ref{thm:main-worst-case}.
To derive Theorem~\ref{thm:main-greedy-externalities} for \fg, we take $\rounds$ to be the set of all minority rounds.

The remainder of this section is dedicated to proving Theorem~\ref{thm:bg_fg}. On a high level, the idea is as follows. As in the proof of Theorem~\ref{thm:bg}, we condition on the high-probability event \eqref{eq:thm:bg-pf-E1} that perturbations are bounded. Specifically, we prove that
\begin{align}\label{eq:thm:bg_fg-cond}
  \left|\; \E{\rReg(T) - \rPReg(T) \given \mE_1} \; \right| \leq
    \tilde O\p{\frac{\sqrt{d}}{\rho^2}} \p{\sqrt{\lambda_{\max}(\pvt)} +
    \frac{1}{\sqrt{\lambda_{\min}(\pvt)}}}.
\end{align}
To prove this statement, we fix round $t$ and compare the action $a_t$ taken by \fg and the predicted action $a'_t$. We observe that the difference in rewards between these two actions can be upper-bounded in terms of $\bmt-\fmt$,
the difference in the $\theta$ estimates with and without knowledge of the prior. (Recall \eqref{eq:BG-est-defn} and \eqref{eq:FG-est-defn} for definitions.)
Specifically, we show that
\begin{equation}
\label{eq:inst_bound_diff}
  \E{
    \theta\tran (x_{a_t, t} - x_{a'_t, t}) \given \mE_1 }
    \le 2R\Exp_{\theta \sim \prior}\b{\|\bmt - \fmt\|_2}.
\end{equation}
The crux of the proof is to show that the difference $\|\bmt - \fmt\|_2$ is small, namely
\begin{equation}
  \label{eq:norm_bound_1_t}
  \E{\|\bmt-\fmt\|_2 \given \mE_1} = \tilde O(1/t),
\end{equation}
ignoring other parameters. Thus, summing over all rounds, we get
\[ \E{\rReg(T) - \rPReg(T) \given
\mE_1} \le O(\log T) = \tilde O(1). \]

\xhdr{Proof of Eq.~\eqref{eq:thm:bg_fg-cond}.}
Let $\regi{t}$ and $\bpregi{t}$ be, resp., instantaneous regret and instantaneous prediction regret at time $t$. Then
  \begin{equation}
    \Exp_{\theta \sim \prior}\b{\rReg(T) - \rPReg(T)}
    = \sum_{t\in \rounds} \Exp_{\theta \sim
    \prior} \b{\regi{t} - \bpregi{t}}.
    \label{eq:reg_time}
  \end{equation}
  Thus, it suffices to bound the differences in instantaneous regret.

  Recall that at time $t$, the chosen action for \fg\ and the predicted action are, resp.,
  \begin{align*}
    \af &= \argmax_{a \in A} x_{a,t}\tran \fmt \\
    \ab &= \argmax_{a \in A} x_{a,t}\tran \bmt.
  \end{align*}
Letting $t_0 - 1 = \lfloor t/Y \rfloor$ be the last round in the previous batch,
we can formulate $\fmt$ and $\bmt$ as 
\begin{align*}
    \fmt &= (\Zto)^{-1} \Xto\tran \vrto \\
    \bmt &= (\Zto + \pvt^{-1})^{-1} (\Xto\tran \vrto + \pvt^{-1} \pmt).
\end{align*}

  Therefore, we have
  \[
    \Exp_{\theta \sim \prior \given h_{t-1}}\b{\regi{t} - \bpregi{t}} = \Exp_{\theta \sim
      \prior \given h_{t-1}} \b{(x_{\ab,t} - x_{\af,t})\tran \bmt} =
      (x_{\ab,t} - x_{\af,t})\tran \bmt,
  \]
  since the mean of the posterior distribution is exactly $\bmt$, and $\bmt$ is
  deterministic given $h_{t-1}$. Taking expectation over $h_{t-1}$, we have
  \[
    \Exp_{\theta \sim \prior}\b{\regi{t} - \bpregi{t}} = \Exp_{\theta \sim \prior}
    \b{(x_{\ab,t} - x_{\af,t})\tran \bmt}.
  \]
  For any fixed $\bmt$ and $\fmt$, since \fg\ chose $\af$ over $\ab$, it must be
  the case that
  \begin{equation}
    x_{\af,t}\tran \fmt \ge x_{\ab,t}\tran \fmt.
    \label{eq:freq_choice}
  \end{equation}
  Therefore,
  \begin{align*}
    (x_{\ab,t} - x_{\af,t})\tran \bmt &= (x_{\ab,t} - x_{\af,t})\tran \fmt +
    (x_{\ab,t} - x_{\af,t})\tran (\bmt - \fmt) \\
    &\le (x_{\ab,t} - x_{\af,t})\tran (\bmt - \fmt)
    \tag{By~\eqref{eq:freq_choice}} \\
    &\le (\|x_{\ab,t}\|_2 + \|x_{\af,t}\|_2)\|\bmt - \fmt\|_2 \\
    &\le 2R\|\bmt - \fmt\|_2
  \end{align*}
Eq.~\eqref{eq:inst_bound_diff} follows.

The crux is to prove \eqref{eq:norm_bound_1_t}: to bound the expected distance between the Frequentist and Bayesian estimates for $\theta$. By expanding
  their definitions, we have
  \begin{align*}
    \bmt - \fmt
    &= (\Zto + \pvt^{-1})^{-1} (\Xto\tran \vrto + \pvt^{-1}
    \pmt) - \Zto^{-1} \Xto\tran \vrto \\
    &= (\Zto + \pvt^{-1})^{-1} \b{\Xto\tran \vrto + \pvt^{-1} \pmt -
    (\Zto + \pvt^{-1})\Zto^{-1} \Xto\tran \vrto} \\
    &= (\Zto + \pvt^{-1})^{-1} \b{\Xto\tran \vrto + \pvt^{-1} \pmt -
    \Xto\tran \vrto  - \pvt^{-1}\Zto^{-1} \Xto\tran \vrto} \\
    &= (\Zto + \pvt^{-1})^{-1} \b{\pvt^{-1} \pmt -
    \pvt^{-1}\Zto^{-1} \Xto\tran \vrto} \\
    &= (\Zto + \pvt^{-1})^{-1} \pvt^{-1} \p{\pmt - \fmt}.
  \end{align*}
  Next, note that
  \begin{align*}
    \|(\Zto + \pvt^{-1})^{-1} \pvt^{-1} (\pmt - \fmt)\|_2
    &\le \|(\Zto + \pvt^{-1})^{-1}\|_2 ~ \|\pvt^{-1} (\pmt - \fmt)\|_2 \\
    &\le \|(\Zto + \pvt)^{-1}\|_2 ~ \p{\|\pvt^{-1}(\pmt - \theta)\|_2
    + \|\pvt^{-1}\|_2 ~ \|\theta - \fmt\|_2}.
  \end{align*}
  By Lemma~\ref{lem:min_ev_sum}, $\lambda_{\min}\p{\Zto + \pvt} \ge
  \lambda_{\min}\p{\Zto}$. Therefore,
  \[
    \|(\Zto + \pvt)^{-1}\|_2 \le \frac{1}{\lambda_{\min}\p{\Zto}},
  \]
  giving us
  \begin{align*}
    \|\bmt - \fmt\|_2
    &\le \frac{\|\pvt^{-1}(\pmt - \theta)\|_2 + \|\pvt^{-1}\|_2
    ~ \|\theta - \fmt\|_2}{\lambda_{\min}(\Zto)} \\
    &\le \frac{\|\pvt^{-1/2}\|_2 \|\pvt^{-1/2}(\pmt - \theta)\|_2 + \|\pvt^{-1/2}\|_2
    ~ \|\theta - \fmt\|_2}{\lambda_{\min}(\Zto)} \\
    &= \frac{\p{\|\pvt^{-1/2}(\pmt - \theta)\|_2 + \sqrt{\lambda_{\min}(\pvt)}
    \|\theta - \fmt\|_2}}{\sqrt{\lambda_{\min}(\pvt)} \lambda_{\min}(\Zto)}.
  \end{align*}

  Next, recall that for
  \[ t_0-1 \ge t_{\min}(\delta) := 160 \tfrac{R^2}{\rho^2} \log \tfrac{2d}{\delta} \cdot \log T \]
 the following bounds hold, each with probability at least $1-\delta$:
  \begin{align*}
    \frac{1}{\lambda_{\min}\p{\Zto}} &\le \frac{32 \log T}{\rho^2
    (t_0-1)}
    \tag{Lemma~\ref{lem:fg_big_cov}} \\
    \|\theta - \fmt\|_2 &\le \frac{\sqrt{2dR (t_0-1)
    \log(d/\delta)}}{\lambda_{\min}(\Zto)} \tag{Lemma~\ref{lem:fmt_close}}
  \end{align*}
 Therefore, fixing $t_0 \geq 1+t_{\min}(\delta/2)$, with probability at least $1-\delta$ we have
  \begin{equation}
    \|\bmt - \fmt\|_2
    \le \frac{32 \log T}{\rho^2 (t_0-1) \sqrt{\lambda_{\min}(\pvt)}}
    \p{\|\pvt^{-1/2}(\pmt - \theta)\|_2 + \frac{64\sqrt{dR
    \log(2d/\delta)} \cdot \log T}{\rho^2 \sqrt{t_0-1}}}.
    \label{eq:fg_bg1}
  \end{equation}
  Note that the high-probability events we need are deterministic given
  $h_{t_0-1}$, and therefore are independent of the perturbations at time $t$.
  This means that Lemma~\ref{lem:exp_reg_ub_er} applies, with $\ell = 0$: conditioned on
  any $h_{t_0-1}$, the expected regret for round $t$ is upper-bounded by
  $2\|\theta\|_2 (1 + \rho(1+\sqrt{2\log K}))$. In particular, this holds for any
  $h_{t_0-1}$ not satisfying the high probability events from
  Lemmas~\ref{lem:fg_big_cov} and~\ref{lem:fmt_close}. Therefore, for all $t \ge
  t_{\min}(\delta)$,
  \begin{align*}
&~~    \Exp_{\theta \sim \prior} \b{\|\bmt - \fmt\|_2}\\
    &\le \Exp_{\theta \sim \prior} \Bigg[(1-\delta) \frac{32 \log T}{\rho^2
      (t_0-1) \sqrt{\lambda_{\min}(\pvt)}} \p{\|\pvt^{-1/2}(\pmt - \theta)\|_2 +
      \frac{64\sqrt{dR \log(2d/\delta)} \cdot \log T}{\rho^2 \sqrt{t_0-1}}} \\
    &\qquad\qquad+ \delta \cdot 2\|\theta\|_2 (1 + \rho(2+\sqrt{2\log K})) \Bigg] \\
    &\le \frac{32 \log T}{\rho^2 (t_0-1) \sqrt{\lambda_{\min}(\pvt)}} \p{\Exp_{\theta \sim
    \prior}\b{\|\pvt^{-1/2} (\pmt - \theta)\|_2} + \frac{64 \sqrt{dR
    \log(2d/\delta)} \cdot \log T}{\rho^2 \sqrt{t_0-1}}} \\
    &\qquad+ \delta \cdot 2(\|\pmt\|_2 + \Exp_{\theta \sim \prior}\b{\|\pmt -
    \theta\|_2}) (1 + \rho(2+\sqrt{2\log K})).
  \end{align*}
  Because $\theta \sim \mc N(\pmt, \pvt)$, we have $\pvt^{-1/2}
  (\pmt - \theta) \sim \mc N(0, I)$. By Lemma~\ref{lem:gaus_norm},
  \[
    \Exp_{\theta \sim \prior} \b{\|\pvt^{-1/2} (\pmt - \theta)\|_2}
    \le \sqrt{d}
    \quad\text{and}\quad
    \Exp_{\theta \sim \prior}\b{\|\pmt - \theta\|_2} \le \sqrt{d
      \lambda_{\max}(\pvt)}.
  \]
  This means
  \begin{align*}
    \Exp_{\theta \sim \prior} \b{\|\bmt - \fmt\|_2}
    &\le \frac{32 \sqrt{d} \log T}{\rho^2 (t_0-1) \sqrt{\lambda_{\min}(\pvt)}} \p{1 +
      \frac{64 \sqrt{R \log(2d/\delta)} \cdot \log T}{\rho^2 \sqrt{t_0-1}}} \\
    &+ \delta \cdot 2(\|\pmt\|_2 + \sqrt{d \lambda_{\max}(\pvt)}) (1 +
    \rho(2+\sqrt{2\log K})).
  \end{align*}
  Since $t_0 = \Omega(t)$, for sufficiently small $\delta$, this
  proves~\eqref{eq:norm_bound_1_t}. 
  
  We need to do a careful computation to complete the proof of Eq.~\eqref{eq:thm:bg_fg-cond}.  We know from~\eqref{eq:inst_bound_diff} that
  \begin{align*}
    \Exp_{\theta \sim \prior}\b{\rReg(T) - \rPReg(T)}
    &\le \sum_{t=1}^T 2R\Exp_{\theta \sim \prior} \b{\|\bmt - \fmt\|_2}.
  \end{align*}
  Choosing $\delta = T^{-2}$, we find that
  \[
    \sum_{t=t_{\min}(T^{-2})}^T \delta \cdot 2(\|\pmt\|_2 + \sqrt{d
    \lambda_{\max}(\pvt)}) (1 + \rho(2+\sqrt{2\log K})) = \tilde O(1),
  \]
  so this term vanishes. Furthermore,
  \[
    \sum_{t=t_{\min}(T^{-2})}^T 2R\frac{32 \sqrt{d} \log T}{\rho^2 (t_0-1)
    \sqrt{\lambda_{\min}(\pvt)}} \p{1 + \frac{64\sqrt{R \log(2d/\delta)} \cdot
    \log T}{\rho^2 \sqrt{t_0-1}}} = \tilde
    O\p{\frac{R\sqrt{d}}{\rho^2\sqrt{\lambda_{\min}(\pvt)}}}
  \]
  since $t_0 \ge t - Y$, and $\sum_{t=1}^T 1/t = O(\log T)$.
  Using the fact that $R = \tilde O(1)$ (since by assumption $\rho \le
  d^{-1/2}$), this is simply
  \[
    \tilde O\p{\frac{\sqrt{d}}{\rho^2\sqrt{\lambda_{\min}(\pvt)}}}.
  \]
  Finally, we note that on the first $t_{\min}(T^{-2}) = \tilde O(1/\rho^2)$
  rounds, the regret bound from Lemma~\ref{lem:exp_reg_ub_er} with $\ell = 0$
  applies, so the total regret difference is at most
  \begin{align*}
    \Exp_{\theta \sim \prior}\b{\rReg(T) - \rPReg(T)}
    &\le \sum_{t=1}^{t_{\min}(T^{-2})}
    \Exp_{\theta \sim \prior}\b{\regi{t} - \bpregi{t}} 
    + \sum_{t=t_{\min}(T^{-2})}^T 2R\Exp_{\theta \sim \prior} \b{\|\bmt - \fmt\|_2}, \\
    &\le t_{\min}(T^{-2}) \cdot 2(\|\pmt\|_2 + \sqrt{d
    \lambda_{\max}(\pvt)})(1 + \rho(2+\sqrt{2 \log K})) 
    + \tilde O\p{\frac{\sqrt{d}}{\rho^2\sqrt{\lambda_{\min}(\pvt)}}} \\
    &= \tilde O\p{\frac{\sqrt{d \lambda_{\max}(\pvt)}}{\rho^2}}
    + \tilde O\p{\frac{\sqrt{d}}{\rho^2\sqrt{\lambda_{\min}(\pvt)}}},
  \end{align*}
which implies Eq.~\eqref{eq:thm:bg_fg-cond}.

\xhdr{Completing the proof of Theorem~\ref{thm:bg_fg} given ~\eqref{eq:thm:bg_fg-cond}.}
  By Theorem~\ref{thm:bg_fg}, this holds whenever all perturbations are bounded by
  $\hat R$, which happens with probability at least $1-\delta_R$. When the bound
  fail, the total regret is at most
  \begin{align*}
    2\b{\p{\|\pmt\|_2 + \sqrt{d\lambda_{\max}(\pvt)}}\p{1 + \rho(2 +
      \sqrt{2 \log K}) + \hat R}}
  \end{align*}
  by Lemma~\ref{lem:exp_reg_ub_er} (with $\ell = \hat R$)
  and Lemma~\ref{lem:gaus_norm}. Since $\delta_R = T^{-2}$, the contribution of
  regret when the high-probability bound fails is $\tilde O(1/T) \le \tilde
  O(1)$.
