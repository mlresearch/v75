Online learning algorithms are a key tool in web search and content optimization, adaptively learning what users want to see. In a typical application, each time a user arrives, the algorithm chooses among various content presentation options (\eg news articles to display), the chosen content is presented to the user, and an outcome (\eg a click) is observed. Such algorithms must balance \emph{exploration} (making potentially suboptimal decisions now for the sake of acquiring information that will improve decisions in the future) and \emph{exploitation} (using information collected in the past to make better decisions now). Exploration could degrade the experience of a current user, but improves user experience in the long run. This exploration-exploitation tradeoff is commonly studied in the online learning framework of \emph{multi-armed bandits}~\citep{Bubeck-survey12}.

Concerns have been raised about whether exploration in such scenarios could be unfair, in the sense that some individuals or groups may experience too much of the downside of exploration without sufficient upside \citep{bird2016exploring}. We formally study these concerns in the \emph{linear contextual bandits} model~\citep{Langford-www10,chu2011contextual}, a standard variant of multi-armed bandits appropriate for content personalization scenarios.  We focus on \emph{externalities} arising due to exploration, that is, undesirable side effects that the presence of one party may impose on another.


We first examine the effects of exploration at a group level.  We introduce the notion of a \emph{group externality} in an online learning system, quantifying how much the presence of one population (which we dub the majority) impacts the rewards of another (the minority). We show that this impact can be negative, and that, in a particular precise sense, no algorithm can avoid it. This cannot be explained by the absence of suitably good policies since our adoption of the linear contextual bandits framework implies the existence of a feasible policy that is simultaneously optimal for everyone. Instead, the problem is inherent to the process of exploration. We come to a surprising conclusion that more data can sometimes lead to worse outcomes for the users of an explore-exploit-based system. \looseness=-1

We next turn to the effect of exploration at an individual level. We interpret exploration as a potential externality imposed on the current user by future users of the system. Indeed, it is only for the sake of the future users that the algorithm would forego the action that currently looks optimal. To avoid this externality, one may use the greedy algorithm that always chooses the action that appears optimal according to current estimates of the problem parameters. While this greedy algorithm performs poorly in the worst case,
it tends to work well in many applications and experiments.\footnote{Both positive and negative findings are folklore. One way to precisely state the negative result is that the greedy algorithm incurs constant per-round regret with constant probability; while results of this form have likely been known for decades,
\citet[Corollary A.2(b)]{competingBandits-itcs16}
proved this for a wide variety of scenarios. Very recently, the good empirical performance has been confirmed by state-of-art experiments in \citet{practicalCB-arxiv18}.}

In a new line of work, \citet{bastani2017exploiting} and \citet{kannan2018smoothed}
analyzed conditions under which inherent diversity in the data makes explicit exploration unnecessary.
\citet{kannan2018smoothed} proved that the greedy algorithm achieves a regret rate of
$\tilde{O}(\sqrt{T})$ in expectation over small perturbations of the context vectors (which ensure sufficient data diversity). This is the best rate that can be achieved in the worst case (\ie for all problem instances, without data diversity assumptions), but it leaves open the possibilities that (i) another algorithm may perform much better than the greedy algorithm on some problem instances, or (ii) the greedy algorithm may perform much better than worst case under the diversity conditions. We expand on this line of work. We prove that under the same diversity conditions, the greedy algorithm almost matches the best possible Bayesian regret rate of \emph{any} algorithm \emph{on the same problem instance}. This could be as low as $\polylog(T)$ for some instances, and, as we prove, at most $\tilde{O}(T^{1/3})$ whenever the diversity conditions hold.


Returning to group-level effects, we show that under the same diversity conditions, the negative group externalities imposed by the majority essentially vanish if one runs the greedy algorithm. Together, our results illustrate a sharp contrast between the high individual and group externalities that exist in the worst case, and the ability to remove all externalities if the data is sufficiently diverse.   \looseness=-1

\xhdr{Additional motivation.} Whether and when explicit exploration is necessary is an important concern in the study of the exploration-exploitation tradeoff. Fairness considerations aside, explicit exploration is expensive. It is wasteful and risky in the short term, it adds a layer of complexity to algorithm design \citep{Langford-nips07,monster-icml14}, and its adoption at scale tends to require substantial systems support and buy-in from management \citep{MWT-WhitePaper-2016,DS-arxiv}. A system based on the greedy algorithm would typically be cheaper to design and deploy.

Further, explicit exploration can run into incentive issues in applications such as recommender systems. Essentially, when it is up to the users which products or experiences to choose and the algorithm can only issue recommendations and ratings, an explore-exploit algorithm needs to provide incentives to explore for the sake of the future users \citep{Kremer-JPE14,Frazier-ec14,Che-13,ICexploration-ec15,Bimpikis-exploration-ms17}. Such incentive guarantees tend to come at the cost of decreased performance, and rely on assumptions about human behavior. The greedy algorithm avoids this problem as it is inherently consistent with the users' incentives.



\xhdr{Additional related work.}
Our research draws inspiration from the growing body of work on fairness in machine learning~\cite[e.g.,][]{dwork2012fairness,hardt2016equality,kleinberg2017inherent,chouldechova2017fair}.  Several other authors have studied fairness in the context of the contextual bandits framework.  Our work differs from the line of research on meritocratic fairness in online learning \citep{kearns2017meritocratic,liu2017calibrated,joseph2016fairness}, which considers the allocation of limited resources such as bank loans and requires that nobody should be passed over in favor of a less qualified applicant. We study a fundamentally different scenario in which there are no allocation constraints and we would like to serve each user the best content possible.  Our work also differs from that of \citet{celis2017fair}, who studied an alternative notion of fairness in the context of news recommendations. According to this notion, all users should have approximately the same probability of seeing a particular type of content (e.g., Republican-leaning articles), regardless of their individual preferences, in order to mitigate the possibility of discriminatory personalization.

The data diversity conditions in \citet{kannan2018smoothed} and this paper are inspired by the smoothed analysis framework of \citet{SmoothedAnalysis-jacm04}, who proved that the expected running time of the simplex algorithm is polynomial for perturbations of any initial problem instance (whereas the worst-case running time has long been known to be exponential). Such disparity implies that very bad problem instances are brittle. 
We find a similar disparity for the greedy algorithm in our setting.



\xhdr{Our results on group externalities.}  A typical goal in online learning is to minimize \emph{regret}, the (expected) difference between the cumulative reward that would have been obtained had the optimal policy been followed at every round and the cumulative reward obtained by the algorithm.  We define a corresponding notion of \emph{minority regret}, the portion of the regret experienced by the minority.  Since online learning algorithms update their behavior based on the history of their observations, minority regret is influenced by the entire population on which an algorithm is run.  If the minority regret is much higher when a particular algorithm is run on the full population than it is when the same algorithm is run on the minority alone, we can view the majority as imposing a negative externality on the minority; the minority population would achieve a higher cumulative reward if the majority were not present. Asking whether this can ever happen
amounts to asking whether access to more data points can ever lead an explore-exploit algorithm to make inferior decisions. One might think that more data should always lead to better decisions and therefore better outcomes for the users.
Surprisingly, we show that this is not the case, even with a standard algorithm.

Consider LinUCB~\citep{Langford-www10,chu2011contextual,abbasi2011improved}, a standard algorithm for linear contextual bandits that is based on the principle of ``optimism under uncertainty.''  We provide a specific problem instance on which, after observing $T$ users, LinUCB would have a minority regret of $\Omega(\sqrt T)$ if run on the full population, but only constant minority regret if run on the minority alone. While stylized, this example is motivated by the problem of providing driving directions to different populations of users, about which fairness concerns have been raised~\citep{bird2016exploring}. Further, the situation is reversed on a slight variation of this example: LinUCB obtains constant minority regret when run on the full population and $\Omega(\sqrt T)$ on the minority alone.  That is, group externalities can be large and positive in some cases, and large and negative in others.

Although these regret rates are specific to LinUCB, we show that this phenomenon is, in some sense, unavoidable. Consider the minority regret of LinUCB when run on the full population and the minority regret that LinUCB would incur if run on the minority alone. We know that one may be much smaller or larger than the other. We ask whether any algorithm could  achieve the minimum of the two on every problem instance. Using a variation of the same problem instance, we prove that this is impossible; in fact, no algorithm could simultaneously approximate both up to any $o(\sqrt{T})$ factor. In other words, an externality-free algorithm would sometimes ``leave money on the table."


In terms of techniques, we rely on the special structure of our example, which can be viewed as an instance of the sleeping bandits problem~\citep{SleepingBandits-ml10}. This simplifies the behavior and analysis of LinUCB, allowing us to obtain the $O(1)$ upper bounds.  The lower bounds are obtained using KL-divergence techniques to show that the two variants of our example are essentially indistinguishable, and an algorithm that performs well on one must obtain $\Omega(\sqrt{T})$ regret on the other. \looseness=-1


\xhdr{Our results on the greedy algorithm.} We consider a version of linear contextual bandits in which the latent weight vector $\theta$ is drawn from a known prior. In each round, an algorithm is presented several actions to choose from, each represented by a \emph{context vector}. The expected reward of an action is a linear product of $\theta$ and the corresponding context vector. The tuple of context vectors is drawn independently from a fixed distribution. In the spirit of smoothed analysis, we assume that this distribution has a small amount of jitter. Formally, the tuple of context vectors is drawn from some fixed distribution, and then a small \emph{perturbation}---small-variance Gaussian noise---is added independently to each coordinate of each context vector. This ensures arriving contexts are diverse. We are interested in Bayesian regret, i.e., regret in expectation over the Bayesian prior. Following the literature, we are primarily interested in the dependence on the time horizon $T$. \looseness=-1

We focus on a batched version of the greedy algorithm, in which new data arrives to the algorithm's optimization routine in small batches, rather than every round. This is well-motivated from a practical perspective---in high-volume applications data usually arrives to the ``learner" only after a substantial delay \citep{MWT-WhitePaper-2016,DS-arxiv}---and is essential for our analysis.

Our main result is that the greedy algorithm matches the Bayesian regret of any algorithm up to polylogarithmic factors, for each problem instance, fixing the Bayesian prior and the context distribution. We also prove that LinUCB achieves regret $\tilde{O}(T^{1/3})$ for each realization of $\theta$. This implies a worst-case Bayesian regret of $\tilde{O}(T^{1/3})$ for the greedy algorithm under the perturbation assumption. \looseness=-1

Our results hold for both natural versions of the batched greedy algorithm, Bayesian and frequentist, henceforth called \BayesGreedy and \FreqGreedy. In \BayesGreedy, the chosen action maximizes expected reward according to the Bayesian posterior. \FreqGreedy estimates $\theta$ using ordinary least squares regression and chooses the best action according to this estimate. The results for \FreqGreedy come with additive polylogarithmic factors, but are stronger in that the algorithm does not need to know the prior. Further, the $\tilde{O}(T^{1/3})$ regret bound for \FreqGreedy is approximately prior-independent, in the sense that it applies even to very concentrated priors such as independent Gaussians with standard deviation on the order of $T^{-2/3}$.

The key insight in our analysis of \BayesGreedy is that any (perturbed) data can be used to simulate any other data, with some discount factor. The analysis of \FreqGreedy requires an additional layer of complexity. We consider a hypothetical algorithm that receives the same data as \FreqGreedy, but chooses actions based on the Bayesian-greedy selection rule. We analyze this hypothetical algorithm using the same technique as \BayesGreedy, and then upper bound the difference in Bayesian regret between the hypothetical algorithm and \FreqGreedy.

Our analyses extend to group externalities and (Bayesian) minority regret. In particular, we circumvent the impossibility result mentioned above. We prove that both \BayesGreedy and \FreqGreedy match the Bayesian minority regret of any algorithm run on either the full population or the minority alone, up to polylogarithmic factors

\xhdr{Detailed comparison with prior work.} We substantially improve over the $\tilde{O}(\sqrt{T})$ worst-case regret bound from \citet{kannan2018smoothed}, at the cost of some additional assumptions. First, we consider Bayesian regret, whereas their regret bound is for each realization of $\theta$.%
\footnote{Equivalently, they allow point priors, whereas our priors must have variance $T^{-O(1)}$.} Second, they allow the context vectors to be chosen by an adversary before the perturbation is applied. Third, they extend their analysis to a somewhat more general model, in which there is a separate latent weight vector for every action (which amounts to a more restrictive model of perturbations). However, this extension relies on the greedy algorithm being initialized with a substantial amount of data. The results of \citet{kannan2018smoothed} do not appear to have implications on group externalities.

\citet{bastani2017exploiting} show that the greedy algorithm achieves logarithmic regret in an alternative linear contextual bandits setting that is incomparable to ours in several important ways.
They consider two-action instances where the actions share a common context vector in each round, but are parameterized by different latent vectors. They ensure data diversity via a strong assumption on the context distribution. This assumption does not follow from our perturbation conditions; among other things, it implies that each action is the best action in a constant fraction of rounds. Further, they assume a version of Tsybakov's \emph{margin condition}, which is known to substantially reduce regret rates in bandit problems \citep[\eg see][]{Zeevi-colt10}.

