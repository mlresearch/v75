\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2012)Agarwal, Bartlett, Ravikumar, and
  Wainwright]{AgarwalBRW12}
Alekh Agarwal, Peter~L. Bartlett, Pradeep Ravikumar, and Martin~J. Wainwright.
\newblock Information-theoretic lower bounds on the oracle complexity of
  stochastic convex optimization.
\newblock \emph{IEEE Transactions on Information Theory}, 2012.

\bibitem[Allen-Zhu(2016)]{Zhu16}
Zeyuan Allen-Zhu.
\newblock Katyusha: The first direct acceleration of stochastic gradient
  methods.
\newblock \emph{CoRR}, abs/1603.05953, 2016.

\bibitem[Anbar(1971)]{anbar1971optimal}
Dan Anbar.
\newblock \emph{On Optimal Estimation Methods Using Stochastic Approximation
  Procedures}.
\newblock University of California, 1971.
\newblock URL \url{http://books.google.com/books?id=MmpHJwAACAAJ}.

\bibitem[Bach(2014)]{Bach14}
Francis~R. Bach.
\newblock Adaptivity of averaged stochastic gradient descent to local strong
  convexity for logistic regression.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, volume 15, 2014.

\bibitem[Bach and Moulines(2011)]{BachM11}
Francis~R. Bach and Eric Moulines.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock In \emph{NIPS 24}, 2011.

\bibitem[Bach and Moulines(2013)]{BachM13}
Francis~R. Bach and Eric Moulines.
\newblock Non-strongly-convex smooth stochastic approximation with convergence
  rate {O}(1/n).
\newblock In \emph{NIPS 26}, 2013.

\bibitem[Bottou and Bousquet(2007)]{BottouB07}
L{\'e}on Bottou and Olivier Bousquet.
\newblock The tradeoffs of large scale learning.
\newblock In \emph{NIPS 20}, 2007.

\bibitem[Cauchy(1847)]{Cauchy1847}
Louis~Augustin Cauchy.
\newblock M\'ethode g\'en\'erale pour la r\'esolution des syst\'emes
  d'\'equations simultanees.
\newblock \emph{C. R. Acad. Sci. Paris}, 1847.

\bibitem[d'Aspremont(2008)]{dAspremont08}
Alexandre d'Aspremont.
\newblock Smooth optimization with approximate gradient.
\newblock \emph{SIAM Journal on Optimization}, 19\penalty0 (3):\penalty0
  1171--1183, 2008.

\bibitem[D{\'e}fossez and Bach(2015)]{DefossezB15}
Alexandre D{\'e}fossez and Francis~R. Bach.
\newblock Averaged least-mean-squares: Bias-variance trade-offs and optimal
  sampling distributions.
\newblock In \emph{AISTATS}, volume~38, 2015.

\bibitem[Devolder et~al.(2013)Devolder, Glineur, and Nesterov]{DevolderGN13}
Olivier Devolder, Fran{\c c}ois Glineur, and Yurii~E. Nesterov.
\newblock First-order methods with inexact oracle: the strongly convex case.
\newblock \emph{CORE Discussion Papers 2013016}, 2013.

\bibitem[Devolder et~al.(2014)Devolder, Glineur, and Nesterov]{DevolderGN14}
Olivier Devolder, Fran{\c c}ois Glineur, and Yurii~E. Nesterov.
\newblock First-order methods of smooth convex optimization with inexact
  oracle.
\newblock \emph{Mathematical Programming}, 146:\penalty0 37--75, 2014.

\bibitem[Dieuleveut and Bach(2015)]{DieuleveutB15}
Aymeric Dieuleveut and Francis~R. Bach.
\newblock Non-parametric stochastic approximation with large step sizes.
\newblock \emph{The Annals of Statistics}, 2015.

\bibitem[Dieuleveut et~al.(2016)Dieuleveut, Flammarion, and
  Bach]{DieuleveutFB16}
Aymeric Dieuleveut, Nicolas Flammarion, and Francis~R. Bach.
\newblock Harder, better, faster, stronger convergence rates for least-squares
  regression.
\newblock \emph{CoRR}, abs/1602.05419, 2016.

\bibitem[Fabian(1973)]{Fabian:1973:AES}
Vaclav Fabian.
\newblock Asymptotically efficient stochastic approximation; the {RM} case.
\newblock \emph{Annals of Statistics}, 1\penalty0 (3), 1973.

\bibitem[Frostig et~al.(2015{\natexlab{a}})Frostig, Ge, Kakade, and
  Sidford]{FrostigGKS15b}
Roy Frostig, Rong Ge, Sham Kakade, and Aaron Sidford.
\newblock Un-regularizing: approximate proximal point and faster stochastic
  algorithms for empirical risk minimization.
\newblock In \emph{ICML}, 2015{\natexlab{a}}.

\bibitem[Frostig et~al.(2015{\natexlab{b}})Frostig, Ge, Kakade, and
  Sidford]{FrostigGKS15}
Roy Frostig, Rong Ge, Sham~M. Kakade, and Aaron Sidford.
\newblock Competing with the empirical risk minimizer in a single pass.
\newblock In \emph{COLT}, 2015{\natexlab{b}}.

\bibitem[Ghadimi and Lan(2012)]{ghadimi2012optimal}
Saeed Ghadimi and Guanghui Lan.
\newblock Optimal stochastic approximation algorithms for strongly convex
  stochastic composite optimization i: A generic algorithmic framework.
\newblock \emph{SIAM Journal on Optimization}, 2012.

\bibitem[Ghadimi and Lan(2013)]{ghadimi2013optimal}
Saeed Ghadimi and Guanghui Lan.
\newblock Optimal stochastic approximation algorithms for strongly convex
  stochastic composite optimization, ii: shrinking procedures and optimal
  algorithms.
\newblock \emph{SIAM Journal on Optimization}, 2013.

\bibitem[Greenbaum(1989)]{Greenbaum89}
Anne Greenbaum.
\newblock Behavior of slightly perturbed lanczos and conjugate-gradient
  recurrences.
\newblock \emph{Linear Algebra and its Applications}, 1989.

\bibitem[Hestenes and Stiefel(1952)]{HestenesS52}
Magnus~R. Hestenes and Eduard Stiefel.
\newblock Methods of conjuate gradients for solving linear systems.
\newblock \emph{Journal of Research of the National Bureau of Standards}, 1952.

\bibitem[Hsu et~al.(2014)Hsu, Kakade, and Zhang]{HsuKZ14}
Daniel~J. Hsu, Sham~M. Kakade, and Tong Zhang.
\newblock Random design analysis of ridge regression.
\newblock \emph{Foundations of Computational Mathematics}, 14\penalty0
  (3):\penalty0 569--600, 2014.

\bibitem[Hu et~al.(2009)Hu, Kwok, and Pan]{HuKP09}
Chonghai Hu, James~T. Kwok, and Weike Pan.
\newblock Accelerated gradient methods for stochastic optimization and online
  learning.
\newblock In \emph{NIPS 22}, 2009.

\bibitem[Jain et~al.(2016)Jain, Kakade, Kidambi, Netrapalli, and
  Sidford]{JainKKNS16}
Prateek Jain, Sham~M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron
  Sidford.
\newblock Parallelizing stochastic approximation through mini-batching and
  tail-averaging.
\newblock \emph{CoRR}, abs/1610.03774, 2016.

\bibitem[Kushner and Clark(1978)]{KushnerClark}
Harold~J. Kushner and Dean~S. Clark.
\newblock \emph{Stochastic Approximation Methods for Constrained and
  Unconstrained Systems.}
\newblock Springer-Verlag, 1978.

\bibitem[Kushner and Yin(2003)]{KushnerY03}
Harold~J. Kushner and George Yin.
\newblock Stochastic approximation and recursive algorithms and applications.
\newblock \emph{Springer-Verlag}, 2003.

\bibitem[Lan(2008)]{Lan08}
G.~Lan.
\newblock An optimal method for stochastic composite optimization.
\newblock \emph{Tech. Report, GaTech.}, 2008.

\bibitem[Lan and Zhou(2015)]{LanZ15}
Guanghui Lan and Yi~Zhou.
\newblock An optimal randomized incremental gradient method.
\newblock \emph{CoRR}, abs/1507.02000, 2015.

\bibitem[Lehmann and Casella(1998)]{lehmann1998theory}
Erich~L. Lehmann and George Casella.
\newblock \emph{Theory of Point Estimation}.
\newblock Springer, 1998.

\bibitem[Lin et~al.(2015)Lin, Mairal, and Harchaoui]{LinMH15}
Hongzhou Lin, Julien Mairal, and Za{\"i}d Harchaoui.
\newblock A universal catalyst for first-order optimization.
\newblock In \emph{NIPS}, 2015.

\bibitem[Needell et~al.(2016)Needell, Srebro, and Ward]{NeedellSW16}
Deanna Needell, Nathan Srebro, and Rachel Ward.
\newblock Stochastic gradient descent, weighted sampling, and the randomized
  kaczmarz algorithm.
\newblock \emph{Mathematical Programming}, 2016.

\bibitem[Nemirovsky and Yudin(1983)]{NemirovskyY83}
Arkadii~S. Nemirovsky and David~B. Yudin.
\newblock \emph{Problem Complexity and Method Efficiency in Optimization}.
\newblock John Wiley, 1983.

\bibitem[Nesterov(1983)]{Nesterov83}
Yurii~E. Nesterov.
\newblock A method for unconstrained convex minimization problem with the rate
  of convergence ${O}(1/k^2)$.
\newblock \emph{Doklady AN SSSR}, 269, 1983.

\bibitem[Nesterov(2004)]{Nesterov04}
Yurii~E. Nesterov.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87 of \emph{Applied Optimization}.
\newblock Kluwer Academic Publishers, 2004.

\bibitem[Nesterov(2012)]{Nesterov12}
Yurii~E. Nesterov.
\newblock Efficiency of coordinate descent methods on huge-scale optimization
  problems.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (2):\penalty0
  341--362, 2012.

\bibitem[Paige(1971)]{Paige71}
Christopher~C. Paige.
\newblock The computation of eigenvalues and eigenvectors of very large sparse
  matrices.
\newblock \emph{PhD Thesis, University of London}, 1971.

\bibitem[Polyak(1964)]{Polyak64}
Boris~T. Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics}, 4,
  1964.

\bibitem[Polyak(1987)]{Polyak87}
Boris~T. Polyak.
\newblock \emph{Introduction to Optimization}.
\newblock Optimization Software, 1987.

\bibitem[Polyak and Juditsky(1992)]{PolyakJ92}
Boris~T. Polyak and Anatoli~B. Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock \emph{SIAM Journal on Control and Optimization}, volume 30, 1992.

\bibitem[Proakis(1974)]{Proakis74}
John~G. Proakis.
\newblock Channel identification for high speed digital communications.
\newblock \emph{IEEE Transactions on Automatic Control}, 1974.

\bibitem[Raginsky and Rakhlin(2011)]{RaginskyR11}
Maxim Raginsky and Alexander Rakhlin.
\newblock Information-based complexity, feedback and dynamics in convex
  programming.
\newblock \emph{IEEE Transactions on Information Theory}, 2011.

\bibitem[Robbins and Monro(1951)]{RobbinsM51}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, vol. 22, 1951.

\bibitem[Roy and Shynk(1990)]{RoyS90}
Sumit Roy and John~J. Shynk.
\newblock Analysis of the momentum lms algorithm.
\newblock \emph{IEEE Transactions on Acoustics, Speech and Signal Processing},
  1990.

\bibitem[Ruppert(1988)]{Ruppert88}
David Ruppert.
\newblock Efficient estimations from a slowly convergent robbins-monro process.
\newblock \emph{Tech. Report, ORIE, Cornell University}, 1988.

\bibitem[Shalev-Shwartz and Zhang(2014)]{ShwartzZ14}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Accelerated proximal stochastic dual coordinate ascent for
  regularized loss minimization.
\newblock In \emph{ICML}, 2014.

\bibitem[Sharma et~al.(1998)Sharma, Sethares, and Bucklew]{SharmaSB98}
Rajesh Sharma, William~A. Sethares, and James~A. Bucklew.
\newblock Analysis of momentum adaptive filtering algorithms.
\newblock \emph{IEEE Transactions on Signal Processing}, 1998.

\bibitem[van~der Vaart(2000)]{Vaart00}
Aad~W. van~der Vaart.
\newblock \emph{Asymptotic Statistics}.
\newblock Cambridge University Publishers, 2000.

\bibitem[Widrow and Stearns(1985)]{WidrowS85}
Bernard Widrow and Samuel~D. Stearns.
\newblock \emph{Adaptive Signal Processing}.
\newblock Englewood Cliffs, NJ: Prentice-Hall, 1985.

\bibitem[Wilson et~al.(2016)Wilson, Recht, and Jordan]{WilsonRJ16}
Ashia~C. Wilson, Benjamin Recht, and Michael~I. Jordan.
\newblock A lyapunov analysis of momentum methods in optimization.
\newblock \emph{CoRR}, abs/1611.02635, 2016.

\bibitem[Woodworth and Srebro(2016)]{WoodworthS16}
Blake Woodworth and Nathan Srebro.
\newblock Tight complexity bounds for optimizing composite objectives.
\newblock \emph{CoRR}, abs/1605.08003, 2016.

\bibitem[Yuan et~al.(2016)Yuan, Ying, and Sayed]{YuanYS16}
Kun Yuan, Bicheng Ying, and Ali~H. Sayed.
\newblock On the influence of momentum acceleration on online learning.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, volume 17, 2016.

\end{thebibliography}
