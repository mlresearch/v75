%\section{Simulations}\label{sec:simulations}
%
%\begin{figure}[t!] % "[t!]" placement specifier just for this example
%\begin{subfigure}{0.33\textwidth}
%\includegraphics[width=\linewidth,height=4cm]{Gaussian-bias.png}
%\caption{Bias Risk} \label{fig:1a}
%\end{subfigure}%\hspace*{\fill}
%\begin{subfigure}{0.33\textwidth}
%\includegraphics[width=\linewidth,height=4cm]{Gaussian-variance.png}
%\caption{Variance Risk} \label{fig:1b}
%\end{subfigure}
%\begin{subfigure}{0.33\textwidth}
%\includegraphics[width=\linewidth,height=4cm]{Gaussian-total.png}
%\caption{Total Risk} \label{fig:1c}
%\end{subfigure}
%\caption{Comparing Accelerated SGD with SGD for the Gaussian distribution. The start of tail-averaging is indicated with a vertical black line. The bias (figure~\ref{fig:1a}) and total risk (figure~\ref{fig:1c}) indicate the superior behavior of accelerated SGD compared to SGD, while the variance (figure~\ref{fig:1b}) of accelerated SGD is a constant factor worse off SGD.} \label{fig:Gaussian}
%\end{figure}
%\begin{figure}[t!] % "[t!]" placement specifier just for this example
%\begin{subfigure}{0.33\textwidth}
%\includegraphics[width=\linewidth,height=4cm]{probability-bias.png}
%\caption{Bias Risk} \label{fig:2a}
%\end{subfigure}%\hspace*{\fill}
%\begin{subfigure}{0.33\textwidth}
%\includegraphics[width=\linewidth,height=4cm]{probability-variance.png}
%\caption{Variance Risk} \label{fig:2b}
%\end{subfigure}
%\begin{subfigure}{0.33\textwidth}
%\includegraphics[width=\linewidth,height=4cm]{probability-total.png}
%\caption{Total Risk} \label{fig:2c}
%\end{subfigure}
%\caption{Comparing Accelerated SGD with SGD for the equal probability discrete distribution. The start of tail-averaging is shown using a vertical black line. The bias (figure~\ref{fig:2a}), variance (figure~\ref{fig:2b}) and total error (figure~\ref{fig:2c}) plots corroborate with theory in that Acceleration does not offer any improvement over SGD for this worst-case example.}\label{fig:Discrete}
%\end{figure}
%\iffalse
%\begin{figure*}[t!]
%	\begin{tabular}{ccc}\hspace*{-10pt}
%		\includegraphics[width=.33\textwidth]{figures1/Gaussian-bias.png}&\hspace*{-20pt}
%		\includegraphics[width=.33\textwidth]{figures1/Gaussian-variance.png}&\hspace*{-20pt}
%		\includegraphics[width=.33\textwidth]{figures1/Gaussian-total.png}\\
%		(a)&(b)&(c)\label{fig1}
%	\end{tabular}
%\caption{Plots (a),(b),(c) represent respectively the behavior of the bias, variance and the total error for the case when the inputs are sampled from a Gaussian Distribution. Note that tail-averaging begins after the point indicated by the black vertical line. We note that for the Gaussian case, $\cnS<<\cnH$ thus indicating huge wins for TASGD (blue) over SGD (red) in terms of the rate of killing the initial error (plot(a)). The variance of TASGD (plot (b)) matches the behavior indicated by the theory bound in that it is worse compared to classic tail-averaged SGD by a constant factor. The total risk (plot (c)) indicates a significantly lower generalization error of TASGD compared to vanilla tail-averaged SGD indicating vastly superior sample complexity.}
%\end{figure*}
%
%\begin{figure*}[t!]\label{fig2}
%	\begin{tabular}{ccc}\hspace*{-10pt}
%		\includegraphics[width=.33\textwidth]{figures1/probability-bias.png}&\hspace*{-20pt}
%		\includegraphics[width=.33\textwidth]{figures1/probability-variance.png}&\hspace*{-20pt}
%		\includegraphics[width=.33\textwidth]{figures1/probability-total.png}\\
%		(d)&(e)&(f)
%	\end{tabular}
%\caption{Plots (d),(e),(f) represent respectively the behavior of the bias, variance and the total error for the case when the inputs are sampled from a Discrete Distribution with equal probabilities of observing each direction. Note that tail-averaging begins after the point indicated by the black vertical line. In this case, $\cnS=\cnH$ thus indicating TASGD (blue) does not improve upon classic tail-averaged SGD (red) in terms of the rate of killing the initial error (plot(d)), as well as in the variance error and total generalization error, thus corroborating with bounds indicated by theory. }
%\end{figure*}
%\fi
%We present an empirical study of the performance of the proposed accelerated SGD method by considering two data distributions, namely, (a) the Gaussian distribution, and  (b) the discrete distribution with equal probability of observing each direction. Recall that our bound for initial error (bias) indicates advantage of using accelerated SGD over SGD only if the condition number $\cnH$ is significantly larger than the statistical condition number $\cnS$ (see Section~\ref{sec:prob}), which implies a significantly faster rate of initial error decay for the Gaussian case and no improvement for the discrete case.
%
%{\bf Methodology:} In each experiment, we generate $n=\cnH$ samples of $(\a_i, b_i)$ pairs, where $\a_i\in \R^{50}$ is sampled from ${\cal D}$ while $b_i$ is sampled from ${\cal N}(\iprod{\a_i}{\vec{x}^*}, \sigma^2)$ with $\sigma=10$. The distribution ${\cal D}$ has second moment $\H=\E{\a_i\otimes \a_i}$ with smallest eigenvalue $\mu=10^{-4}$ while $\|\H\|_2=1$. We run both SGD and  accelerated SGD for a total of $\cnH$ iterations, and begin the tail-averaging phase after $\cnH/10$ iterations. We report excess risk on log scale averaged over $100$ trials where $\x^*$ is fixed, but the optimization is run with freshly sampled data points in each trial. 
%
%We first consider the Gaussian distribution with diagonal covariance. Note that the statistical condition number $\cnS$ for Gaussian distribution is always bounded by $\cnS\leq3d$ irrespective of $\mu$. The condition number $\cnH=\frac{R^2}{\mu}$ in this case is significantly bigger compared to $\sqrt{\cnH\cnS}$, implying significantly faster bias decay for accelerated SGD compared to SGD, which is indeed the behavior observed in figure~\ref{fig:1a}. Moreover, Figure~\ref{fig:1b} presents evidence that the variance of accelerated SGD is not off by more than constant factors off the vanilla SGD. Finally, figure~\ref{fig:1c} lends credence to the claim that accelerated SGD provides a much stronger rate in competing on the overall generalization error compared to classic tail-averaged SGD, thus indicating significantly improved sample complexity.
%
%Next, we consider a distribution over $1$-sparse vectors with exactly same magnitude. That is, the non-zero coordinate $j$ is sampled with probability $p_j$ and then the corresponding $\a_i$ is given by $\vec{e}_j$, the $j$-th canonical basis vector. In this case, $\cnH=\cnS=\frac{1}{p_{min}}$. This implies that the bias decay for accelerated SGD matches that of classic SGD. Figure~\ref{fig:2a},~\ref{fig:2b},~\ref{fig:2c} corroborates this claim that the behavior of accelerated SGD nearly degenerates to classic SGD when considering bias, variance and total error, thus indicating that there exists worst case distributions where acceleration with stochastic oracles does not offer any improvement over classic SGD.
%\iffalse
%However, if each $\a_i$ is sampled from $1$-sparse vectors with different magnitude, i.e., $\a_i = \alpha_j \vec{e}_j$ if $j$-th coordinate is sampled uniformly with probability $p_i=\frac{1}{d}$. Then, $\cnH=\frac{1}{\mu}$ as $R=1$ but $\cnS\leq d$, again indicating significant difference in $\cnH$ and $\cnS$ and hence between bias error bounds for SGD and TASGD. Figure~\ref{fig:fig1} (d) verifies our claim as bias of TASGD decays significantly more rapidly than that of SGD. 
%\fi