\section{Proof Outline}\label{sec:proofoutline}
%\sidford{Section capitalization seems inconsistent. I am not sure which way you prefer things though (or would have fixed it).}
\iffalse We now present a brief outline of the proof of Theorem~\ref{thm:main}.\fi Recall the variables in Algorithm~\ref{algo:TAASGD}. \iffalse Before presenting the proof outline we require some definitions.\fi We begin by defining the centered estimate $\thetat$ as:%\vspace{-2mm}\iffalse We begin by defining the centered estimate $\thetat$ as:\vspace*{-2mm}\fi
\vspace{-0.2cm}
\begin{align*}
\thetat \eqdef \left[\begin{array}{c} \xt - \xs \\ \yt - \xs \end{array}\right]\in\R^{2d}. 
\end{align*}%\vspace*{-2mm}

\vspace{-0.2cm}
\noindent Recall that the stepsizes in Algorithm~\ref{algo:TAASGD} are $\alpha = \frac{3\sqrt{5}\cdot\sqrt{\cnH\cnS}}{1+3\sqrt{5}\cdot\sqrt{\cnH\cnS}}, \beta = \frac{1}{9\sqrt{\cnH\cnS}}, \gamma =  \frac{1}{3\sqrt{5}\cdot \mu \sqrt{\cnH\cnS}}, \delta = \frac{1}{5R^2}$. The accelerated SGD updates of Algorithm~\ref{algo:TAASGD} can be written in terms of $\thetat$ as:%\vspace{-.5mm}
\iffalse
\begin{align*}
\thetat &= \Ahatj \thetat[j-1] + \zetat, \quad \text{where,}\\
\Ahatj &\defeq \begin{bmatrix} 0 & (\eye-\delta\Hh[j])\\ -\alpha(1-\beta)\ \eye & (1+\alpha(1-\beta))\eye-(\alpha\delta+(1-\alpha)\gamma)\Hh[j] \end{bmatrix}\quad \text{and,}\\
\zetat[j] &\eqdef  \left[\begin{array}{c} \delta \cdot \ni[j] \ai[j] \\ (\alpha \delta + (1-\alpha)\gamma) \cdot \ni[j] \ai[j] \end{array}\right].
\end{align*}
\fi

\vspace{-0.2cm}
{\small\begin{align*}
\thetat = \Ahatj \thetat[j-1] + \zetat, &\quad \text{where,}\\
\Ahatj \defeq \begin{bmatrix} 0 & (\eye-\delta\ai[j]\ai[j]\T)\\ -\alpha(1-\beta)\ \eye & (1+\alpha(1-\beta))\eye-(\alpha\delta+(1-\alpha)\gamma)\ai[j]\ai[j]\T \end{bmatrix}&,
\zetat[j] \eqdef  \left[\begin{array}{c} \delta \cdot \ni[j] \ai[j] \\ (\alpha \delta + (1-\alpha)\gamma) \cdot \ni[j] \ai[j] \end{array}\right],
\end{align*}}%

\vspace{-0.2cm}
\noindent where $\epsilon_j=b_j-\iprod{\ai[j]}{\xs}$. The tail-averaged iterate $\bar{\x}_{t,n}$ is associated with $\thetavb\defeq\frac{1}{n-t}\sum_{j=t+1}^n\thetav_j$. Let $\A \eqdef \E{\Ahatj|\mathcal{F}_{j-1}}$, where $\mathcal{F}_{j-1}$ is a filtration generated by $(\a_1,b_1),\cdots,(\a_{j-1},b_{j-1})$. Let $\B,\AL,\AR$ be linear operators acting on a matrix $\S\in\R^{2d\times2d}$ so that $\B\S\eqdef \E{\Ahatj \S \Ahatj\T|\mathcal{F}_{j-1}}$, $\AL\S\eqdef\A\S$, $\AR\S\eqdef\S\A$. Denote $\Sighat\eqdef \E{\zetat \zetat \T|\mathcal{F}_{j-1}}$ and matrices $\G,\Z,\PM$ as:\vspace*{-2mm}
\begin{align*}
\G\defeq\PM \T \mat{Z}\PM , \text{where}, \PM\eqdef \begin{bmatrix} \Id &\zero \\ \frac{-\alpha}{1-\alpha}\Id & \frac{1}{1-\alpha}\Id \end{bmatrix},\ \ \mat{Z}\eqdef\begin{bmatrix} \Id &\zero \\ \zero & {\mu}\inv{\Cov}\end{bmatrix}.
\end{align*}

\vspace{-0.2cm}
\noindent \underline{\textbf{Bias-variance decomposition}}: The proof of theorem~\ref{thm:main} employs the {\em bias-variance} decomposition, which is well known in the context of stochastic approximation (see~\cite{BachM11,FrostigGKS15,JainKKNS16}) and is re-derived in the appendix. The bias-variance decomposition allows for the generalization error to be upper-bounded by analyzing two sub-problems: (a) {\em bias}, analyzing the algorithm's behavior on the {\em noiseless} problem (i.e. $\zetav_j=0\ \forall\ j$ a.s.) while starting at $\thetav_0^{\textrm{bias}}=\thetav_0$ and (b) {\em variance}, analyzing the algorithm's behavior by starting at the solution (i.e. $\thetav_0^{\textrm{variance}}=0$) and allowing the noise $\zetav_{\Bigcdot}$ to drive the process. In a similar manner as $\thetavb$, the bias and variance sub-problems are associated with $\thetavb^{\textrm{bias}}$ and $\thetavb^{\textrm{variance}}$, and these are related as:\vspace*{-2mm}
\begin{align}\label{eqn:bound-covar}
		\E{\thetavb \otimes \thetavb} \preceq 2\cdot\bigg( \E{\thetavb^{\text{bias}} \otimes \thetavb^{\text{bias}}} + \E{\thetavb^{\text{variance}} \otimes \thetavb^{\text{variance}}}\bigg).
\end{align}

\vspace{-0.2cm}
\noindent Since we deal with the square loss, the generalization error of the output $\bar{\x}_{t,n}$ of algorithm~\ref{algo:TAASGD} is:\vspace*{-2mm}
\begin{align}
\label{eq:genErrorExp}
\E{P(\bar{\x}_{t,n})}-P(\xs)=\frac{1}{2}\cdot\iprod{\begin{bmatrix}\Cov&0\\0&0\end{bmatrix}}{\E{\thetavb \otimes \thetavb}},
\end{align}

\vspace{-0.2cm}
\noindent indicating that the generalization error can be bounded by analyzing the bias and variance sub-problem.
\iffalse
\begin{align}
\label{eq:genErrorExp}
\E{P(\bar{\x}_{t,n})}-P(\xs)&=\frac{1}{2}\cdot\iprod{\begin{bmatrix}\Cov&0\\0&0\end{bmatrix}}{\E{\thetavb \otimes \thetavb}}\nonumber\\
&\leq\iprod{\begin{bmatrix}\Cov&0\\0&0\end{bmatrix}}{\E{\thetavb^{\textrm{bias}} \otimes \thetavb^{\textrm{bias}}}}+\iprod{\begin{bmatrix}\Cov&0\\0&0\end{bmatrix}}{\E{\thetavb^{\textrm{variance}} \otimes \thetavb^{\textrm{variance}}}}
\end{align}
\fi
%Equations~\ref{eqn:bound-covar},~\ref{eq:genErrorExp} are well known in stochastic approximation, and have been re-derived in the appendix for the sake of completeness. %Before presenting the lemmas, we require defining the following notation: let $\A \eqdef \E{\Ahatj}$ and $\B$ be an operator acting on matrices $\S\in\R^{2d\times 2d}$ such that $\B \S \eqdef \E{\Ahatj \S \Ahatj\T}$. $\AL$ and $\AR$ denote respectively the left and right multiplication operators of $\A$ i.e., $\S\in\R^{2d\times2d}$, $\AL \S \eqdef \A \S$ and $\AR \S \eqdef \S \A$. Furthermore, let $\Sighat \eqdef \E{\zetat \zetat \T}$, $\PM\eqdef \begin{bmatrix} \Id &\zero \\ \frac{-\alpha}{1-\alpha}\Id & \frac{1}{1-\alpha}\Id \end{bmatrix}$ and let $\G$ represent a positive definite matrix $\G\defeq\PM \begin{bmatrix} \Id &\zero \\ \zero & {\mu}\inv{\Cov} \end{bmatrix} \PM\T$. Recall that the step sizes in Algorithm~\ref{algo:TAASGD} are chosen as $\alpha = \frac{3\sqrt{5}\cdot\sqrt{\cnH\cnS}}{1+3\sqrt{5}\cdot\sqrt{\cnH\cnS}}, \beta = \frac{1}{9\sqrt{\cnH\cnS}}, \gamma =  \frac{1}{3\sqrt{5}\cdot \mu \sqrt{\cnH\cnS}}, \delta = \frac{1}{5R^2}$.
We now present the lemmas that bound the bias error.
\begin{lemma}\label{lem:average-covar-bias}
The covariance $\E{\thetavb^{\textrm{bias}} \otimes \thetavb^{\text{bias}}}$ of the bias part of averaged iterate $\thetavb^{\textrm{bias}}$ satisfies:
\vspace{-0.2cm}
{\small\begin{align*}
\E{\thetavb^{\textrm{bias}} \otimes \thetavb^{\text{bias}}} &=  \frac{1}{(n-t)^2} \bigg( \eyeT + (\eyeT-\AL)^{-1}\AL + (\eyeT-\AR\T)^{-1}\AR\T\bigg) (\eyeT-\BT)^{-1}(\BT^{t+1}-\BT^{n+1}) \left(\thetat[0]\otimes \thetat[0]\right)\\	 &\quad -\frac{1}{(n-t)^2}\sum_{j=t+1}^n\bigg( (\eyeT-\AL)^{-1}\AL^{n+1-j} + (\eyeT-\AR\T)^{-1}(\AR\T)^{n+1-j} \bigg)\BT^j (\thetat[0] \otimes \thetat[0]).
\end{align*}}%
\end{lemma}
\vspace{-0.2cm}The quantity that needs to be bounded in the term above is $\BT^{t+1}\thetav_0\otimes\thetav_0$. Lemma~\ref{lem:main-bias} presents a result that can be applied recursively to bound $\BT^{t+1}\thetav_0\otimes\thetav_0$ ($=\BT^{t+1}\thetav_0^{\textrm{bias}}\otimes\thetav_0^{\textrm{bias}}$ since $\thetav_0^{\textrm{bias}}=\thetav_0$).
\begin{lemma}[Bias contraction] \label{lem:main-bias}
	For any two vectors $\x, \y \in \R^d$, let $\thetav \eqdef \begin{bmatrix}
	\x-\xs \\ \y-\xs
	\end{bmatrix} \in \R^{2d}$. We have: \vspace*{-2mm}
	\begin{align*}
	\iprod{\G}{\B \left(\thetav \thetav \T\right)} \leq \bigg(1-\frac{1}{9\sqrt{\cnH\cnS}}\bigg) \iprod{\G}{\thetav \thetav \T}%\exp\left(\frac{-1}{9\sqrt{\cnH\cnS}}\right) \iprod{\G}{\thetav \thetav \T}.
	\end{align*}
\end{lemma}

\vspace{-0.4cm}
\paragraph{Remarks:}(i) the matrices $\PM$ and $\PM\T$ appearing in $\G$ are due to the fact that we prove contraction using the variables $\x-\xs$ and $\v-\xs$ \iffalse(see Algorithm~\ref{algo:TAASGD})\fi instead of $\x-\xs$ and $\y-\xs$, as used in defining $\thetav$. (ii) The key novelty in lemma~\ref{lem:main-bias} is that while standard analyses of accelerated gradient descent (in the exact first order oracle) use the potential function $\norm{\x-\xs}_{\Cov}^2 + \mu \twonorm{\v - \xs}^2$ (e.g.~\cite{WilsonRJ16}), we consider it crucial for employing the potential function $\twonorm{\x-\xs}^2 + \mu \norm{\v - \xs}_{\inv{\Cov}}^2$ (this potential function is captured using the matrix $\mat{Z}$\iffalse$\begin{bmatrix} \Id &\zero \\ \zero & \mu\inv{\Cov} \end{bmatrix}$\fi) to prove accelerated rates (of $\order{1/\sqrt{\cnH\cnS}}$) for bias decay. 

We now present the lemmas associated with bounding the variance error:
\begin{lemma}\label{lem:average-covar-var}
%	The asymptotic covariance $\E{\thetavb^{\textrm{variance}} \otimes \thetavb^{\text{variance}}}$ of the variance part of averaged iterate $\thetavb^{\textrm{variance}}$ (as $n\to\infty$) satisfies:
	The covariance $\E{\thetavb^{\textrm{variance}} \otimes \thetavb^{\text{variance}}}$ of the variance error $\thetavb^{\textrm{variance}}$ satisfies:
	\iffalse
	\begin{align*}
	\lim_{n \to \infty}\E{\thetavb^{\textrm{variance}} \otimes \thetavb^{\text{variance}}} = \frac{1}{n-t}\big(\eyeT + (\eyeT-\AL)^{-1}\AL + (\eyeT-\AR\T)^{-1}\AR\T\big)(\eyeT-\BT)^{-1}\Sigh.
	\end{align*}
	\fi{\small
	\begin{align*}
	&\E{\thetavb^{\textrm{variance}} \otimes \thetavb^{\text{variance}}} = \frac{1}{n-t}\big(\eyeT + (\eyeT-\AL)^{-1}\AL + (\eyeT-\AR\T)^{-1}\AR\T\big)(\eyeT-\BT)^{-1}\Sigh	\nonumber\\
	&\quad -\frac{1}{(n-t)^2}\big((\eyeT-\AL)^{-2}(\AL-\AL^{n+1-t})+(\eyeT-\AR\T)^{-2}(\AR\T-(\AR\T)^{n+1-t})\big)(\eyeT-\BT)^{-1}\Sigh\nonumber\\
	&\quad -\frac{1}{(n-t)^2}\big(\eyeT + (\eyeT-\AL)^{-1}\AL + (\eyeT-\AR\T)^{-1}\AR\T\big)(\eyeT-\BT)^{-2}(\BT^{t+1}-\BT^{n+1})\Sigh\nonumber\\
	&\quad +\frac{1}{(n-t)^2}\sum_{j=t+1}^n\big((\eyeT-\AL)^{-1}\AL^{n+1-j}+(\eyeT-\AR\T)^{-1}(\AR\T)^{n+1-j}\big)(\eyeT-\BT)^{-1}\BT^j\Sigh.
	\end{align*}}
\end{lemma}
The covariance of the stationary distribution of the iterates i.e., $\lim_{j\to\infty}\thetav_j^{\textrm{variance}}$ requires a precise bound to obtain statistically optimal error rates. Lemma~\ref{lem:main-variance} presents a bound on this quantity. 
\begin{lemma}[Stationary covariance]\label{lem:main-variance}
The covariance of limiting distribution of $\thetav^{\textrm{variance}}$ satisfies:
%	\small{
	\begin{align*}	\E{\thetav_{\infty}^{\textrm{variance}}\otimes\thetav_{\infty}^{\textrm{variance}}}=\inv{\left(\Id - \B\right)} \Sighat &\preceq 5\sigma^2\bigg((2/3)\cdot \big(\frac{1}{\cnS}\Hinv\big)+ (5/6) \cdot(\delta\eye)\bigg)\otimes\begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix}.
	\end{align*}%}
\end{lemma}
A crucial implication of this lemma is that the limiting final iterate $\thetav_{\infty}^{\textrm{variance}}$ has an excess risk $\mathcal{O}(\sigma^2)$. This result naturally lends itself to the (tail-)averaged iterate achieving the minimax optimal rate of $\mathcal{O}(d\sigma^2/n)$. Refer to the appendix~\ref{sec:varianceContraction} and lemma~\ref{lem:var-main-1} for more details in this regard.\vspace*{-2mm}
\iffalse Consider a decomposition of $\thetat$ as $\thetat = \thetat^{\textrm{bias}} + \thetat^{\textrm{variance}}$, where $\thetat^{\textrm{bias}}$ and $\thetat^{\textrm{variance}}$ are defined as follows:
\begin{align}
	\thetat^{\textrm{bias}} \eqdef \Ahatj \thetat[j-1]^{\textrm{bias}} &;\qquad  \thetat[0]^{\textrm{bias}} \eqdef \thetat[0], \mbox{ and } \label{eqn:decomp-bias}\\
	\thetat^{\textrm{variance}} \eqdef \Ahatj \thetat[j-1]^{\textrm{variance}} +\zetat &;\qquad  \thetat[0]^{\textrm{variance}} \eqdef \zero. \label{eqn:decomp-var}
\end{align}
A straightforward inductive argument is sufficient to see that $\thetat = \thetat^{\textrm{bias}} + \thetat^{\textrm{variance}}$ (for more details, refer to the appendix). We note that $\thetat^{\textrm{bias}}$ is obtained by beginning at $\thetav_0^{\textrm{bias}}=\thetav_0$ and running the algorithm on the noiseless problem (i.e. where $\zetav_j=0$ a.s.). On the other hand, $\thetat^{\textrm{variance}}$ is obtained by beginning at the solution $\thetav^{\textrm{variance}}_0=0$, and allowing the noise $\zetav_j$ to drive the process. 

In a similar manner as $\thetat$, the tail-averaged iterate $\thetavb \eqdef \frac{1}{n-t} \sum_{j=t+1}^n \thetat[j]$ can also be written as $\thetavb = \thetavb^{\textrm{bias}} + \thetavb^{\textrm{variance}}$, where $\thetavb^{\textrm{bias}} \eqdef \frac{1}{n-t} \sum_{j=t+1}^n \thetat[j]^{\textrm{bias}}$ and $\thetavb^{\textrm{variance}} \eqdef \frac{1}{n-t} \sum_{j=t+1}^n \thetat[j]^{\textrm{variance}}$. This leads to the following simple upper bound on the covariance matrix of $\thetavb$ (using Cauchy-Shwartz inequality):
\begin{align}\label{eqn:bound-covar}
		\E{\thetavb \otimes \thetavb} \preceq 2\cdot\bigg( \E{\thetavb^{\text{bias}} \otimes \thetavb^{\text{bias}}} + \E{\thetavb^{\text{variance}} \otimes \thetavb^{\text{variance}}}\bigg).
\end{align}
The above inequality is referred to as the {\em bias-variance} decomposition and is well known from previous work~\cite{BachM13,FrostigGKS15,JainKKNS16}, and we re-derive this decomposition for the sake of completeness.

Since we deal with the square loss, a simple taylor expansion argument (for more details, refer to the appendix) reveals that the generalization error is expressed as:\iffalse
Note that $\E{\thetavb \otimes \thetavb}$ is an important quantity for the purpose of obtaining the generalization error of the proposed accelerated algorithm. In particular, since we deal with the square loss, using a taylor expansion argument around the optimum $\xs$ (for more details, see the appendix), we note that the generalization error is expressed as:\fi
\begin{align*}
\E{P(\bar{\x}_{t,n})}-P(\xs)&=\frac{1}{2}\cdot\iprod{\begin{bmatrix}\Cov&0\\0&0\end{bmatrix}}{\E{\thetavb \otimes \thetavb}}\\
&\leq\iprod{\begin{bmatrix}\Cov&0\\0&0\end{bmatrix}}{\E{\thetavb^{\textrm{bias}} \otimes \thetavb^{\textrm{bias}}}}+\iprod{\begin{bmatrix}\Cov&0\\0&0\end{bmatrix}}{\E{\thetavb^{\textrm{variance}} \otimes \thetavb^{\textrm{variance}}}}
\end{align*}
In order to present the two lemmas that characterize the covariance of the bias and variance error, we require the following notation: let $\A \eqdef \E{\Ahatj}$ and $\B$ be an operator acting on $2d \times 2d$ matrices $\S$ such that $\B \S \eqdef \E{\Ahatj \S \Ahatj\T}$. $\AL$ and $\AR$ denote respectively the left and right multiplication operators of $\A$ i.e., for any $2d\times 2d$ matrix $\S$, $\AL \S \eqdef \A \S$ and $\AR \S \eqdef \S \A$. We are now ready to state the two lemmas.
\begin{lemma}\label{lem:average-covar-bias}
	The covariance matrix of the bias part of averaged iterate $\thetavb^{\textrm{bias}}$ satisfies:
	\begin{align*}
	 \E{\thetavb^{\textrm{bias}} \otimes \thetavb^{\text{bias}}} &= \frac{1}{(n-t)^2} \bigg( \eyeT + (\eyeT-\AL)^{-1}\AL + (\eyeT-\AR\T)^{-1}\AR\T\bigg) (\eyeT-\BT)^{-1}(\BT^{t+1}-\BT^{n+1}) \left(\thetat[0]\otimes \thetat[0]\right)\\	 &\quad -\frac{1}{(n-t)^2}\sum_{j=t+1}^n\bigg( (\eyeT-\AL)^{-1}\AL^{n+1-j} + (\eyeT-\AR\T)^{-1}(\AR\T)^{n+1-j} \bigg)\BT^j \thetat[0] \otimes \thetat[0] .
	\end{align*}
\end{lemma}
The main quantity that needs to be bounded in the term above is $\BT^{t+1}\thetav_0\otimes\thetav_0$. Lemma~\ref{lem:main-bias} presents a bound on this expression. 
\begin{lemma}\label{lem:average-covar-var}
	Let $\Sighat \eqdef \E{\zetat \zetat \T}$. The covariance matrix of the variance part of averaged iterate $\thetavb^{\textrm{variance}}$ satisfies:
	\begin{align*}
	&\E{\thetavb^{\textrm{variance}} \otimes \thetavb^{\text{variance}}} = \frac{1}{n-t}\big(\eyeT + (\eyeT-\AL)^{-1}\AL + (\eyeT-\AR\T)^{-1}\AR\T\big)(\eyeT-\BT)^{-1}\Sigh	\nonumber\\
	&\quad -\frac{1}{(n-t)^2}\big((\eyeT-\AL)^{-2}(\AL-\AL^{n+1-t})+(\eyeT-\AR\T)^{-2}(\AR\T-(\AR\T)^{n+1-t})\big)(\eyeT-\BT)^{-1}\Sigh\nonumber\\
	&\quad -\frac{1}{(n-t)^2}\big(\eyeT + (\eyeT-\AL)^{-1}\AL + (\eyeT-\AR\T)^{-1}\AR\T\big)(\eyeT-\BT)^{-2}(\BT^{t+1}-\BT^{n+1})\Sigh\nonumber\\
	&\quad +\frac{1}{(n-t)^2}\sum_{j=t+1}^n\big((\eyeT-\AL)^{-1}\AL^{n+1-j}+(\eyeT-\AR\T)^{-1}(\AR\T)^{n+1-j}\big)(\eyeT-\BT)^{-1}\BT^j\Sigh.
	\end{align*}
\end{lemma}

The term that needs to be bounded in a sharp manner for obtaining the leading order term of the variance is $\inv{\left(\eyeT-\B\right)}\Sigh$. Lemma~\ref{lem:main-variance} presents a bound on this quantity. 

Recall, the step sizes in Algorithm~\ref{algo:TAASGD} are chosen as $\alpha = \frac{3\sqrt{5}\cdot\sqrt{\cnH\cnS}}{1+3\sqrt{5}\cdot\sqrt{\cnH\cnS}}, \beta = \frac{1}{9\sqrt{\cnH\cnS}}, \gamma =  \frac{1}{3\sqrt{5}\cdot \mu \sqrt{\cnH\cnS}}, \delta = \frac{1}{5R^2}$.
%$\alpha = \frac{\sqrt{\cnH\cnHh}}{\ctwo\sqrt{2\cone-\cone^2}+\sqrt{\cnH\cnHh}},\ \  \beta = \cthree\frac{\ctwo\sqrt{2\cone-\cone^2}}{\sqrt{\cnH\cnHh}},\ \  \gamma = \ctwo\frac{\sqrt{2\cone-\cone^2}}{\mu\sqrt{\cnH\cnHh}}, \ \ \delta=\frac{\cone}{\infbound}$ for some $\cone \in (0,2)$, $\ctwo \in (0,1]$, $\cthree\in(0,1]$.

\noindent \underline{\textbf{Bias contraction}}: Let $\G$ denote the PD matrix $\G \eqdef \begin{bmatrix} \Id &\zero \\ \frac{-\alpha}{1-\alpha}\Id & \frac{1}{1-\alpha}\Id \end{bmatrix} \begin{bmatrix} \Id &\zero \\ \zero & {\mu}\inv{\Cov} \end{bmatrix} \begin{bmatrix} \Id & \frac{-\alpha}{1-\alpha}\Id \\ \zero & \frac{1}{1-\alpha}\Id \end{bmatrix}$. 

%Then, the following lemma quantifies the rate of contraction (measured as inner product with $\G$) obtained by applying operator $\BT$.
We are now ready to present the lemma that provides the accelerated rate of bias contraction.
\begin{lemma}[Bias contraction] \label{lem:main-bias}
	For any two vectors $\x, \y \in \R^d$, let $\thetav \eqdef \begin{bmatrix}
	\x-\xs \\ \y-\xs
	\end{bmatrix} \in \R^{2d}$. We have: 
	\begin{align*}
%	\iprod{\G}{\B \left(\thetav \thetav \T\right)} \leq \exp\left(\frac{-\ctwo \cthree \sqrt{2\cone - \cone^2}}{\sqrt{\cnH\cnS}}\right) \iprod{\G}{\thetav \thetav \T}
	\iprod{\G}{\B \left(\thetav \thetav \T\right)} \leq \exp\left(\frac{-1}{9\sqrt{\cnH\cnS}}\right) \iprod{\G}{\thetav \thetav \T}
	\end{align*}
\end{lemma}
There are two points to note here. \rahul{maybe avoid bullets?}
\begin{itemize}
\item The matrices $\begin{bmatrix} \Id &\zero \\ \frac{-\alpha}{1-\alpha}\Id & \frac{1}{1-\alpha}\Id \end{bmatrix}$ and its transpose appearing in $\G$ are due to the fact that we prove contraction using the variables $\x-\xs$ and $\v-\xs$ (see Algorithm~\ref{algo:TAASGD}) instead of employing $\x-\xs$ and $\y-\xs$, which are employed in defining $\thetav$. 
\item The key novelty in this lemma is that while standard analyses of accelerated gradient descent (in the standard first order oracle model) consider the potential function $\norm{\x-\xs}_{\Cov}^2 + \mu \twonorm{\v - \xs}^2$ (see for example~\cite{WilsonRJ16}), we consider the potential function $\twonorm{\x-\xs}^2 + \mu \norm{\v - \xs}_{\inv{\Cov}}^2$ (this potential function corresponds to the matrix $\begin{bmatrix} \Id &\zero \\ \zero & \frac{1}{\mu}\inv{\Cov} \end{bmatrix}$). Using this potential function is crucial in obtaining a geometric rate of $\order{1/\sqrt{\cnH\cnS}}$ in killing the bias.
\end{itemize}
\noindent \underline{\textbf{Bounding variance}}: The key challenge in bounding the variance is in obtaining bounds on the quantity $\inv{\left(\eyeT-\B\right)}\Sigh$, as appearing in Lemma~\ref{lem:average-covar-var}. This term represents the asymptotic covariance of the parameters $\thetat[j]$ with $j\to\infty$. We first note that $\E{\thetat[j]^{\textrm{variance}}}=\zero$ using induction\rahul{More explanation?}. Considering $\E{\thetat[j]\otimes\thetat[j]}$ using equation~\ref{eqn:decomp-var} and unrolling it over $j$ steps, we obtain: %\rahul{some explanation for below?}\praneeth{Is this good now?}
\begin{align*}
\qquad \quad \E{\thetat[j]^{\textrm{variance}}\otimes \thetat[j]^{\textrm{variance}}} &=\E{\otimes_2 \bigg(\Ahatj\thetat[j-1]+\zetat[j]\bigg)}\\ \qquad \quad &= \BT \E{\thetat[j-1]^{\textrm{variance}}\otimes \thetat[j-1]^{\textrm{variance}}} + \Sigh + \E{\zetat {\thetat[j-1]^{\textrm{variance}}}\T {\Ahatj}\T} + \E{{\thetat[j-1]^{\textrm{variance}}} {\Ahatj} \zetat \T}\\\qquad \quad &= \BT \E{\thetat[j-1]^{\textrm{variance}}\otimes \thetat[j-1]^{\textrm{variance}}} + \Sigh\quad\quad\quad\quad \left(\because \E{\thetat[j-1]^{\textrm{variance}}}=\zero \right) \\&= \sum_{k=0}^{j-1} \BT^k \Sigh= \inv{\left(\eyeT-\B\right)}(\eyeT-\BT^j)\Sigh\\
\implies &\lim_{j\to\infty}\E{\thetat[j]^{\textrm{variance}}\otimes \thetat[j]^{\textrm{variance}}}=(\eyeT-\BT)^{-1}\Sigh
\end{align*}
The following lemma obtains a bound on the covariance of the stationary distribution of $\thetav_{\Bigcdot}$. 
\begin{lemma}[Variance bound]\label{lem:main-variance}
	The covariance of the stationary distribution of the iterates $\E{\thetav_{\infty}\otimes\thetav_{\infty}}$ satisfies the following bound:
	\begin{align*}
	\inv{\left(\Id - \B\right)} \Sighat %&\preceq 5\ \sigma^2 \begin{bmatrix}
%	\U_{11} & \U_{12} \\ \U_{12}\T & \U_{22}
%	\end{bmatrix} \nonumber\\
	&\preceq 5\sigma^2\begin{bmatrix} (2/3)\cdot (\frac{1}{\cnS}\Hinv)+ (5/6) \cdot(\delta\eye) & 0 \\ 0 & (2/3)\cdot (\frac{1}{\cnS}\Hinv)+ (5/6) \cdot(\delta\eye)\end{bmatrix}.
	\end{align*}
\end{lemma}
\fi