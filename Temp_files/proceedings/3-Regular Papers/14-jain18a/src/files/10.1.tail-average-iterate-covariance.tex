We begin by considering the first-order Markovian recursion as defined by equation~\ref{eq:mainRec}:
\begin{align*}
\thetav_j&=\Ah_j\thetav_{j-1}+\zetav_j.
\end{align*}
We refer by $\phiv_j$ the covariance of the $j^{\text{th}}$ iterate, i.e.:
\begin{align}
\label{eq:finalIterateCovariance}
\phiv_j \defeq \E{\thetav_j\otimes\thetav_j}
\end{align}
Consider a decomposition of $\thetat$ as $\thetat = \thetat^{\textrm{bias}} + \thetat^{\textrm{variance}}$, where $\thetat^{\textrm{bias}}$ and $\thetat^{\textrm{variance}}$ are defined as follows:
\begin{align}
	\thetat^{\textrm{bias}} \eqdef \Ahatj \thetat[j-1]^{\textrm{bias}} &;\qquad  \thetat[0]^{\textrm{bias}} \eqdef \thetat[0], \mbox{ and } \label{eq:biasRec}\\
	\thetat^{\textrm{variance}} \eqdef \Ahatj \thetat[j-1]^{\textrm{variance}} +\zetat &;\qquad  \thetat[0]^{\textrm{variance}} \eqdef \zero. \label{eq:varianceRec}
\end{align}
We note that 
\begin{align}
\E{\thetav_j^{\textrm{bias}}}=\A\E{\thetav_{j-1}^{\textrm{bias}}}\label{eq:condExpBias},\\
\E{\thetav_j^{\textrm{variance}}}=\A\E{\thetav_{j-1}^{\textrm{variance}}}\label{eq:condExpVar}.
\end{align}
Note equation~\ref{eq:condExpVar} follows using a conditional expectation argument with the fact that $\E{\zetav_k}=0\ \forall\ k$ owing to first order optimality conditions.

Before we prove the decomposition holds using an inductive argument, let us understand what the bias and variance sub-problem intuitively mean. 

Note that the {\em bias} sub-problem (defined by equation~\ref{eq:biasRec}) refers to running algorithm on the noiseless problem (i.e., where, $\zetav_{\Bigcdot}=0$ a.s.) by starting it at $\thetav_0^{\textrm{bias}}=\thetav_0$. The bias essentially measures the dependence of the generalization error on the excess risk of the initial point $\thetav_0$ and bears similarities to convergence rates studied in the context of offline optimization. 

The {\em variance} sub-problem (defined by equation~\ref{eq:varianceRec}) measures the dependence of the generalization error on the noise introduced during the course of optimization, and this is associated with the statistical aspects of the optimization problem. The variance can be understood as starting the algorithm at the solution ($\thetav_0^{\text{variance}}=0$) and running the optimization driven solely by noise. Note that the variance is associated with sharp statistical lower bounds which dictate its rate of decay as a function of the number of oracle calls $n$.

Now, we will prove that the decomposition $\thetat = \thetat^{\textrm{bias}} + \thetat^{\textrm{variance}}$ captures the recursion expressed in equation~\ref{eq:mainRec} through induction. For the base case $j=1$, we see that 
\begin{align*}
\thetav_1&=\Ah_1\thetav_0+\zetav_1\\
&=\underbrace{\Ah_1\thetav_0^{\textrm{bias}}}_{\because\ \thetav_0^{\textrm{bias}}=\thetav_0}+\underbrace{\Ah_1\thetav_0^{\textrm{variance}}}_{=0 ,\ \because\ \thetav_0^{\textrm{variance}}=0}+\zetav_1\\
&=\thetav_1^{\textrm{bias}}+\thetav_1^{\textrm{variance}}
\end{align*}
Now, for the inductive step, let us assume that the decomposition holds in the $j-1^{st}$ iteration, i.e. we assume $\thetav_{j-1}=\thetav_{j-1}^{\textrm{bias}}+\thetav_{j-1}^{\textrm{variance}}$. We will then prove that this relation holds in the $j^{th}$ iteration. Towards this, we will write the recursion:
\begin{align*}
\thetav_j&=\Ah_j\thetav_{j-1}+\zetav_j\\
&=\Ah_j(\thetav_{j-1}^{\textrm{bias}}+\thetav_{j-1}^{\textrm{variance}})+\zetav_j\quad\text{(using the inductive hypothesis)}\\
&=\Ah_j\thetav_{j-1}^{\textrm{bias}}+\Ah_j\thetav_{j-1}^{\textrm{variance}}+\zetav_j\\
&=\thetav_j^{\textrm{bias}}+\thetav_j^{\textrm{variance}}.
\end{align*}
This proves the decomposition holds through a straight forward inductive argument.
\iffalse
We note that $\thetat^{\textrm{bias}}$ is obtained by beginning at $\thetav_0^{\textrm{bias}}=\thetav_0$ and running the algorithm on the noiseless problem (i.e. where $\zetav_j=0$ a.s.). On the other hand, $\thetat^{\textrm{variance}}$ is obtained by beginning at the solution $\thetav^{\textrm{variance}}_0=0$, and allowing the noise $\zetav_j$ to drive the process. 
\fi

In a similar manner as $\thetat$, the tail-averaged iterate $\thetavb \eqdef \frac{1}{n-t} \sum_{j=t+1}^n \thetat[j]$ can also be written as $\thetavb = \thetavb^{\textrm{bias}} + \thetavb^{\textrm{variance}}$, where $\thetavb^{\textrm{bias}} \eqdef \frac{1}{n-t} \sum_{j=t+1}^n \thetat[j]^{\textrm{bias}}$ and $\thetavb^{\textrm{variance}} \eqdef \frac{1}{n-t} \sum_{j=t+1}^n \thetat[j]^{\textrm{variance}}$. Furthermore, the tail-averaged iterate $\thetavb$ and its bias and variance counterparts $\thetavb^{\text{bias}},\thetavb^{\text{variance}}$ are associated with their corresponding covariance matrices $\phivb_{t,n},\phivb_{t,n}^{\text{bias}},\phivb_{t,n}^{\text{variance}}$ respectively. Note that $\phivb_{t,n}$ can be upper bounded using Cauchy-Shwartz inequality as:
\begin{align}\label{eq:tailAvgCovarBound}
		\E{\thetavb \otimes \thetavb} &\preceq 2\cdot\bigg( \E{\thetavb^{\text{bias}} \otimes \thetavb^{\text{bias}}} + \E{\thetavb^{\text{variance}} \otimes \thetavb^{\text{variance}}}\bigg)\nonumber\\
\implies\phivb_{t,n}&\preceq 2\cdot(\phivb_{t,n}^{\text{bias}}+\phivb_{t,n}^{\text{variance}}).
\end{align}
The above inequality is referred to as the {\em bias-variance} decomposition and is well known from previous work~\cite{BachM13,FrostigGKS15,JainKKNS16}, and we re-derive this decomposition for the sake of completeness.
\iffalse
We begin by considering the first-order Markovian recursion as defined by equation~\ref{eq:mainRec}:
\begin{align*}
\thetav_k&=\Ah_k\thetav_{k-1}+\zetav_k.
\end{align*}
The equation above can be unrolled over $k$ steps to write the iterate $\thetav_k$ in terms of the initial conditions $\thetav_0$ and the noise introduced until the $k^{\text{th}}$ iteration, (i.e. $\zetav_j$, $j=1,2,..,k$), and this yields:
\begin{align*}
\thetav_k&=\big(\prod_{l=k}^1\Ah_l\big)\thetav_0 + \sum_{j=1}^k\big(\prod_{l=k}^{j+1}\Ah_l\big)\zetav_j
\end{align*}
Where, $\prod_{l=k}^{k+1}\Ah_l=\eye$. We refer by $\phiv_k$ the covariance of the $k^{\text{th}}$ iterate, i.e.:
\begin{align}
\label{eq:finalIterateCovariance}
\phiv_k \defeq \E{\thetav_k\otimes\thetav_k}
\end{align}
Now, we consider the tail-averaged iterate $\thetavb$:
\begin{align*}
\thetavb&\defeq\frac{1}{n-t}\sum_{k=t+1}^n\thetav_k\\
&=\frac{1}{n-t}\sum_{k=t+1}^n\bigg[ \big(\prod_{l=k}^1\Ah_l\big)\thetav_0 + \sum_{j=1}^k\big(\prod_{l=k}^{j+1}\Ah_l\big)\zetav_j\bigg]\\
&=\frac{1}{n-t}\bigg( \bigg[\sum_{k=t+1}^n\big(\prod_{l=k}^1\Ah_l\big)\bigg]\thetav_0 + \bigg[\sum_{k=t+1}^n\sum_{j=1}^k\big(\prod_{l=k}^{j+1}\Ah_l\big)\zetav_j\bigg]\bigg)
\end{align*}
Which now allows us to define the covariance of the tail-averaged iterate $\phivb_{t,n}$:
\begin{align}
\label{eq:tailAvgCovarBound}
\phivb_{t,n}&=\E{\thetavb\otimes\thetavb}\nonumber\\
&\preceq\frac{2}{(n-t)^2}\cdot\bigg(\E{ \otimes_2\bigg( \bigg[\sum_{k=t+1}^n\big(\prod_{l=k}^1\Ah_l\big)\bigg]\thetav_0\bigg) + \otimes_2\bigg(\bigg[\sum_{k=t+1}^n\sum_{j=1}^l\big(\prod_{l=k}^{j+1}\Ah_l\big)\zetav_j\bigg]\bigg)} \bigg)\nonumber\\
&\defeq 2\cdot(\phivb_{t,n}^{\text{bias}} + \phivb_{t,n}^{\text{variance}}),
\end{align}
where, $\phivb_{t,n}^{\text{bias}}=\E{\thetavb^{\textrm{bias}}\otimes\thetavb^{\textrm{bias}}}$, $\phivb_{t,n}^{\text{variance}}=\E{\thetavb^{\textrm{variance}}\otimes\thetavb^{\textrm{variance}}}$.
Note that we applied the Cauchy-Shwartz inequality for any pair of vectors $\v_1,\v_2$ so that, $\v_1\otimes\v_2+\v_2\otimes\v_1\preceq\v_1\otimes\v_1+\v_2\otimes\v_2$.  Note that equation~\ref{eq:tailAvgCovarBound} clearly indicates that the generalization error exhibits a {\em bias-variance} decomposition, which is described below.

The {\em bias} refers to the dependence of the generalization error on the starting point $\thetav_0$, and bears similarities to convergence rates studied in the context of offline optimization; this can be understood as running the optimization on the {\em noiseless} problem, i.e., where, $\zetav_{\Bigcdot}=0$ a.s. In particular, this sub-problem is characterized by the following first-order recursion:
\begin{align}
\label{eq:biasRec}
\thetav_k^{\text{bias}} = \Ah_k\thetav_{k-1}^{\text{bias}},\quad \thetav_0^{\text{bias}}=\thetav_0.
\end{align} 
The {\em variance} refers to the dependence of the generalization error on the noise introduced during the course of optimization, and this is associated with the statistical aspects of the optimization problem. The variance can be understood as starting the algorithm at the solution ($\thetav_0^{\text{variance}}=0$) and running the optimization driven solely by noise. Note that the variance is associated with sharp statistical lower bounds which dictate its rate of decay as a function of the number of oracle calls $n$.
\begin{align}
\label{eq:varianceRec}
\thetav_k^{\text{variance}} = \Ah_k\thetav_{k-1}^{\text{variance}} + \zetav_k, \quad \thetav_0^{\text{variance}}=0.
\end{align} 
\iffalse
We begin by considering the first-order Markovian recursion as defined by equation~\ref{eq:mainRec}:
\begin{align*}
\thetav_t&=\Ah_t\thetav_{t-1}+\zetav_t.
\end{align*}
By unrolling the recursion above, we have:
\begin{align*}
\thetav_t&=\prod_{j=t}^{1}\Ah_j\cdot\thetav_0+\sum_{l=1}^t\bigg(\prod_{j=t}^{l+1}\Ah_j\bigg)\zetav_l,
\end{align*}
where $\prod_{j=t}^{t+1}\Ah_j \defeq \eye$. Then, we begin by considering the covariance of the final iterate, i.e. $\phiv_t\defeq\E{\thetav_t\otimes\thetav_t}$, which leads us to the first sub-section, which is necessary in order to obtain an expression for the covariance of the tail-averaged iterate. 
\subsection{Covariance of the final iterate and the bias-variance decomposition}
\begin{align}
\label{eq:lpRec}
\phiv_t&=\E{\thetav_t\otimes\thetav_t}\nonumber\\
&=\E{\bigg(\prod_{j=t}^{1}\Ah_j\cdot\thetav_0+\sum_{l=1}^t\bigg(\prod_{j=t}^{l+1}\Ah_j\bigg)\zetav_l\bigg)\otimes\bigg(\prod_{j=t}^{1}\Ah_j\cdot\thetav_0+\sum_{l=1}^t\bigg(\prod_{j=t}^{l+1}\Ah_j\bigg)\zetav_l\bigg)}\nonumber\\
&\preceq2\cdot\E{\bigg(\prod_{j=t}^{1}\Ah_j\bigg)\cdot\thetav_0\otimes\thetav_0\cdot\bigg(\prod_{j=1}^{t}\Ah_j\T\bigg)}
+2\cdot\E{\bigg(\sum_{l=1}^t\big(\prod_{j=t}^{l+1}\Ah_j\big)\zetav_l\bigg)\otimes\bigg(\sum_{m=1}^t\big(\prod_{j'=t}^{m+1}\Ah_{j'}\big)\zetav_m\bigg)},
\end{align}
where the last step follows from the inequality $\u \otimes \v + \v \otimes \u \preceq \u \otimes \u + \v \otimes \v$. 

Equation~\ref{eq:lpRec} implies the covariance of the iterate $\thetav_t$ is sharply characterized by analyzing two sub-problems: (a) the first term of equation~\ref{eq:lpRec}, corresponding to the noiseless problem, also referred to as the {\em bias} risk, and (b) the second term of equation~\ref{eq:lpRec}, corresponding to the problem where we begin at the solution $\thetav^*$ and allow the noise to drive the random process, also referred to as the {\em variance} risk.

Owing to this decomposition, we will write out the covariance of the excess risk $\phiv_t$ in terms of the bias error covariance $\phiv_t^{\text{bias}}$ and the variance error covariance $\phiv_t^{\text{variance}}$:
\begin{align}
\label{eq:bvdecomp}
\phiv_t &\preceq 2\cdot\phiv_t^{\text{bias}} + 2\cdot\phiv_t^{\text{variance}}
\end{align}
where,
\begin{align}
\label{eq:biasLP}
\phiv_t^{\text{bias}}&\eqdef \E{\bigg(\prod_{j=t}^{1}\Ah_j\bigg)\cdot\thetav_0\otimes\thetav_0\cdot\bigg(\prod_{j=1}^{t}\Ah_j\T\bigg)}\nonumber\\
&=\E{\E{\Ah_t\bigg(\prod_{j=t-1}^{1}\Ah_j\bigg)\cdot\thetav_0\otimes\thetav_0\cdot\bigg(\prod_{j=1}^{t-1}\Ah_j\T\bigg)\Ah_t\T\bigg{\vert}\mathcal{F}_{t-1}}}\nonumber\\
&=\BT\cdot\E{\bigg(\prod_{j=t-1}^{1}\Ah_j\bigg)\cdot\thetav_0\otimes\thetav_0\cdot\bigg(\prod_{j=1}^{t-1}\Ah_j\T\bigg)}\nonumber\\
&=\BT^t\cdot\phiv_0.
\end{align}
Next, for the covariance of the variance error, we have:
\begin{align}
\label{eq:varianceLP}
\phiv_t^{\text{variance}}&\eqdef \E{\bigg(\sum_{l=1}^t\big(\prod_{j=t}^{l+1}\Ah_j\big)\zetav_l\bigg)\otimes\bigg(\sum_{m=1}^t\big(\prod_{j'=t}^{m+1}\Ah_{j'}\big)\zetav_m\bigg)}\nonumber\\
&=\sum_{l=1}^t\E{\big(\prod_{j=t}^{l+1}\Ah_j\big)\zetav_l\otimes\zetav_l\big(\prod_{j'=l+1}^{t}\Ah_{j'}\T\big)} \left(\mbox{cross terms are zero since } \E{\zetav_l\middle \vert \mathcal{F}_{l-1}} = 0\right)\nonumber\\
&=\sum_{l=1}^t\E{\E{\Ah_t\bigg(\prod_{j=t-1}^{l+1}\Ah_j\bigg)\cdot\zetav_l\otimes\zetav_l\cdot\bigg(\prod_{j=l+1}^{t-1}\Ah_j\T\bigg)\Ah_t\T\bigg{\vert}\mathcal{F}_{t-1}}}\nonumber\\
&=\sum_{l=1}^t\BT\cdot\E{\bigg(\prod_{j=t-1}^{l+1}\Ah_j\bigg)\cdot\zetav_l\otimes\zetav_l\cdot\bigg(\prod_{j=l+1}^{t-1}\Ah_j\T\bigg)}\nonumber\\
&=\sum_{l=1}^t\BT^{t-l}\cdot\E{\zetav_l\otimes\zetav_l}=\sum_{l=1}^t\BT^{t-l}\Sigh=(\eyeT-\BT)^{-1}(\eyeT-\BT^{l+1})\Sigh,
\end{align}
where the last step assumes that all the eigenvalues of $\BT$ are smaller than $1$, which follows from Lemma~\ref{lem:main-bias}.
%Where we note that the cross-terms involving noise $\zetav_l\otimes\zetav_k$ with $k\ne l$ turning out to be zero owing to straight forward use of the tower rule of conditional expectations and by using first order optimality conditions. %Furthermore, the conditional expectation arguments are straightforward given that the noise $\zetav_l\otimes\zetav_l$ are hit by updates from iteration $l+1$ to $t$.
\fi
\fi
We will now derive an expression for the covariance of the tail-averaged iterate and apply it to obtain the covariance of the bias ($\phivb_{t,n}^{\text{bias}}$) and variance ($\phivb_{t,n}^{\text{variance}}$) error of the tail-averaged iterate.
\subsection{The tail-averaged iterate and its covariance}
We begin by writing out an expression for the tail-averaged iterate $\thetavb$ as: 
\begin{align*}
\thetavb=\frac{1}{n-t}\sum_{j=t+1}^n\thetav_j
\end{align*}
To get the excess risk of the tail-averaged iterate $\thetavb$, we track its covariance $\phivb_{t,n}$:
\begin{align}
\label{eq:tailAvgCovar}
\phivb_{t,n}&=\E{\thetavb\otimes\thetavb}\nonumber\\
&=\frac{1}{(n-t)^2}\sum_{j,l=t+1}^n\E{\thetav_j\otimes\thetav_l}\nonumber\\
&=\frac{1}{(n-t)^2}\sum_j\left(\sum_{l=t+1}^{j-1}\E{\thetav_j\otimes\thetav_l}+\E{\thetav_j\otimes\thetav_j}+\sum_{l=j+1}^n\E{\thetav_j\otimes\thetav_l}\right)\nonumber\\
&=\frac{1}{(n-t)^2}\sum_j\left(\sum_{l=t+1}^{j-1}\A^{j-l}\E{\thetav_l\otimes\thetav_l}+\E{\thetav_j\otimes\thetav_j}+\sum_{l=j+1}^n\E{\thetav_j\otimes\thetav_j}(\A\T)^{l-j}\right) \left(\mbox{ from~\eqref{eqn:theta-det}}\right) \nonumber\\
&=\frac{1}{(n-t)^2}\bigg(\sum_{l=t+1}^n\sum_{j=l+1}^n\A^{j-l}\E{\thetav_l\otimes\thetav_l} + \sum_{j=t+1}^n\E{\thetav_j\otimes\thetav_j}+\sum_{j=t+1}^n\sum_{l=j+1}^n\E{\thetav_j\otimes\thetav_j}(\A\T)^{l-j}\bigg)\nonumber\\
&=\frac{1}{(n-t)^2}\bigg(\sum_{j=t+1}^n\sum_{l=j+1}^n\A^{l-j}\E{\thetav_j\otimes\thetav_j} + \sum_{j=t+1}^n\E{\thetav_j\otimes\thetav_j}+\sum_{j=t+1}^n\sum_{l=j+1}^n\E{\thetav_j\otimes\thetav_j}(\A\T)^{l-j}\bigg)\nonumber\\
&=\frac{1}{(n-t)^2}\bigg(\sum_{j=t+1}^n(\eye-\A)^{-1}(\A-\A^{n+1-j})\E{\thetav_j\otimes\thetav_j} + \sum_{j=t+1}^n\E{\thetav_j\otimes\thetav_j}\nonumber\\&\quad\quad\quad\quad\quad\quad +\sum_{j=t+1}^n\E{\thetav_j\otimes\thetav_j}(\eye-\A\T)^{-1}(\A\T-(\A\T)^{n+1-j})\bigg)\nonumber\\
&=\frac{1}{(n-t)^2}\sum_{j=t+1}^n\bigg( \eyeT + (\eyeT-\AL)^{-1}(\AL-\AL^{n+1-j}) + (\eyeT-\AR\T)^{-1}(\AR\T-(\AR\T)^{n+1-j}) \bigg)\E{\thetav_j\otimes\thetav_j}\nonumber\\
&=\frac{1}{(n-t)^2}\sum_{j=t+1}^n\bigg( \eyeT + (\eyeT-\AL)^{-1}(\AL-\AL^{n+1-j}) + (\eyeT-\AR\T)^{-1}(\AR\T-(\AR\T)^{n+1-j}) \bigg)\phiv_j.
\end{align}
%We can now use the bias-variance decomposition from equation~\ref{eq:bvdecomp} to express $\phiv_j\preceq2\cdot\phiv_j^{\text{bias}}+2\cdot\phiv_j^{\text{variance}}$. 
\iffalse
\praneeth{The below argument is not correct.}
By applying equation~\ref{eq:bvdecomp} to equation~\ref{eq:tailAvgCovar}, we have a bias-variance decomposition for the error of the tail-averaged iterate $\thetavb$. In particular, we have:
\begin{align}
\label{eq:tailAvgCovarBound}
\phivb_{t,n}\preceq 2\cdot\phivb_{t,n}^{\text{bias}}+2\cdot\phivb_{t,n}^{\text{variance}}
\end{align}
Where, we will obtain $\phivb_{t,n}^{\text{bias}}$ and $\phivb_{t,n}^{\text{variance}}$ (defined below) in the following subsections: %using equation~\ref{eq:biasLP} and equation~\ref{eq:varianceLP}, 
\fi
Note that the above recursion can be applied to obtain the covariance of the tail-averaged iterate for the bias ($\phivb_{t,n}^{\text{bias}}$) and variance ($\phivb_{t,n}^{\text{variance}}$) error, since the conditional expectation arguments employed in obtaining equation~\ref{eq:tailAvgCovar} are satisfied by both the recursion used in tracking the bias error (i.e. equation~\ref{eq:biasRec}) and the variance error (i.e. equation~\ref{eq:varianceRec}). This implies that,
\begin{align}
\label{eq:biasTailAvg1}
\phivb_{t,n}^{\text{bias}}&\defeq\frac{1}{(n-t)^2}\sum_{j=t+1}^n\bigg( \eyeT + (\eyeT-\AL)^{-1}(\AL-\AL^{n+1-j}) + (\eyeT-\AR\T)^{-1}(\AR\T-(\AR\T)^{n+1-j}) \bigg)\phiv_j^{\text{bias}}
\end{align}
\begin{align}
\label{eq:varianceTailAvg1}
\phivb_{t,n}^{\text{variance}}&\defeq\frac{1}{(n-t)^2}\sum_{j=t+1}^n\bigg( \eyeT + (\eyeT-\AL)^{-1}(\AL-\AL^{n+1-j}) + (\eyeT-\AR\T)^{-1}(\AR\T-(\AR\T)^{n+1-j}) \bigg)\phiv_j^{\text{variance}}
\end{align}
\subsection{Covariance of Bias error of the tail-averaged iterate}
\begin{proof}[Proof of Lemma~\ref{lem:average-covar-bias}]
To obtain the covariance of the bias error of the tail-averaged iterate, we first need to obtain $\phiv_j^{\text{bias}}$, which we will by unrolling the recursion of equation~\ref{eq:biasRec}:
\begin{align}
\label{eq:biasLP}
\thetav_k^{\text{bias}}&= \Ah_k\thetav_{k-1}^{\text{bias}}\nonumber\\
\implies \phiv_{k}^{\text{bias}} &=\E{\thetav_k^{\text{bias}}\otimes\thetav_k^{\text{bias}}}\nonumber\\
&=\E{\E{\thetav_k^{\text{bias}}\otimes\thetav_k^{\text{bias}}|\mathcal{F}_{k-1}}}\nonumber\\
&=\E{\E{\Ah_k\thetav_{k-1}^{\text{bias}}\otimes\thetav_{k-1}^{\text{bias}}\Ah_k\T|\mathcal{F}_{k-1}}}\nonumber\\
&=\BT\ \E{\thetav_{k-1}^{\text{bias}}\otimes\thetav_{k-1}^{\text{bias}}}=\BT\ \phiv_{k-1}^{\text{bias}}\nonumber\\
\implies\phiv_{k}^{\text{bias}}&=\BT^k\ \phiv_{0}^{\text{bias}}
\end{align}
Next, we recount the equation for the covariance of the bias of the tail-averaged iterate from equation~\ref{eq:biasTailAvg1}:
\begin{align*}
\phivb_{t,n}^{\text{bias}}&=\frac{1}{(n-t)^2}\sum_{j=t+1}^n\bigg( \eyeT + (\eyeT-\AL)^{-1}(\AL-\AL^{n+1-j}) + (\eyeT-\AR\T)^{-1}(\AR\T-(\AR\T)^{n+1-j}) \bigg)\phiv_j^{\text{bias}}
\end{align*}
Now, we substitute $\phiv_j^{\text{bias}}$ from equation~\ref{eq:biasLP}:
\begin{align}
\label{eq:biasTA}
\phivb_{t,n}^{\text{bias}}&=\frac{1}{(n-t)^2}\sum_{j=t+1}^n\bigg( \eyeT + (\eyeT-\AL)^{-1}(\AL-\AL^{n+1-j}) + (\eyeT-\AR\T)^{-1}(\AR\T-(\AR\T)^{n+1-j}) \bigg)\BT^j\phiv_0\nonumber\\
&=\frac{1}{(n-t)^2}\sum_{j=t+1}^n\bigg( \eyeT + (\eyeT-\AL)^{-1}\AL + (\eyeT-\AR\T)^{-1}\AR\T\bigg)\BT^j\phiv_0\nonumber\\
&\qquad-\frac{1}{(n-t)^2}\sum_{j=t+1}^n\bigg( (\eyeT-\AL)^{-1}\AL^{n+1-j} + (\eyeT-\AR\T)^{-1}(\AR\T)^{n+1-j} \bigg)\BT^j\phiv_0\nonumber\\
&=\underbrace{\frac{1}{(n-t)^2}\bigg( \eyeT + (\eyeT-\AL)^{-1}\AL + (\eyeT-\AR\T)^{-1}\AR\T\bigg)(\eyeT-\BT)^{-1}(\BT^{t+1}-\BT^{n+1})\phiv_0}_{\text{Leading order term}}\nonumber\\
&\qquad-\frac{1}{(n-t)^2}\sum_{j=t+1}^n\bigg( (\eyeT-\AL)^{-1}\AL^{n+1-j} + (\eyeT-\AR\T)^{-1}(\AR\T)^{n+1-j} \bigg)\BT^j\phiv_0.
\end{align}
There are two points to note here: (a) The second line consists of terms that constitute the lower-order terms of the bias. We will bound the summation by taking a supremum over $j$. (b) Note that the burn-in phase consisting of $t$ unaveraged iterations allows for a geometric decay of the bias, followed by the tail-averaged phase that allows for a sublinear rate of bias decay. 
\end{proof}
\subsection{Covariance of Variance error of the tail-averaged iterate}
\begin{proof}[Proof of Lemma~\ref{lem:average-covar-var}]
Before obtaining the covariance of the tail-averaged iterate, we note that  $\E{\thetav_j^{\text{variance}}}=0\ \forall\ j$. This can be easily seen since $\thetav_0^{\textrm{variance}}=0$ and $\E{\thetav_k^{\textrm{variance}}}=\A\E{\thetav_{k-1}^{\textrm{variance}}}$ (from equation~\ref{eq:condExpVar}).

Next, in order to obtain the covariance of the variance of the tail-averaged iterate, we first need to obtain $\phiv_j^{\text{variance}}$, and we will obtain this by unrolling the recursion of equation~\ref{eq:varianceRec}:
\begin{align}
\label{eq:varianceLP}
\thetav_k^{\text{variance}}&= \Ah_k\thetav_{k-1}^{\text{variance}}+\zetav_k\nonumber\\
\implies \phiv_{k}^{\text{variance}} &=\E{\thetav_k^{\text{variance}}\otimes\thetav_k^{\text{variance}}}\nonumber\\
&=\E{\E{\thetav_k^{\text{variance}}\otimes\thetav_k^{\text{variance}}|\mathcal{F}_{k-1}}}\nonumber\\
&=\E{\E{\Ah_k\thetav_{k-1}^{\text{variance}}\otimes\thetav_{k-1}^{\text{variance}}\Ah_k\T+\zetav_k\otimes\zetav_k|\mathcal{F}_{k-1}}}\nonumber\\
&=\BT\ \E{\thetav_{k-1}^{\text{variance}}\otimes\thetav_{k-1}^{\text{variance}}}+\Sigh=\BT\ \phiv_{k-1}^{\text{variance}}+\Sigh\nonumber\\
\implies\phiv_{k}^{\text{variance}}&=\sum_{j=0}^{k-1}\BT^j\ \Sigh\nonumber\\
&=(\eye-\BT)^{-1}(\eyeT-\BT^k)\Sigh
\end{align}
Note that the cross terms in the outer product computations vanish owing to the fact that $\E{\thetav_{k-1}^{\textrm{variance}}}=0\ \forall\ k$. We then recount the expression for the covariance of the variance error from equation~\ref{eq:varianceTailAvg1}:
\begin{align*}
\phivb_{t,n}^{\text{variance}}&=\frac{1}{(n-t)^2}\sum_{j=t+1}^n\bigg( \eyeT + (\eyeT-\AL)^{-1}(\AL-\AL^{n+1-j}) + (\eyeT-\AR\T)^{-1}(\AR\T-(\AR\T)^{n+1-j}) \bigg)\phiv_j^{\text{variance}}
\end{align*}
We will substitute the expression for $\phiv_j^{\text{variance}}$ from equation~\ref{eq:varianceLP}.
{\small
\begin{align}
\phivb_{t,n}^{\text{variance}}&=\frac{1}{(n-t)^2}\sum_{j=t+1}^n\bigg( \eyeT + (\eyeT-\AL)^{-1}(\AL-\AL^{n+1-j}) + (\eyeT-\AR\T)^{-1}(\AR\T-(\AR\T)^{n+1-j}) \bigg)(\eyeT-\BT)^{-1}(\eyeT-\BT^j)\Sigh\nonumber
\end{align}
}
Evaluating this summation, we have:
\begin{align}
\label{eq:varianceTA}
\phivb_{t,n}^{\text{variance}}&=\underbrace{\frac{1}{n-t}\big(\eyeT + (\eyeT-\AL)^{-1}\AL + (\eyeT-\AR\T)^{-1}\AR\T\big)(\eyeT-\BT)^{-1}\Sigh}_{\text{Leading order term}}\nonumber\\&-\frac{1}{(n-t)^2}\big((\eyeT-\AL)^{-2}(\AL-\AL^{n+1-t})+(\eyeT-\AR\T)^{-2}(\AR\T-(\AR\T)^{n+1-t})\big)(\eyeT-\BT)^{-1}\Sigh\nonumber\\&-\frac{1}{(n-t)^2}\big(\eyeT + (\eyeT-\AL)^{-1}\AL + (\eyeT-\AR\T)^{-1}\AR\T\big)(\eyeT-\BT)^{-2}(\BT^{t+1}-\BT^{n+1})\Sigh\nonumber\\&+\frac{1}{(n-t)^2}\sum_{j=t+1}^n\big((\eyeT-\AL)^{-1}\AL^{n+1-j}+(\eyeT-\AR\T)^{-1}(\AR\T)^{n+1-j}\big)(\eyeT-\BT)^{-1}\BT^j\Sigh
\end{align}

\end{proof}

Equations~\ref{eq:tailAvgCovarBound},~\ref{eq:biasTA},~\ref{eq:varianceTA} wrap up the proof of lemmas~\ref{lem:average-covar-bias},~\ref{lem:average-covar-var}.

The parameter error of the (tail-)averaged iterate can be obtained using a trace operator 
$\iprod{{\Bigcdot}}{{\Bigcdot}}$ to the tail-averaged iterate's covariance $\phivb_{t,n}$ with the matrix $\begin{bmatrix}\eye&0\\0&0\end{bmatrix}$, i.e. 
\begin{align*}
\|\bar{\x}_{t,n}-\xs\|_2^2=\iprod{\begin{bmatrix}\eye & 0\\ 0 & 0\end{bmatrix}}{\phivb_{t,n}}
\end{align*}
In order to obtain the function error, we note the following taylor expansion of the function $P(\Bigcdot)$ around the minimizer $\xs$:
\begin{align*}
P(\x)&=P(\xs) + \frac{1}{2}\ \|\x-\xs\|_{\nabla^2P(\xs)}^2\\
&=P(\xs) + \frac{1}{2}\ \|\x-\xs\|_{\Cov}^2
\end{align*}
This implies the excess risk can be obtained as:
\begin{align*}
P(\bar{\x}_{t,n})-P(\xs)&=\frac{1}{2}\cdot\iprod{\begin{bmatrix}\H & 0\\ 0 & 0\end{bmatrix}}{\phivb_{t,n}}\\
&\leq\iprod{\begin{bmatrix}\H & 0\\ 0 & 0\end{bmatrix}}{\phivb^{\text{bias}}_{t,n}}+\iprod{\begin{bmatrix}\H & 0\\ 0 & 0\end{bmatrix}}{\phivb^{\text{variance}}_{t,n}}
\end{align*}
\iffalse
We wrap up this section by noting that the function error or parameter error (respectively) can be obtained by applying the trace operator $\iprod{{\Bigcdot}}{{\Bigcdot}}$ to the tail-averaged iterate's covariance $\phivb_{t,n}$: $\iprod{\begin{bmatrix}\H & 0\\ 0 & 0\end{bmatrix}}{\phivb_{t,n}}$ or $\iprod{\begin{bmatrix}\eye & 0\\ 0 & 0\end{bmatrix}}{\phivb_{t,n}}$. 
\fi

\iffalse
We have brought out the leading order terms in the expansion through adding and subtracting terms in the summation.
\begin{align*}
\phivb_{t,N}&=\frac{1}{(n-t)^2}\bigg((\eye-\A)^{-1}\A\sum_{l=t+1}^n\E{\thetav_l\otimes\thetav_l} + \sum_{l=t+1}^n\E{\thetav_l\otimes\thetav_l^T}+\sum_{j=t+1}^n\E{\thetav_j\otimes\thetav_j}(\eye-\A^T)^{-1}\A^T\nonumber\\
&-\sum_{l=t+1}^n\sum_{j=n+1}^{\infty}\A^{j-l}\E{\thetav_l\otimes\thetav_l} -\sum_{j=t+1}^n\sum_{l=n+1}^{\infty}\E{\thetav_j\otimes\thetav_j}(\A^T)^{l-j}\bigg)\nonumber\\
&=\frac{1}{(n-t)^2}\bigg(\left(\eyeT+(\eyeT-\AL)^{-1}\AL+(\eyeT-\AR^T)^{-1}\AR^T\right)\cdot\sum_{l=t+1}^n\E{\thetav_l\otimes\thetav_l}\bigg)\nonumber\\
&-\frac{1}{(n-t)^2}\bigg(\sum_{l=t+1}^n\sum_{j=n+1}^{\infty}\A^{j-l}\E{\thetav_l\otimes\thetav_l} +\sum_{j=t+1}^n\sum_{l=n+1}^{\infty}\E{\thetav_j\otimes\thetav_j}(\A^T)^{l-j}\bigg)
\end{align*}
\fi
\iffalse
Furthermore, we note that these expressions can be converted to precise statements about the bias-variance decomposition. Namely, similar to~\cite{BachM13,DefossezB15,JainKKNS16}, we split the recursion involving $\E{\thetav_l\otimes\thetav_l}$ into two terms, one consisting of terms dependent on initial conditions $\thetav_0\otimes\thetav_0$ (otherwise referred to as the bias), and other terms that depend on the noise level of the problem (namely the variance). We loose atmost a factor of $2$ in the estimate of the generalization error through this decomposition.

\underline{\bf Bias Part}
For the bias part, we consider the noiseless problem, and that implies that
\begin{align*} \E{\thetav_l\otimes\thetav_l}=\B\E{\thetav_{l-1}\otimes\thetav_{l-1}}=\B^l\thetav_0\otimes\thetav_0
\end{align*}
Correspondingly, the bias covariance is:
\begin{align*}
\phivb^{\text{bias}}&=\frac{1}{(n-t)^2}\bigg(\left(\eyeT+(\eyeT-\AL)^{-1}\AL+(\eyeT-\AR^T)^{-1}\AR^T\right)\cdot\sum_{l=t+1}^n\E{\thetav_l\otimes\thetav_l}\bigg)\nonumber\\
&-\frac{1}{(n-t)^2}\bigg(\sum_{l=t+1}^n\sum_{j=n+1}^{\infty}\A^{j-l}\E{\thetav_l\otimes\thetav_l} +\sum_{j=t+1}^n\sum_{l=n+1}^{\infty}\E{\thetav_j\otimes\thetav_j}(\A^T)^{l-j}\bigg)\nonumber\\
&=\frac{1}{(n-t)^2}\bigg(\left(\eyeT+(\eyeT-\AL)^{-1}\AL+(\eyeT-\AR^T)^{-1}\AR^T\right)\cdot(\eyeT-\BT)^{-1}(\BT^{t+1}-\BT^{n+1})\thetav_0\otimes\thetav_0\bigg)\nonumber\\
&-\frac{1}{(n-t)^2}\bigg( (\eyeT-\AL)^{-1} (\eyeT-\AL^{-1}\BT)^{-1}(\AL^{n-t}\BT^{t+1}-\BT^{n+1})\thetav_0\otimes\thetav_0 \ \nonumber\\ &+ (\eyeT-\AR^T)^{-1}(\eyeT-(\AR^T)^{-1}\BT)^{-1}((\AR^T)^{n-t}\BT^{t+1}-\BT^{n+1})\thetav_0\otimes\thetav_0\bigg)
\end{align*}

\underline{\bf Variance Part}
For the variance part, we consider the problem as started from the solution i.e. $\thetav_0=0$ the noise drives the process. This implies that
\begin{align*} \E{\thetav_l\otimes\thetav_l}&=\B\E{\thetav_{l-1}\otimes\thetav_{l-1}}+\Sigh\nonumber\\
&=(\eyeT-\BT)^{-1}(\eyeT-\B^l)\Sigh
\end{align*}
Correspondingly, the covariance of the variance error is:
\begin{align*}
&\phivb^{\text{variance}}=\\
&\frac{1}{n-t}\cdot\bigg(\eyeT+(\eyeT-\AL)^{-1}\AL+(\eyeT-\AR\T)^{-1}\AR\T\bigg)(\eyeT-\BT)^{-1}\Sigh\nonumber\\
&-\frac{1}{(n-t)^2}\cdot\bigg(\eyeT+(\eyeT-\AL)^{-1}\AL+(\eyeT-\AR\T)^{-1}\AR\T\bigg)(\eyeT-\BT)^{-1}(\BT^{t+1}-\BT^{n+1})\Sigh\nonumber\\
&-\frac{1}{n-t}\cdot\bigg((\eyeT-\AL)^{-2}(\AL-\AL^{n-t})+(\eyeT-\AR\T)^{-2}(\AR\T-(\AR^T)^{n-t})\bigg)(\eyeT-\BT)^{-1}\Sigh\nonumber\\
&+\frac{1}{(n-t)^2}\cdot\bigg( (\eyeT-\AL)^{-1}(\eyeT-\AL^{-1}\BT)^{-1}(\AL^{n-t}\BT^{t+1}-\BT^{n+1}) \nonumber\\&+ (\eyeT-\AR\T)^{-1}(\eyeT-(\AR^T)^{-1}\BT)^{-1}((\AR^T)^{n-t}\BT^{t+1}-\BT^{n+1}) \bigg)(\eyeT-\BT)^{-1}\Sigh
\end{align*}
\fi