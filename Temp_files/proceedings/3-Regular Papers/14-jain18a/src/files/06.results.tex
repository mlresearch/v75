%\section{Results}
\iffalse
We can combine the above result and observations with the ideas of~\cite{JainKKNS16} to design a mini-batching, tail-averaging, accelerated SGD algorithm that is highly parallelizable. The pseudocode of this algorithm is presented in Algorithm~\ref{algo:PASGD}, which uses the mini-batched version of Algorithm~\ref{algo:TAASGD} i.e., Algorithm~\ref{algo:MBTAASGD}. Algorithm~\ref{algo:PASGD} consists of three phases: i) initial phase, where the bias is decreased to the same level as noise, ii) middle phase, where doubling mini-batch sizes are used for parallelizability and to decrease variance, and iii) final phase, where tail-averaging is used to improve the variance further. Combining Theorem~\ref{thm:main} with the ideas of~\cite{JainKKNS16}, we obtain the following theorem.
\begin{theorem}\label{thm:par}
	Given $n$ samples $(\ai[1],\bi[1]),\cdots, (\ai[n],\bi[n])$ sampled from a distribution $\D(\R^{d}\times \R)$, Algorithm~\ref{algo:PASGD} run with initial mini-batch size $m=\max\left(\frac{R^2}{\twonorm{\Cov}},{\cnS}\right)$, inner loops $q = \sqrt{\kappa(\Cov)} \log \sqrt{d \kappa(\Cov)}$ and parameters $\alpha = \frac{4\sqrt{\kappa(\Cov)}}{\sqrt{7}+4\sqrt{\kappa(\Cov)}}, \beta = \frac{\sqrt{7}}{4\sqrt{\kappa(\Cov)}}, \gamma =  \frac{\sqrt{7}}{4 \mu \sqrt{\kappa(\Cov)}}, \delta = \frac{1}{4\twonorm{\Cov}}$ outputs a vector $\xtilde$ satisfying:
	\begin{align*}
	\E{L(\xtilde)} \leq \exp\left(-n/qm\right) L(\xt[0]) + \frac{9 \sigma^2 d}{n}.
	\end{align*}
	Furthermore, the depth of Algorithm~\ref{algo:PASGD} in this case is at most $\order{\sqrt{\kappa(\Cov)}\log \frac{n \kappa(\Cov)L(\xt[0])}{\sigma^2 d}}$.
\end{theorem}
\textbf{Remarks}:
\begin{itemize}
	\item	Note that asymptotically, Algorithm~\ref{algo:PASGD} obtains depth that scales as $\order{\sqrt{\kappa(\Cov)}}$ which is the same as that obtained by offline accelerated algorithms.
	\item	There is a work, depth trade-off in the initial phase of Algorithm~\ref{algo:MBTAASGD}. While running Algorithm~\ref{algo:MBTAASGD} with a mini-batch size of $m = \max\left(\frac{R^2}{\twonorm{\Cov}},{\cnS}\right)$ (as in Theorem~\ref{thm:par}) takes $\order{\sqrt{\kappa(\Cov)} \max\left(\frac{R^2}{\twonorm{\Cov}},{\cnS}\right) \log \frac{L(\xt[0])}{\sigma^2}}$ samples and depth $\order{\sqrt{\kappa(\Cov)} \log \frac{L(\xt[0])}{\sigma^2 }}$ to achieve an error of $\order{\sigma^2}$, using a mini-batch size of $\min\left(\frac{R^2}{\twonorm{\Cov}},{\cnS}\right)$ takes $\order{\sqrt{\cnH\cnS}\log \frac{L(\xt[0])}{\sigma^2}}$ samples and depth $\order{\frac{\sqrt{\cnH\cnS}}{\min\left(\frac{R^2}{\twonorm{\Cov}},{\cnS}\right)}\log \frac{L(\xt[0])}{\sigma^2}}$. Note that the sample complexity of the latter is better than that of the former, while the depth of the former is better than that of the latter.
	\item	The above theorem guarantees an asymptotic error of $\order{\frac{\sigma^2 d}{n}}$ which matches the result of Theorem~\ref{thm:main} as well as existing results for SGD~\cite{DefossezB15,JainKKNS16}.
\end{itemize}
\fi