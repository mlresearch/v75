\label{sec:prob}
%This work considers the stochastic approximation problem in equation~\eqref{eq:objFun}. 
%Given access to a stochastic first order oracle (equation~\eqref{eq:stochFirstOrderOracle}), the goal is
%to design a streaming algorithm that minimizes $P(\x)$ at near to the statistically optimal rate.

We now provide our assumptions and main result, before which, we have some notation. For a vector $\x\in\R^d$ and a positive semi-definite matrix $\S\in\R^{d\times d}$ (i.e. $\S\succeq0$), denote $\|\x\|^2_\S \eqdef  \x\T \S \x$.\vspace*{-2mm}
\subsection{Assumptions and Definitions}
Let $\Cov$ denote the second moment matrix of the input, which is also the hessian $\nabla^2P(\x)$ of~\eqref{eq:objFun}:%\vspace*{-1mm}
\vspace{-0.2cm}
\begin{align*}
\Cov \eqdef \Eover{\distr}{\a\otimes\a} = \nabla^2P(\x).
\end{align*}
%Assume:
\vspace{-.8cm}
\begin{enumerate}[leftmargin=*,label=$\mathbf{(\mathcal A\arabic*)}$]
\item \label{asmp:finiteness} \textbf{Finite second and fourth moment:} The second moment matrix $\H$ and the fourth moment tensor $\M\ (=\Eover{\distr}{\a\otimes\a\otimes\a\otimes\a})$ of the input $\a\sim\D$ exist and are finite.
\item \label{asmp:positiveDefinite}\textbf{Positive Definiteness}: The second moment matrix $\Cov$ is strictly positive definite, i.e. $\Cov\succ0$.
\end{enumerate}
%Assumption~\ref{asmp:finiteness}and~\ref{asmp:positiveDefinite} are common in stochastic approximation for the purpose of proving non-asymptotic rates, see for example~\cite{KushnerY03,BachM11,DefossezB15,FrostigGKS15}.

\noindent We assume~\ref{asmp:finiteness} and~\ref{asmp:positiveDefinite}.~\ref{asmp:positiveDefinite} implies that $P(\x)$
is {\em strongly convex} and admits a unique minimizer
$\xs$. Denote the noise $\epsilon$ in a sample $(\a,b)\sim\D$ as: $\n \eqdef \b - \iprod{\a}{\xs}$. First order optimality conditions of $\xs$ imply $\nabla P(\xs) = \E{\n \cdot \a} = 0$.
Let $\Sig$ denote the covariance of gradient at optimum $\xs$ (or {\em noise covariance matrix}), $\Sig \defeq\Eover{\distr}{\widehat{\nabla}P(\xs)\otimes\widehat{\nabla}P(\xs)}=\Eover{\distr}{\n^2\cdot\a\otimes\a}$.

\noindent We define the \emph{noise level} $\sigma^2$, \emph{condition number} $\cnH$, \emph{statistical condition number} $\cnS$ below.\\
\noindent\textbf{Noise level}: The \emph{noise level} is defined to be the smallest positive number $\sigma^2$ such that $\Sig\preceq \sigma^2\H$.\iffalse\vspace*{-2mm}
\begin{align*}
\Sig\preceq\sigma^2\ \Cov.
\end{align*}\fi
The noise level $\sigma^2$ quantifies the amount of noise in the
stochastic gradient oracle and has been utilized in previous work (e.g., see~\cite{BachM11,BachM13}) in providing non-asymptotic bounds for the stochastic approximation problem. In the \emph{homoscedastic} (additive noise) case, where $\n$ is independent of the input $\a$, this condition is satisfied with equality, i.e. $\Sig = \sigma^2\ \Cov$ with $\sigma^2 = \E{\epsilon^2}$.\\
\noindent\textbf{Condition number}: Let $\mu\defeq\lamminH$. $\mu>0$ by \ref{asmp:positiveDefinite}. Now, let $\infbound$ be the smallest positive number such that $\E{\|\a\|^2\ \a\a\T} \preceq \infbound\ \Cov$. The \emph{condition number} $\cnH$ of the distribution $\D$~\citep{DefossezB15,JainKKNS16} is $
\cnH \defeq {\infbound}/{\mu}$.

\noindent \textbf{Statistical condition number}: The \emph{statistical condition number} $\cnS$ is defined as the smallest positive number such that $\E{\Hinvnorm{\a}^2\a\a\T}\preceq \cnS\ \Cov$. 
%\E{(\a\T\H\inv\a)\a\a\T}\preceq \cnS\ \Cov. 

\noindent\textbf{Remarks on $\cnS$ and $\cnH$}:
Unlike $\cnH$, it is straightforward to see that $\cnS$ is affine invariant 
(i.e. unchanged with linear transformations over $\a$). 
Since $\E{\Hinvnorm{\a}^2\a\a\T}\preceq \frac{1}{\mu}
\E{\twonorm{\a}^2\a\a\T}\preceq \cnH \Cov$, we note $\cnS \leq \cnH$. For the discrete case
(from Section~\ref{sec:exp}), it is straightforward
to see that both $\cnH$ and $\cnS$ are equal to $1/\min_i p_{i}$. In
contrast, for the Gaussian case (from Section~\ref{sec:exp}), $\cnS$
 is $\mathcal{O}(d)$, while $\cnH$ is
 $\mathcal{O}(\textrm{Trace}(\Cov)/\mu)$ which may be arbitrarily large (based on
 choice of the coordinate system).

$\cnS$ governs how many samples $\ai$ require to be drawn from $\D$ so
 that the empirical covariance is spectrally close to $\Cov$,
 i.e. for some constant $c>1$, $\Cov/c \preceq \frac{1}{n}\sum_{i=1}^n \a_i\a_i\T \preceq c \Cov$.
 In comparison to the matrix Bernstein inequality where stronger (yet related) moment
 conditions are assumed in order to obtain high probability results,
 our results hold only in expectation (refer to~\citet{HsuKZ14} for this definition, wherein $\cnS$ is referred to as bounded statistical leverage in theorem $1$ and remark $1$).\vspace*{-2mm}
