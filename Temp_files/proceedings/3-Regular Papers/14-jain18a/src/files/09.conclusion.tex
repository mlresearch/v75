\section{Conclusion}\label{sec:conclusion}
This paper introduces an accelerated stochastic gradient method, which presents the first improvement in achieving minimax rates faster than averaged SGD~\citep{RobbinsM51,PolyakJ92,JainKKNS16}/Streaming SVRG~\citep{FrostigGKS15} for the stochastic approximation problem of least squares regression. To obtain this result, the paper presented the need to rethink what acceleration has to offer when working with a stochastic gradient oracle: the statistical condition number (an affine invariant distributional quantity) is shown to characterize the improvements that acceleration offers in the stochastic first order oracle model. In essence, this paper serves to provide the first provable analysis of the claim that fast gradient methods are stable when dealing with statistical errors, in stark contrast to efforts that date to several decades indicating negative results in various statistical or non-statistical settings. %Furthermore, this result yields the first algorithm that attains the optimal statistical rate faster than algorithms such as averaged SGD~\citep{RobbinsM51,Ruppert88,PolyakJ92,JainKKNS16}/streaming SVRG~\citep{FrostigGKS15} for the least squares regression stochastic approximation problem. 
\iffalse
This paper introduces an accelerated stochastic gradient method for the stochastic approximation problem of least squares regression. To obtain this result, the paper presented the need to rethink what acceleration has to offer when working with a stochastic gradient oracle: these thought experiments indicated a need to consider a quantity that captured more fine grained problem characteristics. The statistical condition number (an affine invariant distributional quantity) is shown to characterize the improvements that acceleration offers in the stochastic first order oracle model.

In essence, this paper presents the first known provable analysis of the claim that fast gradient methods are stable when dealing with statistical errors, in stark contrast to negative results in statistical and non-statistical settings~\citep{Paige71,Proakis74,Polyak87,Greenbaum89,RoyS90,SharmaSB98,dAspremont08,DevolderGN13,DevolderGN14,YuanYS16}. Furthermore, this result yields the first algorithm that attains the optimal statistical rate faster than algorithms such as averaged SGD~\citep{Ruppert88,PolyakJ92,JainKKNS16}/streaming SVRG~\citep{FrostigGKS15} for the least squares regression stochastic approximation problem. 
\fi