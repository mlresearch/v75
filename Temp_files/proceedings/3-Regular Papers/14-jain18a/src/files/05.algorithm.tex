\iffalse
\begin{algorithm}[t]
	\caption{ (Tail-Averaged) \textbf{A}ccelerated \textbf{S}tochastic \textbf{G}radient
          \textbf{D}escent (ASGD)}
	\label{algo:TAASGD}
	\begin{algorithmic}[1]
		\INPUT $n$ oracle calls~\ref{eq:stochFirstOrderOracle}, initial point $\xt[0]=\vt[0]$, Unaveraged (burn-in) phase $t$, Step size parameters $\alpha, \beta, \gamma, \delta$
		\FOR{$j = 1, \cdots n$}
		\STATE $\yt[j-1] \leftarrow \alpha \xt[j-1] + (1-\alpha) \vt[j-1]$
		\STATE $\xt[j] \leftarrow \yt[j-1] - \delta \widehat{\nabla} P(\yt[j-1])$
		\STATE $\zt[j-1] \leftarrow \beta \yt[j-1] + (1-\beta) \vt[j-1]$
		\STATE $\vt[j] \leftarrow \zt[j-1] - \gamma \widehat{\nabla} P(\yt[j-1])$
		\ENDFOR
		\OUTPUT $\bar{\x}_{t,n} \leftarrow \frac{1}{n-t}\sum_{j=t+1}^{n} \xt[j]$
	\end{algorithmic}
\end{algorithm}
\fi
%\dontprintsemicolon
%\LinesNumbered
%\RestyleAlgo{boxruled}
%\restylealgo{boxed}
%\RestyleAlgo{boxruled}
%\LinesNumbered
%\RestyleAlgo{boxed}
%\LinesNumbered
\begin{algorithm}[t]
	\caption{(Tail-Averaged) \textbf{A}ccelerated \textbf{S}tochastic \textbf{G}radient
          \textbf{D}escent (ASGD)}
		\KwIn{$n$ oracle calls to~\ref{eq:stochFirstOrderOracle}, initial iterate $\xt[0]=\vt[0]$, Unaveraged (burn-in) phase $t$, Step sizes $\alpha, \beta, \gamma, \delta$}
		\KwOut{$\bar{\x}_{t,n} \leftarrow \frac{1}{n-t}\sum_{j=t+1}^{n} \xt[j]$}
		\For{$j \gets 1$ \KwTo $n$}{
		$\ \ \yt[j-1] \leftarrow \alpha \xt[j-1] + (1-\alpha) \vt[j-1]$\;\\
		$\xt[j] \leftarrow \yt[j-1] - \delta \widehat{\nabla} P(\yt[j-1])$\;\\
		$\zt[j-1] \leftarrow \beta \yt[j-1] + (1-\beta) \vt[j-1]$\;\\
		$\vt[j] \leftarrow \zt[j-1] - \gamma \widehat{\nabla} P(\yt[j-1])$\;
		}
	\label{algo:TAASGD}
\end{algorithm}
\subsection{Algorithm and Main Theorem}\label{sec:results}%\label{sec:algo}
Algorithm~\ref{algo:TAASGD} presents the pseudo code of the proposed algorithm. ASGD can be viewed as a variant of Nesterov's accelerated gradient method~\citep{Nesterov12}, working with a stochastic gradient oracle (equation~\ref{eq:stochFirstOrderOracle}) and with tail-averaging the final $n-t$ iterates. The main result now follows:%\vspace*{-6mm}
\begin{theorem}\label{thm:main}
Suppose ~\ref{asmp:finiteness} and
~\ref{asmp:positiveDefinite} hold. Set $\alpha =
\frac{3\sqrt{5}\cdot\sqrt{\cnH\cnS}}{1+3\sqrt{5}\cdot\sqrt{\cnH\cnS}},
\beta = \frac{1}{9\sqrt{\cnH\cnS}}, \gamma =  \frac{1}{3\sqrt{5}\cdot
  \mu \sqrt{\cnH\cnS}}, \delta = \frac{1}{5R^2}$. After $n$ calls to
the stochastic first order oracle
(equation~\ref{eq:stochFirstOrderOracle}), ASGD
outputs $\bar{\x}_{t,n}$ satisfying:
% the following excess risk guarantee:
\vspace{-0.3cm}
\iffalse
	\begin{align*}
	&\E{P(\bar{\x}_{t,n})}-P(\xs) \leq \underbrace{\UC\cdot\frac{(\cnH\cnS)^{9/4}d\cnH}{(n-t)^2}\cdot\exp\bigg(\frac{-t}{9\sqrt{\cnH\cnS}}\bigg)\cdot\big(P(\x_0)-P(\xs)\big)}_{\text{Leading order bias error}}+\underbrace{5\frac{\sigma^2d}{n-t}}_{\text{Leading order variance error}} + \nonumber\\&\underbrace{\UC\cdot(\cnH\cnS)^{5/4}d\cnH\cdot\exp\left(\frac{-n }{9\sqrt{\cnH\cnS}}\right) \big(P(\x_0)-P(\xs)\big)}_{\text{Exponentially vanishing lower order bias term}} + \underbrace{\UC\cdot\frac{\sigma^2 d}{(n-t)^2} \sqrt{\cnH\cnS}}_{\text{Lower order variance error term}}+\nonumber\\ &{\small \underbrace{\UC\cdot\bigg(\sigma^2d\cdot(\cnH\cnS)^{7/4}\cdot\exp\bigg(\frac{-n}{9\sqrt{\cnH\cnS}}\bigg)+\frac{\sigma^2d}{n-t}(\cnH\cnS)^{11/4}\exp\bigg({-\frac{(n-t-1)}{30\sqrt{\cnH\cnS}}}\bigg) +\frac{\sigma^2d}{(n-t)^2}\cdot\exp\bigg({-\frac{n}{9\sqrt{\cnH\cnS}}}\bigg)\cdot(\cnH\cnS)^{7/2}\cnS\bigg)}_{\text{Exponentially vanishing lower order variance error terms}}},%\alpha^{(n-t-1)/2}
	\end{align*}
\fi
	\begin{align*}
	&\E{P(\bar{\x}_{t,n})}-P(\xs) \leq \underbrace{\UC\cdot\frac{(\cnH\cnS)^{9/4}d\cnH}{(n-t)^2}\cdot\exp\bigg(\frac{-t}{9\sqrt{\cnH\cnS}}\bigg)\cdot\big(P(\x_0)-P(\xs)\big)}_{\text{Leading order bias error}}+\underbrace{5\frac{\sigma^2d}{n-t}}_{\text{Leading order variance error}} + \nonumber\\&\underbrace{\UC\cdot(\cnH\cnS)^{5/4}d\cnH\cdot\exp\left(\frac{-n }{9\sqrt{\cnH\cnS}}\right) \big(P(\x_0)-P(\xs)\big)}_{\text{Exponentially vanishing lower order bias term}} + \underbrace{\UC\cdot\frac{\sigma^2 d}{(n-t)^2} \sqrt{\cnH\cnS}}_{\text{Lower order variance error term}}+\nonumber\\ &{\small \underbrace{\UC\cdot\exp\bigg({-\frac{n}{9\sqrt{\cnH\cnS}}}\bigg)\cdot\bigg(\sigma^2d\cdot(\cnH\cnS)^{7/4}+\frac{\sigma^2d}{(n-t)^2}\cdot(\cnH\cnS)^{7/2}\cnS\bigg)+C\cdot\frac{\sigma^2d}{n-t}(\cnH\cnS)^{11/4}\exp\bigg({-\frac{(n-t-1)}{30\sqrt{\cnH\cnS}}}\bigg)}_{\text{Exponentially vanishing lower order variance error terms}}},%\alpha^{(n-t-1)/2}
	\end{align*}

	\vspace{-0.3cm}
	\noindent 
	where $\UC$ is a universal constant, $\sigma^2$, $\cnH$ and $\cnS$ are the noise level, condition number and statistical condition number respectively.
\end{theorem}
The following corollary holds if the iterates are tail-averaged over the last $n/2$ samples and $n>\mathcal{O}(\sqrt{\cnH\cnS}\log(d\cnH\cnS))$. The second condition lets us absorb lower order terms into leading order terms. 
\begin{corollary}\label{cor:lowerOrder}
Assume the parameter settings of theorem~\ref{thm:main} and
let $t=\lfloor n/2 \rfloor$ and $n>\UC'\sqrt{\cnH\cnS}\log(d\cnH\cnS)$ (for
an appropriate universal constants $\UC,\UC'$).
We have that with $n$ calls to the stochastic first order oracle,
ASGD outputs a vector $\bar{\x}_{t,n}$
satisfying:\vspace*{-2mm}
% the following excess risk guarantee:
	\begin{align*}
	&\E{P(\bar{\x}_{t,n})}-P(\xs) \leq \UC \cdot\exp\bigg(-\frac{n}{20\sqrt{\cnH\cnS}}\bigg)\cdot\big(P(\x_0)-P(\xs)\big)+11\frac{\sigma^2d}{n}.
	\end{align*}
%	\vspace*{-2mm}where $\UC$ is a universal constant.
\end{corollary}

A few remarks about the result of theorem~\ref{thm:main} are due: (i) ASGD decays the initial error at a geometric rate of $\mathcal{O}(1/\sqrt{\cnH\cnS})$ during the unaveraged phase of $t$ iterations, which presents the first improvement over the $\mathcal{O}\left(1/\cnH\right)$ rate offered by SGD~\citep{RobbinsM51}/averaged SGD~\citep{PolyakJ92,JainKKNS16} for the least squares stochastic approximation problem, (ii) the second term in the error bound indicates that ASGD obtains (up to constants) the minimax rate once $n>\mathcal{O}(\sqrt{\cnH\cnS}\log(d\cnH\cnS))$. Note that this implies that Theorem~\ref{thm:main} provides a sharp non-asymptotic analysis (up to $\log$ factors) of the behavior of Algorithm~\ref{algo:TAASGD}.\iffalse (iii) as long as the number of burn-in iterations $t$ and the tail-averaged iterations $n-t$ exceed $\mathcal{O}\left(\sqrt{\cnH\cnS}\cdot\log(d\cnH\cnS) \cdot \log \frac{P(\xt[0])-P(\xs)}{(\sigma^2 d)/n}\right)$, algorithm~\ref{algo:TAASGD} achieves (up to constants) the statistically optimal estimation rates, i.e.:
\begin{align*}
&\E{P(\bar{\x}_{t,n})}-P(\xs) \leq C\ \frac{\sigma^2d}{n},
\end{align*}
where $C$ is a universal constant. This implies that the result in theorem~\ref{thm:main} presents a sharp non-asymptotic analysis (up to $\log$ factors) of the behavior of ASGD.\fi%, and obtains the {\em first} improvement over the rates offered by averaged SGD~\cite{JainKKNS16} and streaming SVRG~\cite{FrostigGKS15} for the stochastic approximation problem.



\subsection{Discussion and Open Problems}\label{sec:resultDiscussion}

%\sidford{I would maybe put this after the Proof outline, but I do not have a strong preference.}
A challenging problem in this context is in formalizing a finite sample size lower bound in the oracle model considered in this work.  Lower bounds in stochastic oracle models have been considered in the literature (see~\cite{NemirovskyY83,RaginskyR11,AgarwalBRW12}), though it is not evident these oracle models and lower bounds are sharp enough to imply statements in our setting (see section~\ref{sec:related} for a discussion of these oracle models).

Let us now understand theorem~\ref{thm:main} in the broader context of stochastic approximation. Under certain regularity conditions, it is known that~\citep{lehmann1998theory,Vaart00} that the rate described in equation~\ref{eq:ERMVarianceAdditive} for the homoscedastic case holds for a broader set of misspecified models (i.e., heteroscedastic noise case), with an appropriate definition of the noise variance. By defining $\sigma^2_{\textrm{ERM}}\eqdef\E{\norm{\widehat{\nabla}P(\xs)}^2_{\inv{\H}}}$, the rate of the ERM is guaranteed to approach $\sigma^2_{\textrm{ERM}}/n$~\citep{lehmann1998theory,Vaart00} in the limit of large $n$, i.e.:\vspace*{-2mm} 
\begin{align}
\label{eq:ERMVariance}
	\lim_{n\to\infty}\frac{\mathbb{E}_{\ST_n}[P_n(\xhat_n^{\textrm{ERM}})]-P(\xs)}{\sigma^2_{\textrm{ERM}}/n} &= 1,
\end{align}

\vspace{-0.2cm}
\noindent where $\xhat_n^{\textrm{ERM}}$ is the ERM over samples $\ST_n=\{\a_i,b_i\}_{i=1}^n$. Averaged SGD~\citep{JainKKNS16} and streaming SVRG~\citep{FrostigGKS15} are known to achieve these rates for the heteroscedastic case. \iffalse Refer to~\cite{FrostigGKS15} for more details.\fi Neglecting constants, Theorem~\ref{thm:main} is guaranteed to achieve the rate of the ERM for the {\em homoscedastic} case (where $\Sig=\sigma^2\H$) and is tight when the bound $\Sig\preceq\sigma^2\H$ is nearly tight (upto constants). We conjecture ASGD achieves the rate of the ERM in the heteroscedastic case by appealing to a more refined analysis as is the case for averaged SGD (see~\cite{JainKKNS16}). It is also an open question to understand acceleration for smooth stochastic approximation (beyond least squares), in situations where the rate represented by equation~\ref{eq:ERMVariance} holds~\citep{PolyakJ92}.
\vspace{-3mm}
\iffalse
Other questions include simplifying the analysis of the proposed algorithm. It is also an important open question to understand the behavior of acceleration for smooth stochastic approximation problems going beyond least squares regression, where the rate represented by equation~\ref{eq:ERMVariance} holds.\fi