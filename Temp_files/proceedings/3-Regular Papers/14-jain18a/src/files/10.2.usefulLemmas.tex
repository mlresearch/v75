\begin{lemma}\label{lem:com3}
	\begin{align*}
	{\left(\Id - \A\T\right)}^{-1} \begin{bmatrix}
	\Cov & \zero \\ \zero & \zero
	\end{bmatrix} = \frac{1}{\g-c\delta}\begin{bmatrix}-(c\eye-\g\Cov)&0\\(\eye-\delta\Cov)&0\end{bmatrix}
	\end{align*}
\end{lemma}
\begin{proof}
Since we assumed that $\Cov$ is a diagonal matrix (with out loss of generality), we note that $\A$ is a block diagonal matrix after a rearrangement of the co-ordinates (via an eigenvalue decomposition).

In particular, by considering the $j^{\text{th}}$ block (denoted by $\A_j$ corresponding to the $j^{\textrm{th}}$ eigenvalue $\lambda_j$ of $\Cov$), we have:
\begin{align*}
\eye-\A_j\T=\begin{bmatrix} 1 & c \\ -(1-\delta\lambda_j) & -(c-\g\lambda_j)\end{bmatrix}
\end{align*}
Implying that the determinant $\Det{\eye-\A_j\T}=(\g-c\delta)\lambda_j$, using which:
\begin{align}
\label{eq:ATInv}
(\eye-\A_j\T)^{-1}&=\frac{1}{(\g-c\delta)\lambda_j}\begin{bmatrix}-(c-\g\lambda_j)&-c\\1-\delta\lambda_j&1\end{bmatrix}
\end{align}
Thus, 
\begin{align*}
(\eye-\A_j\T)^{-1}\begin{bmatrix}\lambda_j&0\\0&0\end{bmatrix}&=\frac{1}{\g-c\delta}\begin{bmatrix}-(c-\g\lambda_j)&0\\(1-\delta\lambda_j)&0 \end{bmatrix}
\end{align*}
Accumulating the results of each of the blocks and by rearranging the co-ordinates, the result follows.
\end{proof}

\begin{lemma}\label{lem:com1}
	\begin{align*}
			\inv{\left(\Id - \A\T\right)} \begin{bmatrix}
		\Cov & \zero \\ \zero & \zero
		\end{bmatrix} \inv{\left(\Id - \A \right)} = \frac{1}{(\g-c\delta)^2}\bigg(\otimes_2\begin{bmatrix} -(c\eye-\g\Cov)\Cov^{-1/2}\\(\eye-\delta\Cov)\Cov^{-1/2}\end{bmatrix}\bigg)
	\end{align*}
\end{lemma}
\begin{proof}

In a similar manner as in lemma~\ref{lem:com3}, we decompose the computation into each of the eigen-directions and subsequently re-arrange the results. In particular, we note:
\begin{align*}
(\eye-\A_j)^{-1}=\frac{1}{(\g-c\delta)\lambda_j}\begin{bmatrix}-(c-\g\lambda_j)&(1-\delta\lambda_j)\\-c&1\end{bmatrix}
\end{align*}
Multiplying the above with the result of lemma~\ref{lem:com3}, we have:
\begin{align*}
(\eye-\A_j\T)^{-1}\begin{bmatrix}\lambda_j&0\\0&0\end{bmatrix}(\eye-\A_j)^{-1}=\frac{1}{(\g-c\delta)^2}\bigg(\otimes_{2}\begin{bmatrix}-(c-\g\lambda_j)\lambda_j^{-1/2}\\(1-\delta\lambda_j)\lambda_j^{-1/2}\end{bmatrix}\bigg)
\end{align*}
From which the statement of the lemma follows through a simple re-arrangement.

\end{proof}

\begin{lemma}\label{lem:com2}
	\begin{align*}
	{\left(\Id - \A\T\right)}^{-2} \A\T \begin{bmatrix}
	\Cov & \zero \\ \zero & \zero
	\end{bmatrix} = \frac{1}{(\g-c\delta)^2}\begin{bmatrix}\Cov^{-1}(-c(1-c)\eye-c\g\Cov)(\eye-\delta\Cov)&0\\\Cov^{-1}((1-c)\eye-c\delta\Cov)(\eye-\delta\Cov)&0\end{bmatrix}
	\end{align*}
\end{lemma}
\begin{proof}
In a similar argument as in previous two lemmas, we analyze the expression in each eigendirection of $\Cov$ through a rearrangement of the co-ordinates. Utilizing the expression of $\eye-\A_j\T$ from equation~\ref{eq:ATInv}, we get:
\begin{align}
\label{eq:intermediateEqn}
(\eye-\A_j\T)^{-1}\A_j\T\begin{bmatrix}\lambda_j&0\\0&0\end{bmatrix}
=\frac{1}{(\g-c\delta)}\begin{bmatrix}-c(1-\delta\lambda_j)&0\\(1-\delta\lambda_j)&0\end{bmatrix}
\end{align}
thus implying:
\begin{align*}
(\eye-\A_j\T)^{-2}\A_j\T\begin{bmatrix}\lambda_j&0\\0&0\end{bmatrix}
=\frac{(1-\delta\lambda_j)}{(\g-c\delta)^2\lambda_j}\begin{bmatrix}-c(1-c)-c\g\lambda_j&0\\(1-c)-c\delta\lambda_j&0\end{bmatrix}
\end{align*}
Rearranging the co-ordinates, the statement of the lemma follows.
\end{proof}

\begin{lemma}\label{lem:eig-A}
	The matrix $\A$ satisfies the following properties:
	\begin{enumerate}
		\item	Eigenvalues $q$ of $\A$ satisfy $\abs{q} \leq \sqrt{\alpha}$, and
		\item	$ \twonorm{\A^k}\leq 3\sqrt{2} \cdot k \cdot \alpha^{\frac{k-1}{2}} \; \forall \; k \geq 1$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	Since the matrix is block-diagonal with $2\times2$ blocks, after a rearranging the coordinates, we will restrict ourselves to bounding the eigenvalues and eigenvectors of each of these $2\times 2$ blocks. Combining the results for different blocks then proves the lemma. Recall that $\A_j = \begin{bmatrix}
	0 & 1 - \delta \lambda_j \\ -c & 1+c - \g \lambda_j
	\end{bmatrix}$.
	
	\textbf{Part I}: Let us first prove the statement about the eigenvalues of $\A$. There are two scenarios here:
	\begin{enumerate}
		\item \emph{Complex eigenvalues}: In this case, both eigenvalues of $\A_j$ have the same magnitude which is given by $\sqrt{\det(\A_j)} = \sqrt{c(1-\delta \lambda_j)}\leq \sqrt{c} \leq \sqrt{\alpha}$.
		\item	\emph{Real eigenvalues}: Let $q_1$ and $q_2$ be the two real eigenvalues of $\A_j$. We know that $q_1+q_2 = \trace{\A_j} = 1 + c - \g\lambda_j > 0$ and $q_1 \cdot q_2 = \det(\A_j) > 0$. This means that $q_1 > 0$ and $q_2 > 0$.
		Now, consider the matrix $\G_j \eqdef (1-\beta) \eye - \A_j = \begin{bmatrix}
		 (1-\beta)  & - 1 + \delta \lambda_j \\ c & -1+{ (1-\beta) (1-\alpha)} + \g \lambda_j
		\end{bmatrix}$. We see that $( (1-\beta) -q_1)( (1-\beta) -q_2) = \det(\G_j) =  (1-\beta) (1-\alpha)\left( (1-\beta) -1\right) +  (1-\beta)  \left(\g-\alpha \delta\right)\lambda_j = (1-\beta)\left(1-\alpha\right)\left(\gamma \lambda_j - \beta\right) \geq 0$. This means that there are two possibilities: either $q_1, q_2 \geq  (1-\beta) $ or $q_1, q_2 \leq  (1-\beta) $. If the second condition is true, then we are done. If not, if $q_1, q_2 \geq  (1-\beta) $, then $\max_i q_i = \frac{\det(\A_j)}{\min_i q_i} \leq \frac{c(1-\delta \lambda_j)}{ (1-\beta) }\leq \alpha (1-\delta \lambda_j)$. Since $\sqrt{\alpha} \geq \alpha \geq 1-\beta$, this proves the first part of the lemma.
	\end{enumerate}

	\textbf{Part II}: Let $\A_j = \V \Q \V\T$ be the Schur decomposition of $\A_j$ where $\Q = \begin{bmatrix}
	q_1 & q \\ 0 & q_2
	\end{bmatrix}$ is an upper triangular matrix with eigenvalues $q_1$ and $q_2$ of $\A_j$ on the diagonal and $\V$ is a unitary matrix i.e., $\V \V\T = \V\T \V = \Id$. We first observe that $\abs{q} \leq \twonorm{\Q} \stackrel{(\zeta_1)}{=} \twonorm{\A_j} \leq \frob{\A_j} \leq \sqrt{6}$, where $(\zeta_1)$ follows from the fact that $\V$ is a unitary matrix. $\V$ being unitary also implies that $\A_j^k = \V \Q^k \V\T$. On the other hand, a simple proof via induction tells us that
	\begin{align*}
		\Q^k = \begin{bmatrix}
		q_1^k & q \left(\sum_{\ell = 1}^{k-1} q_1^{\ell}q_2^{k-\ell}\right) \\ 0 & q_2^k
		\end{bmatrix}.
	\end{align*}
	So, we have $\twonorm{\A_j^k} = \twonorm{\Q^k} \leq \frob{\Q^k} \leq \sqrt{3}k \abs{q} \max\left(\abs{q_1}^{k-1}, \abs{q_2}^{k-1}\right) \leq 3\sqrt{2} \cdot k \cdot \alpha^{\frac{k-1}{2}}$, where we used $\abs{q} \leq \sqrt{6}$ and $\max\left(\abs{q_1},\abs{q_2}\right) \leq \sqrt{\alpha}$.
\end{proof}
Finally, we state and prove the following lemma which is a relation between left and right multiplication operators.
\begin{lemma}
	\label{lem:lhs-psd-lemma}
	Let $\A$ be any matrix with $\AL=\A\otimes\eye$ and $\AR=\eye\otimes\A$ representing its left and right multiplication operators. Then, the following expression holds:
	\begin{align*}
	\bigg(\eyeT + (\eyeT-\AL)^{-1}\AL + (\eyeT-\AR\T)^{-1}\AR\T\bigg)(\eyeT-\AL\AR\T)^{-1}&=(\eyeT-\AL)^{-1}(\eyeT-\AR\T)^{-1}
	\end{align*}
\end{lemma}
\begin{proof}
	Let us assume that $\A$ can be written in terms of its eigen decomposition as $\A = \V\Lambda\V^{-1}$.
	Then the first claim is that $\eyeT,\AL,\AR$ are diagonalized by the same basis consisting of the eigenvectors of $\A$, i.e. in particular, the matrix of eigenvectors of $\eyeT,\AL,\AR$ can be written as $\V\otimes\V$. In particular, this implies, $\forall\ i,j\in\{1,2,...,d\}\times\{1,2,...,d\}$, we have, applying $\vv_i\otimes\vv_j$ to the LHS, we have:
	\begin{align*}
	&\bigg(\eyeT + (\eyeT-\AL)^{-1}\AL + (\eyeT-\AR\T)^{-1}\AR\T\bigg)(\eyeT-\AL\AR\T)^{-1} \vv_i\otimes\vv_j\\
	&=(1-\lambda_i\lambda_j)^{-1}\bigg(\eyeT + (\eyeT-\AL)^{-1}\AL + (\eyeT-\AR\T)^{-1}\AR\T\bigg)\vv_i\otimes\vv_j\\
	&=(1+\lambda_i(1-\lambda_i)^{-1}+\lambda_j(1-\lambda_j)^{-1})\cdot(1-\lambda_i\lambda_j)^{-1}\vv_i\otimes\vv_j
	\end{align*}
	Applying $\vv_i\otimes\vv_j$ to the RHS, we have:
	\begin{align*}
	&(\eyeT-\AL)^{-1}(\eyeT-\AR\T)^{-1}\vv_i\otimes\vv_j\\
	&=(1-\lambda_i)^{-1}(1-\lambda_j)^{-1}\vv_i\otimes\vv_j
	\end{align*}
	The next claim is that for any scalars (real/complex) $x,y~\ne 1$, the following statement holds implying the statement of the lemma:
	\begin{align*}
	(1+(1-x)^{-1}x+(1-y)^{-1}y)\cdot(1-xy)^{-1}=(1-x)^{-1}(1-y)^{-1}
	\end{align*}
\end{proof}

\begin{lemma}\label{lem:G-bound}
	Recall the matrix $\G$ defined as $\G \eqdef \begin{bmatrix} \Id & \frac{-\alpha}{1-\alpha}\Id \\ \zero & \frac{1}{1-\alpha}\Id \end{bmatrix} \begin{bmatrix} \Id &\zero \\ \zero & {\mu}\inv{\Cov} \end{bmatrix} \begin{bmatrix} \Id &\zero \\ \frac{-\alpha}{1-\alpha}\Id & \frac{1}{1-\alpha}\Id \end{bmatrix}$. The condition number of $\G$, $\kappa(\G)$ satisfies $\kappa(\G)\leq \frac{4\cnH}{\sqrt{1-\alpha^2}}$.
\end{lemma}
\begin{proof}
	Since the above matrix is block-diagonal after a rearrangement of coordinates, it suffices to compute the smallest and largest singular values of each block. Let $\lambda_i$ be the $i^{\textrm{th}}$ eigenvalue of $\Cov$. Let $\C \eqdef \begin{bmatrix} 1 & 0 \\ \frac{-\alpha}{1-\alpha} & \frac{1}{1-\alpha} \end{bmatrix}$ and consider the matrix $\G_i \eqdef \C \begin{bmatrix} 1 & 0 \\ 0 & \frac{\mu}{\lambda_i} \end{bmatrix} \C \T$. The largest eigenvalue of $\G_i$ is at most $\singmax{\C}^2$, while the smallest eigenvalue, $\singmin{\G_i}$ is at least $\frac{\mu}{\lambda_i} \cdot \singmin{\C}^2$. We obtain the following bounds on $\singmin{\C}$ and $\singmax{\C}$.
	\begin{align*}
		\singmax{\C} &\leq \frob{\C} \leq \frac{2}{\sqrt{1-\alpha^2}} \quad (\because \; \alpha \leq 1 ) \\
		\singmin{\C} &\geq \frac{\sqrt{\det\left(\C \C\T\right)}}{\frob{\C}} \geq \frac{1}{2}, \\ &\qquad \left(\because \det\left(\C\C\T\right) = \singmax{\C}^2 \singmin{\C}^2\right)
	\end{align*}
	where we used the computation that $\det\left(\C \C\T\right) = \frac{1}{1-\alpha}$. This means that $\singmin{\G_i} \geq \frac{\mu}{2\lambda_i}$ and $\singmax{\G_i} \leq \frac{2}{\sqrt{1-\alpha^2}}$. Combining all the blocks, we see that the condition number of $\G$ is at most $\frac{4\cnH}{\sqrt{1-\alpha^2}}$, proving the lemma.
\end{proof}

