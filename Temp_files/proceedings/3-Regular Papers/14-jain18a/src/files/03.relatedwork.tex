\label{sec:related}
\paragraph{Non-asymptotic Stochastic Approximation:} Stochastic gradient descent (SGD) and its variants are by far the most
widely studied algorithms for the stochastic approximation problem.
While initial works~\citep{RobbinsM51} considered the final iterate of
SGD, later works~\citep{Ruppert88,PolyakJ92} demonstrated that averaged
SGD obtains statistically optimal estimation rates. Several 
works provide non-asymptotic analyses for averaged
SGD and variants~\citep{BachM11,Bach14,FrostigGKS15} for various
stochastic approximation problems. For stochastic approximation with
least squares regression~\citet{BachM13,DefossezB15,NeedellSW16,FrostigGKS15,JainKKNS16}
provide non-asymptotic analysis of the behavior of SGD and its
variants. \cite{DefossezB15,DieuleveutB15} provide non-asymptotic results which achieve the minimax rate on the variance (where the bias is lower order, not geometric).
\cite{NeedellSW16} achieves a geometric rate on the bias (and where the variance is not minimax). \cite{FrostigGKS15,JainKKNS16} obtain both the minimax rate on the variance and a geometric rate on the bias, as seen in equation~\ref{eq:sgd_rate}. 

\iffalse
 The works in \cite{BachM13,DefossezB15} achieve the minimax
rate on the variance, while \cite{NeedellSW16} achieves a geometric
rate of decay on the bias; \cite{FrostigGKS15,JainKKNS16} obtain both
the minimax rate on the variance and a geometric rate of decay on the
bias, as seen in equation~\ref{eq:sgd_rate}. 
\fi

%\rahul{write about \cite{DevolderGN13,Devolder13} as well.}
\paragraph{Acceleration and Noise Stability:}  
While there have been several attempts at understanding if it is
possible to accelerate SGD , the results have been largely negative.
With regards to acceleration with adversarial (non-statistical) errors in the exact first order oracle model,~\cite{dAspremont08} provide negative results and \citet{DevolderGN13,DevolderGN14} provide
lower bounds showing that fast gradient methods do not improve upon
standard gradient methods. There is also a series of works considering
statistical errors.~\cite{Polyak87} suggests that the relative
merits of heavy ball (HB) method~\citep{Polyak64} in the noiseless case
vanish with noise unless strong assumptions on the
noise model are considered; an instance of this is when the
noise variance decays as the iterates approach the minimizer. The
Conjugate Gradient (CG) method~\citep{HestenesS52} is suggested to face
similar robustness issues in the face of statistical
errors~\citep{Polyak87}; this is in addition to the issues that CG is
known to suffer from owing to roundoff errors (due to finite precision
arithmetic)~\citep{Paige71,Greenbaum89}. In the signal processing
literature, where SGD goes by Least Mean Squares
(LMS)~\citep{WidrowS85}, there have been efforts that date to several
decades~\citep{Proakis74,RoyS90,SharmaSB98} which study accelerated LMS
methods (stochastic variants of CG/HB) in the
same oracle model as the one considered by this paper
(equation~\ref{eq:stochFirstOrderOracle}). These efforts consider the
final iterate (i.e. no iterate averaging) of accelerated LMS
methods with a fixed step-size and conclude that while it allows for a
faster decay of the initial error (bias) (which is unquantified), their steady state behavior (i.e. variance) is worse
compared to that of LMS. \citet{YuanYS16} considered a
constant step size accelerated scheme with no iterate averaging in the same oracle model as this paper, and conclude that these do not offer any improvement over standard SGD. More concretely, \citet{YuanYS16} show that the
variance of their accelerated SGD method with a sufficiently small constant step size is the same as that of SGD with a significantly larger step size. Note that none of the these efforts~\citep{Proakis74,RoyS90,SharmaSB98,YuanYS16} achieve minimax
error rates or quantify (any improvement whatsoever on the) rate of bias decay.

\paragraph{Oracle models and optimality:}With regards to notions of optimality, there are (at least) two lines
of thought: one is a statistical objective where the goal is (on every
problem instance) to match the rate of the statistically optimal
estimator~
\citep{anbar1971optimal,Fabian:1973:AES,KushnerClark,PolyakJ92};
another is on obtaining algorithms whose worst case upper bounds
(under various assumptions such as bounded noise) match the lower
bounds provided in ~\cite{NemirovskyY83}.  The work of~\cite{PolyakJ92} are in the former model, where
they show that the distribution of the averaged SGD
estimator matches, on \emph{every} problem, that of the statistically
optimal estimator, in the limit (under appropriate regularization
conditions standard in the statistics literature, where the optimal
estimator is often referred to as the maximum likelihood estimator/the
empirical risk minimizer/an
$M$-estimator~\citep{lehmann1998theory,Vaart00}). Along these lines,
non-asymptotic rates towards statistically optimal estimators are
given
by~\cite{BachM13,Bach14,DefossezB15,DieuleveutB15,NeedellSW16,FrostigGKS15,JainKKNS16}. This work can be seen as improving this non-asymptotic rate (to the
statistically optimal estimation rate) using an accelerated method. As to the latter (i.e. matching the worst-case lower bounds in
~\cite{NemirovskyY83}), there are a number of 
positive results on using accelerated stochastic
optimization procedures; the works
of~\cite{Lan08,HuKP09,ghadimi2012optimal,ghadimi2013optimal,DieuleveutFB16}
match the lower bounds provided in~\cite{NemirovskyY83}. We
compare these assumptions and works in more detail.

In stochastic first order oracle models (see ~\cite{KushnerClark,KushnerY03}), one typically has access to sampled gradients of the form:%\vspace*{-1mm}
\vspace{-0.2cm}
\begin{align}
\widehat{\nabla}P(\x)  = \nabla P(\x) + \etav, \label{eq:stochFirstOrderOracle-diff}
\end{align}

\vspace{-0.2cm}
\noindent where varying assumptions are made on the noise $\etav$. The
worst-case lower bounds in~\cite{NemirovskyY83} are based
on that $\etav$ is bounded; the accelerated methods in~\citet{Lan08,HuKP09,ghadimi2012optimal,ghadimi2013optimal,DieuleveutFB16}
which match these lower bounds in various cases, all assume either
bounded noise or, at least $\E{\|\etav\|^2}$ is finite. In
the least squares setting (such as the one often considered in
practice and also considered in~\citet{PolyakJ92,BachM13,DefossezB15,DieuleveutB15,FrostigGKS15,JainKKNS16}),
this assumption does not hold, since $\E{\|\etav\|^2}$ is
not bounded. To see this, $\etav$ in our oracle model (equation~\ref{eq:stochFirstOrderOracle}) is:
%in our oracle model (equation~\ref{eq:stochFirstOrderOracle}), $\etav$ can be written as:
\vspace{-0.2cm}
\begin{align}
  \label{eq:noiseModel2}
\etav=\widehat{\nabla}P(\x)-\nabla P(\x)=(\a\a\T-\Cov)(\x-\xs)-\epsilon\cdot\a
\end{align}

\vspace{-0.2cm}
\noindent which implies that $\E{\|\etav\|^2}$ is not uniformly bounded (unless
additional assumptions are enforced to ensure that the algorithm's
iterates $\x$ lie within a compact set).  Hence, the assumptions made
in~\cite{HuKP09,ghadimi2012optimal,ghadimi2013optimal,DieuleveutFB16}
do not permit one to obtain finite $n$-sample bounds on the excess
risk.  Suppose we consider the case of $\epsilon=0$, i.e. where the
additive noise is zero and $\b=\a \T \xs$.  For this case, this
paper provides a geometric rate of convergence to the minimizer $\xs$,
while the results of~\cite{ghadimi2012optimal,ghadimi2013optimal,DieuleveutFB16}
at best indicate a $\mathcal{O}(1/n)$ rate. Finally, in contrast to all other existing work, our result is the first to provide finer distribution dependent characteristics of the improvements offered by accelerating SGD (e.g.  refer to the Gaussian and discrete examples in section~\ref{sec:exp}).

\paragraph{Acceleration and Finite Sums:} As a final remark, there have been
results~\citep{ShwartzZ14,FrostigGKS15b,LinMH15,LanZ15,Zhu16} that provide accelerated
rates for {\em offline} stochastic optimization which deal with
minimizing sums of convex functions; these results are almost
tight due to matching lower bounds~\citep{LanZ15,WoodworthS16}. These
results do not immediately translate into rates on the generalization
error.
%In order to convert these statements to generalization error bounds, these results require going through concentration of the ERM as well as a generalization error analysis. 
Furthermore, these algorithms are not streaming, as they require making multiple passes over a
dataset stored in memory. Refer to~\citet{FrostigGKS15} for more details.\vspace*{-2mm}

