While there have been attempts to provide accelerated rates in the stochastic approximation setting, there have been multiple reasons that have impeded progress with regard to this goal:
\begin{itemize}
\item Despite ample practical evidence regarding the effectiveness of momentum (for example~\cite{SutskeverMDH13}), based techniques in stochastic optimization, there have been several negative results~\cite{DevolderGN14} that highlight the notably fragile nature of acceleration when dealing with noisy optimization.
\item While classical averaged SGD has known to provide optimal statistical error rates for the linear least squares regression problem, it has been unclear about how ``fast'' the dependence on the initial error decay could be made in the presence of a stochastic oracle while retaining the statistically optimal rates of classic averaged SGD.
\end{itemize}
We address both these issues by appealing to intuition stemming from results on offline stochastic optimization and matrix concentration, and more specifically the matrix Bernstein inequality and its application in the analysis of random design regression~\cite{HsuKZ14}, that provides a baseline for how ``fast'' an online optimization method can compete in the presence of a stochastic first order oracle.