% !TeX root = main.tex
\section{Weak Correction Model}
\label{sec:wcorr}

  We show how starting from a certification scheme, we can obtain a weak-correction scheme with the same verification complexity
(up to constants).

\begin{theorem}\label{thm:reduction}
    Suppose that there exists a certification scheme for a function $f$ that uses $q(n,\eps)$ verifications and fails with probability $1/3$. Then,
  there exists a weak-correction scheme with verification complexity $O(q(n,\eps) \log(1/\delta))$ that outputs an accurate estimate of the function
  $f$ and fails with  probability $\delta$.
\end{theorem}


%\textcolor{red}{Initially, we are going to assume (for simplicity) that the function $f$ that we want to approximate is increasing. That is, if an incorrect input is removed the approximation of the answer can only get better. However, in the next section we show that this assumption is not necessary. }

  Theorem \ref{thm:reduction} shows that the certification task we defined in section \ref{sec:certification} is already strong enough to perform this
seemingly more challenging task. Intuitively, this is because we can run many rounds of certification until we have enough confidence that we have an
accurate result while we remove from the dataset any invalid record we might find during these rounds. Indeed, a simple way to make the conversion
is to start from a certification scheme with error probability 1/3 and reduce its probability of error to $\delta/n$, by repeating it $\log(n/\delta)$
times. Then use this scheme repeatedly until no more invalid records are detected. By a union bound, the probability of error is at most $\delta$ since
the process takes at most $n$ steps.
  Theorem \ref{thm:reduction} shows a stronger result than the above result showing that the logarithmic dependence on the number of records can be avoided
if the stopping time is more carefully chosen. The full proof of Theorem~\ref{thm:reduction} is presented in Appendix~\ref{app:weak}.
