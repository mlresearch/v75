\section{Applications of Theorem \ref{thm:wcont}}

\input{tsp.tex}

\input{steiner.tex}

\input{wcorrection_appendix.tex}

\section{Missing Proof of Section \ref{sec:scorr}} \label{sec:app:scorr}

\begin{proof}[Proof of Lemma \ref{lem:sumC}]
  We let the random variable $M$ to be the total number of verifications until we found $k$ valid records and let $\mathcal{M}$ be the set of samples that we observed. Also we define 
$Z = \abs{\mathcal{M} \cap \mathcal{T}}/M = k/M$. We claim that
\[ \Prob\left(\frac{\sum_{i \in \Truth} x_i}{\sum_{i \in \Workers} x_i} \in \left[ 1 - \eps, \frac{1}{1 - \eps} \right] \cdot Z \right) \ge 1 - \delta \]

\noindent given that $\abs{\mathcal{M} \cap \mathcal{T}} \ge k$.

  Let $q = \frac{\sum_{i \in \Truth} x_i}{\sum_{i \in \Workers} x_i}$. For tha sake of contradiction let $Z > \frac{1}{1 - \eps} \frac{\sum_{i \in \Truth} x_i}{\sum_{i \in \Workers} x_i}$ then 
$M < (1 - \eps) k / q$. Hence the expected number of valid records if we draw $M$ samples according to the described distribution is at most $(1 - \eps) k$. But know using simple Chernoff 
bounds and the fact that $k \ge \frac{1}{\eps^2}\log (2/\delta)$ we get that with probability at most $\delta / 2$ the number of valid records found is at least $k$. 

  Similarly we can show that if $Z < (1 - \eps) \frac{\sum_{i \in \Truth} x_i}{\sum_{i \in \Workers} x_i}$ then with probability at most $\delta / 2$ the number of valid records found is at most $k$. Hence we have
\begin{align*}
  \Prob(\abs{\mathcal{M} \cap \mathcal{T}} = k) & = \Prob\left(\abs{\mathcal{M} \cap \mathcal{T}} = k \mid q \in \left[ 1 - \eps, \frac{1}{1 - \eps} \right] \cdot Z \right) \Prob\left( q \in \left[ 1 - \eps, \frac{1}{1 - \eps} \right] \cdot Z \right) \\
               & + \Prob\left(\abs{\mathcal{M} \cap \mathcal{T}} = k \mid q < (1 - \eps) Z \right) \Prob\left( q < (1 - \eps) Z \right) \\
               & + \Prob\left(\abs{\mathcal{M} \cap \mathcal{T}} = k \mid q > \frac{1}{1 - \eps} Z \right) \Prob\left( q > \frac{1}{1 - \eps} Z \right)
\end{align*}
\noindent but from the definition of the strong correction scheme $\Prob(\abs{\mathcal{M} \cap \mathcal{T}} = k) = 1$ and as we proved 
\[ \Prob\left(\abs{\mathcal{M} \cap \mathcal{T}} = k \mid q < (1 - \eps) Z \right) \le \delta / 2 ~~ \text{ and } \]
\[ \Prob\left(\abs{\mathcal{M} \cap \mathcal{T}} = k \mid q > \frac{1}{1 - \eps} Z \right) \le \delta / 2 \]
\noindent therefore
\begin{align*}
  \Prob(\abs{\mathcal{M} \cap \mathcal{T}} = k) = 1 & \le \Prob\left(\abs{\mathcal{M} \cap \mathcal{T}} = k \mid q \in \left[ 1 - \eps, \frac{1}{1 - \eps} \right] \cdot Z \right) \Prob\left( q \in \left[ 1 - \eps, \frac{1}{1 - \eps} \right] \cdot Z \right) + \delta \\
\end{align*}
\noindent which implies
\[ \Prob\left( q \in \left[ 1 - \eps, \frac{1}{1 - \eps} \right] \cdot Z \right) \ge 1 - \delta. \]
  This finally implies that our estimator is in the correct range
\[ \Prob\left( Z \cdot \sum_{i \in \Workers} x_i \in \left[ 1 - \eps, \frac{1}{1 - \eps} \right] \cdot \sum_{i \in \Truth} x_i \right) \ge 1 - \delta. \]

  To see that $\Theta\left(\frac{1}{\eps^2}\log (1/\delta)\right)$ are also necessary let $x_1 = \cdots = x_n = 1 / n$ and let 
$\abs{\Truth} = \abs{\Workers} q$ where $q = 1/2$. This instance is identical with estimating the
bias of a Bernoulli random variable with error at most $\eps$ and since all the $x_i$'s are equal we can assume without loss 
of generality that at each step we take a uniform sample from $\Workers$. But it is well known that for
estimating a Bernoulli random variable within $\eps$ with probability of failure at most $\delta$ we need at least 
$\Theta\left(\frac{1}{\eps^2}\log (1/\delta)\right)$ total samples. Half of those samples are expected to be correct samples
and hence the verification complexity for any strong correction scheme is also at least 
$\Theta\left(\frac{1}{\eps^2}\log (1/\delta)\right)$.
\end{proof}


\begin{proof}[Proof of Lemma \ref{lem:maxOfSumC}]
  We consider a partition $\mathcal{J}$ of $\Workers$ into $n/c^2$ sets of size $c^2$ each with $x_i = 1$ for all $i \in \Workers$. Let $S$ be a certification scheme that verifies less than $n / 4 c^2$ records. Then there 
exists a set $A \in \mathcal{J}$ such that
\[ \Prob( S \text{ verifies some } j \in A ) < 1/4. \]
\noindent We prove this by contradiction. Let $\Prob( C \text{ verifies some } j \in A ) \ge 1/4$ for all $A \in \mathcal{J}$. Then 
$\Exp[\text{verification by } C] \ge \sum_{A \in \mathcal{J}} \Prob( C \text{ verifies some } j \in A ) \ge n / 4 c^2$ and hence we have a contradiction on the assumption that $S$ verifies less than $n / 4 c^2$ records. Let $\hat{s}$
be the output estimator of $S$ then we have that
\begin{align*}
  \Prob\left(\hat{s} \in \left[\frac{1}{c}, c\right] \cdot f(\xt)\right) & = \Prob\left(\hat{s} \in \left[\frac{1}{c}, c\right] \cdot f(\xt) \mid  S \text{ verifies some } j \in A\right) \Prob\left( S \text{ verifies some } j \in A\right) \\
                                                                         & + \Prob\left(\hat{s} \in \left[\frac{1}{c}, c\right] \cdot f(\xt) \mid  S \text{ does not verify } A\right) \Prob\left( S \text{ does not verify } A\right) \\
                                                                         & < 1/4 + \Prob\left(\hat{s} \in \left[\frac{1}{c}, c\right] \cdot f(\xt) \mid  S \text{ does not verify } A\right)
\end{align*}
\noindent Now if we fix $Q \subseteq \nats$, we observe that the quantity $\Prob\left(\hat{s} \in Q \mid  S \text{ does not verify } A\right)$ does not depend on $\Truth \cap A$ since we are conditioning on the event that $S$
does not verify any record in $A$. Now let $j_B$ be an arbitrary record from the set $B \in \mathcal{J}$. We consider the following two possibilities for the set $\Truth$.
\[ \Truth_0 = \bigcup_{B \in \mathcal{J}, B \neq A} \{j_B\} \]
\[ \Truth_1 = \Truth_0 \cup A \]
\noindent We observe now that if $\Truth = \Truth_0$ then $f(\xt) = 1$ and if $\Truth = \Truth_1$ then $f(\xt) = c^2$. Now since $\hat{s}$ does not depend on $\Truth \cap A$ given that $S \text{ does not verify } A$ we have that we 
can change $\Truth$ between $\Truth_0$ and $\Truth_1$ without changing the quantity $\Prob\left(\hat{s} \in Q \mid  S \text{ does not verify } A\right)$. Now 
\begin{itemize}
  \item[-] if $\Prob\left(\hat{s} \in [1, c] \mid  S \text{ does not verify } A\right) < 1/2$ then we set $\Truth = \Truth_1$ and 
  \item[-] if $\Prob\left(\hat{s} \in [c(c - 1), c^2] \mid  S \text{ does not verify } A\right) < 1/2$ then we set $\Truth = \Truth_1$. 
\end{itemize}

\noindent Observe that one of the two cases has to be true. In any of these we get that 
\[ \Prob\left(\hat{s} \in \left[\frac{1}{c}, c\right] \cdot f(\xt) \mid  S \text{ does not verify } A\right) < 1/2. \]

\noindent Hence we get that  
\[ \Prob\left(\hat{s} \in \left[\frac{1}{c}, c\right] \cdot f(\xt)\right) < 1/4 + \Prob\left(\hat{s} \in \left[\frac{1}{c}, c\right] \cdot f(\xt) \mid  S \text{ does not verify } A\right) < 3/4 \]
\noindent and therefore $S$ has to verify at least $n / 4 c^2$ records.
\end{proof}

\section{Proof of Theorem \ref{thm:wcont}} \label{sec:app:proofOfLipschitz}

 \begin{proof}
      We set $p_i = \frac{2w_i}{3 f(\xw) \eps}$ and we show that %if we choose to verify worker $i$ with probability $p_i$ then we 
    those values satisfy the LP \eqref{eq:neccLP1}. Thus, if we choose to verify record $i$ with probability $\min\{2p_i, 1\}$, we 
    get a valid $(\varepsilon,\delta)$-certification scheme.

    For any subset $S \subseteq \Workers$ by the  $\vec{w}$-Lipschitz property we get that
    \[ |f(\xw) - f(\vec{x}_{\Workers\setminus S})| \le \sum_{i \in S} w_i \Leftrightarrow \left| \frac{2}{3 \eps} - \frac{2f(\vec{x}_{\Workers\setminus S})}{3 \eps f(\xw)} \right| \le \sum_{i \in S} \frac{2w_i}{3 f(\xw) \eps}. \]

    \noindent Now if $\frac{f(\xw)}{f(\vec{x}_{\Workers\setminus S})} > \frac{1}{1 - \eps}$, we have
    $ \left| \frac{2}{3 \eps} - \frac{2f(\vec{x}_{\Workers\setminus S})}{3 \eps f(\xw)} \right| > \frac 2 3. $

    \noindent Also if $\frac{f(\xw)}{f(\vec{x}_{\Workers\setminus S})} < 1 - \eps$, we have
    $ \left| \frac{2}{3 \eps} - \frac{2f(\vec{x}_{\Workers\setminus S})}{3 \eps f(\xw)} \right| >
    \frac{2}{3 \eps (1-\eps)} - \frac{2}{3 \eps}  \ge \frac 2 3. $
    
    \noindent Therefore when $\frac{f(\xw)}{f(\vec{x}_{\Workers\setminus S})} \notin \left[1 - \eps, \frac{1}{1 - \eps}\right]$, we have
    $\sum_{i \in S} p_i = \sum_{i \in S} \frac{2w_i}{3 f(\xw) \eps} \ge \left| \frac{2}{3 \eps} - \frac{2f(\vec{x}_{\Workers\setminus S})}{3 \eps f(\xw)} \right| \ge \frac{2}{3}$.

    \noindent This means that LP \eqref{eq:neccLP1} is satisfied. % with $\delta = 1/2$.
    Now we can apply Theorem \ref{thm:optInstV} and we conclude that the certification scheme that verifies each record independently 
    with probability $\min\{2p_i, 1\}$, where $2p_i = \frac{4w_i}{3 f(\xw) \eps}$, verifies at most 
    $\frac{4\sum_{i \in \Workers} w_i}{3 f(\xw) \eps}$ records and has probability of success at least $2/3$. In order to get 
    probability of success $\delta$ we instead verify each record $i$ with probability $2 p_i \log(1/\delta)$ and the theorem follows.
  \end{proof}

\section{Complete Statement and applications of Theorem \ref{thm:sCorrection}} \label{sec:app:applicationsStrongCorrection}

  More precisely we are given an input $\xw = (x_1, x_2 , \cdots , x_n)$ of length $n$, where every $x_i$ belongs in some set 
$\Domain$. In this section, we will fix $\Domain = [\mathcal{D}]^d$ for some $\mathcal{D} = n^{O(1)}$ to be the discretized 
$d$-dimensional Euclidean space. Our goal is to compute the value of a symmetric function 
$f : \Domain^n \rightarrow \mathbb{R}_+$ with input $\vec x \in \Domain^n$. We assume that all $x_i$ are distinct and define 
$\Support \subseteq \Domain$ as the set $\Support = \{x_i : i \in \Workers\}$. Since we consider symmetric functions $f$, it 
is convenient to extend the definition of $f$ to sets $f(\Support) = f(x)$.

  The \emph{conditional sampling model} allows such queries of small description complexity to be performed. In particular, 
the algorithm is given access to an oracle $\Cond(C)$ that takes as input a function $C: \Domain \rightarrow \{0,1\}$ and 
returns a tuple $(i, x_i)$ with $C(x_i) = 1$ with $i$ chosen uniformly at random from the subset 
$\{ j \in [n] \mid C(x_j) = 1 \}$. If no such tuple exists the oracle returns $\bot$.

  The main result of this section is a reduction from any algorithm that uses conditional sampling to a strong correction
scheme.

\begin{theorem}
    An algorithm that uses $k$ conditional samples to compute a function $f$ can produce a strong correction scheme with 
  verification cost $k$.
\end{theorem}

\begin{proof}
    We will show how we can implement one conditional sample using only one verification. We take all the values of the
  records $x_1, \dots, x_n$ and we randomly shuffle them to get $x_{\pi_1}, \dots, x_{\pi_n}$. Then we take one by one 
  the records $x_{\pi_i}$ with this new order and we check if $C(x_{\pi_i}) = 1$. If yes then we verify $x_{\pi_i}$ and
  if it is valid we return it as the result of the conditional sampling oracle. If $\pi_i$ is invalid then we just ignore
  this records without any cost and we proceed with the next record. If we finish the records and we found no valid record 
  $x_{\pi_j}$ such that $C(x_{\pi_j}) = 1$, then we return $\bot$. It is easy to see that this procedure produces at every 
  step a verified conditional sample. Since the conditional sampling algorithm has only this access to the data we get that
  any guarantees of the conditional sampling immediately transfer to this corresponding strong correction scheme.
\end{proof}

  The above result gives a general framework for designing strong correction schemes for several computational and learning 
problems. We give some of these examples below that are based on the work of \cite{GouleakisTZ2017}. For other distributional 
learning tasks, one can use the conditional sampling algorithms of \cite{CanonneRS14} to get efficient strong correction 
schemes. Some applications of Theorem \ref{thm:sCorrection} can be found in Appendix \ref{sec:app:applicationsStrongCorrection}.
\paragraph{$k$-means Clustering}
  Let $\Domain$ be a metric space with distance metric $d : \Domain \times \Domain \rightarrow \reals$, i.e. $d(x, y)$ 
represents the distance between $x$ and $y$. Given a set of \textit{centers} $Ct$ we define the distance of a point $x$ from 
$Ct$ to be $d(x, Ct) = \min_{c \in Ct} d(x, c)$. Now given a set of $n$ input points $\Support \subseteq \Domain$ and a set of 
centers $Ct \subseteq \Omega$ we define the cost of $Ct$ for $\Support$ to be 
$d(\Support, Ct) = \sum_{x \in \Support} d(x, Ct)$. The $k$-means problem is the problem of minimizing the 
\textit{squared cost} $d^2(\Support, Ct) = \sum_{x \in \Support} d^2(x, Ct)$ over the choice of centers $Ct$ subject to the 
constraint $|Ct| = k$. We assume that the diameter of the metric space is $\Delta = \max_{x, y \in \Support} d(x, y)$. In this 
setting we assume that the records contain the points in the $d$-dimensional metric space.

\begin{corollary} \label{cor:kmeans}
    Let $x_1, x_2, \dots, x_n$ be the points in the $d$-dimensional metric space $\Domain$ stored in the records $\Workers$
  and $f(\xw)$ be the optimal $k$-means clustering of the points $\xw$. There exists a strong correction scheme with 
  $\tilde{O}(k^2 \log n \log ( k / \delta ))$ verifications that guarantees a constant approximation of the value optimal 
  clustering, with probability of failure at most $\delta$.
\end{corollary}

  The proof of this corollary is based on the Theorem \ref{thm:sCorrection} and the Theorem 2 from \cite{GouleakisTZ2017}.

\paragraph{Euclidean Minimum Spanning Tree}
  Given a set of points $\xw$ in $\reals^d$, the minimum spanning tree problem in Euclidean space ask to compute the a spanning
tree $T$ on the points minimizing the sum of weights of the edges. The weight of an edge between two points is equal to their 
Euclidean distance. We will focus on a simpler variant of the problem which is to compute just the weight of the best possible 
spanning tree, i.e. estimate the quantity
$\min_{\text{tree } T} \sum_{(x,x') \in T} \|x - x'\|_2$.

\begin{corollary} \label{cor:mst}
    Let $x_1, x_2, \dots, x_n$ be the points in $\reals^d$ stored in the records $\Workers$ and 
  $f(\xw) = \min_{\text{tree } T} \sum_{(x,x') \in T} \|x - x'\|_2$. There exists a strong correction scheme with 
  $\tilde O(d^3 \log^4 n / \eps^7 )\cdot \log(1/\delta)$ verifications that guarantees an $(1 + \eps)$-approximation of
  the weight of the minimum spanning tree, with probability of failure at most $\delta$.
\end{corollary}

  The proof of this corollary is based on the Theorem \ref{thm:sCorrection} and the Theorem 3 from \cite{GouleakisTZ2017}.

\textbf{Remark.} Observe that the value of the MST gives a 2-approximation of the metric TSP and the metric Steiner Tree 
problems. Hence Corollary \ref{cor:mst} implies efficient strong correction schemes that achieve constant approximation for 
those problems as well.
