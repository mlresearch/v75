% !TEX root = main.tex
Online learning with partial information dates back to the seminal work of \cite{Lai1985}. They consider a stochastic version, where losses come from fixed distributions. We focus on the case where the losses are selected adversarially, i.e. they do not come from a distribution and may be adaptive to the algorithm's choices. This was first studied by \cite{Auer2003} who provided the EXP3 algorithm for pure bandits and the EXP4 algorithm for learning with expert advice (a generalization of the contextual bandits of \cite{Langford2007}). They focus on uniform regret bounds, i.e. that grow as a function of time $o(T)$, and bound mostly the expected performance, but such guarantees can also be derived with high probability \citep{Auer2003,AudibertB10,pmlr-v15-beygelzimer11a}. Data-dependent guarantees are easily derived from the above algorithms for the case of maximizing some reward  as even getting reward $0$ with probability of $\epsilon$ only causes an $\eps$ fraction of loss in utility. In contrast, incurring high cost with a small probability $\epsilon$ can dominate the loss of the algorithm, if the best arm has small loss. In this paper we develop 
data-dependent guarantees for partial information algorithm for the cases of losses. There are a few specialized algorithms that achieve such small-loss guarantees for the case of bandits for pseudo-regret, e.g. by ensuring that the estimated losses of all arms remain close \citep{Allenberg2006,Neu15_semibandits} or using a stronger regularizer \citep{RakhlinS13predictablesequences,FosterLLST16}, but all of these methods neither offer high probability small-loss guarantees even for the bandit setting, nor extend to graph-based feedback. Our technique allows us to develop small-loss bounds on actual regret with high probability. 

The graph-based
partial information that we examine in this paper 
was introduced by \cite{MannorS11} who provided ELP, a linear programming based algorithm achieving
$\widetilde{\bigO}{(\sqrt{\alpha T})}$ regret for undirected graphs. \cite{AlonCGM13,AlonCGMMS} provided variants of Exp3  (Exp3-SET) that recovered the previous bound via what they call \emph{explicit exploration}. Following this work, there have been multiple results on this setting, e.g.\citep{AlonCBDK15,Cohen2016, KocakNV16, tossou:aaai2017b}, but prior to our work, there was no small-loss guarantee for the feedback graph setting that could exploit the graph structure. To obtain a regret bound depending on the graph structure, the above techniques upper bound the losses of the arms by the maximum loss which results in a dependence on the time horizon $T$ instead of $L^{\star}$. Addressing this, we achieve regret that scales with an appropriate problem dimension, the size of the maximum independent set $\alpha$, instead of ignoring the extra information and only depending on the number of arms as all small-loss results of prior work.

Biased estimators have been used prior to our work for achieving better regret guarantees. The freezing technique of \cite{Allenberg2006} can be thought of as the first use of biased estimators.  Their \emph{GREEN} algorithm uses freezing in the context of the multiplicative weights algorithm for the case of pure bandits. Freezing keeps the range of estimated losses bounded and when used with the multiplicative weights algorithm, also keeps the cumulative estimated losses very close, which ensures that one does not lose much in the application of the full information algorithm. Using these facts, \cite{Allenberg2006} achieved small-loss guarantees for  pseudo-regret in the classical multi-armed bandit setting. 
An approach very close to freezing is the \emph{implicit exploration} of \cite{Kock2014EfficientLB} that adds a term in the denominator of the estimator making the estimator biased, even for the selected arms. 
The \emph{FPL-TrIX} algorithm of \cite{Neu15_semibandits} 
is based on the Follow the Perturbed Leader algorithm using implicit exploration together with truncating the perturbations to  guarantee that the estimated losses of all actions are close to each other and the \emph{geometric resampling} technique of \cite{NeuB13} to obtain these estimated losses. His analysis provides small-loss regret bounds for pseudo-regret, but does not extend to high-probability guarantees. The \emph{EXP3-IX} algorithm of  \cite{Kock2014EfficientLB}combines implicit exploration with multiplicative weights to obtain, via the analysis of \cite{Neu2015_implicit}, high-probability uniform bounds. Focusing on uniform regret bounds, exploration and truncation were presented as strictly superior to freezing. In this paper, we show an important benefit of the freezing technique: it can be extended to handle feedback graphs (via our dual-thresholding).
We also combine freezing with multiplicative weights to develop an algorithm we term \emph{GREEN-IX} which achieves optimal high-probability small-loss
$\widetilde{\bigO}(\sqrt{d L^{\star}})$ for the pure bandit setting. Finally, combining freezing with the truncation idea, we obtain the corresponding result for semi-bandits; in contrast, the geometric resampling analysis does not seem to extend to high probability since it does not provide a handle on the variance of the estimated loss. 