\begin{thebibliography}{22}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Applegate and Kannan(1991)]{applegate_kannan}
David Applegate and Ravi Kannan.
\newblock Sampling and integration of near log-concave functions.
\newblock In \emph{Proceedings of the twenty-third annual ACM symposium on
  Theory of computing}, pages 156--163. ACM, 1991.

\bibitem[Belloni et~al.(2015)Belloni, Liang, Narayanan, and
  Rakhlin]{Simulated_Annealing_Nonassymptotic}
Alexandre Belloni, Tengyuan Liang, Hariharan Narayanan, and Alexander Rakhlin.
\newblock Escaping the local minima via simulated annealing: Optimization of
  approximately convex functions.
\newblock In \emph{Conference on Learning Theory}, pages 240--265, 2015.

\bibitem[Blum and Rivest(1989)]{blum1989training}
Avrim Blum and Ronald~L Rivest.
\newblock Training a 3-node neural network is {NP}-complete.
\newblock In \emph{Advances in neural information processing systems}, pages
  494--501, 1989.

\bibitem[Bubeck et~al.(2015)Bubeck, Eldan, and Lehec]{BubeckEL15}
Sebastien Bubeck, Ronen Eldan, and Joseph Lehec.
\newblock Finite-time analysis of projected {L}angevin {M}onte {C}arlo.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1243--1251, 2015.

\bibitem[Chen(2015)]{chen_thesis}
Ruobing Chen.
\newblock \emph{Stochastic derivative-free optimization of noisy functions}.
\newblock Ph.D. Thesis, Lehigh University, 2015.

\bibitem[Chen et~al.(2015)Chen, Menickelly, and Scheinberg]{chen2015stochastic}
Ruobing Chen, Matt Menickelly, and Katya Scheinberg.
\newblock Stochastic optimization using a trust-region method and random
  models.
\newblock \emph{Mathematical Programming}, pages 1--41, 2015.

\bibitem[Cliffe et~al.(2011)Cliffe, Giles, Scheichl, and
  Teckentrup]{cliffe2011multilevel}
K~Andrew Cliffe, Mike~B Giles, Robert Scheichl, and Aretha~L Teckentrup.
\newblock Multilevel {M}onte {C}arlo methods and applications to elliptic
  {PDE}s with random coefficients.
\newblock \emph{Computing and Visualization in Science}, 14\penalty0
  (1):\penalty0 3, 2011.

\bibitem[Conrad et~al.(2018)Conrad, Davis, Marzouk, Pillai, and
  Smith]{conrad2018parallel}
Patrick~R Conrad, Andrew~D Davis, Youssef~M Marzouk, Natesh~S Pillai, and Aaron
  Smith.
\newblock Parallel local approximation {MCMC} for expensive models.
\newblock \emph{SIAM/ASA Journal on Uncertainty Quantification}, 6\penalty0
  (1):\penalty0 339--373, 2018.

\bibitem[Duchi et~al.(2015)Duchi, Jordan, Wainwright, and
  Wibisono]{duchi2015optimal}
John~C Duchi, Michael~I Jordan, Martin~J Wainwright, and Andre Wibisono.
\newblock Optimal rates for zero-order convex optimization: The power of two
  function evaluations.
\newblock \emph{IEEE Transactions on Information Theory}, 61\penalty0
  (5):\penalty0 2788--2806, 2015.

\bibitem[Hanson and Wright(1971)]{hanson1971bound}
David~Lee Hanson and Farroll~Tim Wright.
\newblock A bound on tail probabilities for quadratic forms in independent
  random variables.
\newblock \emph{The Annals of Mathematical Statistics}, 42\penalty0
  (3):\penalty0 1079--1083, 1971.

\bibitem[Hazan et~al.(2016)Hazan, Levy, and Shalev-Shwartz]{hazan2016graduated}
Elad Hazan, Kfir~Yehuda Levy, and Shai Shalev-Shwartz.
\newblock On graduated optimization for stochastic non-convex problems.
\newblock In \emph{International Conference on Machine Learning}, pages
  1833--1841, 2016.

\bibitem[Jebalia and Auger(2008)]{jebalia2008multiplicative}
Mohamed Jebalia and Anne Auger.
\newblock On multiplicative noise models for stochastic search.
\newblock In \emph{International Conference on Parallel Problem Solving from
  Nature}, pages 52--61. Springer, 2008.

\bibitem[Jebalia et~al.(2011)Jebalia, Auger, and Hansen]{jebalia2011log}
Mohamed Jebalia, Anne Auger, and Nikolaus Hansen.
\newblock Log-linear convergence and divergence of the scale-invariant ($1+
  1$)-{ES} in noisy environments.
\newblock \emph{Algorithmica}, 59\penalty0 (3):\penalty0 425--460, 2011.

\bibitem[Kirkpatrick et~al.(1983)Kirkpatrick, Gelatt, and
  Vecchi]{kirkpatrick1983optimization}
Scott Kirkpatrick, C~Daniel Gelatt, and Mario~P Vecchi.
\newblock Optimization by simulated annealing.
\newblock \emph{science}, 220\penalty0 (4598):\penalty0 671--680, 1983.

\bibitem[Lee and Vempala(2017)]{lee2017convergence}
Yin~Tat Lee and Santosh~S Vempala.
\newblock Convergence rate of {R}iemannian {H}amiltonian {M}onte {C}arlo and
  faster polytope volume computation.
\newblock \emph{To appear in Proceedings of STOC 2018, arXiv preprint
  arXiv:1710.06261}, 2017.

\bibitem[Lov{\'a}sz and Simonovits(1993)]{lovasz1993random}
L{\'a}szl{\'o} Lov{\'a}sz and Mikl{\'o}s Simonovits.
\newblock Random walks in a convex body and an improved volume algorithm.
\newblock \emph{Random structures \& algorithms}, 4\penalty0 (4):\penalty0
  359--412, 1993.

\bibitem[Raginsky et~al.(2017)Raginsky, Rakhlin, and
  Telgarsky]{raginsky2017non}
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky.
\newblock Non-convex learning via stochastic gradient {L}angevin dynamics: a
  nonasymptotic analysis.
\newblock In \emph{Conference on Learning Theory}, pages 1674--1703, 2017.

\bibitem[Risteski and Li(2016)]{risteski2016algorithms}
Andrej Risteski and Yuanzhi Li.
\newblock Algorithms and matching lower bounds for approximately-convex
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4745--4753, 2016.

\bibitem[Rudelson and Vershynin(2013)]{rudelson2013hanson}
Mark Rudelson and Roman Vershynin.
\newblock Hanson-{W}right inequality and sub-{G}aussian concentration.
\newblock \emph{Electron. Commun. Probab}, 18\penalty0 (82):\penalty0 1--9,
  2013.

\bibitem[Singer and Vondr{\'a}k(2015)]{singer2015information}
Yaron Singer and Jan Vondr{\'a}k.
\newblock Information-theoretic lower bounds for convex optimization with
  erroneous oracles.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3204--3212, 2015.

\bibitem[Welling and Teh(2011)]{welling2011bayesian}
Max Welling and Yee~W Teh.
\newblock Bayesian learning via stochastic gradient {L}angevin dynamics.
\newblock In \emph{Proceedings of the 28th International Conference on Machine
  Learning (ICML-11)}, pages 681--688, 2011.

\bibitem[Zhang et~al.(2017)Zhang, Liang, and Charikar]{hitting_times}
Yuchen Zhang, Percy Liang, and Moses Charikar.
\newblock A hitting time analysis of stochastic gradient {L}angevin dynamics.
\newblock In \emph{Conference on Learning Theory}, pages 1980--2022, 2017.

\end{thebibliography}
