\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{applegate_kannan,singer2015information,risteski2016algorithms,hitting_times}
\citation{chen_thesis}
\citation{chen2015stochastic,jebalia2008multiplicative,jebalia2011log}
\jmlr@workshop{31st Annual Conference on Learning Theory}
\jmlr@title{Convex Optimization with Unbounded Nonconvex Oracles}{Convex Optimization with Unbounded Nonconvex Oracles using Simulated Annealing}
\jmlr@author{\Name {Oren Mangoubi}\\ \addr \'{E}cole Polytechnique F\'{e}d\'{e}rale de Lausanne (EPFL), Switzerland \AND \Name {Nisheeth K. Vishnoi}\\ \addr \'{E}cole Polytechnique F\'{e}d\'{e}rale de Lausanne (EPFL), Switzerland }{\Name {Oren Mangoubi}\\ \addr \'{E}cole Polytechnique F\'{e}d\'{e}rale de Lausanne (EPFL), Switzerland \AND \Name {Nisheeth K. Vishnoi}\\ \addr \'{E}cole Polytechnique F\'{e}d\'{e}rale de Lausanne (EPFL), Switzerland }
\newlabel{jmlrstart}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.0.1}}
\citation{conrad2018parallel,cliffe2011multilevel}
\citation{chen2015stochastic}
\citation{chen2015stochastic}
\citation{applegate_kannan}
\newlabel{def:noise_both2}{{1}{2}{Introduction}{theorem.0.1.1}{}}
\newlabel{eq:model_add_mult2}{{1}{2}{Introduction}{equation.0.1.1}{}}
\newlabel{problem1}{{1}{2}{Introduction}{problem.1}{}}
\citation{Simulated_Annealing_Nonassymptotic}
\citation{Simulated_Annealing_Nonassymptotic}
\citation{hitting_times}
\citation{singer2015information,hazan2016graduated,risteski2016algorithms}
\citation{applegate_kannan,hitting_times}
\citation{Simulated_Annealing_Nonassymptotic}
\citation{Simulated_Annealing_Nonassymptotic}
\citation{hitting_times}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Our contributions}{3}{subsection.0.1.1}}
\citation{applegate_kannan}
\citation{blum1989training}
\citation{hitting_times}
\citation{hitting_times}
\newlabel{thm:summary}{{2}{4}{Our contributions}{theorem.0.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}On the assumption that $F(x^\star )=0.$}{4}{subsection.0.1.2}}
\newlabel{rem:binarysearch}{{1.2}{4}{On the assumption that $F(x^\star )=0.$}{subsection.0.1.2}{}}
\citation{hitting_times}
\citation{hitting_times}
\citation{kirkpatrick1983optimization}
\citation{Simulated_Annealing_Nonassymptotic}
\citation{Simulated_Annealing_Nonassymptotic}
\citation{Simulated_Annealing_Nonassymptotic}
\citation{Simulated_Annealing_Nonassymptotic}
\citation{Simulated_Annealing_Nonassymptotic}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Short summary of techniques}{5}{subsection.0.1.3}}
\citation{Simulated_Annealing_Nonassymptotic}
\citation{raginsky2017non,BubeckEL15,welling2011bayesian,lee2017convergence}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Organization of the rest of the paper}{6}{subsection.0.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Overview of Our Contributions}{6}{section.0.2}}
\newlabel{sec:overview}{{2}{6}{Overview of Our Contributions}{section.0.2}{}}
\@writefile{toc}{\contentsline {paragraph}{The model and the problem.}{6}{section*.1}}
\newlabel{eq:noise_model}{{2}{6}{The model and the problem}{equation.0.2.2}{}}
\citation{hitting_times}
\@writefile{toc}{\contentsline {paragraph}{Our algorithm.}{7}{section*.2}}
\@writefile{toc}{\contentsline {paragraph}{Description of the Markov chain in a single epoch.}{7}{section*.3}}
\newlabel{update_rule}{{3}{7}{Description of the Markov chain in a single epoch}{equation.0.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Steps in bounding the running time.}{7}{section*.4}}
\citation{hitting_times}
\citation{hitting_times}
\citation{hitting_times}
\citation{lovasz1993random}
\citation{lovasz1993random}
\citation{hitting_times}
\@writefile{toc}{\contentsline {paragraph}{Bounding the hitting time and the Cheeger constant.}{8}{section*.5}}
\@writefile{toc}{\contentsline {paragraph}{Bounding the Cheeger constant.}{8}{section*.6}}
\citation{hitting_times}
\citation{hitting_times}
\@writefile{toc}{\contentsline {paragraph}{Requirements on the temperature to bound the Cheeger constant.}{9}{section*.7}}
\@writefile{toc}{\contentsline {paragraph}{Quantitative error and running time bounds.}{10}{section*.8}}
\citation{hitting_times}
\citation{Simulated_Annealing_Nonassymptotic}
\citation{Simulated_Annealing_Nonassymptotic}
\citation{hitting_times}
\citation{hitting_times}
\@writefile{toc}{\contentsline {paragraph}{Drift bounds and initialization.}{11}{section*.9}}
\@writefile{toc}{\contentsline {paragraph}{Another coupled toy chain.}{11}{section*.10}}
\@writefile{toc}{\contentsline {paragraph}{Rounding the sub-level sets.}{11}{section*.11}}
\citation{duchi2015optimal}
\citation{hitting_times}
\citation{hitting_times}
\@writefile{toc}{\contentsline {paragraph}{Smoothing a non-differentiable noisy oracle.}{12}{section*.12}}
\bibstyle{plain}
\bibdata{annealing}
\bibcite{applegate_kannan}{{1}{1991}{{Applegate and Kannan}}{{}}}
\bibcite{Simulated_Annealing_Nonassymptotic}{{2}{2015}{{Belloni et~al.}}{{Belloni, Liang, Narayanan, and Rakhlin}}}
\bibcite{blum1989training}{{3}{1989}{{Blum and Rivest}}{{}}}
\bibcite{BubeckEL15}{{4}{2015}{{Bubeck et~al.}}{{Bubeck, Eldan, and Lehec}}}
\bibcite{chen_thesis}{{5}{2015}{{Chen}}{{}}}
\bibcite{chen2015stochastic}{{6}{2015}{{Chen et~al.}}{{Chen, Menickelly, and Scheinberg}}}
\bibcite{cliffe2011multilevel}{{7}{2011}{{Cliffe et~al.}}{{Cliffe, Giles, Scheichl, and Teckentrup}}}
\bibcite{conrad2018parallel}{{8}{2018}{{Conrad et~al.}}{{Conrad, Davis, Marzouk, Pillai, and Smith}}}
\bibcite{duchi2015optimal}{{9}{2015}{{Duchi et~al.}}{{Duchi, Jordan, Wainwright, and Wibisono}}}
\bibcite{hanson1971bound}{{10}{1971}{{Hanson and Wright}}{{}}}
\bibcite{hazan2016graduated}{{11}{2016}{{Hazan et~al.}}{{Hazan, Levy, and Shalev-Shwartz}}}
\bibcite{jebalia2008multiplicative}{{12}{2008}{{Jebalia and Auger}}{{}}}
\bibcite{jebalia2011log}{{13}{2011}{{Jebalia et~al.}}{{Jebalia, Auger, and Hansen}}}
\bibcite{kirkpatrick1983optimization}{{14}{1983}{{Kirkpatrick et~al.}}{{Kirkpatrick, Gelatt, and Vecchi}}}
\bibcite{lee2017convergence}{{15}{2017}{{Lee and Vempala}}{{}}}
\bibcite{lovasz1993random}{{16}{1993}{{Lov{\'a}sz and Simonovits}}{{}}}
\bibcite{raginsky2017non}{{17}{2017}{{Raginsky et~al.}}{{Raginsky, Rakhlin, and Telgarsky}}}
\bibcite{risteski2016algorithms}{{18}{2016}{{Risteski and Li}}{{}}}
\bibcite{rudelson2013hanson}{{19}{2013}{{Rudelson and Vershynin}}{{}}}
\bibcite{singer2015information}{{20}{2015}{{Singer and Vondr{\'a}k}}{{}}}
\bibcite{welling2011bayesian}{{21}{2011}{{Welling and Teh}}{{}}}
\bibcite{hitting_times}{{22}{2017}{{Zhang et~al.}}{{Zhang, Liang, and Charikar}}}
\citation{duchi2015optimal}
\citation{hitting_times}
\@writefile{toc}{\contentsline {section}{\numberline {A}Preliminaries}{15}{section.0.A}}
\newlabel{sec:preliminaries}{{A}{15}{Preliminaries}{section.0.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Notation}{15}{subsection.0.A.1}}
\newlabel{sec:notation}{{A.1}{15}{Notation}{subsection.0.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Assumptions on the convex objective function and the constraint set}{15}{subsection.0.A.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}A smoothed oracle from a non-smooth one}{15}{subsection.0.A.3}}
\newlabel{sec:smoothed_from_nonsmooth}{{A.3}{15}{A smoothed oracle from a non-smooth one}{subsection.0.A.3}{}}
\newlabel{eq:assumption}{{4}{15}{A smoothed oracle from a non-smooth one}{equation.0.A.4}{}}
\newlabel{eq:smoother}{{5}{15}{A smoothed oracle from a non-smooth one}{equation.0.A.5}{}}
\citation{hitting_times}
\@writefile{toc}{\contentsline {section}{\numberline {B}Our Contribution}{16}{section.0.B}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Our Algorithm}{16}{subsection.0.B.1}}
\newlabel{sec:algorithm}{{B.1}{16}{Our Algorithm}{subsection.0.B.1}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:SGLD}{{1}{16}{Stochastic gradient Langevin dynamics (SGLD) \relax }{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Stochastic gradient Langevin dynamics (SGLD) \relax }}{16}{algorithm.1}}
\newlabel{alg:annealing}{{2}{16}{Simulated annealing SGLD \relax }{algorithm.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Simulated annealing SGLD \relax }}{16}{algorithm.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Statement of Our Main Theorem}{17}{subsection.0.B.2}}
\newlabel{sec:theorem}{{B.2}{17}{Statement of Our Main Theorem}{subsection.0.B.2}{}}
\newlabel{thm:main}{{3}{17}{Statement of Our Main Theorem}{theorem.0.B.3}{}}
\citation{hitting_times}
\@writefile{toc}{\contentsline {section}{\numberline {C}Proofs}{18}{section.0.C}}
\newlabel{sec:proofs}{{C}{18}{Proofs}{section.0.C}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Assumptions about the smooth oracle}{18}{subsection.0.C.1}}
\newlabel{sec:smooth_assumptions}{{C.1}{18}{Assumptions about the smooth oracle}{subsection.0.C.1}{}}
\newlabel{eq:assumption_noise}{{6}{18}{Assumptions about the smooth oracle}{equation.0.C.6}{}}
\newlabel{assumption:A}{{1}{18}{Assumptions about the smooth oracle}{assumption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Conductance and bounding the Cheeger constant}{18}{subsection.0.C.2}}
\newlabel{sec:conductance}{{C.2}{18}{Conductance and bounding the Cheeger constant}{subsection.0.C.2}{}}
\citation{hitting_times}
\newlabel{def:Cheeger}{{4}{19}{Conductance and bounding the Cheeger constant}{theorem.0.C.4}{}}
\newlabel{def:epsilon_close}{{5}{19}{Conductance and bounding the Cheeger constant}{theorem.0.C.5}{}}
\newlabel{lemma:cheeger2}{{6}{19}{Conductance and bounding the Cheeger constant}{theorem.0.C.6}{}}
\newlabel{eq:ratio4}{{7}{19}{Conductance and bounding the Cheeger constant}{equation.0.C.7}{}}
\citation{lovasz1993random}
\newlabel{eq:half}{{8}{20}{Conductance and bounding the Cheeger constant}{equation.0.C.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3}Bounding the escape probability}{20}{subsection.0.C.3}}
\newlabel{sec:drift}{{C.3}{20}{Bounding the escape probability}{subsection.0.C.3}{}}
\citation{hitting_times}
\newlabel{alg:SGLD_metropolis}{{3}{21}{Lazy Metropolis-adjusted SGLD \relax }{algorithm.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Lazy Metropolis-adjusted SGLD \relax }}{21}{algorithm.3}}
\newlabel{def:coupling}{{7}{21}{Bounding the escape probability}{theorem.0.C.7}{}}
\newlabel{thm:drift}{{8}{22}{Bounding the escape probability}{theorem.0.C.8}{}}
\newlabel{eq:drift2}{{9}{22}{Bounding the escape probability}{equation.0.C.9}{}}
\newlabel{eq:drift3}{{10}{22}{Bounding the escape probability}{equation.0.C.10}{}}
\newlabel{eq:drift4}{{11}{22}{Bounding the escape probability}{equation.0.C.11}{}}
\newlabel{lemma:drift}{{9}{22}{Bounding the escape probability}{theorem.0.C.9}{}}
\newlabel{eq:drift1}{{12}{23}{Bounding the escape probability}{equation.0.C.12}{}}
\newlabel{eq:warmstart}{{13}{23}{Bounding the escape probability}{equation.0.C.13}{}}
\newlabel{eq:ratio}{{14}{23}{Bounding the escape probability}{equation.0.C.14}{}}
\newlabel{eq:ratio2}{{15}{23}{Bounding the escape probability}{equation.0.C.15}{}}
\newlabel{eq:ratio3}{{16}{23}{Bounding the escape probability}{equation.0.C.16}{}}
\citation{hitting_times}
\newlabel{lemma:drift2}{{10}{24}{Bounding the escape probability}{theorem.0.C.10}{}}
\newlabel{eq:coupling}{{17}{24}{Bounding the escape probability}{equation.0.C.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.4}Comparing noisy functions}{24}{subsection.0.C.4}}
\newlabel{sec:compare}{{C.4}{24}{Comparing noisy functions}{subsection.0.C.4}{}}
\newlabel{lemma:compare}{{11}{24}{Comparing noisy functions}{theorem.0.C.11}{}}
\newlabel{eq:b1}{{18}{25}{Comparing noisy functions}{equation.0.C.18}{}}
\newlabel{eq:b2}{{19}{25}{Comparing noisy functions}{equation.0.C.19}{}}
\newlabel{eq:b3}{{20}{25}{Comparing noisy functions}{equation.0.C.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.5}Bounding the error and running time: The smooth case}{25}{subsection.0.C.5}}
\newlabel{sec:result_smooth}{{C.5}{25}{Bounding the error and running time: The smooth case}{subsection.0.C.5}{}}
\citation{hitting_times}
\newlabel{thm:error}{{12}{26}{Bounding the error and running time: The smooth case}{theorem.0.C.12}{}}
\newlabel{eq:delta2}{{21}{27}{Bounding the error and running time: The smooth case}{equation.0.C.21}{}}
\newlabel{eq:delta}{{22}{27}{Bounding the error and running time: The smooth case}{equation.0.C.22}{}}
\newlabel{eq:subset}{{23}{27}{Bounding the error and running time: The smooth case}{equation.0.C.23}{}}
\newlabel{eq:delta3}{{24}{27}{Bounding the error and running time: The smooth case}{equation.0.C.24}{}}
\newlabel{eq:F}{{25}{28}{Bounding the error and running time: The smooth case}{equation.0.C.25}{}}
\newlabel{eq:noise}{{26}{28}{Bounding the error and running time: The smooth case}{equation.0.C.26}{}}
\newlabel{eq:conductance}{{27}{28}{Bounding the error and running time: The smooth case}{equation.0.C.27}{}}
\newlabel{eq:eta}{{28}{28}{Bounding the error and running time: The smooth case}{equation.0.C.28}{}}
\citation{hitting_times}
\citation{hitting_times}
\newlabel{eq:conductance2}{{29}{29}{Bounding the error and running time: The smooth case}{equation.0.C.29}{}}
\newlabel{eq:hitting_time}{{30}{29}{Bounding the error and running time: The smooth case}{equation.0.C.30}{}}
\newlabel{eq:hitting_time2}{{31}{30}{Bounding the error and running time: The smooth case}{equation.0.C.31}{}}
\newlabel{eq:contraction2}{{32}{30}{Bounding the error and running time: The smooth case}{equation.0.C.32}{}}
\newlabel{eq:contraction}{{33}{30}{Bounding the error and running time: The smooth case}{equation.0.C.33}{}}
\newlabel{eq:tilda}{{34}{30}{Bounding the error and running time: The smooth case}{equation.0.C.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.6}The non-smooth case}{31}{subsection.0.C.6}}
\newlabel{sec:smoothing}{{C.6}{31}{The non-smooth case}{subsection.0.C.6}{}}
\newlabel{lemma:smooth_gradient}{{13}{31}{The non-smooth case}{theorem.0.C.13}{}}
\citation{hitting_times}
\citation{hitting_times}
\newlabel{max_value}{{14}{32}{The non-smooth case}{theorem.0.C.14}{}}
\newlabel{eq:a1}{{35}{32}{The non-smooth case}{equation.0.C.35}{}}
\newlabel{lemma:Hessian}{{15}{32}{The non-smooth case}{theorem.0.C.15}{}}
\newlabel{lemma:noise_smooth}{{16}{32}{The non-smooth case}{theorem.0.C.16}{}}
\citation{hanson1971bound}
\citation{rudelson2013hanson}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.7}Rounding the domain of the Markov Chain}{33}{subsection.0.C.7}}
\newlabel{lemma:contraint_round}{{17}{33}{Rounding the domain of the Markov Chain}{theorem.0.C.17}{}}
\newlabel{eq:roundness1}{{36}{34}{Rounding the domain of the Markov Chain}{equation.0.C.36}{}}
\newlabel{eq:Hanson}{{37}{34}{Rounding the domain of the Markov Chain}{equation.0.C.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.8}Proof of Main Result (Theorem \ref  {thm:main})}{34}{subsection.0.C.8}}
\newlabel{sec:proof_of_main_result}{{C.8}{34}{Proof of Main Result (Theorem \ref {thm:main})}{subsection.0.C.8}{}}
\newlabel{eq:k_max}{{38}{35}{Proof of Main Result (Theorem \ref {thm:main})}{equation.0.C.38}{}}
\newlabel{eq:xi_k}{{39}{35}{Proof of Main Result (Theorem \ref {thm:main})}{equation.0.C.39}{}}
\newlabel{eq:i_max}{{40}{35}{Proof of Main Result (Theorem \ref {thm:main})}{equation.0.C.40}{}}
\citation{hitting_times}
\newlabel{eq:eta_k}{{41}{36}{Proof of Main Result (Theorem \ref {thm:main})}{equation.0.C.41}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Figures}{37}{section.0.D}}
\newlabel{sec:figures}{{D}{37}{Figures}{section.0.D}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces To quickly escape a local minimizer $x^\circ $ of ``depth" $\beta $, a Markov chain must run at a temperature $\beta $. At this temperature, the Markov chain will concentrate in a sub-level set of height $d\beta $. This sub-level set does not have a narrow bottleneck, so a Markov chain running at temperature $\beta $ will quickly escape the local minimum at $x^\circ $.\relax }}{37}{figure.caption.14}}
\newlabel{fig:level_sets}{{1}{37}{To quickly escape a local minimizer $x^\circ $ of ``depth" $\beta $, a Markov chain must run at a temperature $\beta $. At this temperature, the Markov chain will concentrate in a sub-level set of height $d\beta $. This sub-level set does not have a narrow bottleneck, so a Markov chain running at temperature $\beta $ will quickly escape the local minimum at $x^\circ $.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  (a) Optimization of a convex function $F$ (green) with noisy oracle $\mathaccentV {hat}05E{F}$ (black) under bounded additive noise. Since the gap between the noise bounds (dashed lines) is constant, the Markov chain (red) can be run at a single temperature that is both hot enough to quickly escape any local minimum but also cold enough so that the Markov chain eventually concentrates near the global minimum. (b) and (c) {Optimization of a convex function $F$ (green) with noisy oracle $\mathaccentV {hat}05E{F}$ (black) when both additive and multiplicative noise are present, if we run the Markov chain at single a fixed temperature. If the temperature is hot enough to escape even the deepest local minima (b), then the Markov chain will not concentrate near the global minimum, leading to a large error. If instead the Markov chain is run at a colder temperature (c), it will take a very long time to escape the deeper local minima.} (d) Optimization of a convex function $F$ (green) with noisy oracle $\mathaccentV {hat}05E{F}$ (black) under both additive and multiplicative noise, when using a gradually decreasing temperature. If multiplicative noise is present the local minima of $\mathaccentV {hat}05E{F}$ are very deep for large values of $F$. To quickly escape the deeper local minima, the Markov chain is started at a high temperature. As the Markov chain concentrates in regions where $F$ is smaller, the local minima become shallower, so the temperature may be gradually decreased while still allowing the Markov chain to escape nearby local minima. As the temperature is gradually decreased, the Markov chain concentrates in regions with successively smaller values of $\mathaccentV {hat}05E{F}$. \relax }}{38}{figure.caption.15}}
\newlabel{fig:annealing}{{2}{38}{(a) Optimization of a convex function $F$ (green) with noisy oracle $\hat {F}$ (black) under bounded additive noise. Since the gap between the noise bounds (dashed lines) is constant, the Markov chain (red) can be run at a single temperature that is both hot enough to quickly escape any local minimum but also cold enough so that the Markov chain eventually concentrates near the global minimum. (b) and (c) {Optimization of a convex function $F$ (green) with noisy oracle $\hat {F}$ (black) when both additive and multiplicative noise are present, if we run the Markov chain at single a fixed temperature. If the temperature is hot enough to escape even the deepest local minima (b), then the Markov chain will not concentrate near the global minimum, leading to a large error. If instead the Markov chain is run at a colder temperature (c), it will take a very long time to escape the deeper local minima.} (d) Optimization of a convex function $F$ (green) with noisy oracle $\hat {F}$ (black) under both additive and multiplicative noise, when using a gradually decreasing temperature. If multiplicative noise is present the local minima of $\hat {F}$ are very deep for large values of $F$. To quickly escape the deeper local minima, the Markov chain is started at a high temperature. As the Markov chain concentrates in regions where $F$ is smaller, the local minima become shallower, so the temperature may be gradually decreased while still allowing the Markov chain to escape nearby local minima. As the temperature is gradually decreased, the Markov chain concentrates in regions with successively smaller values of $\hat {F}$. \relax }{figure.caption.15}{}}
\newlabel{jmlrend}{{D}{39}{end of Convex Optimization with Unbounded Nonconvex Oracles}{section*.16}{}}
