%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Oliver Hinder at 2018-06-05 12:10:55 -0700 


%% Saved with string encoding Unicode (UTF-8) 


@string{ieeeit = {IEEE Transactions on Information Theory}}

@string{mathprog = {Mathematical Programming}}

@string{nips21 = {Advances in Neural Information Processing Systems 21}}

@string{nips26 = {Advances in Neural Information Processing Systems 26}}

@string{nips27 = {Advances in Neural Information Processing Systems 27}}

@string{siopt = {SIAM Journal on Optimization}}


@article{carmon2017lowerII,
	Author = {Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
	Date-Added = {2018-05-22 00:03:22 +0000},
	Date-Modified = {2018-05-22 00:04:01 +0000},
	Journal = {arXiv preprint arXiv:1711.00841},
	Keywords = {me},
	Title = {Lower Bounds for Finding Stationary Points II: First-Order Methods},
	Year = {2017}}

@article{carmon2017lower,
	Author = {Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
	Date-Added = {2018-05-22 00:03:14 +0000},
	Date-Modified = {2018-05-22 00:04:07 +0000},
	Journal = {arXiv preprint arXiv:1710.11606},
	Keywords = {me},
	Title = {Lower bounds for finding stationary points {I}},
	Year = {2017}}

@electronic{knitroOptimalPower,
	Author = {Todd Plantenga},
	Date-Added = {2018-05-02 17:20:26 +0000},
	Date-Modified = {2018-05-02 17:29:42 +0000},
	Month = {October},
	Title = {{KNITRO} for Nonlinear Optimal Power Flow Applications},
	Url = {https://www.artelys.com/downloads/pdf/composants-numeriques/knitro/papers/case_OPF.pdf},
	Year = {2006},
	Bdsk-Url-1 = {https://www.artelys.com/downloads/pdf/composants-numeriques/knitro/papers/case_OPF.pdf}}

@article{bukhsh2013local,
	Author = {Bukhsh, Waqquas A and Grothey, Andreas and McKinnon, Ken IM and Trodden, Paul A},
	Date-Added = {2018-05-02 16:44:37 +0000},
	Date-Modified = {2018-05-02 16:44:37 +0000},
	Journal = {IEEE Transactions on Power Systems},
	Number = {4},
	Pages = {4780--4788},
	Publisher = {IEEE},
	Title = {Local solutions of the optimal power flow problem},
	Volume = {28},
	Year = {2013}}

@article{bahn1995cutting,
	Author = {Bahn, Olivier and du Merle, Olivier and Goffin, J-L and Vial, J-P},
	Date-Added = {2018-05-02 05:01:44 +0000},
	Date-Modified = {2018-05-02 05:01:44 +0000},
	Journal = {Mathematical Programming},
	Number = {1-3},
	Pages = {45--73},
	Publisher = {Springer},
	Title = {A cutting plane method from analytic centers for stochastic programming},
	Volume = {69},
	Year = {1995}}

@article{padberg1991branch,
	Author = {Padberg, Manfred and Rinaldi, Giovanni},
	Date-Added = {2018-05-02 04:43:19 +0000},
	Date-Modified = {2018-05-02 04:43:19 +0000},
	Journal = {SIAM review},
	Number = {1},
	Pages = {60--100},
	Publisher = {SIAM},
	Title = {A branch-and-cut algorithm for the resolution of large-scale symmetric traveling salesman problems},
	Volume = {33},
	Year = {1991}}

@article{birgin2017worst,
	Author = {Birgin, Ernesto G and Gardenghi, JL and Mart{\'\i}nez, Jos{\'e} Mario and Santos, Sandra Augusta and Toint, Ph L},
	Date-Added = {2018-04-13 00:46:04 +0000},
	Date-Modified = {2018-04-13 00:46:06 +0000},
	Journal = {Mathematical Programming},
	Keywords = {pth},
	Number = {1-2},
	Pages = {359--368},
	Publisher = {Springer},
	Title = {Worst-case evaluation complexity for unconstrained nonlinear optimization using high-order regularized models},
	Volume = {163},
	Year = {2017}}

@article{cartis2017improved,
	Author = {Cartis, Coralia and Gould, Nicholas IM and Toint, Philippe L},
	Date-Added = {2018-04-13 00:28:28 +0000},
	Date-Modified = {2018-04-13 00:42:27 +0000},
	Journal = {arXiv preprint arXiv:1708.04044},
	Keywords = {pth},
	Title = {Improved second-order evaluation complexity for unconstrained nonlinear optimization using high-order regularized models},
	Year = {2017}}

@article{demmel2007fast,
	Author = {Demmel, James and Dumitriu, Ioana and Holtz, Olga},
	Date-Added = {2018-04-11 22:16:22 +0000},
	Date-Modified = {2018-04-11 22:16:22 +0000},
	Journal = {Numerische Mathematik},
	Number = {1},
	Pages = {59--91},
	Publisher = {Springer},
	Title = {Fast linear algebra is stable},
	Volume = {108},
	Year = {2007}}

@article{press1992numerical,
	Author = {Press, William H and Teukolsky, Saul A and Vetterling, William T and Flannery, Brian P},
	Date-Added = {2018-04-10 07:05:29 +0000},
	Date-Modified = {2018-04-10 07:05:29 +0000},
	Journal = {New York, NY: Press Syndicate of the University of Cambridge},
	Title = {Numerical recipes in FORTRAN 77, vol. 1},
	Year = {1992}}

@article{jin2017accelerated,
	Author = {Jin, Chi and Netrapalli, Praneeth and Jordan, Michael I},
	Date-Added = {2018-02-15 23:04:23 +0000},
	Date-Modified = {2018-02-15 23:04:25 +0000},
	Journal = {arXiv preprint arXiv:1711.10456},
	Keywords = {fast-nonconvex-theory},
	Title = {Accelerated Gradient Descent Escapes Saddle Points Faster than Gradient Descent},
	Year = {2017}}

@article{royer2017complexity,
	Author = {Royer, Cl{\'e}ment W and Wright, Stephen J},
	Date-Added = {2018-02-15 22:53:33 +0000},
	Date-Modified = {2018-02-15 22:53:40 +0000},
	Journal = {arXiv preprint arXiv:1706.03131},
	Keywords = {fast-nonconvex-theory},
	Title = {Complexity analysis of second-order line-search algorithms for smooth nonconvex optimization},
	Year = {2017}}

@article{cartis2016universal,
	Author = {Cartis, Coralia and Gould, Nicholas IM and Toint, Philippe L},
	Date-Added = {2018-02-15 22:44:37 +0000},
	Date-Modified = {2018-04-13 00:42:35 +0000},
	Journal = {Preprint RAL-P-2016-010, Rutherford Appleton Laboratory, Chilton, England},
	Keywords = {pth},
	Title = {Universal regularization methods--varying the power, the smoothness and the accuracy},
	Year = {2016}}

@article{goffin1997solving,
	Author = {Goffin, J-L and Gondzio, Jacek and Sarkissian, Robert and Vial, J-P},
	Date-Added = {2018-02-15 20:56:55 +0000},
	Date-Modified = {2018-02-15 20:56:57 +0000},
	Journal = {Mathematical programming},
	Keywords = {cutting-plane},
	Number = {1},
	Pages = {131--154},
	Publisher = {Springer},
	Title = {Solving nonlinear multicommodity flow problems by the analytic center cutting plane method},
	Volume = {76},
	Year = {1997}}

@article{atkinson1995cutting,
	Author = {Atkinson, David S and Vaidya, Pravin M},
	Date-Added = {2018-02-15 20:48:52 +0000},
	Date-Modified = {2018-02-15 20:48:55 +0000},
	Journal = {Mathematical Programming},
	Keywords = {cutting-plane},
	Number = {1-3},
	Pages = {1--43},
	Publisher = {Springer},
	Title = {A cutting plane algorithm for convex programming that uses analytic centers},
	Volume = {69},
	Year = {1995}}

@article{shor1977cut,
	Annote = {ellipsoid method},
	Author = {Shor, Naum Z},
	Date-Added = {2018-02-15 20:41:12 +0000},
	Date-Modified = {2018-02-15 20:45:16 +0000},
	Journal = {Cybernetics},
	Keywords = {cutting-plane},
	Number = {1},
	Pages = {94--96},
	Publisher = {Springer},
	Title = {Cut-off method with space extension in convex programming problems},
	Volume = {13},
	Year = {1977}}

@article{CenterGravity,
	Author = {A. Yu Levin},
	Date-Added = {2018-02-15 20:37:36 +0000},
	Date-Modified = {2018-02-15 20:38:55 +0000},
	Journal = {Soviet Math. Doklady},
	Keywords = {cutting-plane},
	Title = {On an algorithm for the minimization of convex functions},
	Year = {1965}}

@inproceedings{vaidya1989new,
	Author = {Vaidya, Pravin M},
	Booktitle = {Foundations of Computer Science, 1989., 30th Annual Symposium on},
	Date-Added = {2018-02-15 20:19:00 +0000},
	Date-Modified = {2018-02-15 20:19:34 +0000},
	Keywords = {cutting-plane},
	Organization = {IEEE},
	Pages = {338--343},
	Title = {A new algorithm for minimizing convex functions over convex sets},
	Year = {1989}}

@article{khachiyan1980polynomial,
	Author = {Khachiyan, Leonid G},
	Date-Added = {2018-02-15 20:17:24 +0000},
	Date-Modified = {2018-02-15 20:17:24 +0000},
	Journal = {USSR Computational Mathematics and Mathematical Physics},
	Number = {1},
	Pages = {53--72},
	Publisher = {Elsevier},
	Title = {Polynomial algorithms in linear programming},
	Volume = {20},
	Year = {1980}}

@inproceedings{lee2015faster,
	Author = {Lee, Yin Tat and Sidford, Aaron and Wong, Sam Chiu-wai},
	Booktitle = {Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on},
	Date-Added = {2018-02-15 20:15:03 +0000},
	Date-Modified = {2018-02-15 20:20:00 +0000},
	Keywords = {cutting-plane},
	Organization = {IEEE},
	Pages = {1049--1065},
	Title = {A faster cutting plane method and its implications for combinatorial and convex optimization},
	Year = {2015}}

@inproceedings{carmon2017convex,
	Author = {Carmon, Yair and Duchi, John C. and Hinder, Oliver and Sidford, Aaron},
	Booktitle = {Proceedings of 34th International Conference on Machine Learning},
	Date-Added = {2017-08-14 01:30:33 +0000},
	Date-Modified = {2017-11-23 20:31:34 +0000},
	Keywords = {me},
	Pages = {654-663},
	Title = {`{C}onvex Until Proven Guilty': {d}imension-Free Acceleration of Gradient Descent on Non-Convex Functions},
	Year = {2017}}

@article{polaknote,
	Author = {Polak, E and Ribi{\`e}re, G},
	Date-Added = {2017-02-23 18:49:52 +0000},
	Date-Modified = {2017-02-23 19:30:39 +0000},
	Journal = {Rev. Fr. Inform. Rech. Oper. v16},
	Pages = {35--43},
	Title = {Note sur la convergence de directions conjug{\'e}es},
	Year = {1969}}

@article{beck2009gradient,
	Author = {Beck, Amir and Teboulle, Marc},
	Date-Added = {2017-02-23 07:45:14 +0000},
	Date-Modified = {2017-02-23 07:45:14 +0000},
	Journal = {Convex optimization in signal processing and communications},
	Pages = {42--88},
	Title = {Gradient-based algorithms with applications to signal recovery},
	Year = {2009}}

@article{beaton1974fitting,
	Author = {Beaton, Albert E and Tukey, John W},
	Date-Added = {2017-02-23 05:18:52 +0000},
	Date-Modified = {2017-02-23 05:18:52 +0000},
	Journal = {Technometrics},
	Number = {2},
	Pages = {147--185},
	Publisher = {Taylor \&amp; Francis Group},
	Title = {The fitting of power series, meaning polynomials, illustrated on band-spectroscopic data},
	Volume = {16},
	Year = {1974}}

@incollection{RumelhartHiWi86,
	Author = {Rumelhart, David E. and Geoffrey E. Hinton and Ronald J. Williams},
	Booktitle = {Parallel Distributed Processing -- Explorations in the Microstructure of Cognition},
	Chapter = 8,
	Comment = {Classic paper introducing the generalized delta rule (back-propagation).},
	Editor = {David E. Rumelhart and James L. McClelland},
	Pages = {318--362},
	Publisher = {MIT Press},
	Title = {Learning Internal Representations by Error Propagation},
	Year = 1986}

@inproceedings{MairalBaPoSaZi08,
	Author = {Julien Mairal and Francis Bach and Jean Ponce and Guillermo Sapiro and Andrew Zisserman},
	Booktitle = nips21,
	Title = {Supervised Dictionary Learning},
	Year = 2008}

@article{BeckTe09,
	Author = {Amir Beck and Marc Teboulle},
	Journal = {SIAM Journal of Imaging Sciences},
	Number = 1,
	Pages = {183--202},
	Title = {A fast iterative shrinkage-thresholding algorithm for linear inverse problems},
	Volume = 2,
	Year = 2009}

@article{o2015adaptive,
	Author = {O'Donoghue, Brendan and Cand{\`e}s, Emmanuel},
	Date-Added = {2017-02-19 19:06:22 +0000},
	Date-Modified = {2017-02-19 19:06:22 +0000},
	Journal = {Foundations of Computational Mathematics},
	Number = {3},
	Pages = {715--732},
	Publisher = {Springer},
	Title = {Adaptive restart for accelerated gradient schemes},
	Volume = {15},
	Year = {2015}}

@article{powell1970survey,
	Author = {Powell, MJD},
	Date-Added = {2017-02-15 07:00:09 +0000},
	Date-Modified = {2017-02-15 07:00:09 +0000},
	Journal = {SIAM Review},
	Number = {1},
	Pages = {79--97},
	Publisher = {SIAM},
	Title = {A survey of numerical methods for unconstrained optimization},
	Volume = {12},
	Year = {1970}}

@article{hager2006survey,
	Author = {Hager, William W and Zhang, Hongchao},
	Date-Added = {2017-02-15 06:55:53 +0000},
	Date-Modified = {2017-02-15 06:55:53 +0000},
	Journal = {Pacific Journal of Optimization},
	Number = {1},
	Pages = {35--58},
	Title = {A survey of nonlinear conjugate gradient methods},
	Volume = {2},
	Year = {2006}}

@article{liu1989limited,
	Author = {Liu, Dong C and Nocedal, Jorge},
	Date-Added = {2017-02-15 06:55:01 +0000},
	Date-Modified = {2017-02-15 06:55:01 +0000},
	Journal = {Mathematical Programming},
	Number = {1},
	Pages = {503--528},
	Publisher = {Springer},
	Title = {On the limited memory {BFGS} method for large scale optimization},
	Volume = {45},
	Year = {1989}}

@article{schmidhuber2015deep,
	Author = {Schmidhuber, J{\"u}rgen},
	Date-Added = {2017-02-15 06:50:44 +0000},
	Date-Modified = {2017-02-15 06:50:44 +0000},
	Journal = {Neural networks},
	Pages = {85--117},
	Publisher = {Elsevier},
	Title = {Deep learning in neural networks: An overview},
	Volume = {61},
	Year = {2015}}

@inproceedings{kingma2014adam,
	Author = {Kingma, Diederik and Ba, Jimmy},
	Booktitle = {International Conference on Learning Representations},
	Title = {Adam: A method for stochastic optimization},
	Year = {2015}}

@article{bottou2016optimization,
	Author = {Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
	Date-Added = {2017-02-15 06:45:45 +0000},
	Date-Modified = {2017-02-15 06:46:18 +0000},
	Journal = {arXiv preprint arXiv:1606.04838},
	Keywords = {mom-SGD},
	Title = {Optimization methods for large-scale machine learning},
	Year = {2016}}

@article{sutskever2013importance,
	Author = {Sutskever, Ilya and Martens, James and Dahl, George E and Hinton, Geoffrey E},
	Date-Added = {2017-02-15 06:42:41 +0000},
	Date-Modified = {2017-02-15 06:43:07 +0000},
	Journal = {ICML (3)},
	Keywords = {mom-SGD},
	Pages = {1139--1147},
	Title = {On the importance of initialization and momentum in deep learning.},
	Volume = {28},
	Year = {2013}}

@book{NocedalWr06,
	Author = {Jorge Nocedal and Stephen J. Wright},
	Publisher = {Springer},
	Title = {Numerical Optimization},
	Year = 2006}

@article{CandesLiSo15,
	Author = {Emmanuel J. Cand{\`e}s and Xiaodong Li and Mahdi Soltanolkotabi},
	Journal = ieeeit,
	Number = 4,
	Pages = {1985--2007},
	Title = {Phase Retrieval via {W}irtinger Flow: Theory and Algorithms},
	Volume = 61,
	Year = 2015}

@article{koren2009matrix,
	Author = {Koren, Yehuda and Bell, Robert and Volinsky, Chris},
	Date-Added = {2017-02-14 23:42:12 +0000},
	Date-Modified = {2017-02-14 23:42:12 +0000},
	Journal = {Computer},
	Number = {8},
	Publisher = {IEEE},
	Title = {Matrix factorization techniques for recommender systems},
	Volume = {42},
	Year = {2009}}

@article{lee1999learning,
	Author = {Lee, Daniel D and Seung, H Sebastian},
	Date-Added = {2017-02-14 23:40:58 +0000},
	Date-Modified = {2017-02-14 23:40:58 +0000},
	Journal = {Nature},
	Number = {6755},
	Pages = {788--791},
	Publisher = {Nature Publishing Group},
	Title = {Learning the parts of objects by non-negative matrix factorization},
	Volume = {401},
	Year = {1999}}

@article{rosenbrock1960automatic,
	Author = {Rosenbrock, HoHo},
	Date-Added = {2017-02-14 01:56:10 +0000},
	Date-Modified = {2017-02-14 01:56:10 +0000},
	Journal = {The Computer Journal},
	Number = {3},
	Pages = {175--184},
	Publisher = {Br Computer Soc},
	Title = {An automatic method for finding the greatest or least value of a function},
	Volume = {3},
	Year = {1960}}

@article{carmon2016accelerated,
	Author = {Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
	Date-Added = {2017-02-08 06:06:37 +0000},
	Date-Modified = {2018-05-22 00:04:54 +0000},
	Journal = {arXiv preprint arXiv:1611.00756},
	Keywords = {fast-nonconvex-theory; me},
	Title = {Accelerated methods for non-convex optimization},
	Year = {2016}}

@book{RockafellarWe98,
	Address = {New York},
	Author = {R. T. Rockafellar and R. J. B. Wets},
	Publisher = {Springer},
	Title = {Variational Analysis},
	Year = {1998}}

@book{AdlerTa09,
	Author = {Robert J. Adler and Jonathan E. Taylor},
	Publisher = {Springer},
	Title = {Random fields and geometry},
	Year = 2009}

@article{SunQuWr15,
	Author = {Ju Sun and Qing Qu and John Wright},
	Journal = {arXiv:1510.06096 [math.OC]},
	Title = {When Are Nonconvex Problems Not Scary?},
	Year = 2015}

@incollection{Nesterov00,
	Author = {Yurii Nesterov},
	Booktitle = {High Performance Optimization},
	Pages = {405--440},
	Publisher = {Springer},
	Series = {Applied Optimization},
	Title = {Squared Functional Systems and Optimization Problems},
	Volume = 33,
	Year = 2000}

@article{MurtyKa87,
	Author = {Katta Murty and Santosh Kabadi},
	Comment = {Shows how even deciding whether the point $x = 0$ is a local minimum for the optimization problem $\min. x^T A x$ s.t. $x \ge 0$, where $A$ is indefinite, is NP hard. Reduces problem to subset sum.},
	Journal = mathprog,
	Pages = {117--129},
	Title = {Some {NP}-Complete Problems in Quadratic and Nonlinear Programming},
	Volume = 39,
	Year = 1987}

@article{cartis2011adaptive,
	Author = {Cartis, Coralia and Gould, Nicholas IM and Toint, Philippe L},
	Journal = {Mathematical Programming},
	Number = {2},
	Pages = {245--295},
	Publisher = {Springer},
	Title = {Adaptive cubic regularisation methods for unconstrained optimization. {P}art {I}: motivation, convergence and numerical results},
	Volume = {127},
	Year = {2011}}

@article{bianconcini2015use,
	Author = {Bianconcini, Tommaso and Liuzzi, Giampaolo and Morini, Benedetta and Sciandrone, Marco},
	Journal = {Computational Optimization and Applications},
	Number = {1},
	Pages = {35--57},
	Publisher = {Springer},
	Title = {On the use of iterative methods in cubic regularization for unconstrained optimization},
	Volume = {60},
	Year = {2015}}

@article{agarwal2016finding,
	Author = {Agarwal, Naman and Allen-Zhu, Zeyuan and Bullins, Brian and Hazan, Elad and Ma, Tengyu},
	Date-Modified = {2018-05-22 00:08:55 +0000},
	Journal = {Symposium on Theory of Computing},
	Keywords = {fast-nonconvex-theory},
	Title = {Finding Approximate Local Minima for Nonconvex Optimization in Linear Time},
	Year = {2017}}

@inproceedings{orecchia2012approximating,
	Author = {Orecchia, Lorenzo and Sachdeva, Sushant and Vishnoi, Nisheeth K},
	Booktitle = {Proceedings of the forty-fourth annual ACM symposium on Theory of computing},
	Date-Added = {2016-11-02 18:37:04 +0000},
	Date-Modified = {2016-11-02 18:37:04 +0000},
	Organization = {ACM},
	Pages = {1141--1160},
	Title = {Approximating the exponential, the lanczos method and an O (m)-time spectral algorithm for balanced separator},
	Year = {2012}}

@article{bubeck2015geometric,
	Author = {Bubeck, S{\'e}bastien and Lee, Yin Tat and Singh, Mohit},
	Date-Added = {2016-10-31 17:04:14 +0000},
	Date-Modified = {2016-10-31 17:04:14 +0000},
	Journal = {arXiv preprint arXiv:1506.08187},
	Title = {A geometric alternative to {N}esterov's accelerated gradient descent},
	Year = {2015}}

@article{huo2016asynchronous,
	Author = {Huo, Zhouyuan and Huang, Heng},
	Date-Added = {2016-10-31 06:33:11 +0000},
	Date-Modified = {2016-10-31 06:33:11 +0000},
	Journal = {arXiv preprint arXiv:1604.03584},
	Title = {Asynchronous Stochastic Gradient Descent with Variance Reduction for Non-Convex Optimization},
	Year = {2016}}

@book{NemirovskiYu83,
	Author = {A. Nemirovski and D. Yudin},
	Comment = {Huge book that shows all the lower complexity results for convex optimization. Very very difficult and also hard to find.},
	Publisher = {Wiley},
	Title = {Problem Complexity and Method Efficiency in Optimization},
	Year = 1983}

@article{ge2011note,
	Author = {Ge, Dongdong and Jiang, Xiaoye and Ye, Yinyu},
	Date-Added = {2016-10-29 02:35:15 +0000},
	Date-Modified = {2016-10-29 02:35:15 +0000},
	Journal = {Mathematical programming},
	Number = {2},
	Pages = {285--299},
	Publisher = {Springer},
	Title = {A note on the complexity of $L_p$ minimization},
	Volume = {129},
	Year = {2011}}

@article{GhadimiLa13,
	Author = {Saeed Ghadimi and Guanghui Lan},
	Journal = siopt,
	Number = 4,
	Pages = {2341--2368},
	Title = {Stochastic first- and zeroth-order methods for nonconvex stochastic programming},
	Volume = 23,
	Year = 2013}

@inproceedings{JohnsonZh13,
	Author = {Rie Johnson and Tong Zhang},
	Booktitle = nips26,
	Date-Modified = {2018-02-15 23:04:40 +0000},
	Keywords = {fast-nonconvex-theory},
	Title = {Accelerating Stochastic Gradient Descent using Predictive Variance Reduction},
	Year = 2013}

@article{ye1998complexity,
	Author = {Ye, Yinyu},
	Date-Added = {2016-10-29 02:13:15 +0000},
	Date-Modified = {2016-10-29 02:13:15 +0000},
	Journal = {Mathematical Programming},
	Number = {2},
	Pages = {195--211},
	Publisher = {Springer},
	Title = {On the complexity of approximating a {KKT} point of quadratic programming},
	Volume = {80},
	Year = {1998}}

@article{zhuEvenFaster,
	Author = {Zeyuan {Allen Zhu} and Yuanzhi Li},
	Journal = {arXiv preprint arXiv:1607.03463},
	Title = {Even Faster {SVD} Decomposition Yet Without Agonizing Pain},
	Url = {http://arxiv.org/abs/1607.03463},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1607.03463}}

@article{garber2016faster,
	Author = {Garber, Dan and Hazan, Elad and Jin, Chi and Kakade, Sham M and Musco, Cameron and Netrapalli, Praneeth and Sidford, Aaron},
	Journal = {arXiv preprint arXiv:1605.08754},
	Title = {Faster Eigenvector Computation via Shift-and-Invert Preconditioning},
	Year = {2016}}

@article{wang2016linear,
	Author = {Wang, Jiulin and Xia, Yong},
	Doi = {10.1007/s11590-016-1070-0},
	Journal = {Optimization Letters},
	Keywords = {cubic},
	Title = {A linear-time algorithm for the trust region subproblem based on hidden convexity},
	Volume = {Online First},
	Year = {2016},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s11590-016-1070-0}}

@article{hazan2016linear,
	Author = {Hazan, Elad and Koren, Tomer},
	Journal = {Mathematical Programming},
	Keywords = {cubic},
	Number = 1,
	Pages = {363--381},
	Publisher = {Springer},
	Title = {A linear-time algorithm for trust region problems},
	Volume = 158,
	Year = {2016}}

@article{pearlmutter1994fast,
	Author = {Pearlmutter, Barak A},
	Journal = {Neural computation},
	Number = {1},
	Pages = {147--160},
	Publisher = {MIT Press},
	Title = {Fast exact multiplication by the {H}essian},
	Volume = {6},
	Year = {1994}}

@article{schraudolph2002fast,
	Author = {Schraudolph, Nicol N},
	Journal = {Neural computation},
	Number = {7},
	Pages = {1723--1738},
	Publisher = {MIT Press},
	Title = {Fast curvature matrix-vector products for second-order gradient descent},
	Volume = {14},
	Year = {2002}}

@inproceedings{williams2012multiplying,
	Author = {Williams, Virginia Vassilevska},
	Booktitle = {Proceedings of the forty-fourth annual ACM symposium on Theory of computing},
	Date-Added = {2016-10-26 21:39:50 +0000},
	Date-Modified = {2016-10-26 21:39:50 +0000},
	Organization = {ACM},
	Pages = {887--898},
	Title = {Multiplying matrices faster than {C}oppersmith-{W}inograd},
	Year = {2012}}

@article{WangGiEl16,
	Author = {Gang Wang and Georgios B. Giannakis and Yonina C. Eldar},
	Journal = {arXiv:1605.08285 [stat.ML]},
	Title = {Solving Systems of Random Quadratic Equations via Truncated Amplitude Flow},
	Year = 2016}

@book{Nesterov04,
	Author = {Y. Nesterov},
	Publisher = {Kluwer Academic Publishers},
	Title = {Introductory Lectures on Convex Optimization},
	Year = {2004}}

@inproceedings{shalev2014accelerated,
	Author = {Shalev-Shwartz, Shai and Zhang, Tong},
	Booktitle = {ICML},
	Date-Added = {2016-10-24 16:48:39 +0000},
	Date-Modified = {2016-10-26 22:13:05 +0000},
	Keywords = {proximal},
	Pages = {64--72},
	Title = {Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized Loss Minimization.},
	Year = {2014}}

@inproceedings{lin2015universal,
	Author = {Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
	Booktitle = {Advances in Neural Information Processing Systems},
	Date-Added = {2016-10-24 16:48:12 +0000},
	Date-Modified = {2016-10-26 22:13:08 +0000},
	Keywords = {proximal},
	Pages = {3384--3392},
	Title = {A universal catalyst for first-order optimization},
	Year = {2015}}

@inproceedings{frostig2015regularizing,
	Author = {Frostig, Roy and Ge, Rong and Kakade, Sham M and Sidford, Aaron},
	Booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML)},
	Date-Added = {2016-10-24 03:48:45 +0000},
	Date-Modified = {2016-10-26 22:12:19 +0000},
	Keywords = {proximal},
	Title = {Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization},
	Year = {2015}}

@article{allen2014linear,
	Author = {Allen-Zhu, Zeyuan and Orecchia, Lorenzo},
	Date-Added = {2016-10-23 23:44:34 +0000},
	Date-Modified = {2016-10-26 22:12:26 +0000},
	Journal = {arXiv preprint arXiv:1407.1537},
	Keywords = {ACG},
	Title = {Linear coupling: An ultimate unification of gradient and mirror descent},
	Year = {2014}}

@inproceedings{ge2015escaping,
	Author = {Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
	Booktitle = {Proceedings of The 28th Conference on Learning Theory},
	Date-Added = {2016-10-23 22:35:59 +0000},
	Date-Modified = {2017-01-24 18:08:23 +0000},
	Keywords = {*, strict-saddle},
	Pages = {797--842},
	Title = {Escaping from saddle points---online stochastic gradient for tensor decomposition},
	Year = {2015}}

@inproceedings{lee2016gradient,
	Author = {Lee, Jason D and Simchowitz, Max and Jordan, Michael I and Recht, Benjamin},
	Booktitle = {29th Annual Conference on Learning Theory (COLT)},
	Date-Modified = {2017-01-24 18:13:14 +0000},
	Keywords = {strict-saddle},
	Pages = {1246---1257},
	Title = {Gradient descent only converges to minimizers},
	Year = {2016}}

@article{nesterov1983method,
	Author = {Nesterov, Yurii},
	Journal = {Soviet Mathematics Doklady},
	Keywords = {ACG; convex},
	Number = {2},
	Pages = {372--376},
	Title = {A method of solving a convex programming problem with convergence rate ${O}(1/k^2)$},
	Volume = {27},
	Year = {1983}}

@article{birgin2015worst,
	Author = {Birgin, EG and Gardenghi, JL and Mart{\i}nez, JM and Santos, SA and Toint, Ph L},
	Date-Added = {2016-10-22 18:24:21 +0000},
	Date-Modified = {2016-10-22 18:24:26 +0000},
	Journal = {Report naXys-05-2015, University of Namur, Belgium},
	Keywords = {nonconvex},
	Title = {Worst-case evaluation complexity for unconstrained nonlinear optimization using high-order regularized models},
	Year = {2015}}

@article{cartis2010complexity,
	Abstract = {It is shown that the steepest-descent and Newton's methods for unconstrained nonconvex optimization under standard assumptions may both require a number of iterations and function evaluations arbitrarily close to $O(\epsilon^{-2})$ to drive the norm of the gradient below $\epsilon$. This shows that the upper bound of $O(\epsilon^{-2})$ evaluations known for the steepest descent is tight and that Newton's method may be as slow as the steepest-descent method in the worst case. The improved evaluation complexity bound of $O(\epsilon^{-3/2})$ evaluations known for cubically regularized Newton's methods is also shown to be tight.

Read More: http://epubs.siam.org/doi/abs/10.1137/090774100},
	Author = {Cartis, Coralia and Gould, Nicholas IM and Toint, Ph L},
	Date-Added = {2016-10-19 05:06:11 +0000},
	Date-Modified = {2016-10-19 05:07:04 +0000},
	Journal = {{SIAM} journal on optimization},
	Keywords = {nonconvex; cubic},
	Number = {6},
	Pages = {2833--2852},
	Publisher = {SIAM},
	Title = {On the complexity of steepest descent, {N}ewton's and regularized {N}ewton's methods for nonconvex unconstrained optimization problems},
	Volume = {20},
	Year = {2010}}

@article{anandkumar2016efficient,
	Author = {Anandkumar, Anima and Ge, Rong},
	Date-Added = {2016-10-19 02:08:47 +0000},
	Date-Modified = {2016-10-19 02:11:52 +0000},
	Journal = {arXiv preprint arXiv:1602.05908},
	Keywords = {cu\left( ic; nonconvex; eigenvalues; *},
	Title = {Efficient approaches for escaping higher order saddle points in non-convex optimization},
	Year = {2016}}

@book{BoydVa04,
	Author = {Stephen Boyd and Lieven Vandenberghe},
	Publisher = {Cambridge University Press},
	Title = {Convex Optimization},
	Year = {2004}}

@article{parikh2014proximal,
	Author = {Parikh, Neal and Boyd, Stephen P and others},
	Date-Added = {2016-10-18 03:43:18 +0000},
	Date-Modified = {2016-10-18 03:44:11 +0000},
	Journal = {Foundations and Trends in optimization},
	Keywords = {convex, ACG, projected},
	Number = {3},
	Pages = {127--239},
	Title = {Proximal Algorithms.},
	Volume = {1},
	Year = {2014}}

@article{bubeck2014convex,
	Author = {Bubeck, S{\'e}bastien},
	Date-Added = {2016-10-18 03:18:12 +0000},
	Date-Modified = {2016-10-18 03:18:34 +0000},
	Journal = {arXiv preprint arXiv:1405.4980},
	Keywords = {convex; ACG},
	Title = {Convex optimization: Algorithms and complexity},
	Year = {2014}}

@article{NesterovGradSmall2012,
	Annote = {Short discussion article. Very much worth reading.},
	Author = {Yurii Nesterov},
	Date-Added = {2016-10-17 21:21:30 +0000},
	Date-Modified = {2016-10-24 04:09:48 +0000},
	Journal = {Optima 88},
	Keywords = {cubic; nonconvex; ACG; convex; proximal},
	Title = {How to Make the Gradients Small},
	Year = {2012}}

@article{nesterov2006cubic,
	Abstract = {In this paper, we provide theoretical analysis for a cubic regularization of Newton method as
applied to unconstrained minimization problem. For this scheme, we prove general local convergence results. However, the main contribution of the paper is related to global worst-case complexity bounds for different problem classes including some nonconvex cases. It is shown that the search direction can be computed by standard linear algebra technique.},
	Author = {Nesterov, Yurii and Polyak, Boris T},
	Date-Added = {2016-10-17 20:17:03 +0000},
	Date-Modified = {2016-10-17 20:19:10 +0000},
	Journal = {Mathematical Programming},
	Keywords = {cubic, nonconvex},
	Number = {1},
	Pages = {177--205},
	Publisher = {Springer},
	Title = {Cubic regularization of {N}ewton method and its global performance},
	Volume = {108},
	Year = {2006}}

@article{ghadimi2015generalized,
	Abstract = {In this paper, we present a generic framework to extend existing uniformly optimal convex
programming algorithms to solve more general nonlinear, possibly nonconvex, optimization problems. The basic idea is to incorporate a local search step (gradient descent or Quasi-Newton iteration) into these uniformly optimal convex programming methods, and then enforce a monotone decreasing property of the function values computed along the trajectory. Algorithms of these types will then achieve the best known complexity for nonconvex problems, and the optimal complexity for convex ones without requiring any problem parameters. As a consequence, we can have a unified treatment for a general class of nonlinear programming problems regardless of their convexity and smoothness level. In particular, we show that the accelerated gradient and level methods, both originally designed for solving convex optimization problems only, can be used for solving both convex and nonconvex problems uniformly. In a similar vein, we show that some well-studied techniques for nonlinear programming, e.g., Quasi-Newton iteration, can be embedded into optimal convex optimization algorithms to possibly further enhance their numerical performance. Our theoretical and algorithmic developments are complemented by some promising numerical results obtained for solving a few important nonconvex and nonlinear data analysis problems in the literature.},
	Author = {Ghadimi, Saeed and Lan, Guanghui and Zhang, Hongchao},
	Date-Added = {2016-10-17 20:10:54 +0000},
	Date-Modified = {2016-10-17 20:11:17 +0000},
	Journal = {arXiv preprint arXiv:1508.07384},
	Keywords = {ACG; nonconvex},
	Title = {Generalized Uniformly Optimal Methods for Nonlinear Programming},
	Year = {2015}}

@article{ghadimi2016accelerated,
	Abstract = {In this paper, we generalize the well-known Nesterov's accelerated gradient
(AG) method, originally designed for convex smooth optimization, to solve
nonconvex and possibly stochastic optimization problems. We demonstrate that by
properly specifying the stepsize policy, the AG method exhibits the best known rate of
convergence for solving general nonconvex smooth optimization problems by using
first-order information, similarly to the gradient descent method. We then consider an
important class of composite optimization problems and show that the AG method can
solve them uniformly, i.e., by using the same aggressive stepsize policy as in the convex
case, even if the problem turns out to be nonconvex. We demonstrate that the AG
method exhibits an optimal rate of convergence if the composite problem is convex,
and improves the best known rate of convergence if the problem is nonconvex. Based
on the AG method, we also present new nonconvex stochastic approximation methods
and show that they can improve a few existing rates of convergence for nonconvex
stochastic optimization. To the best of our knowledge, this is the first time that the
convergence of the AG method has been established for solving nonconvex nonlinear
programming in the literature.},
	Author = {Ghadimi, Saeed and Lan, Guanghui},
	Date-Added = {2016-10-17 20:04:34 +0000},
	Date-Modified = {2016-10-17 20:05:13 +0000},
	Journal = {Mathematical Programming},
	Keywords = {ACG; nonconvex; *},
	Number = {1-2},
	Pages = {59--99},
	Publisher = {Springer},
	Title = {Accelerated gradient methods for nonconvex nonlinear and stochastic programming},
	Volume = {156},
	Year = {2016}}

@inproceedings{NIPS2015_5728,
	Abstract = {Nonconvex and nonsmooth problems have recently received considerable attention in signal/image processing, statistics and machine learning. However, solving the nonconvex and nonsmooth optimization problems remains a big challenge. Accelerated proximal gradient (APG) is an excellent method for convex programming. However, it is still unknown whether the usual APG can ensure the convergence to a critical point in nonconvex programming. In this paper, we extend APG for general nonconvex and nonsmooth programs by introducing a monitor that satisfies the sufficient descent property. Accordingly, we propose a monotone
APG and a nonmonotone APG. The latter waives the requirement on monotonic reduction of the objective function and needs less computation in each iteration. To the best of our knowledge, we are the first to provide APG-type algorithms for general nonconvex and nonsmooth problems ensuring that every accumulation point is a critical point, and the convergence rates remain O(k^2) when the problems are convex, in which k is the number of iterations. Numerical results testify to the advantage of our algorithms in speed.},
	Annote = {``To the best of our knowledge, we are the first to provide APG-type algorithms
for general nonconvex and nonsmooth problems ensuring that every accumulation
point is a critical point, and the convergence rates remain O(k^2) when the problems
are convex''},
	Author = {Li, Huan and Lin, Zhouchen},
	Booktitle = {Advances in Neural Information Processing Systems 28},
	Keywords = {ACG, nonconvex},
	Pages = {379--387},
	Title = {Accelerated Proximal Gradient Methods for Nonconvex Programming},
	Year = {2015},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/5728-accelerated-proximal-gradient-methods-for-nonconvex-programming.pdf}}

@article{allen2015improved,
	Abstract = {Many classical algorithms are found until several years later to outlive the confines in which they were conceived, and continue to be relevant in unforeseen settings. In this paper, we show that SVRG is one such method: being originally designed for strongly convex objectives, it is also very robust in non-strongly convex or sum-of-non-convex settings.
More precisely, we provide new analysis to improve the state-of-the-art running times in both settings by either applying SVRG or its novel variant. Since non-strongly convex objectives include important examples such as Lasso or logistic regression, and sum-of-non-convex objectives include famous examples such as stochastic PCA and is even believed to be related to training deep neural nets, our results also imply better performances in these applications.},
	Author = {Allen-Zhu, Zeyuan and Yuan, Yang},
	Date-Added = {2016-10-17 19:50:11 +0000},
	Date-Modified = {2016-10-22 19:14:19 +0000},
	Journal = {arXiv preprint arXiv:1506.01972},
	Keywords = {nonconvex; stochastic},
	Title = {Improved SVRG for non-strongly-convex or sum-of-non-convex objectives},
	Year = {2015}}

@article{reddi2016stochastic,
	Abstract = {We study nonconvex finite-sum problems and analyze stochastic variance reduced gradient (SVRG) methods for them. SVRG and related methods have recently surged into prominence for convex optimization given their edge over stochastic gradient descent (SGD); but their theoretical analysis almost exclusively assumes convexity. In contrast, we prove non-asymptotic rates of convergence (to stationary points) of SVRG for nonconvex optimization, and show that it is provably faster than SGD and gradient descent. We also analyze a subclass of nonconvex problems on which SVRG attains linear convergence to the global optimum. We extend our analysis to mini-batch variants of SVRG, showing (theoretical) linear speedup due to mini-batching in parallel settings.
},
	Author = {Reddi, Sashank J and Hefny, Ahmed and Sra, Suvrit and P{\'o}cz{\'o}s, Barnab{\'a}s and Smola, Alex},
	Date-Added = {2016-10-17 19:49:05 +0000},
	Date-Modified = {2016-10-22 19:14:12 +0000},
	Journal = {arXiv preprint arXiv:1603.06160},
	Keywords = {nonconvex, stochastic},
	Title = {Stochastic Variance Reduction for Nonconvex Optimization},
	Year = {2016}}

@inproceedings{DefazioBaLa14,
	Author = {Aaron Defazio and Francis Bach and Simon Lacoste-Julien},
	Booktitle = nips27,
	Title = {{SAGA}: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives},
	Year = 2014}

@article{allen2016variance,
	Abstract = {We consider the fundamental problem in non-convex optimization of efficiently reaching a stationary point. In contrast to the convex case, in the long history of this basic problem, the only known theoretical results on first-order non-convex optimization remain to be full gradient descent that converges in O(1/ε) iterations for smooth objectives, and stochastic gradient descent that converges in O(1/ε2) iterations for objectives that are sum of smooth functions. We provide the first improvement in this line of research. Our result is based on the variance reduction trick recently introduced to convex optimization, as well as a brand new analysis of variance reduction that is suitable for non-convex optimization. For objectives that are sum of smooth functions, our first-order minibatch stochastic method converges with an O(1/ε) rate, and is faster than full gradient descent by Ω(n1/3). 
We demonstrate the effectiveness of our methods on empirical risk minimizations with non-convex loss functions and training neural nets.},
	Author = {Allen-Zhu, Zeyuan and Hazan, Elad},
	Date-Added = {2016-10-17 19:48:21 +0000},
	Date-Modified = {2016-10-22 19:14:15 +0000},
	Journal = {arXiv preprint arXiv:1603.05643},
	Keywords = {nonconvex, *; stochastic},
	Title = {Variance reduction for faster non-convex optimization},
	Year = {2016}}

@article{kuczynski1992estimating,
	Author = {Kuczynski, Jacek and Wozniakowski, Henryk},
	Date-Added = {2016-10-17 18:11:50 +0000},
	Date-Modified = {2016-10-17 18:12:04 +0000},
	Journal = {SIAM journal on matrix analysis and applications},
	Keywords = {eigenvalues},
	Number = {4},
	Pages = {1094--1122},
	Publisher = {SIAM},
	Title = {Estimating the largest eigenvalue by the power and Lanczos algorithms with a random start},
	Volume = {13},
	Year = {1992}}

@article{Ho-NguyenKi16,
	Author = {Nam Ho-Nguyen and Fatma K{\i}l{\i}n{c}-Karzan},
	Journal = {arXiv preprint arXiv:1603.03366},
	Title = {A Second-Order Cone Based Approach for Solving the Trust-Region Subproblem and Its Variants},
	Year = 2016}

@article{lecun2015deep,
	Author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	Journal = {Nature},
	Number = {7553},
	Pages = {436--444},
	Publisher = {Nature Research},
	Title = {Deep learning},
	Volume = {521},
	Year = {2015}}

@unpublished{Nemirovski1999,
	Author = {Arkadi Nemirovski},
	Note = {Technion -- Israel Institute of Technology},
	Title = {Optimization {II}: Standard Numerical Methods for Nonlinear Continuous Optimization},
	Url = {http://www2.isye.gatech.edu/~nemirovs/Lect_OptII.pdf},
	Year = 1999,
	Bdsk-Url-1 = {http://www2.isye.gatech.edu/~nemirovs/Lect_OptII.pdf}}

@misc{lecun1998mnist,
	Author = {Le{C}un, Yann and Cortes, Corinna and Burges, Christopher {JC}},
	Title = {The {MNIST} database of handwritten digits},
	Year = {1998}}

@inproceedings{goodfellow2014qualitatively,
	Author = {Goodfellow, Ian J and Vinyals, Oriol and Saxe, Andrew M},
	Booktitle = {International Conference on Learning Representations},
	Title = {Qualitatively characterizing neural network optimization problems},
	Year = {2015}}

@inproceedings{glorot2010understanding,
	Author = {Glorot, Xavier and Bengio, Yoshua},
	Booktitle = {Aistats},
	Pages = {249--256},
	Title = {Understanding the difficulty of training deep feedforward neural networks.},
	Volume = {9},
	Year = {2010}}
