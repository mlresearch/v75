
%\def\format{COLT}

\newif\ifCOLT
\COLTtrue
\newif\ifSIAM
\SIAMfalse
\newif\ifARXIV
\ARXIVfalse
%%%%%
%% 
%%%%%
\newif\ifARXIVorSIAM
\ifSIAM
\ARXIVorSIAMtrue
\fi
\ifARXIV
\ARXIVorSIAMtrue
\fi
%
\newif\ifCOLTorSIAM
\ifSIAM
\COLTorSIAMtrue
\fi
\ifCOLT
\COLTorSIAMtrue
\fi

\ifCOLT
\documentclass[12pt,final]{colt2018} % colt style
\fi
\ifSIAM
\documentclass[review]{siamart1116}
\fi
\ifARXIV
\documentclass[11pt]{article}
\usepackage{graphicx,fancyhdr,amsmath,amssymb,subfig,url}
\usepackage{amsthm}
\fi
\ifARXIVorSIAM
\usepackage[numbers,square]{natbib}
\fi

\usepackage[algo2e]{algorithm2e} 

%\documentclass[12pt]{article} % arvix style
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

%%%
%%% PREAMBLE BEGIN

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e
\usepackage{physics}
\usepackage{float}
\usepackage{enumerate}
\usepackage{thm-restate}
\usepackage{times}

\newtheorem{assumption}{Assumption}
%\newtheorem{lem}{lem}
%\newtheorem{lem}[lemma]{lem}

\declaretheorem[name=Theorem]{thm}
\declaretheorem[name=Lemma]{lem}

\newcommand{\ones}{\mathbf 1}
\newcommand{\zeros}{\mathbf 0}

\newcommand{\reals}{{\mbox{\bf R}}}
\newcommand{\integers}{{\mbox{\bf Z}}}
\newcommand{\symm}{{\mbox{\bf S}}}  % symmetric matrices
\newcommand{\plusLog}{\mathop{\rm log^{+}}}


\newcommand{\nullspace}{{\mathcal N}}
\newcommand{\range}{{\mathcal R}}
\newcommand{\Rank}{\mathop{\bf Rank}}
%\newcommand{\Tr}{\mathop{\bf Tr}}
\newcommand{\diag}{\mathop{\bf diag}}
\newcommand{\card}{\mathop{\bf card}}
%\newcommand{\rank}{\mathop{\bf rank}}
\newcommand{\conv}{\mathop{\bf conv}}
\newcommand{\prox}{\mathbf{prox}}

\newcommand{\Expect}{\mathop{\bf E{}}}
\newcommand{\Prob}{\mathop{\bf Prob}}
\newcommand{\Co}{{\mathop {\bf Co}}} % convex hull
\newcommand{\dist}{\mathop{\bf dist{}}}
\newcommand{\argmin}{\mathop{\rm argmin}}
\newcommand{\argmax}{\mathop{\rm argmax}}
\newcommand{\epi}{\mathop{\bf epi}} % epigraph
\newcommand{\Vol}{\mathop{\bf vol}}
\newcommand{\dom}{\mathop{\bf dom}} % domain
\newcommand{\intr}{\mathop{\bf int}}
\newcommand{\sign}{\mathop{\bf sign}}

\newcommand{\cf}{{\it cf.}}
\newcommand{\eg}{{\it e.g.}}
\newcommand{\ie}{{\it i.e.}}
\newcommand{\etc}{{\it etc.}}

\newcommand\ind[1]{^{(#1)}}
\newcommand\st[1]{_{(#1)}}
\def\LipZero{L_{0}}
\def\LipFirst{L_{1}}
\def\LipSecond{L_{2}}
\def\LipFirstHat{\hat{L}_{1}}
\def\epsHat{\hat{\epsilon}}
\def\fHat{\hat{f}}
\def\LipThird{L_{3}}
\def\pLip{L_{p}}
\def\e{e}

\def\SetLip{Q}

\def\N{N}
\def\Radius{R}
\def\Dim{d}

\def\TimeCenter{T_{C}}
\def\TimeGrad{T_{1}}
\def\TimeHess{T_{2}}
\def\TimeCube{T_{3}}
\def\TimeP{T_{p}}

  \def\xBest{x_{\text{best}}}
  \def\fProx{f_{\text{prox}}}
\newcommand\vol[1]{\mathbf{vol}\left(#1\right)}
\newcommand\ball[2]{\mathbf{B}_{#1}\left(#2\right)}
\def\R{\mathbb{R}}
\def\zerovec{\vec{0}}

\newcommand\hinder[1]{{\color{red} #1}}


\title{Cutting plane methods can be extended into nonconvex optimization$^1$}

\ifCOLT
 \coltauthor{\Name{Oliver Hinder} \Email{ohinder@stanford.edu}\\
 \addr Management Science and Engineering Department. Stanford.
  }
\fi

\ifARXIVorSIAM
\author{Oliver Hinder\thanks{Supported at Stanford by the PACCAR Inc Stanford Graduate Fellowship and the Dantzig-Lieberman fellowship.}}
\def\cite{\citet}
\fi



\begin{document}

\maketitle

\addtocounter{footnote}{1}
\footnotetext{Extended abstract. Full paper version appears \href{https://arxiv.org/pdf/1805.08370.pdf}{arXiv:1805.08370}.}

\begin{abstract}
We show that it is possible to obtain an $O(\epsilon^{-4/3})$ runtime --- including computational cost --- for finding $\epsilon$-stationary points of nonconvex functions using cutting plane methods. This improves on the best known epsilon dependence achieved by cubic regularized Newton of $O(\epsilon^{-3/2})$ as proved by Nesterov and Polyak (2006)\nocite{nesterov2006cubic}. Our techniques utilize the convex until proven guilty principle proposed by Carmon, Duchi, Hinder, and Sidford (2017)\nocite{carmon2017convex}.
\end{abstract}

\begin{keywords}
optimization, cutting plane methods, nonconvex, stationary point, local minima
\end{keywords}

\vspace{0.5cm}
\noindent We consider the problem of finding an $\epsilon$-stationary point $x$ of the function $f : \reals^{\Dim} \rightarrow \reals$ starting from some point $x\ind{0}$, i.e.,
$$\| \grad f(x) \| \le \epsilon$$
under the assumptions that $f(x\ind{0}) - \inf_{z} f(z)$ is bounded below and the function has Lipschitz first and third derivatives. It is well-known that gradient descent achieves an $\epsilon^{-2}$ runtime when the first derivatives are Lipschitz. This was improved to $\epsilon^{-3/2}$ by \cite{nesterov2006cubic} using cubic regularized Newton when the  second derivatives are Lipschitz. However, each iteration of cubic regularized Newton is more expensive --- it requires Hessian evaluations and solving a linear system. This observation has inspired a line of work developing gradient based methods that improve on the worst-case runtime of gradient descent \citep*{agarwal2016finding,carmon2016accelerated,carmon2017convex,jin2017accelerated,royer2017complexity}. These methods have cheap `gradient' iteration costs and runtime bounds worse than cubic regularization but better than gradient descent. If low accuracy is desired in high dimensions these gradient based methods are preferable.

What about the regime where the dimension is low but we want to obtain high accuracy? In this case it might be acceptable to have iteration costs that scale polynomially with the dimension if that enables an algorithm with significantly less iterations. Under the assumption that the first and third derivatives to be Lipschitz, our main result is an algorithm that takes
$$
\tilde{O}( ( \TimeGrad + \Dim^{\omega}) \Dim  \epsilon^{-4/3} )
$$
time to find an $\epsilon$-stationary point, where $\TimeP$ which refers to cost of one evaluating the function and its first $p$ derivatives and $O(d^{\omega})$ denotes the runtime for a linear system solve. To prove our result we utilize the `convex until proven guilty' principle proposed by \cite{carmon2017convex} to adapt cutting plane methods to find stationary points of nonconvex functions. Cutting plane methods are traditionally used to optimize general convex functions in low dimensions to high accuracy.

Our result can be contrasted with the results of \cite*{birgin2017worst} who gives a runtime of $O( (\TimeCube + ?)  \epsilon^{-(p+1)/p} )$ where the $?$ denotes the cost of finding a stationary point of a $p$th order regularized problem. Letting $p = 1$ gives gradient descent and $p=2$ cubic regularized Newton. However, for $p > 2$ all known methods for solving $p$th order have $\epsilon$-dependencies that cause the computational runtime to scale at best with $O(\epsilon^{-3/2})$ corresponding to cubic regularized Newton. Therefore our major contribution is to show it is possible to obtain an $O(\epsilon^{-4/3})$ runtime --- including computational cost --- for finding $\epsilon$-stationary points of nonconvex functions. See Table~\ref{table-results} for a comparison of our results with existing results.
\begin{table}[H]
\begin{tabular}{ |c|p{4.0cm}|r|p{3.2cm}| } 
 \hline
Lipschitz  & method & runtime & dimension-free lower bound \citep{carmon2017lower,carmon2017lowerII}  \\ 
\hline
$\grad f$  & gradient descent & $\TimeGrad \epsilon^{-2}$ & $\TimeGrad \epsilon^{-2}$  \\ 
$\grad f, \grad^2 f$  & \cite{carmon2017convex} & $\TimeGrad \epsilon^{-7/4}$ & $\TimeGrad \epsilon^{-12/7}$ \\ 
$\grad f, \grad^3 f$    &  \cite{carmon2017convex} & $\TimeGrad  \epsilon^{-5/3}$ & $\TimeGrad \epsilon^{-8/5}$  \\ 
$\grad^2 f$  & cubic reg. \cite{nesterov2006cubic} & $(\TimeHess + \Dim^{\omega}) \epsilon^{-3/2}$ & $\TimeHess \epsilon^{-3/2}$  \\ 
$\grad^p f$  & $p$th reg. \cite{birgin2017worst}. & $(\TimeP + ?) \epsilon^{-\frac{p+1}{p}}$ &$\TimeP \epsilon^{-(p+1)/p}$ \\ 
$\grad f, \grad^3 f$  &  This paper. Thm~1. & $( (\TimeGrad + \Dim^{\omega}) \Dim + \TimeHess)  \epsilon^{-4/3}$ &  \\ 
$\grad f, \grad^3 f$  & This paper. Thm~2. & $(\TimeCube + \Dim^{4}) \epsilon^{-4/3}$ & $\TimeCube \epsilon^{-4/3}$  \\ 
 \hline
\end{tabular}
\caption{Comparison of the runtime of different algorithms for finding stationary points of nonconvex functions. The question mark is a placeholder for the time to solve a $p$th order regularization problem.}\label{table-results}
\end{table}

%\newpage


%\blfootnote{A footnote without marker}

\ifSIAM
\bibliographystyle{abbrvnat}
\fi

\bibliography{../library-cutting-convex-until-guilty.bib}

\end{document}
