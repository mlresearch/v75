%!TEX root = paper.tex

\subsection{Proofs from \pref{sec:logistic}}

\begin{lemma}
\label{lem:multiclass_lipschitz}
The generalized multiclass logisitic loss is $2L$-Lipschitz with respect to $\ls_{\infty}$ norm.
\end{lemma}
\begin{proof}
It is straightforward to verify the identity
\[
\grad{}_z\ls(z,y) = \prn*{\sum_{k}y_k}\mb{\sigma}(z) - y.
\]
It follows that $\nrm*{\grad{}_z\ls(z,y)}_{1}\leq{}\nrm*{y}_{1}\nrm*{\mb{\sigma}(z)}_{1} + \nrm*{y}_1\leq{}2L$. By duality, this implies $2L$-Lipschitzness with respect to $\ls_{\infty}$.
\end{proof}

\begin{lemma}
\label{lem:monomial_concave}
The function $f(x) = \prod_{k\in\brk{d}}x_{k}^{\alpha_{k}}$ is concave over $\bbR_{+}^{d}$ whenever $\alpha_k\geq{}0\;\forall{}k$ and $\sum_{k\in\brk{d}}\alpha_{k}\leq{}1$.
\end{lemma}
\begin{proof}
We will prove that the Hessian of $f$ is negative semidefinite. The Hessian can be written as
\[
\grad^{2}f(x) = f(x)\cdot{}G(x),
\]
where the matrix $G(x)\in\bbR^{d\times{}d}$ is given by $G(x)_{ii} = \alpha_i(\alpha_i-1)x_{i}^{-2}$ and $G(x)_{ij}=\alpha_i\alpha_jx_i^{-1}x_j^{-1}$. Since $f$ is nonnegative, it suffices to show that $G$ is negative semidefinite. Using the reparameterization $y_i=x_i^{-1}$ and the notation $\had$ for the element-wise product, we can write
\[
G(y) = (\alpha\had{}y)^{\tens{}2} - \diag(\alpha\had{}y^{2}).
\]
For any fixed $y\in\bbR^{d}_{+}$ and any $v\in\bbR^{d}$, we have
\begin{align*}
\tri*{v, G(y)v} &= \prn*{\sum_{k=1}^{d}\alpha_ky_kv_k}^{2} - \sum_{k=1}^{d}\alpha_{k}y_k^{2}v_k^{2} \\
&\leq{} \prn*{\sum_{k=1}^{d}\alpha_ky_k^{2}v_k^{2}}\prn*{\sum_{k=1}^{d}\alpha_k} - \sum_{k=1}^{d}\alpha_{k}y_k^{2}v_k^{2}\\
&\leq{} 0.
\end{align*}
The first inequality above uses Cauchy-Schwarz and the second uses that $\sum\alpha_k\leq{}1$.
\end{proof}

\begin{proof}[\pfref{prop:generalized_multiclass_log_mixable}]
We first show that the generalized multiclass log loss $\ls_{\textrm{log}}(p, y)\ldef-\sum_{k\in\brk{K}}y_{k}\log(p_k)$ is $1/L$-mixable over predictions $p \in \Delta_K$ and outcomes $y \in \cY$. Recall that to show $\eta$-mixability it is sufficient to demonstrate that $\ls$ is $\eta$-exp-concave with respect to $p$ (e.g. \citep{PLG}) for any $y \in \cY$.

Observe that we have
\[
e^{-\eta{}\ls(p,y)} = \prod_{k\in\brk{K}}p_{k}^{\eta{}y_k}.
\]

When $\eta\leq{}1/L$, we have $\sum_{k\in\brk{K}} \eta{}y_k\leq{}1$. Since $p \in \Delta_K$ and by the definition of $\cY$, \pref{lem:monomial_concave} implies the function $p\mapsto\prod_{k\in\brk{K}}p_{k}^{\eta{}y_k}$ is concave, which proves the result.

Exp-concavity implies that for any distribution ${\tilde{\pi}}$ over $\Delta_K$, the predicition $p_{\tilde{\pi}}=\En_{p\sim{}{\tilde{\pi}}}\brk*{p}$ certifies the inequality
\[
\En_{p \sim {\tilde{\pi}}}[\exp(-\eta{}\ls_{\textrm{log}}(p, y))] \leq \exp(-\eta\ls_{\textrm{log}}(p_{\tilde{\pi}}, y)) 
\quad{}y\in\cY.
\]
Now, turning to the multiclass logistic loss $\ls: \R^K \times \cY \rightarrow \R$ defined as $\ls(z, y) = -\sum_{k\in\brk{K}}y_{k}\log(\mb{\sigma}(z)_{k})$, let $\pi$ be any distribution on $\R^K$. Let $\tilde{\pi}$ be the induced distribution on $\Delta_K$ via the softmax function, i.e. a sample from $\tilde{\pi}$ is generated by sampling $z \sim \pi$ and computing $p = \mb{\sigma}(z)$. Then define $z_\pi = \mb{\sigma}^{+}\prn*{\En_{z \sim \pi}\brk*{\mb{\sigma}(z)}}$. Since $\mb{\sigma}(z_\pi) = \En_{z \sim \pi}\brk*{\mb{\sigma}(z)} = p_{\tilde{\pi}}$ and $\ls(z, y) = \ls_{\textrm{log}}(\mb{\sigma}(z), y)$, the above inequality implies that
\[
\En_{z \sim \pi}[\exp(-\eta{}\ls(z, y))] \leq \exp(-\eta\ls(z_{\pi}, y)) 
\quad{}y\in\cY.
\]
\end{proof}

\begin{lemma}
  \label{lem:logistic_bounded}
  Suppose a strategy $(\zt_{t}))_{t\leq{}n}$ guarantees a regret inequality
  \[
    \sum_{t=1}^{n}\ls(\zt_t,y_t) - \inf_{f\in\cF}\sum_{t=1}^{n}\ls(f(x_t),y_t) \leq{} \mathbf{R}.
  \]
  Then for $0\leq\mu\leq{}1/2$ the strategy $\hat{z}_{t} \ldef \mb{\sigma}^{+}\prn*{\smooth\prn*{\mb\sigma(\zh_t)}}$ guarantees
  \[
    \sum_{t=1}^{n}\ls(\zt_t,y_t) - \inf_{f\in\cF}\sum_{t=1}^{n}\ls(f(x_t),y_t) \leq{} \mathbf{R} + 2\mu\sum_{t=1}^{n}\nrm*{y_t}_{1}.
  \]
  and satisfies $\nrm*{\hat{z}_{t}}_{\infty}\leq\log(K/\mu)$.

\end{lemma}
\begin{proof}[\pfref{lem:logistic_bounded}]

 We write regret as
\begin{align*}
&\sum_{t=1}^{n}\ls(\zh_t,y_t) - \inf_{f\in\cF}\sum_{t=1}^{n}\ls(f(x_t),y_t) \\ &= 
\sum_{t=1}^{n}\ls(\zt_t,y_t) - \inf_{f\in\cF}\sum_{t=1}^{n}\ls(f(x_t),y_t) + \sum_{t=1}^{n}\ls(\zh_t,y_t) - \sum_{t=1}^{n}\ls(\zt_t,y_t)\\
% &\leq{} \mathbf{R}  + \sum_{t=1}^{n}\ls(\zh_t,y_t) - \sum_{t=1}^{n}\ls(\zt_t,y_t) \\
&\leq{} \mathbf{R} + \sum_{t=1}^{n}\ls(\zh_t,y_t) - \sum_{t=1}^{n}\ls(\zt_t,y_t).
\end{align*}
For the last two terms, fix any round $t$ and define $\tilde{p} = \mb{\sigma}(\zt_t)$. Since $\mb{\sigma}(\zh_t) = (1-\mu) \tilde{p} + \mu \mb{1}/K$, we have
\[
\ls(\zh_t,y_t) - \ls(\zt_t,y_t)
= \sum_{k\in\brk{K}}y_{t,k}\log\prn*{
\frac{\tilde{p}_k}{(1-\mu)\tilde{p}_k + \mu/K}
} \leq{} \log\prn*{
\frac{1}{1-\mu}
}\sum_{k\in\brk{K}}y_{t,k} \leq{} 2\mu\nrm*{y_t}_{1}.
\]
The last inequality uses that $\log(1/(1-x))\leq{}2x$ for $x\leq{}1/2$. Summing up over all rounds $t$ gives us the desired regret bound.

To establish boundedness of the predictions, recall that $\mb{\sigma}_{k}^{+}(p) = \log(p_k)$. Letting $p = (1-\mu)\En_{W\sim{}P_t}\brk*{\mb{\sigma}(Wx_t)} + \mu\ones{}/K$, it clearly holds that $p_{k}\geq{}\mu/K$, and so $|\mb{\sigma}_{k}^{+}(p)|\leq{} \log(K/\mu)$.
\end{proof}


\begin{proof}[\pfref{thm:multiclass_logistic_regret}]
Let $\eta=1/L$. Let $\zt_{t}=\mb{\sigma}^{+}\prn*{\En_{W\sim{}P_t}\brk*{\mb{\sigma}(Wx_t)}}$ --- that is, the prediction for the setting $\mu=0$. We will first establish a regret bound for the case $\mu=0$, then reduce the general case to it by approximation.

First observe that due to mixability for $\eta\leq{}1/L$ (from \pref{prop:generalized_multiclass_log_mixable}), we have
\[
\sum_{t=1}^{n}\ls(\zt_t,y_t) \leq{} -\frac{1}{\eta}\sum_{t=1}^{n}\log\prn*{\int_{\cW}\exp(-\eta\ls(Wx_t, y_t))dP_t(W)}.
\]
Let $Z_{t}=\int_{\cW}\exp\prn{-\eta\sum_{s=1}^{t}\ls(Wx_s, y_s)}dW$ with the convention $Z_0=\int_{\cW}dW$. Using the definition of $P_t$, the right-hand-side in the displayed equation above is then equal to
\[
-\frac{1}{\eta}\sum_{t=1}^{n}\log(Z_{t}/Z_{t-1}) = -\frac{1}{\eta}\log(Z_{n}/Z_{0}) = -\frac{1}{\eta}\log\prn*{\int_{\cW}\exp\prn*{-\eta\sum_{t=1}^{n}\ls(Wx_t, y_t)}dW} + \frac{1}{\eta}\log(\vol(\cW))
\]
We will focus on coming up with an upper bound on the term $-\log\prn*{\int_{\cW}\exp\prn*{-\eta\sum_{t=1}^{n}\ls(Wx_t, y_t)}dW}$. Let $W^{\star}=\argmin_{W\in{}\cW}\sum_{t=1}^{n}\ls(Wx_t,y_t)$. Fix $\theta\in[0,1)$ and let $S=\crl*{\theta{}W^{\star} + (1-\theta)W\mid{}W\in{}\cW}\subseteq{}\cW$. To upper bound the negative-log-integral term, we will lower bound the integral appearing inside.
\begin{align*}
&\int_{\cW}\exp\prn*{-\eta\sum_{t=1}^{n}\ls(Wx_t, y_t)}dW \geq \int_{S}\exp\prn*{-\eta\sum_{t=1}^{n}\ls(Wx_t, y_t)}dW.
\intertext{Using a change of variables and noting that since $W\in\bbR^{K\times{}d}$ the Jacobian of the mapping $W\mapsto (1-\theta)W + \theta{}W^{\star}$ has determinant $(1-\theta)^{\wdim}$, the right-hand-side above equals}
&= (1-\theta)^{\wdim}\int_{\cW}\exp\prn*{-\eta\sum_{t=1}^{n}\ls((\theta{}W^{\star} + (1-\theta)W)x_t, y_t)}dW.
\intertext{Observe that $\nrm*{(\theta{}W^{\star} + (1-\theta)W)x_t - W^{\star}x_t}_{\infty} = (1-\theta)\max_{k\in\brk{K}}\abs*{\tri*{W^{\star}_{k}-W_{k},x_t}}\leq{} 2(1-\theta)B\nrm*{x_t}_{\star}$. 
Using this observation with the $2L$-Lipschitzness of $\ls$ with respect to $\ls_{\infty}$ from \pref{lem:multiclass_lipschitz} implies that the above displayed expression is at most
}
&(1-\theta)^{D_{\cW}}\int_{\cW}\exp\prn*{-\eta\sum_{t=1}^{n}\ls(W^{\star}x_t, y_t) - 4(1-\theta)BL\eta\sum_{t=1}^{n}\nrm*{x_t}_{\star}}dW. \\
&= (1-\theta)^{D_{\cW}}\cdot\mathrm{Vol}(\cW)\cdot\exp\prn*{-\eta\sum_{t=1}^{n}\ls(W^{\star}x_t, y_t)}\cdot\exp\prn*{ - 4(1-\theta)BL\eta\sum_{t=1}^{n}\nrm*{x_t}_{\star}}.
\end{align*}

Combining all of the observations so far, we have proven the following regret bound:
\begin{align*}
&\sum_{t=1}^{n}\ls(\yh_t,y_t) - \sum_{t=1}^{n}\ls(W^{\star}x_t,y_t) \\ &
\begin{aligned}
\leq{}& \frac{1}{\eta}\log(\vol(\mathcal{W})) - \sum_{t=1}^{n}\ls(W^{\star}x_t,y_t) \\ &+ \frac{1}{\eta}\underbrace{\prn*{
D_{\cW}\log\prn*{\frac{1}{1-\theta}} - \log(\vol(\mathcal{W}))
+ \eta\sum_{t=1}^{n}\ls(W^{\star}x_t,y_t)
+ 4(1-\theta)BL\eta\sum_{t=1}^{n}\nrm*{x_t}_{\star}
}}_{\text{Bound on negative log-integral-exp.}}
\end{aligned}
\\
&= \frac{D_{\cW}}{\eta}\log\prn*{\frac{1}{1-\theta}} + 4(1-\theta)BL\sum_{t=1}^{n}\nrm*{x_t}_{\star}.
\end{align*}
To conclude, we choose $\theta$ to satisfy $1-\theta=\min\crl*{D_{\cW}/(B\sum_{t=1}^{n}\nrm*{x_t}_{\star}), 1}$. Note that regardless of which argument obtains the minimum, we have $4(1-\theta)BL\sum_{t=1}^{n}\nrm*{x_t}_{\star} \leq{} 4D_{\cW}L$. The choice of $\theta$ also means that $\log\prn*{\frac{1}{1-\theta}} = \log\prn*{1\vee{}B\sum_{t=1}^{n}\nrm*{x_t}_{\star}/D_{\cW}}$. This leads to a final bound of
\[
D_{\cW}L\cdot{}\log\prn*{1\vee{}\frac{B\sum_{t=1}^{n}\nrm*{x_t}_{\star}}{D_{\cW}}} + 4D_{\cW}L.
\]
To simplify we upper bound this by
\[
5D_{\cW}L\cdot{}\log\prn*{\frac{B\sum_{t=1}^{n}\nrm*{x_t}_{\star}}{D_{\cW}} + e} = 5D_{\cW}L\cdot{}\log\prn*{\frac{BRn}{D_{\cW}} + e}.
\]

To handle the general case where $\mu>0$ we simply appeal to \pref{lem:logistic_bounded} and use that $\mb{\sigma}(\mb{\sigma}^{+}(p)) = p\;\forall{}p\in\Delta_{K}$.

\end{proof}

We now state the proof of \pref{thm:logb_lower_bound}. This proof is a simple corollary of \pref{thm:margin_lb}, a lower bound on mistakes for online binary classification with a margin. \pref{thm:margin_lb} is proven in the remainder of this section of the appendix. To begin, we need the following definition:
\begin{definition}
Let $\cF:\cX\to\brk{-1,1}$ be some function class. A dataset $(x_1,y_1),\ldots,(x_n,y_n)\in \cup_{t=1}^{n}\cX\times{}\pmo$ is shattered with $\gamma$ margin if there exists $f\in\cF$ such that
\[
f(x_t)y_t\geq{}\gamma.
\]
\end{definition}



\begin{proof}[\pfref{thm:logb_lower_bound}]
Let $\zh_t$ for $t \in [n]$ be the sequence of predictions made by the algorithm for a sequence of examples $(x_t, y_t)$, for $t \in [n]$. It is easy to check that
\begin{align*}
	\sum_{t=1}^{n}\ls_\text{bin}(\zh_t, y_t) \geq{} \log(2)\sum_{t=1}^{n}\ind\crl*{\sgn(\zh_t)\neq{}y_t}.
\end{align*}

Let $1/\gamma=B/\log(n)$. From \pref{thm:margin_lb}, it holds that whenever $\gamma\leq{}O(1/\sqrt{d})$, there exists an adversarial sequence $(x_t, y_t)$, for $t \in [n]$, for which 
\[
\sum_{t=1}^{n}\ind\crl*{\sgn(\yh_t)\neq{}y_t}\geq{}\frac{d}{4}\floor*{\log_{2}\prn*{\frac{1}{5\gamma{}d^{1/2}}}},
\]
and for which the dataset is $\gamma$-shattered by some $w\in\bbR^{d}$ with $\nrm*{w}_2\leq{}1$. Since the dataset is $\gamma$-shattered we also have
\[
\inf_{w:\nrm*{w}_2\leq{}B}\sum_{t=1}^{n}\ls_\text{bin}(\tri*{w,x_t}, y_t)\leq{}\sum_{t=1}^{n}\log(1+e^{-\gamma{}B})=\sum_{t=1}^{n}\log\prn*{1+\frac{1}{n}}\leq{}1.
\]
This yields the desired lower bound on the regret.
\end{proof}


\begin{theorem}
\label{thm:margin_lb}
Fix a margin $\gamma\in(0,\frac{1}{4\sqrt{5d}}]$. Then for any randomized strategy $(\yh_t)_{t\leq{}n}$ there exists an adversary $(x_t)_{t\leq{}n}$, $(y_t)_{t\leq{}n}$ with $\nrm*{x_t}_2\leq{}2$ for which
\begin{equation}
\En\brk*{\sum_{t=1}^{n}\ind\crl*{\sgn(\yh_t)\neq{}y_t}} \geq{} \frac{d}{4}\floor*{\log_{2}\prn*{\frac{1}{5\gamma{}d^{1/2}}}},
\end{equation}
and the data sequence is realizable by a unit vector $w\in\bbR^{d+1}$ with margin $\gamma$.
\end{theorem}
\begin{remark}
This lower bound only applies in the regime where $\frac{1}{\gamma^{2}}\geq{}d$, meaning that it does not contradict the dimension-independent Perceptron bound.
\end{remark}
To prove \pref{thm:margin_lb}, we first state a standard lower bound based on Littlestone's dimension.

\begin{definition}
An $\cX$-valued tree is a sequence of mappings $\mb{x}_{t}:\pmo^{t-1}\to\cX$ for $1\leq{}t\leq{}n$.
\end{definition}
We use the abbreviation of $\mb{x}_{t}(\eps) = \mb{x}_{t}(\eps_{1},\ldots,\eps_{t-1})$ for such a tree, where $\eps\in\pmo^{n}$.

\begin{lemma}
\label{lem:fat_shattering}
Let $\cF:\cX\to\brk{-1,1}$ be some function class. Suppose there exists a $\cX$-valued tree $\mb{x}$ of depth $D_{\gamma}$ such that
\begin{equation}
\label{eq:tree_shattered}
\forall{}\eps\in\pmo^{D_{\gamma}}\;\;\exists{}f\in\cF\;\;\;\textrm{s.t.}\;\;\; f(\mb{x}_t(\eps))\eps_{t}\geq{}\gamma.
\end{equation}
Then
\[
    \inf_{q_1,\ldots,q_n}\sup_{\substack{(x_1, y_1),\ldots, (x_n,y_n)\\\textnormal{separable with $\gamma$ margin}}}\En_{\yh_1\sim{}q_t,\ldots,\yh_n\sim{}q_n}\brk*{
    \sum_{t=1}^{n}\ind\crl*{\yh_t\neq{}y_t}
    } \geq{} \frac{1}{2}\min\crl*{D_{\gamma}, n},
\]
where the infimum and supremum above are understood to range over policies.

\end{lemma}
\begin{proof}[\pfref{lem:fat_shattering}]
Suppose that $n\leq{}D_{\gamma}$. We will sample Rademacher random variables $\eps\in\pmo^{n}$ and play $y_t=\eps_t$ and $x_t=\mb{x}_t(\eps_{1:t-1})$. This immediately implies that the expected number of mistakes is equal to $\frac{n}{2}$. Moreover, since $n\leq{}D_{\gamma}$, the assumption in the statement of the lemma implies that  there exists $f\in\cF$ such that $f(\mb{x}_t(\eps))y_t\geq{}\gamma$, so the data is indeed separable with $\gamma$ margin.

If $n>D_{\gamma}$ we can follow the strategy above, then continue to play $(x_{D_{\gamma}}, y_{D_{\gamma}})$ for all $t>D_{\gamma}$.
\end{proof}

\begin{proof}[\pfref{thm:margin_lb}]
By \pref{lem:fat_shattering} it suffices to exhibit a tree $\mb{x}$ for which \pref{eq:tree_shattered} is satisfied with $D_{\gamma}=\Omega(d\log(1/(\sqrt{d}\gamma)))$.

We first restate a well-known tree instance for the one-dimensional case. Consider a class of thresholds $\cF_{\textrm{thresh}}=\crl*{f_{\theta}:\brk*{0,1}\to\pmo}$ defined by $f_{\theta}(z)=1-2\ind\crl*{x<\theta}$. The claim is as follows: For any $\delta\in(0,1]$, there exists a $\brk*{0,1}$-valued tree $\mb{z}$ of depth $D_{\delta}\ldef\floor*{\log_{2}(2/\delta)}$ such that
\begin{enumerate}
\item 
$\forall{}\eps\in\pmo^{D_{\delta}}\;\;\exists{}\theta\;\;\;\textrm{s.t.}\;\;\; f_{\theta}(\mb{z}_t(\eps))\eps_{t}=1$.
\item $\abs*{\mb{z}_t(\eps) - \mb{z}_s(\eps)}\geq{}\delta\;\;\forall{}s\neq{}t$.
\end{enumerate}
The construction is as follows. Let $u_{1}=1$, $l_1=0$. Recursively for $t=1,\ldots,n$:
\begin{itemize}
\item $\mb{z}_{t}(\eps_{1:t-1})=\frac{l_t + u_t}{2}$. 
\item If $\eps_t=-1$ set $l_{t+1}=\mb{z}_t(\eps_{1:t-1})$ and $u_{t+1}=u_t$, else set $u_{t+1}=\mb{z}_t(\eps_{1:t-1})$ and $l_{t+1}=l_t$.
\end{itemize}
Under this construction the sequence $\mb{z}_1(\cdot),\ldots, \mb{z}_{D_{\delta}}(\eps_{1:D_{\delta}-1})$ can always be shattered. Furthermore $\mb{z}^{\star}(\eps)\ldef\mb{z}_{D_{\delta}+1}(\eps_{1:D_{\delta}})$ satisfies the additional property that $\mb{z}_t>\mb{z}^{\star}(\eps)\implies{}\eps_t=1$ and $\mb{z}_t<\mb{z}^{\star}(\eps)\implies\eps_t=-1$. Also, $\abs*{\mb{z}^{\star}-\mb{z}_t}\geq\frac{\delta}{2}\;\forall{}t\leq{}D_{\delta}$.

We now show how to extend this instance to $d+1$ dimensions for any $d\geq{}1$. The approach is to concatenate $d$ instances of the $\mb{z}$ tree constructed above, one for each of the first $d$ coordinates. The final coordinate is left as a constant so that a bias can be implemented.

Let $n=d\cdot{}D_{\delta}$ be the tree depth for our $d+1$-dimensional instance. For any time $t$, let $k\in\brk{d}$ and $\tau\in\brk{D_{\delta}}$ be such that $t=(k-1)D_{\delta} + \tau$. Let any sequence $\eps\in\pmo^{n}$ be partitioned as $(\mb{\eps}^{1},\ldots,\mb{\eps}^{d})$ with each $\mb{\eps}^{k}\in{}\pmo^{D_{\delta}}$. Letting $e_k$ denote the $k$th standard basis vector, we define a shattered tree $\mb{x}$ as follows:
\[
\mb{x}_{t}(\eps_{1:t-1}) = e_{d+1} + e_{k}\mb{z}_{\tau}(\mb{\eps}^{k}_{1:\tau-1}).
\]
We construct a vector $w\in\bbR^{d+1}$ whose sign correctly classifies each $\mb{x}_t$ as follows:
\begin{itemize}
\item $w_{d+1}=-\delta$.
\item $w_{k}=\delta/\mb{z}^{\star}(\mb{\eps}^{k})$.
\end{itemize}
For any $t=(k-1)D_{\delta} + \tau$ this choice gives
\[
\tri*{w, \mb{x}_{t}(\eps)}\eps_t = \delta(\mb{z}_{\tau}(\mb{\eps}^{k}_{1:\tau-1})/\mb{z}^{\star}(\mb{\eps}^{k})-1)\eps_t.
\]
As described above, $\mb{z}_t>\mb{z}^{\star}(\eps)\implies{}\eps_t=1$ and $\mb{z}_t<\mb{z}^{\star}(\eps)\implies\eps_t=-1$, which immediately implies that the inner product is always non-negative, and so the dataset is shattered. Using that $\abs*{\mb{z}^{\star}(\eps)-\mb{z}_t(\eps)}\geq{}\frac{\delta}{2}$ and that both numbers lie in $\brk*{0,1}$, we can lower bound the magnitude with which the shattering takes place:
\[
\abs*{\mb{z}_{\tau}(\eps^{k}_{1:\tau-1})/\mb{z}^{\star}(\mb{\eps}^{k})-1} = \frac{1}{\mb{z}^{\star}(\mb{\eps}^{k})}\abs*{\mb{z}_{\tau}(\eps^{k}_{1:\tau-1})-\mb{z}^{\star}(\mb{\eps}^{k})}\geq{}\frac{1}{\mb{z}^{\star}(\mb{\eps}^{k})}\frac{\delta}{2}\geq{}\frac{\delta}{4},
\]
and so the shattering takes place with margin at least $\delta^{2}/4$.

Lastly, the norm of $w$ is given by
\[
\nrm*{w}_{2}=\sqrt{\delta^{2} + \sum_{k=1}^{d}\prn*{\frac{\delta}{\mb{z}^{\star}(\mb{\eps}^{k})}}^{2}}
\leq{} \sqrt{\delta^{2} + 4d} \leq{} \sqrt{5d},
\]
where the first inequality uses that $\mb{z}^{\star}(\eps)\geq{}\delta/2$ and the second uses that $d\geq{}1$

Rescaling, we have that the vector $w/\nrm*{w}_{2}$ shatters the tree with margin at least $\frac{\delta^{2}}{4\sqrt{5d}}$. To rephrase the result as a function of a desired margin: For any margin $\gamma\in(0,\frac{1}{4\sqrt{5d}}]$, setting $\delta=\sqrt{\gamma4\sqrt{5d}}\leq{}1$, we have constructed a tree of depth $\floor*{\log_{2}(2/\sqrt{\gamma4\sqrt{5d}})}$ that can be shattered with margin $\gamma$.

\end{proof}

\input{appendix_bandits}

\input{appendix_online_boosting}

\input{appendix_o2b}

\subsection{Details from \pref{sec:general_class}}
\label{app:general_class}


For this section we let $\ls$ denote the unweighted multiclass logistic loss: the multiclass logistic loss defined in \pref{sec:prelims} for the special case where $\cY=\crl*{e_i}_{i\in\brk{K}}$.
Before proving \pref{thm:logistic_minimax} we need a few preliminaries. First, we state a version of the Aggregating Algorithm with the logistic loss for finite classes.
\begin{lemma}
  \label{lem:aggregating_finite}
  Let $\cF$ be any finite class of sequences of the form $f=(f_{t})_{t\leq{}n}$ with $f_t\in\bbR^{K}$, where each $f_t$ is available at time $t$ and may depend on $y_{1:t-1}$. Define a strategy
  \begin{enumerate}
  \item $P_t(f)\propto\exp\prn*{-\sum_{s=1}^{t-1}\ls(f_s, y_s)}$ (so $P_{1}=\mathrm{Uniform}(\cF)$).
  \item $\hat{z}_t = \mb{\sigma}^{+}(\mathrm{smooth}_{\frac{1}{n}}(\En_{f\sim{}P_t}\brk*{\mb{\sigma}(f_t)}))$.
  \end{enumerate}
  This strategy enjoys a regret bound of
  \begin{equation}
    \label{eq:logistic_finite}
    \sum_{t=1}^{n}\ls(\hat{z}_t, y_t) - \min_{f\in\cF}\sum_{t=1}^{n}\ls(f_t, y_t) \leq{} \log\abs*{\cF} + 2.
  \end{equation}
  Furthermore, the predictions satisfy $\nrm*{\hat{z}_t}_{\infty}\leq{}\log(Kn)$.
\end{lemma}
\begin{proof}[\pfref{lem:aggregating_finite}]
  First consider the closely related strategy $\wt{z}_t \ldef \mb{\sigma}^{+}(\En_{f\sim{}P_t}\brk*{\mb{\sigma}(f(x_t))})$. In light of the $1$-mixability for the logistic loss proven in \pref{prop:unweighted-mixability}, $\wt{z}_t$ is precisely the finite class version of the Aggregating Algorithm, which guarantees \citep{PLG}:
  \[
    \sum_{t=1}^{n}\ls(\wt{z}_t, y_t) - \min_{f\in\cF}\sum_{t=1}^{n}\ls(f_t, y_t) \leq{} \log\abs*{\cF}.
  \]
  To establish the final result we simply appeal to \pref{lem:logistic_bounded}, using that $\mb{\sigma}(\mb{\sigma}^{+}(p)) = p\;\forall{}p\in\Delta_{K}$.
  \end{proof}


% At first glance this appears challenging because data, which is typically required to build non-pointwise covers, is not available a-priori. This issue can be overcome by moving to the minimax dual of the online learning game, but the sequential covers considered in the dual are improper and tree-valued, and consequently the right generalization of the previous argument for partitioning into small-radius sub-classes is tricky to define. \pref{thm:logistic_minimax} overcomes this issue. 

We now formally define a multiclass generalization of a sequential cover.

\begin{definition}
For any set $\cZ$, a $\mathcal{Z}$-valued $K$-ary tree of depth $n$ is a sequence $\mathbf{z} = (\mathbf{z}_1,\ldots,\mathbf{z}_n)$ of $n$ mappings with $\mathbf{z}_t: [K]^{t-1} \to \mathcal{Z}$. 
\end{definition}
\begin{definition}\label{def:cover}
A set $V$  of $\mathbb{R}^K$-valued $K$-ary trees is an $\alpha$-cover (w.r.t. the $L_p$ norm) of $\F$ on an $\X$-valued $K$-ary tree $\x$ of depth $n$ with loss $\ls$ if
$$
\forall f \in \F, ~y \in [K]^n,  ~\exists \vv \in V ~\textnormal{s.t.}~ \left(\frac{1}{n} \sum_{t=1}^n \max_{y'_t\in\brk*{K}}\left|\ell(f(\x_t(y)),y'_t) - \ell(\vv_t(y),y'_t) \right|^p\right)^{1/p} \le \alpha.
$$ 
\end{definition}
\begin{definition}
The $L_p$ covering number of $\F$ on tree $\x$ is defined as
$$
\mathcal{N}_p(\alpha,\ell \circ \F,\x) \ldef \min\{|V| : V \textrm{ is an }\alpha\textnormal{-cover of $\cF$ on $\x$ w.r.t. the $L_p$ norm} \}.
$$
Further, define $\mathcal{N}_p(\alpha,\ell \circ \F) = \sup_\x \mathcal{N}_p(\alpha,\ell \circ \F,\x)$. 
\end{definition}
If $K = 2$ then the above definition coincides with the definition of sequential cover in \cite{RakSriTew14jmlr} which was defined for real valued function classes. 


  
We also need a slight generalization of the notion of covering number defined in \pref{def:cover} for intermediate results.
  
  \begin{definition}\label{def:cover_general}
  Let $U$ be a collection of $\bbR^{K}$-valued $K$-ary trees. A set $V$  of $\mathbb{R}^K$-valued $K$-ary trees is an $\alpha$-cover with respect to the $L_p$ norm for $U$ if
\[
\forall \uu\in{}U, ~y \in [K]^n,  ~\exists \vv \in V ~\textnormal{s.t.}~ \left(\frac{1}{n} \sum_{t=1}^n \max_{y'_t\in\brk*{K}}\left|\ell(\uu_t(y),y'_t) - \ell(\vv_t(y),y'_t) \right|^p\right)^{1/p} \le \alpha.
\]
\end{definition}
\begin{definition}
\label{def:covering_number_general}
The $L_p$ covering number for a collection of trees $U$ with loss $\ls$ is
\[
\mathcal{N}_p(\alpha, \ls\circ{}U) \ldef \min\{|V| : V \textrm{ is an }\alpha\textnormal{-cover of $U$ w.r.t. the } L_p\textnormal{ norm} \}.
\]
\end{definition}

  \begin{proof}[\pfref{thm:logistic_minimax}]
    Define a subset of the output space:
  \[
    \cZ \ldef{} \crl*{z\in\bbR^{K}\mid{} \nrm*{z}_{\infty} \leq{} \log(Kn)}.
  \]
  
We move to an upper bound on the minimax value by restricting predictions to $\cZ$:
\begin{align*}
  \mathcal{V}_n(\F) & = \dtri*{\sup_{x_t\in\cX} \inf_{\hat{z}_t\in\bbR^{K}} \max_{y_t \in [K]}}_{t=1}^n\left[ \sum_{t=1}^n \ell(\hat{z}_t,y_t) - \inf_{f \in \mathcal{F}} \sum_{t=1}^n \ell(f(x_t),y_t)\right]\\
                    & \leq{} \dtri*{\sup_{x_t\in\cX} \inf_{\hat{z}_t\in\cZ} \max_{y_t \in [K]}}_{t=1}^n\left[ \sum_{t=1}^n \ell(\hat{z}_t,y_t) - \inf_{f \in \mathcal{F}} \sum_{t=1}^n \ell(f(x_t),y_t)\right].
\end{align*}
Note that $\cZ$ is a compact subset of a separable metric space and that $\ls$ is convex with respect to $\hat{z}$. Therefore, using repeated application of minimax theorem following \cite{RakSriTew10}\footnote{See \cite{RakSriTew10} for an extensive discussion of the technicalities.} the minimax value can be written as:
\begin{align}
  &= \dtri*{\sup_{x_t\in\cX} \sup_{p_t \in \Delta_{K}} \inf_{\hat{z}_t\in\cZ} \mathbb{E}_{y_t \sim p_t}}_{t=1}^n\left[ \sum_{t=1}^n \ell(\hat{z}_t,y_t) - \inf_{f \in \mathcal{F}} \sum_{t=1}^n \ell(f(x_t),y_t)\right].\notag
    \intertext{Now we perform a standard manipulation of the $\sup$ and loss terms as in \cite{RakSriTew10}:}
&= \dtri*{\sup_{x_t\in\cX} \sup_{p_t \in \Delta_{K}}  \mathbb{E}_{y_t \sim p_t}}_{t=1}^n\left[ \sum_{t=1}^n \inf_{\hat{z}_t\in\cZ} \mathbb{E}_{y_t \sim p_t}\left[\ell(\hat{z}_t,y_t)\right] - \inf_{f \in \mathcal{F}} \sum_{t=1}^n \ell(f(x_t),y_t)\right]\label{eq:inf_inside}\\
&= \sup_{\x, \p} \mathbb{E}_{y \sim \p}\left[ \sum_{t=1}^n \inf_{\hat{z}_t\in\cZ} \mathbb{E}_{y_t \sim \p_t(y)}\left[\ell(\hat{z}_t,y_t)\right] - \inf_{f \in \mathcal{F}} \sum_{t=1}^n \ell(f(\x_t(y)),y_t)\right]\label{eq:logistic_tree}.
\end{align}
In the final line above we have introduced new notation. $\x$ and $\p$ are $\X$- and $\Delta_K$-valued $K$-ary trees of depth $n$. That is, $\x = (\x_1,\ldots,\x_n)$ where $\x_t : [K]^{t-1} \to \X$ and similarly for the tree $\p=(\p_1,\ldots,\p_n)$,  $\p_{t}:\brk*{K}^{t-1}\to\Delta_{K}$. The notation ``$y \sim \p$'' refers to the process in which we first draw $y_1 \sim \p_1$, then draw $y_t \sim \p_t(y_1,\ldots,y_{t-1})$ for subsequent timesteps $t$. We also overload the notation as $\p_{t}(y)\ldef\p_{t}(y_{1:t-1})$, and likewise for $\x$.

With this notation, \pref{eq:logistic_tree} is seen to be \pref{eq:inf_inside} rewritten using that at time $t$, based on draw of previous $y$s, $x_t$ and $p_t$ are chosen to maximize the remaining game value; this process be represented via $K$-ary tree.

Note that the sequence $(\hat{z}_{t})_{t\leq{}n}$ being minimized over in \pref{eq:inf_inside} can depend on the full trees $\x$ and $\p$, but that it is adapted to the path $(y_t)_{t\leq{}n}$, meaning that the value at time $t$ ($\hat{z}_{t}$) can only depend on the $\yr[t-1]$. This property is imporant because the choice we exhibit for $(\hat{z}_{t})_{t\leq{}n}$ will indeed depend on the full trees.

In light of the discussion in \pref{sec:general_class}, the key advantage of having moved to the dual game above is that we can condition on the $K$-ary tree $\x$ and cover $\F$ only on this tree. Let $V^\gamma$ be a minimal $\gamma$-sequential cover of $\ell \circ \mathcal{F}$ on the tree $\mathbf{x}$ with respect to the $L_2$ norm (in the sense of \pref{def:cover}). 

Keeping the tree $\x$ fixed, for each tree $\vv \in V^\gamma$, each $f \in \F$, we define a class of trees $\cF_{\vv}$ ``centered'' at $\vv$---in a sense that will be made precise in a moment---via the following procedure.

\begin{itemize}[leftmargin=*]
\item $\cF_{\vv} = \emptyset$.
\item For each $f\in\cF$ and $y\in\brk*{K}^{n}$ with $\sqrt{\frac{1}{n} \sum_{t=1}^n \max_{y''_t\in\brk*{K}}(\ell(f(\x_t(y)),y''_t) - \ell(\vv_t(y),y''_t) )^2} \le \gamma$: 
\begin{itemize}
\item Define a $\bbR^{K}$-valued $K$-ary tree $\uu_{f,y}$ via: For each $y'\in\brk*{K}^{n}$, 
\[
(\uu_{f,y})_{t}(y') \ldef f(\x_{t}(y'))\ind\crl*{y'_1=y_1,\ldots,y'_{t-1}=y_{t-1}} + \vv_{t}(y')\ind\crl*{\neg{}(y'_1=y_1,\ldots,y'_{t-1}=y_{t-1})}.
\]
In other words, $\uu_{f,y}$ is equal to $f\circ\x$ on the path $y$, and equal to $\vv$ everywhere else.
\item Add $\uu_{f,y}$ to $\cF_{\vv}$.
\end{itemize}
\end{itemize}

The class $\cF_{\vv}$ has two important properties which are formally proven in an auxiliary lemma, \pref{lem:fv_properties}: First, its $L_2$ covering number is (up to low order terms) bounded in terms of the $L_2$ covering number of the class $\cF\circ{}\x$, so it has similar complexity to this class. Second, its $L_2$ radius is bounded by $\gamma$, in the sense that its covering number at scale $\gamma$ is at most $1$.

Note that on any path $y \in [K]^n$ and for each $f \in \F$, there exist $\vv \in V^\gamma$ and $\uu \in \F_\vv$ such that $f(\x_t(y)) = \uu_{t}(y)$. This is because a $\vv$ that is $\gamma$-close to $f$ on the path $y$ through $\x$ is guaranteed by the cover property of $V^{\gamma}$, and so we can take $\uu_{f,y}$ in $\cF_{\vv}$ as the desired $\uu$. This implies that
$$
\inf_{f \in \F }\sum_{t=1}^n \ell(f(\x_t(y)),y_t) \ge \min_{\vv \in V^\gamma} \inf_{\uu \in \F_\vv }\sum_{t=1}^n \ell(\uu_t(y),y_t).
$$

With this we are ready to return to the minimax rate. We already established that
\begin{align}
 \mathcal{V}_n(\F)  &\leq \sup_{\x, \p} \mathbb{E}_{y \sim \p}\left[ \sum_{t=1}^n \inf_{\hat{z}_t\in\cZ} \mathbb{E}_{y_t \sim \p_t(y)}\left[\ell(\hat{z}_t,y_t)\right] - \inf_{f \in \mathcal{F}} \sum_{t=1}^n \ell(f(\x_t(y)),y_t)\right]. \notag 
 \intertext{We now move to an upper bound based on the constructions for the tree collections $V^{\gamma}$ and $\crl*{\cF_{\vv}}_{\vv\in{}V^{\gamma}}$. These collections depend only on the tree $\x$ at the outer supremum above. Writing the choice of these collections as an infimum to make its dependence on the other quantities in the random process as explicit as possible, and using the containment just shown:}
  & \le \sup_{\x}\inf_{V^{\gamma}}\inf_{\crl*{\cF_{\vv}}_{\vv\in{}V^{\gamma}}}\sup_{\p}\mathbb{E}_{y \sim \p}\left[ \sum_{t=1}^n \inf_{\hat{z}_t\in\cZ} \mathbb{E}_{y_t \sim \p_t(y)}\left[\ell(\hat{z}_t,y_t)\right] - \min_{\vv \in V^\gamma} \inf_{\uu \in \mathcal{F}_\vv} \sum_{t=1}^n \ell(\uu_t(y),y_t)\right]. \notag
\end{align}
For the last time in the proof, we introduce a new collection of trees. For each $\vv\in{}V^{\gamma}$ we introduce a $\cZ$-valued $K$-ary tree $\yhtree^{\vv}$, with $\yhtree^{\vv}_{t}:\brk*{K}^{t-1}\to\cZ$. We postpone explicitly constructing the trees for now, but the reader may think of each tree $\yhtree^{\vv}$ as representing the optimal strategy for the set $\cF_{\vv}$ in a sense that will be made precise in a moment.
{\small
\begin{align}
%    \intertext{}
  & \begin{aligned}= \sup_{\x}\inf_{V^{\gamma}}\inf_{\crl*{\cF_{\vv}}_{\vv\in{}V^{\gamma}}}\inf_{\crl*{\yhtree^{\vv}}_{\vv\in{}V^{\gamma}}}\sup_{\p} \mathbb{E}_{y \sim \p}\biggl[& \sum_{t=1}^n \inf_{\hat{z}_t\in\cZ} \mathbb{E}_{y_t \sim \p_t(y)}\left[\ell(\hat{z}_t,y_t)\right] \\
    &- \min_{\vv \in V^\gamma}\left\{ \sum_{t=1}^n \ell(\yhtree_t^\vv(y),y_t) - \sum_{t=1}^n \ell(\yhtree_t^\vv(y),y_t) +  \inf_{\uu \in \mathcal{F}_\vv} \sum_{t=1}^n \ell(\uu_t(y),y_t)\right\}\biggr]\end{aligned} \notag \\
& \leq\sup_{\x}\inf_{V^{\gamma}}\inf_{\crl*{\cF_{\vv}}_{\vv\in{}V^{\gamma}}}\inf_{\crl*{\yhtree^{\vv}}_{\vv\in{}V^{\gamma}}}\left\{\begin{aligned}~& \underbrace{\sup_{\p} \mathbb{E}_{y \sim \p}\biggl[ \sum_{t=1}^n \inf_{\hat{z}_t\in\cZ} \mathbb{E}_{y_t \sim \p_t(y)}\left[\ell(\hat{z}_t,y_t)\right] - \min_{\vv \in V^\gamma}\sum_{t=1}^n \ell(\yhtree_t^\vv(y),y_t)\biggr]}_{(\star)} \\
  & + \underbrace{\sup_{\p}\mathbb{E}_{y \sim \p}\left[ \max_{\vv \in V^\gamma}\left\{ \sum_{t=1}^n \ell(\yhtree_t^\vv(y),y_t) -  \inf_{\uu \in \mathcal{F}_\vv} \sum_{t=1}^n \ell(\uu_t(y),y_t)\right\}\right]}_{(\star\star)}
  \end{aligned}\right\}.\label{eq:interval}
\end{align}}

We now bound terms $(\star)$ and $(\star\star)$ individually by instantiating specific choices for $(\hat{z}_t)_{t\leq{}n}$ and $\crl*{\yhtree^{\vv}}$.

\paragraph{Term $(\star)$}
We select $(\hat{z}_t)_{t\leq{}n}$ using the Aggregating Algorithm as configured in \pref{lem:aggregating_finite}, taking $\cF$ to be the finite collection of sequences $\crl*{\yhtree^{\vv}}_{\vv\in{}V^{\gamma}}$. Since each tree has the property that $\yhtree^{\vv}_{t}$ only depends on $y_{1:t-1}$, \pref{lem:aggregating_finite} indeed applies, which means that for any sequence $\yr[n]\in\brk*{K}^{n}$ of labels the algorithm deterministically satisfies the regret inequality
\[
\sum_{t=1}^n \ell(\hat{z}_t,y_t) - \min_{\vv \in V^\gamma}\sum_{t=1}^n \ell(\yhtree_t^\vv(y),y_t) \leq{} \log\abs*{V^{\gamma}} + 2.
\]
Since the algorithm guarantees $\nrm*{\hat{z}_{t}}_{\infty}\leq{}\log(Kn)$, one can verify that $\hat{z}_{t}\in\cZ$. Furthermore, $\hat{z}_{t}$ depends only on $\yr[t-1]$, and so the predictions of the Aggregating Algorithm are a valid choice for the infimum in $(\star)$. This implies that
\[
(\star) \leq{} \sup_{\x}\log\abs*{V^{\gamma}} + 2 \leq \log\cN_{2}(\gamma, \ls\circ\cF) + 2,
\]
since the regret inequality holds for every possible draw of $\yr[n]$ in the expression $(\star)$.

\paragraph{Term $(\star\star)$}

First, observe that each tree class $\cF_{\vv}$ is uniformly bounded in the sense that \[\sup_{\uu\in\cF_{\vv}}\sup_{y\in\brk*{K}^{n}}\max_{t\in\brk*{n}}\nrm*{\uu_{t}(y)}_{\infty}<\infty.\] This holds because $\uu_{t}(y)$ is either equal to $\vv_{t}(y)$, which is finite, or is equal to $f(\x_{t}(y))$ for some $f\in\cF$, and the class $\cF$ was already assumed to be uniformly bounded.

To bound this term we need a variant of the sequential Rademacher complexity regret bound of \citep{RakSriTew10}, which shows that there exists a deterministic strategy for competing against any collection of trees. This is proven in the auxiliary \pref{lem:rademacher_strategy} following this proof.

In particular, for each tree class $\cF_{\vv}$, there exists a deterministic strategy $\hat{y}_{t}^{\vv}$ that guarantees the inequality
\[
  \sum_{t=1}^n \ell(\hat{y}^{\vv}_{t},y_t) -  \inf_{\uu \in \mathcal{F}_\vv} \sum_{t=1}^n \ell(\uu_t(y),y_t) \leq
  2\cdot\max_{\y,\y'}\Enn_{\eps}\sup_{\uu\in{}\cF_{\vv}}\left[ \sum_{t=1}^n \eps_{t}\ell(\uu_{t}(\y_{1:t-1}(\eps)),\y'_t(\eps))\right] + 2,
 \]
 holds for every sequence, where the supremum on the right-hand-side ranges over $\brk*{K}$-valued binary trees. Futhermore, $\yh_{t}^{\vv}$ is guaranteed by \pref{lem:rademacher_strategy} to lie in the class $\cZ$. We choose this strategy for the collection $\crl*{\yhtree^{\vv}}$ being minimized over in \pref{eq:interval}. Since the regret inequality from \pref{lem:rademacher_strategy} holds deterministically for all sequences $y$ for each $\vv$, we have that
 \[
   (\star\star) \leq{} 2\cdot\max_{\vv\in{}V^{\gamma}}\max_{\y,\y'}\Enn_{\eps}\sup_{\uu\in{}\cF_{\vv}}\left[ \sum_{t=1}^n \eps_{t}\ell(\uu_{t}(\y_{1:t-1}(\eps)),\y'_t(\eps))\right] + 2.
 \]
 For each choice of $\vv$, $\y$, $\y'$ at the outer supremum, we define a class of real-valued trees $W_{\vv, \y, \y'}$  via  $\crl*{(\ww_{t})_{t\leq{}n}\;:\;\ww_{t}(\eps) \ldef \ls(\uu_{t}(\y(\eps_{1:t-1})), \y'_{t}(\eps)) \mid{} \uu\in\cF_{\vv}}$. \pref{lem:chaining_trees} then implies
 \[
   (\star\star) \leq{} 2\max_{\vv\in{}V^{\gamma}}\max_{\y,\y'}\inf_{\alpha>0}\crl*{
      4\alpha{}n + 12\int_{\alpha}^{\mathrm{rad}_{2}(W_{\vv,\y,\y'})}\sqrt{n\log\cN_{2}(\delta, W_{\vv,\y,\y'})}d\delta
    } + 2,
 \]
 with the real-valued covering number $\cN_{2}$ and radius $\mathrm{rad}_{2}$ defined as in \pref{lem:chaining_trees}.

 We now show how to bound this covering number in terms of the covering number for $\cF_{\vv}$. Suppose that $Z$ is a collection of $\bbR^{K}$-valued $K$-ary trees that form a $\delta$-cover for $\cF_{\vv}$ in the sense of \pref{def:cover_general}. Then we have
 \begin{align*}
   &\sup_{\uu\in\cF_{\vv}}\max_{\eps\in\pmo^{n}}\inf_{\zz\in{}Z}\sqrt{\frac{1}{n}\sum_{t=1}^{n}\prn*{\ls(\uu_t(\y(\eps)),\y'_t(\eps)) - \ls(\zz_{t}(\y(\eps)),\y'_t(\eps))}^{2}} \\
   &\leq{} \sup_{\uu\in\cF_{\vv}}\max_{\eps\in\pmo^{n}}\inf_{\zz\in{}Z}\sqrt{\frac{1}{n}\sum_{t=1}^{n}\max_{y'_{t}\in\brk{K}}\prn*{\ls(\uu_t(\y(\eps)),y'_t) - \ls(\zz_t(\y (\eps)),y'_t)}^{2}} \\
   &\leq{} \sup_{\uu\in\cF_{\vv}}\max_{y\in\brk*{K}^{n}}\inf_{\zz\in{}Z}\sqrt{\frac{1}{n}\sum_{t=1}^{n}\max_{y'_{t}\in\brk{K}}\prn*{\ls(\uu_t(y),y'_t) - \ls(\zz_t(y),y'_t)}^{2}}\\
   &\leq{} \delta.
 \end{align*}
 This implies that for any cover of $\cF_{\vv}$ in the sense of \pref{def:cover_general} we can construct a cover for $W_{\vv,\y,\y'}$ at the same scale using the construction $\crl*{(\ww_{t})_{t\leq{}n}\;:\;\ww_{t}(\eps) \ldef \ls(\zz_{t}(\y(\eps_{1:t-1})), \y'_{t}(\eps)) \mid{} \zz\in{}Z}$. Consequently, we have
  \[
    (\star\star) \leq{} 2\max_{\vv\in{}V^{\gamma}}\inf_{\alpha>0}\crl*{
      4\alpha{}n + 12\int_{\alpha}^{\mathrm{rad}_{2}(\cF_{\vv})}\sqrt{n\log\cN_{2}(\delta, \ls\circ\cF_{\vv})}d\delta
    } + 2.
  \]
  In light of \pref{lem:fv_properties}, this is further upper bounded by
  \begin{align*}
    (\star\star) &\leq{} 2\inf_{\alpha>0}\crl*{
      4\alpha{}n + 12\int_{\alpha}^{\gamma}\sqrt{n\log\prn*{\cN_{2}(\delta, \ls\circ\cF, \x)n}}d\delta
                   } + 2 \\
    &\leq{} 2\inf_{\alpha>0}\crl*{
      4\alpha{}n + 12\int_{\alpha}^{\gamma}\sqrt{n\log\prn*{\cN_{2}(\delta, \ls\circ\cF)n}}d\delta
    } + 2.    
  \end{align*}

\paragraph{Final bound}
Combining $(\star)$ and $(\star\star)$, we have
\[
  \mathcal{V}_n(\F)
  \leq{} \log\cN_{2}(\gamma, \ls\circ\cF) + \inf_{\gamma\geq{}\alpha>0}\crl*{
      8\alpha{}n + 24\int_{\alpha}^{\gamma}\sqrt{n\log\prn*{\cN_{2}(\delta, \ls\circ\cF)n}}d\delta
    } + 4.
\]
for any fixed $\gamma$. Optimizing over $\gamma$ yields the result.
\end{proof}

\begin{lemma}
\label{lem:fv_properties}
Let $\cF_{\vv}$ be defined as in the proof of \pref{thm:logistic_minimax} for trees $\vv$ and $\x$ and scale $\gamma$. Then it holds that
\begin{enumerate}
\item $\mathcal{N}_2(\gamma,\ell \circ \cF_{\vv} ) \leq{} 1$.
\item $\mathcal{N}_2(\alpha,\ell \circ \cF_{\vv} ) \leq{} n\cdot{}\mathcal{N}_2(\alpha,\ell \circ \cF, \x )$ for all $\alpha>0$.
\end{enumerate}

\end{lemma}
\begin{proof}[\pfref{lem:fv_properties}]~\\
\textbf{First claim}~~~~
This is essentially by construction. Recall that each element of $\cF_v$ is of the form
\[
(\uu_{f,y})_{t}(y') \ldef f(\x_{t}(y'))\ind\crl*{y'_1=y_1,\ldots,y'_{t-1}=y_{t-1}} + \vv_{t}(y')\ind\crl*{\neg{}(y'_1=y_1,\ldots,y'_{t-1}=y_{t-1})}.
\]
for some path $y\in\brk*{K}^n$ and $f\in\cF$ for which
\begin{equation}
\label{eq:radius_gamma}
\sqrt{\frac{1}{n} \sum_{t=1}^n \max_{y''_t\in\brk*{K}}(\ell(f(\x_t(y)),y''_t) - \ell(\vv_t(y),y''_t) )^2} \le \gamma.
\end{equation}
These properties imply that $\crl*{\vv}$ is a sequential $\gamma$-cover. Indeed, using the explicit form for $\mb{u}_{f,y}$ above, it can be seen that for each path $y'\in\brk*{K}^{n}$, there exists some time $1<\tau\leq{}n+1$ such that
\[
(\uu_{f,y})_{t}(y') = \left\{
\begin{array}{ll}
f(\x_{t}(y')), & \textrm{ if } t < \tau,\\
\vv_{t}(y'), & \textrm{ if } t \geq{} \tau.
\end{array}
\right.
\]
it also holds that $y_{t}=y'_{t}$ for all $t<\tau-1$.

Using this representation we have that for any path $y'\in\brk*{K}^{n}$:
\begin{align*}
&\sqrt{\frac{1}{n} \sum_{t=1}^n \max_{y''_t\in\brk*{K}}(\ell((\uu_{f,y})_{t}(y'),y''_t) - \ell(\vv_t(y'),y''_t) )^2}\\
&= \sqrt{\frac{1}{n} \sum_{t=1}^{\tau-1} \max_{y''_t\in\brk*{K}}(\ell(f(\x_{t}(y'),y''_t) - \ell(\vv_t(y'),y''_t) )^2}.
\end{align*}
Now use that $\x_{1},\ldots,\x_{\tau-1}$ and $\vv_1,\ldots,\vv_{\tau-1}$ only depend on $y'_1,\ldots,y'_{\tau-2}$, and that $y'_1,\ldots,y'_{\tau-2} = y_1,\ldots,y_{\tau-2}$:
\begin{align*}
&= \sqrt{\frac{1}{n} \sum_{t=1}^{\tau-1} \max_{y''_t\in\brk*{K}}(\ell(f(\x_{t}(y),y''_t) - \ell(\vv_t(y),y''_t) )^2} \\
&\leq \sqrt{\frac{1}{n} \sum_{t=1}^{n} \max_{y''_t\in\brk*{K}}(\ell(f(\x_{t}(y),y''_t) - \ell(\vv_t(y),y''_t) )^2} \\
&\leq{} \gamma.
\end{align*}

\textbf{Second claim}~~~~
Let $V$ be a cover for $\ls\circ\cF$ on $\x$ of size $\mathcal{N}_2(\alpha,\ell \circ \cF, \x)$. Assume $\abs*{V}<\infty$ as the claim holds trivially otherwise. We will construct from $V$ a cover $\wt{V}$ for $\ls\circ\F_{\vv}$ with the following procedure:
\begin{itemize}
\item $\wt{V}=\emptyset$.
\item For each $K$-ary $\bbR^{K}$-valued tree $\zz\in{}V$ and each time $\tau\in\crl*{2,\ldots,n+1}$:
\begin{itemize}
\item Construct tree $K$-ary $\bbR^{K}$-valued tree $\zz^{(\tau)}$ via
\[
\zz^{(\tau)}_{t}(y) = \zz_{t}(y)\ind\crl*{t<\tau} + \vv_{t}(y)\ind\crl*{t\geq{}\tau}.
\]
\item Add $\zz^{(\tau)}$ to $\wt{V}$.
\end{itemize}
\end{itemize}
Clearly $\abs*{\wt{V}}\leq{}n\cdot\abs*{V}$. We now show that $\wt{V}$ is an $\alpha$-cover for $\ls\circ\cF_{\vv}$.

Let $\uu_{f,y}$ be an element of $\cF_{\vv}$ of the form described in the proof of the first claim and let $y'\in\brk*{K}^{n}$ be a particular path. Let $\tau$ be such that $(\uu_{f,y})_{t}(y') = f(\x_{t}(y'))\ind\crl*{t<\tau} + \vv_{t}(y')\ind\crl*{t\geq{}\tau}$. Let $\zz\in{}V$ be $\alpha$-close to $f$ on the path $y'$ through $\x$, i.e.
\[
\sqrt{\frac{1}{n} \sum_{t=1}^{n} \max_{y''_t\in\brk*{K}}(\ell(f(\x_{t}(y'),y''_t) - \ell(\zz_t(y'),y''_t) )^2} \leq{} \alpha.
\]
Existence of such a $\zz$ is guaranteed by the cover property of $V$. We will show that $\zz^{(\tau)}$ is $\alpha$-close to $\uu_{f,y}$ on $y'$. Indeed, we have
\begin{align*}
&\sqrt{\frac{1}{n} \sum_{t=1}^n \max_{y''_t\in\brk*{K}}(\ell((\uu_{f,y})_{t}(y'),y''_t) - \ell(\zz^{(\tau)}_t(y'),y''_t) )^2}\\
&= \sqrt{\frac{1}{n} \sum_{t=1}^{\tau-1} \max_{y''_t\in\brk*{K}}(\ell(f(\x_{t}(y'),y''_t) - \ell(\zz_{t}(y'),y''_t) )^2
+ \frac{1}{n} \sum_{t=\tau}^{n} \max_{y''_t\in\brk*{K}}(\ell(\vv_{t}(y'),y''_t) - \ell(\vv_t(y'),y''_t) )^2} \\
&= \sqrt{\frac{1}{n} \sum_{t=1}^{\tau-1} \max_{y''_t\in\brk*{K}}(\ell(f(\x_{t}(y'),y''_t) - \ell(\zz_{t}(y'),y''_t) )^2} \\
&\leq{} \sqrt{\frac{1}{n} \sum_{t=1}^{n} \max_{y''_t\in\brk*{K}}(\ell(f(\x_{t}(y'),y''_t) - \ell(\zz_{t}(y'),y''_t) )^2} \\
&\leq{} \alpha.
\end{align*}
Since this argument works for any $\uu_{f,y}\in\cF_{\vv}$ this establishes that $\wt{V}$ is an $\alpha$-cover of $\cF_{\vv}$.
\end{proof}

The next lemma is almost the same as the sequential Rademacher complexity bound in \cite{RakSriTew10}, with the only technical difference being that the learner competes with a class of trees rather than a class of fixed functions. It is proven using the same argument as in that paper.
\begin{lemma}
  \label{lem:rademacher_strategy}
  Let $U$ be any collection of $\bbR^{K}$-valued $K$-ary trees of depth $n$. Suppose that $C\ldef\sup_{\uu\in{}U}\sup_{y\in\brk*{K}^{n}}\max_{t\in\brk*{n}}\nrm*{\uu_{t}(y)}_{\infty}<\infty$. Then there exists a strategy $\hat{z}_{t}$ that guarantees
  \[
    \sum_{t=1}^{n}\ls(\zh_t, y_t) - \inf_{\uu\in{}U}\sum_{t=1}^{n}\ls(\uu_{t}(y), y_t) \leq{} 2\cdot\max_{\y,\y'}\Enn_{\eps}\sup_{\uu\in{}U}\left[ \sum_{t=1}^n \eps_{t}\ell(\uu_{t}(\y_{1:t-1}(\eps)),\y'_t(\eps))\right] + 2,
  \]
  where $\y$ and $\y'$ are $\brk*{K}$-valued binary trees of depth $n$ and $\eps=(\eps_1,\ldots,\eps_n)$ are Rademacher random variables.

  Furthermore, the predictions $(\hat{z}_{t})_{t\leq{}n}$ satisfy $\nrm*{\hat{z}_{t}}_{\infty}\leq{}\log(Kn)$.

\end{lemma}
\begin{proof}[\pfref{lem:rademacher_strategy}]
      Define $\cZ \ldef{} \crl*{z\in\bbR^{K}\mid{} \nrm*{z}_{\infty} \leq{} C}$. The minimax optimal regret amongst deterministic strategies taking values in $\cZ$ is given by
\begin{align*}
  \mathcal{V}_n(U) & \ldef \dtri*{\inf_{\hat{z}_t\in\bbR^{K}} \max_{y_t \in [K]}}_{t=1}^n\left[ \sum_{t=1}^n \ell(\hat{z}_t,y_t) - \inf_{\uu\in{}U} \sum_{t=1}^n \ell(\uu_t(y),y_t)\right].
\end{align*}
Once again, this proof closely follows the sequential Rademacher complexity bound from \cite{RakSriTew10}.
We only sketch the first few steps for this proof as they are identical to the first few steps of the proof of \pref{thm:logistic_minimax}, which is admissible due to compactness of $\cZ$. Using the minimax swap as in that theorem, we can move to an upper bound of
\begin{align*}
  &\leq \dtri*{\sup_{p_t \in \Delta_{K}}  \mathbb{E}_{y_t \sim p_t}}_{t=1}^n\left[ \sum_{t=1}^n \inf_{\hat{z}_t\in\cZ} \mathbb{E}_{y_t \sim p_t}\left[\ell(\hat{z}_t,y_t)\right] - \inf_{\uu\in{}U} \sum_{t=1}^n \ell(\uu_{t}(y),y_t)\right] \\
  &= \dtri*{\sup_{p_t \in \Delta_{K}}  \mathbb{E}_{y_t \sim p_t}}_{t=1}^n\sup_{\uu\in{}U}\left[ \sum_{t=1}^n \inf_{\hat{z}_t\in\cZ} \mathbb{E}_{y_t \sim p_t}\left[\ell(\hat{z}_t,y_t)\right] - \sum_{t=1}^n \ell(\uu_{t}(y),y_t)\right].
    \intertext{Now we choose $\hat{z}_{t}$ to match the value of $\uu_{t}(y) = \uu_{t}(\yr[t-1])$, which is possible by definition of $\cZ$:}
  &\leq \dtri*{\sup_{p_t \in \Delta_{K}}  \mathbb{E}_{y_t \sim p_t}}_{t=1}^n\sup_{\uu\in{}U}\left[ \sum_{t=1}^n \mathbb{E}_{y_t \sim p_t}\left[\ell(\uu_{t}(y),y_t)\right] - \sum_{t=1}^n \ell(\uu_{t}(y),y_t)\right].
    \intertext{Using Jensen's inequality, we pull the conditional expectaitons in the first term outside the supremum over $\uu$ by introducing a tangent sequence $(y_t')_{t\leq{}n}$, where $y'_{t}$ follows the distribution $p_{t}$ conditioned on $\yr[t-1]$.}
  &\leq \dtri*{\sup_{p_t \in \Delta_{K}}  \mathbb{E}_{y_t,y'_t \sim p_t}}_{t=1}^n\sup_{\uu\in{}U}\left[ \sum_{t=1}^n \ell(\uu_{t}(y),y'_t) - \sum_{t=1}^n \ell(\uu_{t}(y),y_t)\right].
    \intertext{Since $y_t$ and $y'_t$ are conditionally i.i.d., we can introduce a Rademacher random variable $\eps_{t}$ at each timestep $t$ as follows:}
  &= \dtri*{\sup_{p_t \in \Delta_{K}} \mathbb{E}_{y_t,y'_t \sim p_t}\Enn_{\eps_t}}_{t=1}^n\sup_{\uu\in{}U}\left[ \sum_{t=1}^n \eps_{t}\prn*{\ell(\uu_{t}(y),y'_t) - \ell(\uu_{t}(y),y_t)}\right].
    \intertext{To decouple the arguments to the losses from the arugments to the tree $\uu$, we move to a pessimistic upper bound:}
  &\leq \dtri*{\sup_{p_t \in \Delta_{K}} \mathbb{E}_{y_t\sim p_t}\max_{y'_t,y''_t\in\brk*{K}}\Enn_{\eps_t}}_{t=1}^n\sup_{\uu\in{}U}\left[ \sum_{t=1}^n \eps_{t}\prn*{\ell(\uu_{t}(y),y'_t) - \ell(\uu_{t}(y),y''_t)}\right] \\
  &= \dtri*{\max_{y_t,y'_t,y''_t\in\brk*{K}}\Enn_{\eps_t}}_{t=1}^n\sup_{\uu\in{}U}\left[ \sum_{t=1}^n \eps_{t}\prn*{\ell(\uu_{t}(y),y'_t) - \ell(\uu_{t}(y),y''_t)}\right]. \\
  \intertext{We now complete the symmetrization as follows:}
  &\leq \dtri*{\max_{y_t,y'_t,y''_t\in\brk*{K}}\Enn_{\eps_t}}_{t=1}^n\sup_{\uu\in{}U}\left[ \sum_{t=1}^n \eps_{t}\ell(\uu_{t}(y),y'_t)\right]
    + \dtri*{\max_{y_t,y'_t,y''_t\in\brk*{K}}\Enn_{\eps_t}}_{t=1}^n\sup_{\uu\in{}U}\left[ \sum_{t=1}^n \eps_{t}\ell(\uu_{t}(y),y''_t)\right] \\
  &= 2\cdot\dtri*{\max_{y_t,y'_t\in\brk*{K}}\Enn_{\eps_t}}_{t=1}^n\sup_{\uu\in{}U}\left[ \sum_{t=1}^n \eps_{t}\ell(\uu_{t}(y),y'_t)\right]\\
  &= 2\cdot\max_{\y,\y'}\Enn_{\eps}\sup_{\uu\in{}U}\left[ \sum_{t=1}^n \eps_{t}\ell(\uu_{t}(\y_{1:t-1}(\eps)),\y'_t(\eps))\right].
\end{align*}
In the last line $\y$ and $\y'$ are taken to be $\brk*{K}$-valued binary trees of depth $n$, so that $\y_{t}(\eps) = \y_{t}(\eps_{1},\ldots\eps_{t-1})$ and likewise for $\y'$.

Finally, to guarantee the boundedness of predictions claimed in the lemma statement, we apply \pref{lem:logistic_bounded} to the minimax optimal strategy, for which we just showed regret is bounded by the sequential Rademacher complexity.
\end{proof}

The last auxiliary lemma in this section is a slight variant of the Dudley entropy integral bound for sequential Rademacher complexity. This lemma can be extracted from the proof of Theorem 4 in \cite{rakhlin2015ptrf}. We do not repeat the proof here.
\begin{lemma}
  \label{lem:chaining_trees}
  Let $W$ be a collection of $\bbR$-valued binary trees. Define $\cN_{p}(\alpha, W)$ to be the size of the smallest class of trees $V$ such that
  \begin{equation}
    \label{eq:cover_real}
    \forall \ww\in{}W, \eps\in\pmo^{n},  ~\exists \vv \in V ~\textnormal{s.t.}~ \left(\frac{1}{n} \sum_{t=1}^n\prn*{\ww_{t}(\eps) - \vv_{t}(\eps)}^p\right)^{1/p} \le \alpha.
  \end{equation}
  Let $\mathrm{rad}_{p}(W)\ldef{}\min\crl*{\alpha\mid{}\cN_{p}(\alpha, W) = 1}$. Then it holds that
  \begin{equation}
    \label{eq:chaining_real}
    \Enn_{\eps}\sup_{\ww\in{}W}\sum_{t=1}^{n}\eps_{t}\ww_{t}(\eps) \leq{} \inf_{\alpha>0}\crl*{
      4\alpha{}n + 12\int_{\alpha}^{\mathrm{rad}_{2}(W)}\sqrt{n\log\cN_{2}(\delta, W)}d\delta
    }.
    \end{equation}

\end{lemma}


\subsection{Details from \pref{sec:log_loss}}
\label{app:logloss}

We first define a suitable notion of sequential cover for the log loss setting:
\begin{definition}
  \label{def:cover_real}
  For a fixed $\cX$-valued binary tree $\x$, define $\cN_{\infty}(\alpha, \cF, \x)$ to be the size of the smallest set of $\brk*{0,1}$-valued binary trees $V$ such that
  \[
    \forall f \in \F, ~\eps \in \pmo^{n},  ~\exists \vv \in V ~\textnormal{s.t.}~ \max_{t\in\brk*{n}}\abs*{f(\x_{t}(\eps)) - \vv_{t}(\eps)} \leq{} \alpha.
  \]Further, define $\mathcal{N}_{\infty}(\alpha,\F) = \sup_\x \mathcal{N}_{\infty}(\alpha,\F,\x)$. 
\end{definition}
We also require a generalization of \pref{def:cover_real} for general tree classes.
\begin{definition}
  \label{def:cover_real_tree}
  For a class of $\brk*{0,1}$-valued binary trees $U$, define $\cN_{\infty}(\alpha, U)$ to be the size of the smallest set of $\brk*{0,1}$-valued binary trees $V$ such that
  \[
    \forall \uu \in U, ~\eps \in \pmo^{n},  ~\exists \vv \in V ~\textnormal{s.t.}~ \max_{t\in\brk*{n}}\abs*{\uu_{t}(\eps) - \vv_{t}(\eps)} \leq{} \alpha.
  \]
\end{definition}

We now turn to the proof of \pref{thm:logloss_minimax}. It follows the same structure as the proof in \pref{app:general_class} with a few technical differences related the slightly different notion of cover used and the non-Lipschitzness of the log loss. We first give one more definition.

\begin{definition}
  For any $\delta\in(0,1/2]$, we define the truncation to the range $\brk*{\delta, 1-\delta}$ via $\clip(p) = \max\crl*{\delta, \min\crl*{1-\delta, p}}$.
\end{definition}

The following proposition is a simple consequence of the fact that $\clip$ is $1$-Lipschitz.
\begin{proposition}
  \label{prop:clip_covering}
  For any class of trees $U$ and any $\delta\in(0,1/2]$, $\cN_{\infty}(\alpha, \clip\circ{}U)\leq{}\cN_{\infty}(\alpha, U)$.
\end{proposition}

\begin{proof}[\pfref{thm:logloss_minimax}]
  The proof is very similar to that of \pref{thm:logistic_minimax}. When it would otherwise be repetitive we will only sketch details and instead refer back to the proof of that theorem.

  To begin, fix $\delta\in(0,1/2]$. We will work with the clipped class $\cF^{\delta} = \clip\circ{}\cF$ just as in \cite{PLG}. It was shown there that
  \[
    \cV_{n}^{\log}(\cF) \leq{} \cV_{n}^{\log}(\cF^{\delta}) + \delta{}n.
  \]

  With this restriction, we proceed exactly as in the proof of \pref{thm:logistic_minimax}. First, restrict the learner's predictions to $\brk*{\delta,1-\delta}$ to guarantee boundedness of the loss:
  \begin{align*}
  \mathcal{V}^{\mathrm{log}}_n(\F^{\delta}) & = \dtri*{\sup_{x_t\in\cX} \inf_{\lpred_t\in\brk*{0,1}} \max_{y_t \in \crl*{0,1}}}_{t=1}^n\left[ \sum_{t=1}^n \logloss(\lpred_t,y_t) - \inf_{f \in \mathcal{F}^{\delta}} \sum_{t=1}^n \logloss(f(x_t),y_t)\right]\\
                    & \leq{} \dtri*{\sup_{x_t\in\cX} \inf_{\lpred_t\in\brk*{\delta,1-\delta}} \max_{y_t \in \crl*{0,1}}}_{t=1}^n\left[ \sum_{t=1}^n \logloss(\lpred_t,y_t) - \inf_{f \in \mathcal{F}^{\delta}} \sum_{t=1}^n \logloss(f(x_t),y_t)\right].
\end{align*}
Since compactness holds, we can apply the minimax theorem and manipulate terms in the same fashion as in the proof of \pref{thm:logistic_minimax} to arrive at the following expression
\begin{align}
&= \sup_{\x, \p} \mathbb{E}_{y \sim \p}\left[ \sum_{t=1}^n \inf_{\lpred_t\in\brk*{\delta,1-\delta}} \mathbb{E}_{y_t \sim \p_t(y)}\left[\logloss(\lpred_t,y_t)\right] - \inf_{f \in \mathcal{F}} \sum_{t=1}^n \logloss(f(\x_t(y)),y_t)\right]\label{eq:logloss_tree}.
\end{align}
In the final line above $\x$ and $\p$ are $\X$- and $\Delta_{\crl*{0,1}}$-valued binary trees (indexed by $\zo$) of depth $n$. The notation ``$y \sim \p$'' refers to the process in which we first draw $y_1 \sim \p_1$, then draw $y_t \sim \p_t(y_1,\ldots,y_{t-1})$ for subsequent timesteps $t$.

Let $V^\gamma$ be a minimal $\gamma$-sequential cover of $\mathcal{F}$ on the tree $\mathbf{x}$ with respect to the $L_{\infty}$ norm in the sense of \pref{def:cover_real}. 

Following the proof of \pref{thm:logistic_minimax}, we define a collection of $\brk*{\delta,1-\delta}$-valued binary trees for each element of $V^{\gamma}$, with the tree $\x$ fixed. For each tree $\vv \in V^\gamma$, each $f \in \F^{\delta}$, we define a class of trees $\cF_{\vv}^{\delta}$ as follows:

\begin{itemize}[leftmargin=*]
\item Initially $\cF^{\delta}_{\vv} = \emptyset$.
\item For each $f\in\Fclip$ and $y\in\crl*{0,1}^{n}$ with $\max_{t\in\brk*{n}}\abs*{f(\x_{t}(y))-\vv_{t}(y)} \le \gamma$: 
\begin{itemize}
\item Define a $\brk*{\delta, 1-\delta}$-valued binary tree $\uu_{f,y}$ via: For each $y'\in\pmo^{n}$, 
\[
  (\uu_{f,y})_{t}(y'_{1:t-1}) \ldef f(\x_{t}(y'))\ind\crl*{y'_1=y_1,\ldots,y'_{t-1}=y_{t-1}} + \vv_{t}(y')\ind\crl*{\neg{}(y'_1=y_1,\ldots,y'_{t-1}=y_{t-1})}.
\]
(So that $\uu_{f,y}$ is equal to $f\circ\x$ on the path $y$, and equal to $\vv$ everywhere else.)
\item Add $\uu_{f,y}$ to $\cF_{\vv}^{\delta}$.
\end{itemize}
\end{itemize}

Just like the construction in \pref{thm:logistic_minimax}, $\cF_{\vv}^{\delta}$ has two properties: Its $L_{\infty}$ covering number is bounded in terms of the $L_{\infty}$ covering number of the class $\Fclip\circ{}\x$, and its $L_{\infty}$ radius is bounded by $\gamma$. These properties are stated in \pref{lem:fv_properties_logloss}.

On any path $y \in \zo^n$ and for each $f \in \F$, there exist $\vv \in V^\gamma$ and $\uu \in \F_\vv^{\delta}$ such that $f(\x_t(y)) = \uu_{t}(y)$. This is because a $\vv$ that is $\gamma$-close to $f$ on the path $y$ through $\x$ is guaranteed by the cover property of $V^{\gamma}$, and so we can take $\uu_{f,y}$ in $\cF_{\vv}$ as the desired $\uu$. This implies that
$$
\inf_{f \in \F }\sum_{t=1}^n \logloss(f(\x_t(y)),y_t) \ge \min_{\vv \in V^\gamma} \inf_{\uu \in \F_\vv^{\delta} }\sum_{t=1}^n \logloss(\uu_t(y),y_t).
$$
Returning to the minimax rate, all the properties of the tree families we have established so far imply
\begin{align}
\notag & \mathcal{V}^{\mathrm{log}}_n(\F^{\delta})  \\ & \le \sup_{\x}\inf_{V^{\gamma}}\inf_{\crl*{\cF_{\vv}^{\delta}}_{\vv\in{}V^{\gamma}}}\sup_{\p}\mathbb{E}_{y \sim \p}\left[ \sum_{t=1}^n \inf_{\lpred_t\in\brk*{\delta, 1-\delta}} \mathbb{E}_{y_t \sim \p_t(y)}\left[\logloss(\lpred_t,y_t)\right] - \min_{\vv \in V^\gamma} \inf_{\uu \in \mathcal{F}_\vv^{\delta}} \sum_{t=1}^n \logloss(\uu_t(y),y_t)\right]. \notag
\end{align}
As in the proof of \pref{thm:logistic_minimax}, we introduce a family of trees representing the minimax optimal strategy competing with each tree class $\cF_{\vv}^{\delta}$. For each $\vv\in{}V^{\gamma}$, we introduce a $\brk*{\delta, 1-\delta}$-valued binary tree $\phtree^{\vv}$, with $\phtree^{\vv}_{t}:\zo^{t-1}\to\deltarange$.
{\small
\begin{align}
  & \begin{aligned}= \sup_{\x}\inf_{V^{\gamma}}\inf_{\crl*{\cF_{\vv}^{\delta}}_{\vv\in{}V^{\gamma}}}\inf_{\crl*{\phtree^{\vv}}_{\vv\in{}V^{\gamma}}}\sup_{\p} \mathbb{E}_{y \sim \p}\biggl[& \sum_{t=1}^n \inf_{\hat{p}_t\in\deltarange} \mathbb{E}_{y_t \sim \p_t(y)}\left[\logloss(\hat{p}_t,y_t)\right] \\
    &\hspace{-3em}- \min_{\vv \in V^\gamma}\left\{ \sum_{t=1}^n \logloss(\phtree_t^\vv(y),y_t) - \sum_{t=1}^n \logloss(\phtree_t^\vv(y),y_t) +  \inf_{\uu \in \mathcal{F}_\vv^{\delta}} \sum_{t=1}^n \logloss(\uu_t(y),y_t)\right\}\biggr].\end{aligned} \notag \\
& \leq\sup_{\x}\inf_{V^{\gamma}}\inf_{\crl*{\cF_{\vv}^{\delta}}_{\vv\in{}V^{\gamma}}}\inf_{\crl*{\phtree^{\vv}}_{\vv\in{}V^{\gamma}}}\left\{\begin{aligned}~& \underbrace{\sup_{\p} \mathbb{E}_{y \sim \p}\biggl[ \sum_{t=1}^n \inf_{\hat{p}_t\in\deltarange} \mathbb{E}_{y_t \sim \p_t(y)}\left[\logloss(\hat{p}_t,y_t)\right] - \min_{\vv \in V^\gamma}\sum_{t=1}^n \logloss(\phtree_t^\vv(y),y_t)\biggr]}_{(\star)} \\
  & + \underbrace{\sup_{\p}\mathbb{E}_{y \sim \p}\left[ \max_{\vv \in V^\gamma}\left\{ \sum_{t=1}^n \logloss(\phtree_t^\vv(y),y_t) -  \inf_{\uu \in \mathcal{F}_\vv^{\delta}} \sum_{t=1}^n \logloss(\uu_t(y),y_t)\right\}\right]}_{(\star\star)}
  \end{aligned}\right\}.\label{eq:star_logloss}
\end{align}}

We now bound the terms $(\star)$ and $(\star\star)$ individually as follows:

\paragraph{Term $(\star)$}
We select $(\hat{p}_t)_{t\leq{}n}$ using the Aggregating Algorithm as configured in \pref{lem:vovk_logloss}, with $W$ as the finite collection of sequences $\crl*{\phtree^{\vv}}_{\vv\in{}V^{\gamma}}$. This is possible because $\phtree^{\vv}_{t}$ only depends on $y_{1:t-1}$.
\[
\sum_{t=1}^n \logloss(\hat{p}_t,y_t) - \min_{\vv \in V^\gamma}\sum_{t=1}^n \logloss(\phtree_t^\vv(y),y_t) \leq{} \log\abs*{V^{\gamma}} + 2.
\]
Since the algorithm's predictions lie in $\deltarange$ they are a valid choice for the infimum in $(\star)$. This implies that
\[
(\star) \leq{} \sup_{\x}\log\abs*{V^{\gamma}}\leq \log\cN_{\infty}(\gamma, \cF^{\delta}).
\]

\paragraph{Term $(\star\star)$}

First, note that we can take each tree class $\cF_{\vv}^{\delta}$ to be $\deltarange$-valued without loss of generality. We exhibit a deterministic strategy for each class by invoking the generic minimax regret bound \pref{lem:logloss_covering}. Since the collection is $\deltarange$-valued, the lemma guarantees existence of a deterministic strategy $(\hat{p}_t)_{t\leq{}n}$ with a regret bound of
{\small
  \begin{align*}
    &\sum_{t=1}^{n}\logloss(\hat{p}_t, y_t) - \inf_{\uu\in{}\cF_{\vv}^{\delta}}\sum_{t=1}^{n}\logloss(\uu_{t}(y), y_t) \\
    &\leq{}
    2n\delta\log(1/\delta) \\
    &~~~~+ \frac{C}{\delta}\log\cN_{\infty}(\gamma, \cF_{\vv}^{\delta}) + \inf_{\alpha\in(0, \gamma]}\crl*{
      \frac{4n\alpha}{\delta} + 30\sqrt{\frac{2n}{\delta}}\int_{\alpha}^{\gamma}\sqrt{\log\cN_{\infty}(\rho,\cF_{\vv}^{\delta})}d\rho
      + \frac{8}{\delta}\int_{\alpha}^{\gamma}\log\cN_{\infty}(\rho,\cF_{\vv}^{\delta})d\rho
      }.
  \end{align*}}

  By \pref{lem:fv_properties_logloss}, $\mathcal{N}_{\infty}(\gamma, \cF_{\vv}^{\delta} ) \leq{} 1$, and so we can drop the leading covering number term in the bound:
  {\small
  \[
    \leq{}
    2n\delta\log(1/\delta)
    + \inf_{\alpha\in(0, \gamma]}\crl*{
      \frac{4n\alpha}{\delta} + 30\sqrt{\frac{2n}{\delta}}\int_{\alpha}^{\gamma}\sqrt{\log\cN_{\infty}(\rho,\cF_{\vv}^{\delta})}d\rho
      + \frac{8}{\delta}\int_{\alpha}^{\gamma}\log\cN_{\infty}(\rho,\cF_{\vv}^{\delta})d\rho
    }.
  \]}
  \pref{lem:fv_properties_logloss} also implies that we can upper bound the covering number in terms of that of $\cF^{\delta}$:
    {\small\[
    \leq{}
    2n\delta\log(1/\delta)
    + \inf_{\alpha\in(0, \gamma]}\crl*{
      \frac{4n\alpha}{\delta} + 30\sqrt{\frac{2n}{\delta}}\int_{\alpha}^{\gamma}\sqrt{\log(n\cN_{\infty}(\rho,\cF^{\delta}, \x))}d\rho
      + \frac{8}{\delta}\int_{\alpha}^{\gamma}\log(n\cN_{\infty}(\rho,\cF^{\delta}, \x))d\rho
    }.
  \]}
  Since the regret inequality holds deterministically and uniformly for all sequences $y$ for each $\vv$, we have that
{\small\begin{align*}
   &(\star\star) \\&    \leq{}
    2n\delta\log(1/\delta)
    + \inf_{\alpha\in(0, \gamma]}\crl*{
      \frac{4n\alpha}{\delta} + 30\sqrt{\frac{2n}{\delta}}\int_{\alpha}^{\gamma}\sqrt{\log(n\cN_{\infty}(\rho,\cF^{\delta}, \x))}d\rho
      + \frac{8}{\delta}\int_{\alpha}^{\gamma}\log(n\cN_{\infty}(\rho,\cF^{\delta}, \x))d\rho
    }.
 \end{align*}}

  \paragraph{Final bound}
We combine $(\star)$ and $(\star\star)$, take the supremum over $\x$, and apply \pref{prop:clip_covering} to conclude that $\mathcal{V}^{\mathrm{log}}_n(\F)$ is bounded by
{\small
\begin{align*}
  &3n\delta\log(1/\delta)
  + \log\cN_{\infty}(\gamma, \cF)\\
    &~~~~+ \inf_{\alpha\in(0, \gamma]}\crl*{
      \frac{4n\alpha}{\delta}
       + 30\sqrt{\frac{2n}{\delta}}\int_{\alpha}^{\gamma}\sqrt{\log(n\cN_{\infty}(\rho,\cF))}d\rho
      + \frac{8}{\delta}\int_{\alpha}^{\gamma}\log(n\cN_{\infty}(\rho,\F))d\rho
    }.
\end{align*}}
The theorem statement uses that we are free to choose any value for $\delta$ and $\gamma$.
\end{proof}

The remaining lemmas in this section mirror those used in the proof of \pref{thm:logistic_minimax}, with the most substantive difference being that we required a more refined chaining bound for general classes under the log loss from \cite{RakSri15}. We omit their proofs.

\begin{lemma}
\label{lem:fv_properties_logloss}
Let $\cF_{\vv}^{\delta}$ be defined as in the proof of \pref{thm:logistic_minimax} for trees $\vv$ and $\x$ and scale $\gamma$. Then it holds that
\begin{enumerate}
\item $\mathcal{N}_{\infty}(\gamma, \cF_{\vv}^{\delta} ) \leq{} 1$.
\item $\mathcal{N}_{\infty}(\alpha, \cF^{\delta}_{\vv} ) \leq{} n\cdot{}\mathcal{N}_{\infty}(\alpha, \cF^{\delta}, \x )$ for all $\alpha>0$.
\end{enumerate}
\end{lemma}
Note that the covering number (\pref{def:cover_real_tree}) was defined for trees indexed by $\pmo^{n}$, but trees in $\cF_{\vv}^{\delta}$ are indexed by $\zo^{n}$. We overload the covering number in the natural way in the lemma above and subsequent lemmas.

\begin{lemma}[\cite{PLG}]
  \label{lem:vovk_logloss}
  Let $W$ be any class of $\deltarange$-valued binary trees of depth $n$. Then Vovk's Aggregating Algorithm configured with $W$ as a benchmark class of experts generates predictions $(\hat{p}_t)_{t\leq{}n}$ that enjoy regret
  \begin{equation}
    \label{eq:logistic_finite}
    \sum_{t=1}^{n}\logloss(\hat{p}_t, y_t) - \min_{\ww\in{}W}\sum_{t=1}^{n}\logloss(\ww_{t}(y), y_t) \leq{} \log\abs*{\cW}.
  \end{equation}
  Furthermore, the predictions $(\hat{p}_t)_{t\leq{}n}$ lie in $\deltarange$.
\end{lemma}

\begin{lemma}[Extracted from \cite{RakSri15}]
  \label{lem:logloss_covering}
  Let $W$ be any class of $\deltarange$-valued binary trees of depth $n$. Then there exists a deterministic prediction strategy $(\hat{p}_t)_{t\leq{}n}$ that enjoys regret
  \begin{align*}
    &\sum_{t=1}^{n}\logloss(\hat{p}_t, y_t) - \inf_{\ww\in{}W}\sum_{t=1}^{n}\logloss(\ww_{t}(y), y_t) \\
    &\leq{}
    2n\delta\log(1/\delta)
    + \frac{C}{\delta}\log\cN_{\infty}(\gamma, W) \\ 
    &~~~~+ \inf_{\alpha\in(0, \gamma]}\crl*{
      \frac{4n\alpha}{\delta} + 30\sqrt{\frac{2n}{\delta}}\int_{\alpha}^{\gamma}\sqrt{\log\cN_{\infty}(\rho,W)}d\rho
      + \frac{8}{\delta}\int_{\alpha}^{\gamma}\log\cN_{\infty}(\rho,W)d\rho
      },
  \end{align*}
  for all $\gamma>0$ and for some absolute constant $C>0$. The predictions $(\hat{p}_t)_{t\leq{}n}$ lie in $\deltarange$.
\end{lemma}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
