%!TEX root = paper.tex

In this section we show that our analysis techniques 
% developed for the general function class bounds in the previous section 
can also be used to obtain improved rates for prediction with the \emph{log loss} $\logloss:\Delta_{K}\times{}\brk*{K}\to\bbR$, defined via $\logloss(p, y)=-\log(p_{y})$. Characterizing optimal rates for online prediction with the log loss is a fundamental problem \citep{mf-up-98}, but there have been very few successful attempts to provide rates for general classes of functions. \cite{CesLug99} studied the multiclass case,\footnote{In literature on log loss the class size $K$ we use is typically referred to as the \emph{alphabet size}.} but provide bounds only in terms of pointwise covering numbers; this can lead to vacuous bounds even for well-behaved classes such as Hilbert spaces. More recently, \cite{RakSri15} provided a bound for general classes in terms of sequential covering numbers, but their bound is known to not be tight for certain classes (see the discussion in their Section 6). We improve on their rates uniformly.

Note that the problems of learning with the logistic loss and learning with the log loss can easily be mapped onto each other to provide coarse rates.
One can trivially write $\logloss(p,y)$ as 
$\ell(\mb{\sigma}^{+}(p),y)$ for any distribution $p \in \Delta_K$, and likewise it holds that $\ls(z,y)=\logloss(\mb{\sigma}(z),y)$ for any $z\in\bbR^{K}$. To obtain rates for competing with a class $\cF:\cX\to\Delta_{K}$ under the log loss, we can use this relationship to get a bound by applying \pref{thm:logistic_minimax} with the class $\mb{\sigma}^{+} \circ \F$. This bound improves over \cite{RakSri15} in the low complexity regime, though it is worse for high complexity classes.

By combining the style of proof in \pref{thm:logistic_minimax} with key technical observations from \cite{RakSri15}, we provide a bound on minimax rate for log loss that both uniformly improves on the rate in \cite{RakSri15} for binary outcome case and also extends in general to $K>2$. For brevity we present results only for the binary case. In this case we can restrict to real-valued outputs: We let $\logloss:\brk*{0,1}\times{}\crl*{0,1}\to\bbR$ be defined by $\logloss(p, y) = -y\log(p)-(1-y)\log(1-p)$, and take both $\cF$ and the learner's predictions to be $\brk*{0,1}$-valued. The minimax regret for learning with the log loss is given by

\begin{equation} \label{eq:minimax_logloss}
\mathcal{V}^{\mathrm{log}}_n(\F) = \dtri*{\sup_{x_t\in\cX} \inf_{\lpred_t\in\brk*{0,1}} \max_{y_t \in \crl*{0,1}}}_{t=1}^n\left[ \sum_{t=1}^n \ell(\lpred_t,y_t) - \inf_{f \in \mathcal{F}} \sum_{t=1}^n \ell(f(x_t),y_t)\right].
\end{equation}

The following theorem provides an upper bound on the minimax regret in terms of $L_\infty$ covering numbers $\mathcal{N}_{\infty}(\alpha,\F)$ (definition deferred to \pref{app:logloss}).
\begin{theorem}
\label{thm:logloss_minimax}
For any class $\cF\subseteq{}\brk*{0,1}^{\cX}$ and any $\delta\in(0,1/2]$, $\mathcal{V}^{\mathrm{log}}_n(\F)$ is bounded by
\[
\tilde{O}\prn*{\inf_{\gamma\geq{}\alpha>0}\crl*{ \log\cN_{\infty}(\gamma, \cF)
    +     \frac{\alpha{}n}{\delta}
      + \sqrt{\frac{n}{\delta}}\int_{\alpha}^{\gamma}\sqrt{\log\cN_{\infty}(\rho,\cF)}d\rho
      + \frac{1}{\delta}\int_{\alpha}^{\gamma}\log\cN_{\infty}(\rho,\F)d\rho}
     + \delta{}n
    }.\]
where $\tilde{O}$ supresses $\log(n)$ and $\log(1/\delta)$ factors.
\end{theorem}
Comparing to \cite[Theorem 4]{RakSri15}, the only difference is that their bound has an extra $\frac{1}{\delta}$ factor in the leading $\log\cN_{\infty}(\gamma,\cF)$ term above. \pref{thm:logloss_minimax} is strictly better for low-complexity classes, e.g. when $\log\cN_{\infty}(\gamma,\cF)\asymp\prn*{\frac{C}{\gamma}}^{p}$ for $p\leq{}1$.
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
