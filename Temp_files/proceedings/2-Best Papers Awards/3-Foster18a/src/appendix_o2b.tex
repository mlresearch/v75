\subsection{Proof from \pref{sec:online_to_batch}}
\label{app:o2b_proof}

\begin{theorem}
\label{thm:o2b_high_prob}
Let $\cF$ be a class of functions $f: \cX \rightarrow \Delta_K$. Suppose there is an online multiclass learning algorithm over $\cF$ using the log loss that for any data sequence $(x_t, y_t) \in \cX \times [K]$ for $t = 1, 2, \ldots, n$ produces distributions $p_t \in \Delta_K$ such that the following regret bound holds:
\[ \sum_{t=1}^n \logloss(p_t, y_t) - \inf_{f \in \cF}\sum_{t=1}^n \logloss(f(x_t), y_t) \leq R(n).\]
Here $R(n)$ is some function of $n$ and other relevant problem dependent parameters. Then for any given $\delta > 0$ and any (unknown) distribution $\cD$ over $\cX \times [K]$, it is possible to construct a predictor $g: \cX \rightarrow \Delta_K$ using $n$ samples $\{(x_t, y_t)\}_{t=1}^n$ drawn from $\cD$ such that with probability at least $1-\delta$, the excess risk of $g$ is bounded as
\[ \En_{(x, y)}[\logloss(g(x), y)] \leq \inf_{f \in \cF} \En_{(x, y)}[\logloss(f(x), y)] + O\prn*{\frac{\log\prn*{\frac{1}{\delta}}R\prn*{\frac{n}{\log(1/\delta)}} + \log(Kn)\log\prn*{\frac{\log(n)}{\delta}}}{n}}.
\]
\end{theorem}

\begin{proof}[Proof of \pref{thm:o2b_high_prob}]
Recall that the standard online-to-batch conversion \citep{helmboldwarmuth} produces an (improper) predictor using $n$ data samples by running the online algorithm on those samples and stopping at a random time. Then predictor is online algorithm with its the internal state frozen. This predictor has excess risk bounded by the average regret over $n$ rounds, in expectation over the $n$ data samples.

The algorithm to generate the predictor $g$ with the specified excess risk bound in the theorem statement is given below:
\begin{enumerate}
\item Let $M=\lceil\log(2/\delta)\rceil$. Produce $M$ predictors $h_{1},\ldots,h_{M}: \cX \rightarrow \Delta_K$ by using the online-to-batch conversion on the online multiclass learning algorithm run using $M$ disjoint sets of $n/2M$ samples each. Call the $i$th such set of samples $S_{i}$
\item For $i \in [M]$, define $\tilde{h}_{i}: \cX \rightarrow \Delta_K$ as $\tilde{h}_i(x) = \smooth\prn*{h_i(x)}$ for $\mu = \frac{R(n/M)}{2n/M}$.
\item Construct an online convex optimization instance as follows. The learner's decision set is $\Delta_M$, the set of all distributions on $[M]$. For every data point $(x, y) \in \cX \times [K]$, associate the loss function $\ls_{(x, y)}: \Delta_M \rightarrow \R$ defined as $\ls_{(x, y)}(q) = -\log(\En_{i \sim q}[(\tilde{h}_{i}(x))_y])$. These loss functions are $1$-exp-concave, so run the EWOO algorithm \citep{hazan2007logarithmic} using the remaining $n/2$ examples sequentially to generate loss functions. Let $\bar{q}$ be the average of all the distributions in $\Delta_M$ generated by EWOO. Define $g := \En_{i \sim \bar{q}}[\tilde{h}_i]$.
\end{enumerate}

We now proceed to analyse the excess risk of $g$. First, using the regret bound for the online multiclass learning algorithm, and in-expecation bound on the excess risk for online-to-batch conversion, for every $i \in [M]$, we have 
\[ \En_{S_i}\brk*{\En_{(x, y)}[\ls_\text{log}(h_i(x), y)]} \leq \inf_{f \in \cF} \En_{(x, y)}[\ls_\text{log}(f(x), y)] + \frac{R(n/M)}{n/M}.\]
For any $p \in \Delta_K$, if $\tilde{p} = \smooth(p)$, then for any $y \in [K]$ we have $-\log(\tilde{p}_y) + \log(p_y) = \log(\frac{p_y}{(1-\mu)p_y + \mu/K}) \leq 2\mu$. So for every $i \in [M]$, we have
\[ \En_{S_i}\brk*{\En_{(x, y)}[\ls_\text{log}(\tilde{h}_i(x), y)]} \leq \En_{S_i}\brk*{\En_{(x, y)}[\ls_\text{log}(h_i(x), y)]} + 2\mu.\]
Putting the above two bounds together, using the specified value of $\mu$ and an application of Markov's inequality, with probability at least $1 - e^{-M} = 1 - \frac{\delta}{2}$, there exists some $i^\star \in [M]$ such that
\begin{equation} \label{eq:markov}
	\En_{(x, y)}[\ls_\text{log}(\tilde{h}_{i^\star}(x), y)] \leq \inf_{f \in \cF} \En_{(x, y)}[\ls_\text{log}(f(x), y)] + \frac{2eR(n/M)}{n/M}.
\end{equation}


The EWOO algorithm in step 3 of the procedure enjoys a regret bound of $O(M\log(n))$ (the online convex	optimization problem is an instance of online portfolio selection over $M$ instruments, see \citep{hazan2007logarithmic}).  Furthermore, the application of $\smooth$ makes the range for the log loss be bounded by $\log(K/\mu)$. Thus, by Corollary 2 of \cite{mehta2016fast}, with probability at least $1-\frac{\delta}{2}$,
\begin{align}
\En_{(x, y)}[\ls_{\text{log}}(g(x), y)] &= \En_{(x, y)}[-\log(\En_{i \sim \bar{q}}[(\tilde{h}_i(x))_y])] \notag\\ 
&\leq \En_{(x, y)}[-\log((\tilde{h}_{i^\star}(x))_y)] + O\prn*{\frac{M\log(n) + \log(K/\mu)\log(\log(n)/\delta)}{n}} \label{eq:ewoo-bound}
\end{align}
Note that $\ls_\text{log}(\tilde{h}_{i^\star}(x), y) = -\log((\tilde{h}_{i^\star}(x))_y)$. Applying the union bound and combining inequalities \eqref{eq:markov} and \eqref{eq:ewoo-bound} with some simplification of the bounds using the value of $M$, with probability at least $1-\delta$ we have
\begin{align*}
\En_{(x, y)}[\ls_{\text{log}}(g(x), y)] &\leq \inf_{f \in \cF} \En_{(x, y)}[\ls_\text{log}(f(x), y)] + O\prn*{\frac{\log\prn*{\frac{1}{\delta}}R\prn*{\frac{n}{\log(1/\delta)}} + \log(Kn)\log\prn*{\frac{\log(n)}{\delta}}}{n}}.
\end{align*}
\end{proof}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
