% !TEX root = paper.tex

Before the present work, the issue of improving on the $O(e^{B})$ fast rate for logistic regression was not addressed even in the batch statistical learning setting. This is perhaps not surprising since the proper lower bound proven by \cite{hazan2014logistic} applies in this setting as well.

Using our improved online algorithm as a starting point, we will show that it is possible to obtain a predictor with excess risk bounded in \emph{high-probability} by $O(d\log(Bn)/n)$ for the batch logistic regression problem. While it is quite straightforward to show that the standard online-to-batch conversion technique applied to \pref{alg:mixing_multiclass} provides a predictor that obtains such an excess risk bound in expectation, obtaining a high-probability bound is far less trivial, as we must ensure that deviations scale at most as $O(\log(B))$. Indeed, a different algorithm is necessary, and our approach is to use a modified version of the ``boosting the confidence'' scheme proposed by \cite{mehta2016fast} for exp-concave losses. Our main result for linear classes is \pref{thm:high_prob_logn} below. For notational convenience will use the shorthand $\En_{(x, y)}[\cdot]$ to denote $\En_{(x, y) \sim \cD}[\cdot]$ where $\cD$ is an unknown distribution over $\cX \times [K]$.

\begin{theorem}[High-probability excess risk bound]
\label{thm:high_prob_logn}
Let $\cD$ be an unknown distribution over $\cX \times [K]$. For any $\delta > 0$ and $n$ samples $\{(x_t, y_t)\}_{t=1}^n$ drawn from $\cD$, we can construct  $g: \cX \rightarrow \bbR^{K}$ such that w.p. at least $1-\delta$, the excess risk $\En_{(x, y)}[\ls(g(x), y)]-\inf_{W\in\cW} \En_{(x, y)}\brk*{\ls(Wx, y)}$ is bounded by
\begin{align*}
O\prn*{\frac{dK\log\prn*{\frac{BRn}{\log(1/\delta)dK} + e}\log\prn*{\frac{1}{\delta}} + \log(Kn)\log\prn*{\frac{\log(n)}{\delta}}}{n}}.
\end{align*}
\end{theorem}
\pref{thm:high_prob_logn} is a consequence of the more general \pref{thm:o2b_high_prob}---stated and proved in \pref{app:o2b_proof}---concerning prediction with the log loss $\ls_\text{log}: \Delta_K \times [K] \rightarrow \R$ defined as $\ls_\text{log}(p, y) = -\log(p_y)$. The theorem asserts that we can convert any online algorithm for multiclass learning with log loss that predicts distributions in $\Delta_K$ for any given input into a predictor for the batch problem with an excess bound essentially equal to the average regret with high probability.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
