% !TEX root = paper.tex

We start by providing a simple proof of the mixability of the multiclass logisitic loss function for the case when the outcomes $y$ is a class in $\brk{K}$ (i.e. the unweighted case).
\begin{proposition} \label{prop:unweighted-mixability}
The unweighted multiclass logistic loss $\ls: \R^K \times \brk{K} \rightarrow \R$ defined as $\ls(z, y) = -\log(\mb{\sigma}(z)_y)$ is $1$-mixable.
\end{proposition}
\begin{proof}
The proof is by construction. Given a distribution $\pi$ on $\R^K$, define $z_\pi = \mb{\sigma}^{+}(\En_{z \sim \pi}[\mb{\sigma}(z)])$. Now, for any $y \in \brk{K}$, we have $\En_{z \sim \pi}[\exp(-\ls(z, y))] = \En_{z \sim \pi}[\mb{\sigma}(z)_y] = \mb{\sigma}(z_\pi)_y = \exp(-\ls(z_\pi, y))$. The second equality above uses the fact that for any $p \in \Delta_K$, $\mb{\sigma}(\mb{\sigma}^{+}(p)) = p$. Thus, $\ls$ is $1$-mixable.
\end{proof}
With a little more work, we can prove that the weighted multiclass logistic loss function is also mixable with a constant that inversely depends on the total weight. The proof appears in \pref{app:proofs}.\vspace{-.5em}
\begin{proposition} \label{prop:generalized_multiclass_log_mixable}
Let $\cY\ldef\crl*{y\in\bbR^{K}_{+}\mid{} \nrm*{y}_{1}\leq{}L}$ for some parameter $L > 0$. The weighted multiclass logistic loss $\ls: \R^K \times \cY \rightarrow \R$ defined as $\ls(z, y) = -\sum_{k\in\brk{K}}y_{k}\log(\mb{\sigma}(z)_{k})$ is $\frac{1}{L}$-mixable. For any distribution $\pi$ on $\R^K$, the mixed prediction $z_\pi = \mb{\sigma}^{+}(\En_{z \sim \pi}[\mb{\sigma}(z)])$ certifies $\frac{1}{L}$-mixability of $\ls$.
\end{proposition}

We are now ready to state a variant of Vovk's Aggregating Algorithm, \pref{alg:mixing_multiclass} for the online multiclass logistic regression problem from \pref{sec:prelims}, operating over a class of linear predictors parameterized by weight matrices $W$ in some convex set $\cW$. The algorithm and its regret bound (proved in \pref{app:proofs}) are given in some generality that is useful for applications.

\begin{algorithm}[h]
\caption{}
\label{alg:mixing_multiclass}
\begin{algorithmic}[1]
\Procedure{}{decision set $\cW$, smoothing parameter $\mu\in\brk{0,1/2}$.}
\State Initialize $P_1$ to be the uniform distribution over $\cW$.
\For{$t=1,\ldots,n$}
\State{Obtain $x_t$ and predict $\zh_{t}=\mb{\sigma}^{+}\prn*{\smooth\prn*{\En_{W\sim{}P_t}\brk*{\mb{\sigma}(Wx_t)}}}$.}
\State Obtain $y_t$ and define $P_{t+1}$ as the distribution over $\cW$ with density \hspace*{1in} $P_{t+1}(W) \propto \exp\prn{-\tfrac{1}{L}\textstyle{\sum}_{s=1}^{t}\ls(Wx_s, y_s)}$.
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}


\begin{theorem}
\label{thm:multiclass_logistic_regret}
The regret of \pref{alg:mixing_multiclass} is bounded by
\begin{equation}
\label{eq:regret_main}
\sum_{t=1}^{n}\ls(\zh_t,y_t) - \inf_{W\in\cW}\sum_{t=1}^{n}\ls(Wx_t,y_t) \leq{} 5LD_{\cW}\cdot{}\log\prn*{\frac{BRn}{D_{\cW}} + e} + 2\mu\sum_{t=1}^{n}\nrm*{y_t}_{1},
\end{equation}
where $D_{\cW}\ldef{}\mathrm{dim}(\cW)\leq{}dK$ is the linear-algebraic dimension of $\cW$.
The predictions $(\zh_t)_{t\leq{}n}$ generated by the algorithm satisfy $\nrm*{\zh_t}_{\infty} \leq{} \log(K/\mu)$.
\end{theorem}
Increasing the smoothing parameter $\mu$ only degrades the performance of \pref{alg:mixing_multiclass}. However, smoothing ensures that each prediction $\zh_t$ is bounded, which is important for our applications.

For the special case of multiclass prediction when $y \in [K]$, this algorithm enjoys a regret bound of $O(dK\log(\frac{BRn}{dK}+e))$. It thus provides a positive resolution to the open problem of \citet{mcmahan2012open} (in fact, with an exponentially better dependence on $B$ than what the open problem asked for), using improper predictions to circumvent the lower bound of \citet{hazan2014logistic}.

Turning to efficient implementation, it has been noted (e.g. \citep{hazan2007logarithmic}) that log-concave sampling or integration techniques \citep{lovasz2006fast, lovasz2007geometry} can be applied to compute the expectation in \pref{alg:mixing_multiclass} in polynomial time. The following proposition makes this idea rigorous\footnote{A subtlety is that since $\hat{z}_t$ is evaluated inside the nonlinear logistic loss we cannot exploit linearity of expectation.} and is proven formally in \pref{app:efficient}. We note that this is not a practical algorithm, however, and obtaining a truly practical algorithm with a modest polynomial dependence on the dimension is a significant open problem.
% There is precedent for this kind of algorithm: the Online Newton Step algorithm of \citet{hazan2007logarithmic} was developed as a practically efficient alternative to Cover's Universal Portfolios algorithm, which can also be viewed as an instance of the Aggregating Algorithm.
\begin{proposition}
\label{prop:alg_polytime}
\pref{alg:mixing_multiclass} can be implemented approximately so that the regret bound \pref{eq:regret_main} is obtained up to additive constants in time $\mathrm{poly}(d, n, B, R, K, L)$.
\end{proposition}

Finally, to conclude this section we state a lower bound, which shows that the $\log(B)$ factor in the regret bound in \pref{thm:multiclass_logistic_regret} cannot be improved for most values of $B$. This lower bound is by reduction to learning halfspaces with a margin in a Perceptron-type setting: We first show that \pref{alg:mixing_multiclass} can be configured to give a mistake bound of $O\prn*{d\log(\log(n)/\gamma)}$ for binary classification with halfspaces and margin $\gamma$,\footnote{It is a folklore result that this type of margin bound can be obtained by running a variant of the ellipsoid method online.} then give a lower bound against this type of rate. 

For simplicity, the lower bound is only stated in the binary outcome settting and we use the standard definition of the binary logistic loss, $\ls_\text{bin}$ from \pref{sec:prelims}. The proof is in \pref{app:proofs}.
\begin{theorem}[Lower bound]
\label{thm:logb_lower_bound}
Consider the binary logistic regression problem over the class of linear predictors with parameter set $\cW = \{w \in \R^{d}|\ \nrm*{w}_2 \leq B\}$ with $B=\Omega(\sqrt{d}\log(n))$. Then for any algorithm for prediction with the binary logistic loss, there is a sequence of examples $(x_t, y_t) \in \R^d \times \{-1, 1\}$ for $t \in [n]$ with $\nrm*{x_t}_2\leq{}1$ such that the regret of the algorithm is $\Omega\prn*{
d\log\prn*{
\frac{B}{\sqrt{d}\log(n)}
}
}$.
\end{theorem}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
