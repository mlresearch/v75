\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adamczak et~al.(2010)Adamczak, Litvak, Pajor, and
  Tomczak-Jaegermann]{adamczak2010quantitative}
Rados{\l}aw Adamczak, Alexander Litvak, Alain Pajor, and Nicole
  Tomczak-Jaegermann.
\newblock Quantitative estimates of the convergence of the empirical covariance
  matrix in log-concave ensembles.
\newblock \emph{Journal of the American Mathematical Society}, 23\penalty0
  (2):\penalty0 535--561, 2010.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{bartlett2017spectrally}
Peter Bartlett, Dylan~J Foster, and Matus Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock Technical report, Technical Report Preprint, 2017.

\bibitem[{Bhojanapalli} et~al.(2016){Bhojanapalli}, {Neyshabur}, and
  {Srebro}]{bhojanapalli2016personal}
S.~{Bhojanapalli}, B.~{Neyshabur}, and N.~{Srebro}.
\newblock {Global Optimality of Local Search for Low Rank Matrix Recovery}.
\newblock \emph{ArXiv e-prints}, May 2016.

\bibitem[Bousquet and Elisseeff(2002)]{bousquet2002stability}
Olivier Bousquet and Andr{\'e} Elisseeff.
\newblock Stability and generalization.
\newblock \emph{Journal of Machine Learning Research}, 2\penalty0
  (Mar):\penalty0 499--526, 2002.

\bibitem[Brutzkus et~al.(2017)Brutzkus, Globerson, Malach, and
  Shalev-Shwartz]{brutzkus2017sgd}
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz.
\newblock Sgd learns over-parameterized networks that provably generalize on
  linearly separable data.
\newblock \emph{arXiv preprint arXiv:1710.10174}, 2017.

\bibitem[Candes(2008)]{candes2008RIP}
Emmanuel~J Candes.
\newblock The restricted isometry property and its implications for compressed
  sensing.
\newblock \emph{Comptes Rendus Mathematique}, 346\penalty0 (9-10):\penalty0
  589--592, 2008.

\bibitem[Cand{\`e}s and Recht(2009)]{candes2009exact}
Emmanuel~J Cand{\`e}s and Benjamin Recht.
\newblock Exact matrix completion via convex optimization.
\newblock \emph{Foundations of Computational mathematics}, 9\penalty0
  (6):\penalty0 717--772, 2009.

\bibitem[Cand{\`e}s et~al.(2011)Cand{\`e}s, Li, Ma, and
  Wright]{candes2011robust}
Emmanuel~J Cand{\`e}s, Xiaodong Li, Yi~Ma, and John Wright.
\newblock Robust principal component analysis?
\newblock \emph{Journal of the ACM (JACM)}, 58\penalty0 (3):\penalty0 11, 2011.

\bibitem[Chen et~al.(2015)Chen, Chi, and Goldsmith]{chen2015exact}
Yuxin Chen, Yuejie Chi, and Andrea~J Goldsmith.
\newblock Exact and stable covariance estimation from quadratic sampling via
  convex programming.
\newblock \emph{IEEE Transactions on Information Theory}, 61\penalty0
  (7):\penalty0 4034--4059, 2015.

\bibitem[Cisse et~al.(2017)Cisse, Bojanowski, Grave, Dauphin, and
  Usunier]{cisse2017parseval}
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas
  Usunier.
\newblock Parseval networks: Improving robustness to adversarial examples.
\newblock In \emph{International Conference on Machine Learning}, pages
  854--863, 2017.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh2017sharp}
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio.
\newblock Sharp minima can generalize for deep nets.
\newblock \emph{arXiv preprint arXiv:1703.04933}, 2017.

\bibitem[Dziugaite and Roy(2017)]{dziugaite2017computing}
Gintare~Karolina Dziugaite and Daniel~M Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock \emph{arXiv preprint arXiv:1703.11008}, 2017.

\bibitem[{Ge} and {Ma}(2017)]{ge2017on}
R.~{Ge} and T.~{Ma}.
\newblock {On the Optimization Landscape of Tensor Decompositions}.
\newblock \emph{ArXiv e-prints}, June 2017.

\bibitem[Ge et~al.(2016)Ge, Lee, and Ma]{ge2016matrix}
Rong Ge, Jason~D. Lee, and Tengyu Ma.
\newblock Matrix completion has no spurious local minimum.
\newblock \emph{Advances in Neural Information Processing Systems (NIPS)},
  2016.
\newblock URL \url{http://arxiv.org/abs/1605.07272}.

\bibitem[Ge et~al.(2017)Ge, Jin, and Zheng]{ge2017no}
Rong Ge, Chi Jin, and Yi~Zheng.
\newblock No spurious local minima in nonconvex low rank problems: A unified
  geometric analysis.
\newblock \emph{arXiv preprint arXiv:1704.00708}, 2017.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar2017implicit}
Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and
  Nathan Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock \emph{arXiv preprint arXiv:1705.09280}, 2017.

\bibitem[Hardt and Ma(2017)]{hardt17identity}
Moritz Hardt and Tengyu Ma.
\newblock Identity matters in deep learning.
\newblock In \emph{5th International Conference on Learning Representations
  (ICLR 2017)}, 2017.

\bibitem[Hardt et~al.(2015)Hardt, Recht, and Singer]{hardt2015train}
Moritz Hardt, Benjamin Recht, and Yoram Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock \emph{arXiv preprint arXiv:1509.01240}, 2015.

\bibitem[Hardt et~al.(2016)Hardt, Ma, and Recht]{hardt2016gradient}
Moritz Hardt, Tengyu Ma, and Benjamin Recht.
\newblock Gradient descent learns linear dynamical systems.
\newblock \emph{CoRR}, abs/1609.05191, 2016.
\newblock URL \url{http://arxiv.org/abs/1609.05191}.

\bibitem[Jose et~al.(2017)Jose, Cisse, and Fleuret]{jose2017kronecker}
Cijo Jose, Moustpaha Cisse, and Francois Fleuret.
\newblock Kronecker recurrent units.
\newblock 2017.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Krogh and Hertz(1992)]{krogh1992simple}
Anders Krogh and John~A Hertz.
\newblock A simple weight decay can improve generalization.
\newblock In \emph{Advances in neural information processing systems}, pages
  950--957, 1992.

\bibitem[Kueng et~al.(2017)Kueng, Rauhut, and Terstiege]{kueng2017low}
Richard Kueng, Holger Rauhut, and Ulrich Terstiege.
\newblock Low rank matrix recovery from rank one measurements.
\newblock \emph{Applied and Computational Harmonic Analysis}, 42\penalty0
  (1):\penalty0 88--116, 2017.

\bibitem[Livni et~al.(2014)Livni, Shalev-Shwartz, and
  Shamir]{livni2014computational}
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir.
\newblock On the computational efficiency of training neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  855--863, 2014.

\bibitem[Mukherjee et~al.(2006)Mukherjee, Niyogi, Poggio, and
  Rifkin]{mukherjee2006learning}
Sayan Mukherjee, Partha Niyogi, Tomaso Poggio, and Ryan Rifkin.
\newblock Learning theory: stability is sufficient for generalization and
  necessary and sufficient for consistency of empirical risk minimization.
\newblock \emph{Advances in Computational Mathematics}, 25\penalty0
  (1):\penalty0 161--193, 2006.

\bibitem[Neyshabur et~al.(2014)Neyshabur, Tomioka, and
  Srebro]{neyshabur2014search}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock \emph{arXiv preprint arXiv:1412.6614}, 2014.

\bibitem[Neyshabur et~al.(2017{\natexlab{a}})Neyshabur, Bhojanapalli,
  McAllester, and Srebro]{neyshabur2017pac}
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro.
\newblock A pac-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock \emph{arXiv preprint arXiv:1707.09564}, 2017{\natexlab{a}}.

\bibitem[Neyshabur et~al.(2017{\natexlab{b}})Neyshabur, Bhojanapalli, and
  Srebro]{neyshabur2017exploring}
Behnam Neyshabur, Srinadh Bhojanapalli, and Nati Srebro.
\newblock Exploring generalization in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5943--5952, 2017{\natexlab{b}}.

\bibitem[Polyak(1963)]{polyak1963gradient}
Boris~Teodorovich Polyak.
\newblock Gradient methods for minimizing functionals.
\newblock \emph{Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki},
  3\penalty0 (4):\penalty0 643--653, 1963.

\bibitem[Recht(2011)]{recht2011simpler}
Benjamin Recht.
\newblock A simpler approach to matrix completion.
\newblock \emph{The Journal of Machine Learning Research}, 12:\penalty0
  3413--3430, 2011.

\bibitem[Recht et~al.(2010)Recht, Fazel, and Parrilo]{recht2010guaranteed}
Benjamin Recht, Maryam Fazel, and Pablo~A Parrilo.
\newblock Guaranteed minimum-rank solutions of linear matrix equations via
  nuclear norm minimization.
\newblock \emph{SIAM review}, 52\penalty0 (3):\penalty0 471--501, 2010.

\bibitem[Shalev-Shwartz et~al.(2010)Shalev-Shwartz, Shamir, Srebro, and
  Sridharan]{shalev2010learnability}
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan.
\newblock Learnability, stability and uniform convergence.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Oct):\penalty0 2635--2670, 2010.

\bibitem[{Soltanolkotabi} et~al.(2017){Soltanolkotabi}, {Javanmard}, and
  {Lee}]{2017arXiv170704926S}
M.~{Soltanolkotabi}, A.~{Javanmard}, and J.~D. {Lee}.
\newblock {Theoretical insights into the optimization landscape of
  over-parameterized shallow neural networks}.
\newblock \emph{ArXiv e-prints}, July 2017.

\bibitem[Soudry and Carmon(2016)]{soudry2016no}
Daniel Soudry and Yair Carmon.
\newblock No bad local minima: Data independent training error guarantees for
  multilayer neural networks.
\newblock \emph{arXiv preprint arXiv:1605.08361}, 2016.

\bibitem[Soudry et~al.(2017)Soudry, Hoffer, and Srebro]{soudry2017implicit}
Daniel Soudry, Elad Hoffer, and Nathan Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{arXiv preprint arXiv:1710.10345}, 2017.

\bibitem[Srebro and Jaakkola(2013)]{srebro2003weighted}
Nathan Srebro and Tommi Jaakkola.
\newblock Weighted low-rank approximations.
\newblock In \emph{ICML}, 2013.

\bibitem[Srebro and Shraibman(2005)]{srebro2005rank}
Nathan Srebro and Adi Shraibman.
\newblock Rank, trace-norm and max-norm.
\newblock In \emph{International Conference on Computational Learning Theory},
  pages 545--560. Springer, 2005.

\bibitem[Srebro et~al.(2011)Srebro, Sridharan, and
  Tewari]{srebro2011universality}
Nati Srebro, Karthik Sridharan, and Ambuj Tewari.
\newblock On the universality of online mirror descent.
\newblock In \emph{Advances in neural information processing systems}, pages
  2645--2653, 2011.

\bibitem[Sun et~al.(2016)Sun, Qu, and Wright]{sun2016phase}
Ju~Sun, Qing Qu, and John Wright.
\newblock A geometric analysis of phase retrieval.
\newblock \emph{Forthcoming}, 2016.

\bibitem[Tu et~al.(2015)Tu, Boczar, Soltanolkotabi, and Recht]{tu2015low}
Stephen Tu, Ross Boczar, Mahdi Soltanolkotabi, and Benjamin Recht.
\newblock Low-rank solutions of linear matrix equations via {P}rocrustes flow.
\newblock \emph{arXiv preprint arXiv:1507.03566}, 2015.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, and
  Recht]{wilson2017marginal}
Ashia~C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin
  Recht.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock \emph{arXiv preprint arXiv:1705.08292}, 2017.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2016.

\bibitem[Zhang et~al.(2017)Zhang, Lee, Wainwright, and
  Jordan]{zhang2017learnability}
Yuchen Zhang, Jason Lee, Martin Wainwright, and Michael Jordan.
\newblock On the learnability of fully-connected neural networks.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 83--91, 2017.

\bibitem[Zheng and Lafferty(2016)]{zheng2016convergence}
Qinqing Zheng and John Lafferty.
\newblock Convergence analysis for rectangular matrix completion using
  burer-monteiro factorization and gradient descent.
\newblock \emph{arXiv preprint arXiv:1605.07051}, 2016.

\bibitem[Zhong et~al.(2015)Zhong, Jain, and Dhillon]{zhong2015efficient}
Kai Zhong, Prateek Jain, and Inderjit~S Dhillon.
\newblock Efficient matrix sensing using rank-1 gaussian measurements.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pages 3--18. Springer, 2015.

\end{thebibliography}
