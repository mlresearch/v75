\section{Missing proofs in Section~\ref{sec:quadratic}}\label{sec:proofs:q}
\begin{proof}[Proof of Lemma~\ref{lem:gaussian_concentration}]

Let us first consider the case when $X$ is a rank-1 matrix.  Suppose $X  = aa^{\top}$  with $\| a \|= 1$. We then have:
\begin{align}
\frac{1}{m} \sum_{i = 1}^m \langle A_i, X \rangle  A_i 1_{|\langle A_i, X \rangle| \leq R}&= \frac{1}{m} \sum_{i = 1}^m \langle A_i, a a^{\top} \rangle A_i \nonumber
\\
&=  \frac{1}{m} \sum_{i = 1}^m\langle x_i, a \rangle^2  x_i x_i^{\top} 1_{\langle x_i, a \rangle^2 \leq R^2}\nonumber
\end{align}
We define: 
 \begin{align}
 H(x_1, \cdots, x_m) := \sup_{u, v \in \mathbb{R}^d, \| u \|_2=  \| v\|_2 = 1} \left| \frac{1}{m}\sum_{i = 1}^m \langle x_i, u \rangle^2 \langle x_i , v \rangle^2 1_{\langle x_i,u  \rangle^2 \leq R^2} - 2 \langle u, v \rangle^2  - 1\right|\nonumber
\end{align}


It suffices to bound $H$ because by definition, for every $X  = aa^{\top}$ being a rank one matrix, with $\| a \|= 1$, we have that  
\begin{align}
\Norm{\frac{1}{m} \sum_i\langle x_i, a \rangle^2  x_i x_i^{\top} 1_{\langle x_i, a \rangle^2 \leq R^2} - 2X  - I}\leq  H(x_1, \cdots, x_m) \nonumber
 \end{align}
Let us further decompose $H$ into two terms:
\begin{align}
H(x_1, \cdots, x_m) &\leq  \sup_{u, v \in \mathbb{R}^d, \| u \|_2=  \| v\|_2 = 1} \left| \frac{1}{m}\sum_{i = 1}^m\langle x_i, u \rangle^2 \langle x_i , v \rangle^2 1_{\langle x_i,u  \rangle^2 \leq R^2 } 1_{\langle x_i,v  \rangle^2 \leq R^2 } - 2 \langle u, v \rangle^2 - 1 \right|\nonumber
\\
& + \sup_{u, v \in \mathbb{R}^d, \| u \|_2=  \| v\|_2 = 1} \left| \frac{1}{m}\sum_{i = 1}^m \langle x_i, u \rangle^2 \langle x_i , v \rangle^2   1_{\langle x_i,u  \rangle^2 \leq R^2} 1_{\langle x_i,v  \rangle^2 > R^2 }  \right|\label{eqn:split}
\end{align}
 
 
Let us bound the two term separately. For the first term, for every unit vectors $u, v$, we define functions $f_{u, v}: \mathbb{R}^d \to \mathbb{R}$ as $f_{u, v}(x) = \langle  u, x \rangle \langle v, x \rangle  1_{\langle x_i,u  \rangle^2 \leq R^2} 1_{\langle x_i,v  \rangle^2 \leq R^2 }$. We have that $f_{u, v}(x) \leq R^4$. Thus by the symmetrization technique and the contraction principle(e.g.,  see Corollary 4.7 in~\cite{adamczak2010quantitative}), we have:
\begin{align}
\E\left[ \sup_{u, v \in \mathbb{R}^d, \| u \|_2=  \| v\|_2 = 1} \left| \sum_{i  = 1}^m \left(f_{u, v}(x_i)^2 - \E\left[f_{u, v}(x_i)^2\right] \right) \right|\right] \leq 8R^2\E\left[  \sup_{u, v \in \mathbb{R}^d, \| u \|_2=  \| v\|_2 = 1} \left| \sum_{i = 1}^m \veps_i f_{u, v}(x_i)\right| \right]\nonumber
\end{align}
 where $\{\veps_i\}_{i = 1}^m$ is a set of i.i.d.  Rademacher random variables.  We can further bound the right hand side of the inequality above by:
 \begin{align}
 \E\left[  \sup_{u, v \in \mathbb{R}^d, \| u \|_2=  \| v\|_2 = 1} \left| \sum_{i = 1}^m \veps_i f_{u, v}(x_i)\right| \right] &\leq \E\left[  \Norm{\sum_{i = 1}^m \veps_i x_i x_i^{\top}}\right]\nonumber
 \end{align}
 A standard bound on the norm of Gaussian random matrices gives us: $\E\left[  \Norm{\sum_{i = 1}^m \veps_i x_i x_i^{\top}}\right] \lesssim \sqrt{md}$. Therefore, we conclude that 
 \begin{align}
 \E\left[ \sup_{u, v \in \mathbb{R}^d, \| u \|_2=  \| v\|_2 = 1} \left| \sum_{i  = 1}^m \left(f_{u, v}(x_i)^2 - \E\left[f_{u, v}(x_i)^2 \right] \right) \right|\right]  \lesssim R^2 \sqrt{md}\nonumber
 \end{align}
 Now let us consider the expectation of $f_{u, v}(x_i)^2$, a direct calculation shows that 
 \begin{align}
\left| \E[f_{u, v}(x_i)^2]  - 2\langle u, v \rangle^2 - 1 \right| &=\left| \E\left[\langle x_i, u \rangle^2 \langle x_i , v \rangle^2 1_{\langle x_i,u  \rangle^2 \leq R^2 } 1_{\langle x_i,v  \rangle^2 \leq R^2 }\right] - \E\left[\langle x_i, u \rangle^2 \langle x_i , v \rangle^2  \right]\right|\nonumber
\\
& \leq \E\left[\langle x_i, u \rangle^2 \langle x_i , v \rangle^2 1_{\langle x_i,u  \rangle^2 \leq R^2 } 1_{\langle x_i,v  \rangle^2 > R^2 }\right]\nonumber
+ \E\left[\langle x_i, u \rangle^2 \langle x_i , v \rangle^2 1_{\langle x_i,u  \rangle^2 > R^2 } \right]\nonumber
\\
& \leq 2 \E[\langle x_i, u \rangle^2 \langle x_i , v \rangle^2 1_{\langle x_i,u  \rangle^2 > R^2 } ]\nonumber
 \end{align}
An elementary calculation of Gaussian variables gives us: 
\begin{align}
 \E[\langle x_i, u \rangle^2 \langle x_i , v \rangle^2 1_{\langle x_i,u  \rangle^2 > R^2 } ] \lesssim R^4 e^{- R^2/2}
\end{align}
\noindent Putting everything together, for $R \geq 1$ we are able to bound the first term of equation~\eqref{eqn:split} by:
{\small 
\begin{align}
\E\left[ \sup_{u, v \in \mathbb{R}^d, \| u \|_2=  \| v\|_2 = 1} \left| \frac{1}{m}\sum_{i = 1}^m\langle x_i, u \rangle^2 \langle x_i , v \rangle^2 1_{\langle x_i,u  \rangle^2 \leq R^2 } 1_{\langle x_i,v  \rangle^2 \leq R^2 } - 2 \langle u, v \rangle^2 - 1 \right|  \right]\lesssim R^4 \left( \sqrt{\frac{d}{m}}+ e^{- R^2 /2} \right) \label{eqn:211}
\end{align}}
Moreover, for every $u, v \in \mathbb{R}^d, \| u \|=  \| v\| = 1$ we know that $f_{u, v}(x)  \leq R^2$, we can apply~\cite[Lemma 4.8]{adamczak2010quantitative} to transform the bound above into a high probability bound. We have that for every $s \in [0, 1]$, with probability at least $1 - e^{-\Omega(s^2 m / (dR^4))}$, 
{\small 
\begin{align}
 \sup_{u, v \in \mathbb{R}^d, \| u \|_2=  \| v\|_2 = 1} \left| \frac{1}{m}\sum_{i = 1}^m\langle x_i, u \rangle^2 \langle x_i , v \rangle^2 1_{\langle x_i,u  \rangle^2 \leq R^2 } 1_{\langle x_i,v  \rangle^2 \leq R^2 } - 2 \langle u, v \rangle^2 - 1 \right| \lesssim R^4 \left( \sqrt{\frac{d}{m}}+ e^{- R^2 /2} \right) + s \nonumber
\end{align}
}
Picking $s = \left(\frac{R^2 \sqrt{d}}{\sqrt{m}} \log \frac{1}{q}  \right)$ with $R =  \Theta \left(\log\left(\frac{1}{ \delta}\right) \right)$ we obtain that 
\begin{align}
\sup_{u, v \in \mathbb{R}^d, \| u \|_2=  \| v\|_2 = 1} \left| \frac{1}{m}\sum_{i = 1}^m\langle x_i, u \rangle^2 \langle x_i , v \rangle^2 1_{\langle x_i,u  \rangle^2 \leq R^2 } 1_{\langle x_i,v  \rangle^2 \leq R^2 } - 2 \langle u, v \rangle^2 - 1 \right| \leq \delta\nonumber
\end{align}
For the second term of equation~\eqref{eqn:split} , we have that 
\begin{align}
& \sup_{u, v \in \mathbb{R}^d, \| u \|_2=  \| v\|_2 = 1} \left| \frac{1}{m}\sum_{i = 1}^m \langle x_i, u \rangle^2 \langle x_i , v \rangle^2   1_{\langle x_i,u  \rangle^2 \leq R^2} 1_{\langle x_i,v  \rangle^2 > R^2 }  \right|\nonumber \\
 & \leq  \sup_{ v \in \mathbb{R}^d, \| v\|_2 = 1} \left| \frac{1}{m}\sum_{i = 1}^m R^2 \langle x_i , v \rangle^2   1_{\langle x_i,v  \rangle^2 > R^2 }  \right|\nonumber
\end{align}
By \cite[Theorem 3.6 and Remark 3.10]{adamczak2010quantitative}, we have that for every $s > 0$, with probability at $1 - e^{-\Omega(s \sqrt{d})}$:
\begin{align}
\sup_{ v \in \mathbb{R}^d, \| v\|_2 = 1} \left| \frac{1}{m}\sum_{i = 1}^m R^2 \langle x_i , v \rangle^2   1_{\langle x_i,v  \rangle^2 > R^2 }  \right| \lesssim R^2 s^2 \left( \frac{d^2}{m} + \frac{d^2}{m} s^2 R^{-2} \log^2 \frac{m}{n} \right)
\end{align}

Taking $s =\Omega\left( \frac{ \log \frac{1}{q}}{\sqrt{d}}\right)$, putting everything together we prove the Lemma for the case when $\bX=aa^\top$ is rank one. 

For $\bX$ of general rank, the proof follows by decomposing $\bX$ to a sum of rank one singular vectors and apply triangle inequality directly. 
\end{proof}
