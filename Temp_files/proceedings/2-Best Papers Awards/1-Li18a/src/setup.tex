\subsection{Setup and Main Results}
Let $\bXg$ be an unknown rank-$r$ symmetric positive semidefinite (PSD) matrix in $\R^{d\times d}$ that we aim to recover. Let $\bA_1, \cdots, \bA_m \in \mathbb{R}^{d \times d}$ be $m$ given symmetric measurement matrices.\footnote{Given that the matrix $\bXg$ is symmetric, we can assume that $A_i$'s are symmetric without loss of generality: Because $\inner{A_i, \bXg} = \inner{\frac{1}{2}(A_i+A_i^\top), \bXg}$ for any symmetric matrix $\bXg$, we can always replace $A_i$ by $\frac{1}{2}(A_i+A_i^\top)$.} We assume that the label vector $y\in \R^m$ is generated by linear measurements 
$$y_i = \langle \bA_i, \bXg \rangle.$$
Here $\inner{A, B} = \trace(A^\top B)$ denotes the inner product of two matrices. Our goal is to recover the matrix $\bXg$. \footnote{Our analysis can naturally handle a small amount of Gaussian noise in the label vector $y$, but for simplicity we only work with the noiseless case. }

Without loss of generality, we assume that $\bXg$ has spectral norm 1.  Let $\sigma_{r}(\bX)$ denote the $r-th$ singular value of a matrix $\bX$, and let $\kappa = 1/\sigma_{r}(\bXg)$ be the condition number of $\bXg$. We focus on the regime where $r\ll d$ and $m \approx d\cdot\poly(r \log d) \ll d^2$.

Let $U\in \mathbb{R}^{d\times d}$ be a matrix variable. We consider the following mean squared loss objective function with over-parameterization:
\begin{align}
\min_{U \in \mathbb{R}^{d \times d}} f(\bU) = \frac{1}{2m}\sum_{i = 1}^m \left(y_i- \langle \bA_i, \bU \bU^{\top} \rangle\right)^2\label{eqn:obj}
\end{align}
Since the label is generated by $y_i = \inner{A_i, \bXg}$, any matrix $U$ satisfying $UU^\top = \bXg$ is a local minimum of $f$ with zero training error. These are the ideal local minima that we are shooting for.  However, because the number of parameters $d^2$ is much larger than the number of observation $m$, there exist other choices of $U$ satisfying $f(U) = 0$ but $UU^\top \neq \bXg$. 

A priori, such over-parameterization will cause over-fitting. However, we will show that the following gradient descent algorithm with small initialization converges to a desired local minimum, instead of other non-generalizable local minima:
\begin{align}
& U_0 = \alpha B, \textup{   where $B\in \R^{d\times d}$ is any orthonormal matrix}\nonumber\\
& U_{t+1} = U_t - \eta \nabla f(U_t)\label{eqn:init}
\end{align}
The following theorem assumes the measurements matrices $A_1,\dots, A_m$ satisfy restricted isometry property (RIP),  which is formally defined in Section~\ref{sec:prelim}. Casual readers may simply assume that the entries of $A_i$'s are drawn i.i.d from standard normal distribution\footnote{Or equivalently, as discussed in the previous footnote, causal readers may assume $A_i = \frac{1}{2}(Q_i+Q_i^\top)$ where $Q_i$ is from standard normal distribution. Such symmetrization doesn't change the model since $\inner{Q_i, \bXg} = \inner{\frac{1}{2}(Q_i+Q_i^\top), \bXg}$ } and the number of observations $m \lesssim dr^{2}\log^3 d$: it's known~\cite{recht2010guaranteed} that in this case $A_1,\dots, A_m$ meet the requirement of the following theorem, that is, they satisfy $(4r,\delta)$-RIP with $\delta \lesssim 1/(\sqrt{r}\log d)$ with high probability.
\footnote{Technically, to get such RIP parameters that depends on $r$, one would need to slightly modify the proof of~\cite[Theorem 4.2]{recht2010guaranteed} at the end to get the dependency of $m$ on $\delta$. } 

\begin{thm}\label{thm:intro-main}
	Let $c$ be a sufficiently small absolute constant. Assume that the set of measurement matrices $(A_1,\dots, A_m)$ satisfies $(4r,\delta)$-restricted isometry property (defined in Section~\ref{sec:prelim} formally) with $\delta \le c/(\kappa^3\sqrt{r}\log^2 d)$. Suppose the initialization and learning rate satisfy $0< \alpha \le c\min\{\delta\sqrt{r}\kappa, 1/d\}$ and $\eta \le c\delta$. Then for \emph{every} $(\kappa\log (\frac d {\alpha}))/\eta\lesssim T \lesssim  1/(\eta\sqrt{d\kappa \alpha})$, we have
	$$\norm{U_TU_T^\top-\bXg}_F^2 \lesssim {\alpha} \sqrt d / \kappa^2. $$
\end{thm}

Note that the recovery error $\norm{U_TU_T^\top-\bXg}_F^2$ can be viewed as the test error (defined in Equation \eqref{eq_test_error} formally) --- it's the expectation of the test error on a fresh measurement $A_j$ drawn from the standard normal distribution. The theorem above shows that gradient descent can provide an algorithmic regularization so that the generalization error depends on the size of the initialization $\alpha$, instead of the number of parameters. Because the convergence is not very sensitive to the initialization, we can choose small enough $\alpha$ (e.g., $1/d^{5}$) to get approximately zero generalization error. Moreover, when $\alpha$ is small, gradient descent can run for a long period of time without overfitting the data. We show in Section~\ref{sec:exp} that empirically indeed the generalization error depends on the size of the initialization and gradient descent is indeed very stable. 

The analysis also applies to \textit{stochastic gradient descent}, as long as each batch of the measurement matrices satisfies RIP.\footnote{Smaller batch size should also work when the learning rate is sufficiently small, although its analysis seems to require more involved techniques and is left for future work.} We also remark that our theory suggests that early stopping for this problem is not necessary when the initialization is small enough --- the generalization error bounds apply until $1/(\eta\sqrt{d\kappa \alpha})$ iterations. We corroborate this with empirical results in Section~\ref{sec:exp}. 

We remark that we achieve a good iteration complexity bound $1/\eta \approx 1/\delta\approx \sqrt{r}$ for the gradient descent algorithm, which was not known in previous work even for low-rank parameterization, nor for the case with infinite samples (which is the PCA problem).   Part of the technical challenges is to allow finite step size $\eta$ and inverse-poly initialization $\alpha$ (instead of exponentially small initialization). The dependency of $\delta$ on $\kappa$ and $r$ in the theorem is possibly not tight. We conjecture that $\delta$ only needs to be smaller than an absolute constant, which is left for future work. 

\paragraph{Insights of the analysis:} Interestingly, our analysis ideas seem to be different from other previous work in a conceptual sense. The analysis of the logistic regression case~\cite{soudry2017implicit} relies on that the iterate eventually moves to infinity. The folklore analysis of the algorithmic regularization of SGD for least squares and the analysis in~\cite{gunasekar2017implicit} for the matrix regression with commutable measurements both follow the two-step plan: a) the iterates always stays on a low-rank manifold that only depends on the inputs (the measurement matrices) but not on the label vector $y$; b) generalization follows from the low complexity of the low-rank manifold. Such input-dependent but label-independent manifold doesn't seem to exist in the setting when $A_i$'s are random. 

Instead, we show that the iterates stay in the set of matrices with approximate rank smaller or equal to the minimal possible rank that can fit the data, which is a set that depends on the labels $y$ but not on the inputs $A_i$'s.  
We implicitly exploit the fact that gradient descent on the population risk with small initialization only searches through the space of solutions with a \textit{lower} rank than that of the true matrix $\bXg$.  The population risk is close to the empirical risk on matrices with rank smaller than or equal to the true rank.  Hence, we can expect the learning dynamics of the empirical risk  to be similar to that of the population risk, and therefore the iterates of GD on the empirical risk remain approximately low-rank as well. Generalization then follows straightforwardly from the low-rankness of the iterates. See Section~\ref{sec:rank1} for more high-level discussions.

We note that the factorized parameterization also plays an important role here. The intuition above would still apply if we replace $UU^\top$ with a single variable $X$ and run gradient descent in the space of $X$ with small enough initialization. However, it will converge to a solution that \textit{doesn't} generalize.  The discrepancy comes from another crucial property of the factorized parameterization: it provides certain denoising effect that encourages the empirical gradient to have a smaller eigenvalue tail. This ensures the eigenvalues tails of the iterates to grow sufficiently slowly. This point will be more precise in Section~\ref{sec:rank1} once we apply the RIP property. In section~\ref{sec:exp}, we also empirically demonstrate that GD in the original space of $\bXg$ with projection to the PSD cone doesn't provide as good generalization performance as GD in the factorized space.  

Finally, we remark that the cases with rank $r> 1$ are technically much more challenging than the rank-1 case. For the rank-1 case, we show that the spectrum of $U_t$ remains small in a fixed rank-$(d-1)$ subspace, which is exactly the complement of the column span of $\bXg$. Hence the iterates are approximately rank one. By contrast, for the rank-$r$ case, a direct extension of this proof strategy only gives a much weaker result compared to Theorem \ref{thm:intro-main}. Instead, we identify an \textit{adaptive}  rank-$(d-r)$ subspace in which $U_t$ remains small. Clearly, the best choice of this adaptive subspace is the subspace of the least $(d-r)$ left singular vectors of $U_t$. However, we use a more convenient surrogate. We refer the reader to Section~\ref{sec:mainproof} for detailed descriptions.



\subsection{Extensions to Neural Networks with Quadratic Activations}

Our results can be applied to learning one-hidden-layer neural networks with quadratic activations. We setup the notations and state results below and defer the details to Section~\ref{sec:quadratic}. 

Let $x\in \R^d$ be the input and $U^\star\in \R^{d\times r}$ be the first layer weight matrix. We assume that the weight on the second layer is simply the all one's vector $\mathbf{1}\in \R^r$.  Formally, the label $y$ is assumed to be generated by 
\begin{align}
y = \mathbf{1}^\top q({U^\star}^\top x)  \label{eqn:qnn}
\end{align}
where $q(\cdot)$ is the element-wise quadratic function. For simplicity, we assume that $x$ comes from standard normal distribution $\mathcal{N}(0,\Id_{d\times d})$. It's not hard to see that the representational power of the hypothesis class with $r=d$ is the same as those with $r > d$. Thus we only focus on the case when $r \le d$.
For the purpose of this paper, the most interesting regime is the scenario when $r\ll d$. 

We use an over-parameterized model with a variable $U\in \R^{d\times d}$. The prediction $\hat{y}$ is parameterized by $\hat{y} = \mathbf{1}^\top q(U^\top x) $, 
and we use the mean squared error $(y-\hat{y})^2$ as the loss function. We use a variant of stochastic gradient descent (or gradient descent) on the mean squared loss. 

The following theorem shows that the learned model will generalize with $\tilde{O}(dr^{5} \kappa^6)$ examples, despite that the number of parameters $d^2$ can be much larger than the number of samples (when $d \gg r$ or $r$ is considered as a constant).\footnote{The dependency on $r$ here is likely to be loose. Although we note that this is the first bound of this kind for this problem that shows over-parameterized models can still generalize. } We will start with an initialization $U_0$ in the same way as in equation~\eqref{eqn:init}, and denote $U_1,\dots, U_T$ as the iterates. Let $\kappa$ be the condition number of $\bUg {\bUg}^\top$. 



\begin{thm}\label{thm:main-quadratic}
Given $\tilde{O}(dr^5\kappa^6)$ examples, 	a variant of gradient descent (Algorithm~\ref{alg:aqnn} in Section~\ref{sec:quadratic}) with initialization $\alpha \lesssim \min\{1/d, 1/(r^2\kappa^4\log^2 d)\}$ and learning rate $\eta \lesssim \frac{1}{\kappa^3 r^{1.5} \log^2 d}$ returns a solution with generalization error at most $O(d\kappa \alpha)$ at any iteration $t$ such that  $(\kappa\log (d/\alpha))/\eta \lesssim t \lesssim 1/(\eta\sqrt{d\kappa \alpha})$. 
\end{thm}

\noindent The same analysis also applies to stochastic gradient descent as long as the batch size is at least $\gtrsim dr^5\kappa^6$. The analysis exploits the connection (pointed out by ~\cite{2017arXiv170704926S}) between neural networks with quadratic activations and matrix sensing with rank-1 measurements~\cite{kueng2017low, zhong2015efficient,chen2015exact}: one can view $xx^\top$ as the measurement matrix in matrix sensing. However, these measurements don't satisfy the RIP property.  We will modify the learning algorithm slightly to cope with it. See Section~\ref{sec:quadratic} for details. 


\noindent {\bf Organization:} 
The rest of this paper is organized as follows:
In Section \ref{sec:prelim}, we define notations and present a review of the restricted isometry property. 
In Section \ref{sec:rank1}, we lay out the key theoretical insights towards proving Theorem \ref{thm:intro-main} and give the analysis for the rank-1 case as a warm-up. 
In Section \ref{sec:mainproof}, we outline the main steps for proving Theorem \ref{thm:intro-main} and Section~\ref{sec:proofprop} completes the proofs of these steps. Section~\ref{sec:quadratic} and Section~\ref{sec:proofs:q} give the proof of Theorem~\ref{thm:main-quadratic}. Section~\ref{sec:exp} contains numeric simulations. 
Finally, Section \ref{sec:rip} provide the proofs of concentration properties we have used.
\paragraph{Notations:}
Let $\Id_U$ denotes the projection to the column span of $U$, and let $\Id$ denotes the identity matrix.  Let $U^+$ denote the Moore-Penrose pseudo-inverse of the matrix $U$. Let $\Norm{\cdot}$ denotes the Euclidean norm of a vector and spectral norm of a matrix.  Let $\Norm{\cdot}_F$ denote the Frobenius norm of a matrix. 
Suppose $A\in \R^{m\times n}$, then $\sigma_{\max}(A)$ denote its largest singular value and $\sigma_{\min}(A)$ denotes its $\min\{m,n\}$-th largest singular value. Alternatively, we have $\sigma_{\min}(A) = \min_{x:\|x\|=1}\Norm{Ax}$. Let $\inner{A, B} = \trace(A^\top B)$ denote the inner product of two matrices. We use $\sin(A,B)$ to denote the sine of the principal angles between the columns spaces of $A$ and $B$. 

Unless explicitly stated otherwise, $O(\cdot)$-notation hides absolute multiplicative constants.
Concretely, every occurrence of $O(x)$ is a placeholder for some function $f(x)$ that satisfies $\forall x\in \R,\, |{f(x)}|\le C|x|$ for some absolute constant $C>0$. Similarly, $a\lesssim b$ means that there exists an absolute constant $C> 0$ such that $a \lesssim Cb$. We use the notation $\poly(n)$ as an abbreviation for $n^{O(1)}$.  

\section{Preliminaries and Related Work}\label{sec:prelim}
Recall that we assume $\bXg$ is rank-$r$ and positive semidefinite. Let $\bXg = \bUg \bSigmag\bUg^{\top}$ be the eigen-decomposition of $\bXg$, where $\bUg \in \mathbb{R}^{d \times r}$ is an orthonormal matrix and $\bSigmag \in \mathbb{R}^{r \times r}$ is a diagonal matrix. The assumptions that $\|\bXg\| =1$ and $\sigma_{r}(\bXg) = 1/\kappa$ translate to that $\forall i \in [r],  1/\kappa \le \Sigma^\star_{ii}\le 1$. 
Under the above notations, we see that the target solution for the variable $U$ is equal to $U = \bUg {\Sigma^\star}^{1/2}R$ where $R$ can be arbitrary orthonormal matrix. 
For convenience, we define the matrix $M_t$ as
\begin{align}
M_t = \frac{1}{m}\sum_{i = 1}^m\inner{\bA_i, \bU_t\bU_t^\top - \bXg}\bA_i \label{eqn:Mt}
\end{align}
\noindent Then the update rule can be rewritten as 
\begin{align}
U_{t+1} = (\Id- \eta M_t)U_t\label{eqn:def-Ut}
\end{align}
\noindent where $\Id$ is the identity matrix. One of the key challenges is to understand how the matrix $\Id - \eta M_t$ transforms $U_t$, so that $U_0$ converges the target solution $\bUg {\Sigma^\star}^{1/2}R$ quickly.

Suppose that $A_1,\dots, A_m$ are drawn from Gaussian distribution and  optimistically suppose that they are \textit{independent} with $U_t$. Then, we have that $M_t \approx U_tU_t^\top - \bXg, $ since the expectation of $M_t$ with respect to the randomness of $A_i$'s is equal to $U_tU_t^\top - \bXg$. However, they are two fundamental issues with this wishful thinking: a) obviously $U_t$ depends on $A_i$'s heavily for $t> 1$, since in every update step $A_i$'s are used; b) even if $A_i$'s are independently with $U_t$, there are not enough $A_i$'s to guarantee $M_t$ concentrates around its mean $U_tU_t^\top - \bXg$ in Euclidean norm. To have such concentration, we need $m > d^2$, whereas we only have $m = d \times \poly(r \log d)$ samples. 


\paragraph{Restricted isometry propety:} The restricted isometry property (RIP) allows us to partially circumvent both the technical issues a) and b) above. It says that using the set of linear measurement matrices $A_1,\dots, A_m$, we can preserve the Frobenius norm of any rank-$r$ matrices approximately.


\begin{defn}\label{def:rip}(Restricted isometry property~\cite{recht2010guaranteed})  A set of linear measurement matrices  $A_1,\dots, A_m$ in $\mathbb{R}^{d\times d}$ satisfies $(r,\delta)$-restricted isometry property (RIP) if for any $d\times d$ matrix $X$ with rank at most $r$, we have
	\begin{align}
	(1-\delta)\norm{X}_F^2 \le \frac{1}{m}\sum_{i = 1}^m \inner{A_i, X}^2 \leq (1+\delta)\norm{X}_F^2 \mper \label{eqn:RIP}
	\end{align}
\end{defn}
\noindent The crucial consequence of RIP that we exploit in this paper is the meta statement as follows: 
\begin{align}
\textup{$\mathcal{M}(Q) := \frac{1}{m}\sum_{i = 1}^m\inner{\bA_i, Q}\bA_i$ behaves like $Q$ for approximately low-rank $Q$} \label{eqn:meta}\end{align}

\noindent We will state several lemmas below that reflect the principle above. The following lemma says that $\langle\mathcal{M}(X), Y\rangle$ behaves like $\langle X, Y\rangle$ for low-rank matrices $X$ and $Y$. 

\begin{lem}\cite[Lemma 2.1]{candes2008RIP}\label{lem:RIP3}
	Let $\{ A_i \}_{i=1}^m$ be a family of matrices in $\Real^{d \times d}$
	that satisfy $(r, \delta)$-restricted isometry property.
	Then for any matrices $X, Y \in \Real^{d \times d}$ with rank at most $r$,
	we have:
	\[ \bigabs{\frac 1 m \sum_{i=1}^m \innerProduct{A_i}{X} \innerProduct{A_i}{Y}    - \innerProduct{X}{Y} }
	\le \delta \normFro{X} \normFro{Y} \]
\end{lem}

\noindent The following lemma says that $\mathcal{M}(X)$ behaves like $X$ when multiplied by a matrix $R$ with small operator norm.  
\begin{lem}\label{lem:property_1}
	Let $\{A_i\}_{i=1}^m$ be a family of matrices in $\Real^{d \times d}$ that
	satisfy $(r, \delta)$-restricted isometry property.
	Then for any matrix $X \in \mathbb{R}^{d \times d}$ of rank at most $r$,
	and any matrix $R \in \mathbb{R}^{d \times d'}$, where $d'$ can be any positive integer,
	we have:
	\[ \bignorm{ \frac{1}{m}\sum_{i = 1}^m \langle \bA_i , \bX \rangle \bA_i  \bR - \bX \bR } \leq  \delta \normFro{X} \cdot \norm{R}. \]
\end{lem}
\noindent Lemma~\ref{lem:property_1} is proved in Section~\ref{sec:rip}\footnote{We suspect that Lemma \ref{lem:property_1} is already known, however we haven't been able to find a reference.}. We can also extend Lemma \ref{lem:property_1} to the cases when $X$ has a higher rank (see Lemma~\ref{lem:RIP4} and Lemma~\ref{lem:property_2}). The bounds are not as strong as above (which is inevitable because we only have $m$ measurements), but are useful when $X$ itself is relatively small. 



\subsection{Related Work}

\paragraph{Generalization theory beyond uniform convergence: } 
This work builds upon the remarkable work of Gunasekar et al.~\cite{gunasekar2017implicit}, which raises the conjecture of the implicit regularization in matrix factorization models and provides theoretical evidence for the simplified setting where the measurements matrices are commutable.  Implicit regularization of gradient descent is studied in the logistic regression setting by Soudry et al.~\cite{soudry2017implicit}.

Recently, the work of Hardt et al.~\cite{hardt2015train} studies the implicit regularization provided by stochastic gradient descent through uniform stability~\cite{bousquet2002stability,mukherjee2006learning,shalev2010learnability}. Since the analysis therein is independent of the training labels and therefore it may give pessimistic bounds~\cite{zhang2017learnability}. Brutzkus et al.~\cite{brutzkus2017sgd} use a compression bound to show network-size independent generalization bounds of one-hidden-layer neural networks on linearly separable data. 

Bartlett et al.~\cite{bartlett2017spectrally}, Neyshabur et al.~\cite{neyshabur2017pac}, and Cisse et al.~\cite{cisse2017parseval} recently prove spectrally-normalized margin-based generalization bounds for neural networks. Dziugaite and Roy~\cite{dziugaite2017computing} provide non-vacuous generalization bounds for neural networks from PCA-Bayes bounds. As pointed out by Bartlett et al.~\cite{bartlett2017spectrally}, it's still unclear why SGD without explicit regularization can return a large margin solution. This paper makes progress on explaining the regularization power of gradient descent, though on much simpler non-linear models. 


\paragraph{Matrix factorization problems: }
Early works on matrix sensing and matrix factorization problems use convex relaxation  (nuclear norm minimization) approaches and obtain tight sample complexity bounds~\cite{recht2010guaranteed, srebro2005rank,candes2009exact,recht2011simpler,candes2011robust}. Tu et al.~\cite{tu2015low} and Zheng and Lafferty~\cite{zheng2016convergence} analyze the convergence of non-convex optimization algorithms from spectral initialization. The recent work of Ge et al.~\cite{ge2016matrix} and Bhojanapalli et al. \cite{bhojanapalli2016personal}
shows that the non-convex objectives on matrix completion and matrix sensing with low-rank parameterization don't have any spurious local minima, and stochastic gradient descent algorithm on them converges to the global minimum. 
Such a phenomenon was already known for the PCA problem and recently shown for phase retrieval, robust PCA,
and random tensor decomposition as well (e.g., see~\cite{srebro2003weighted, ge2016matrix,bhojanapalli2016personal,ge2017no,ge2017on,sun2016phase} and references therein). 
Soltanolkotabi et al.~\cite{2017arXiv170704926S} analyzes the optimization landscape of over-parameterized one-hidden-layer neural networks with quadratic activations. 
Empirically, Jose et al.~\cite{jose2017kronecker} show that factorized parameterizations of recurrent neural networks provide additional regularization effect. %Th%e connection of simplified quadratic neural networks to rank-1 matrix sensing and low-rank covariance estimation from quadratic sampling~\cite{kueng2017low, zhong2015efficient,chen2015exact} is pointed out by ~\cite{2017arXiv170704926S}. 



