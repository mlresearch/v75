\section{Proof Outline of Rank-$r$ Case}\label{sec:mainproof}

In this section we outline the proof of Theorem~\ref{thm:intro-main}. The proof is significantly more sophisticated than the rank-1 case (Theorem ~\ref{thm:rank1}), because the top $r$ eigenvalues of the iterates grow at different speed.
Hence we need to align the signal and error term in the right way so that the signal term grows monotonically.
Concretely, we will decompose the iterates into a signal and an error term according to a dynamic subspace, as we outline below. Moreover, the generalization error analysis here is also tighter than Theorem~\ref{thm:rank1}.  
We first state a slightly stronger version of Theorem~\ref{thm:intro-main}:

\begin{thm}\label{thm:technical-main}
	There exists a sufficiently small absolute constant $c > 0$ such that the following is true.
	For every $\alpha \in (0, c/d)$, assume that the set of measurement matrices $(A_1,\dots, A_m)$ satisfies $(r,\delta)$-restricted isometry property with $\delta \le c/(\kappa^3\sqrt{r} \log^2 \frac {d}{\alpha})$, $\eta \le c\delta$, and let $T_0$ be equal to $\max\left\{\frac{\kappa\log(d/\alpha)}{\eta}, \frac 1 {\eta \sqrt{d\kappa\alpha}}\right\}$.
	For every $t\lesssim T_0$,
	$$\Norm{U_tU_t^\top-\bXg}_F^2 \leq (1 - \eta /(8 \kappa))^{t - T_0} +O(\alpha \sqrt d / \kappa^2).$$
	As a consequence, for $T_1 =\Theta((\kappa\log (\frac d {\alpha}))/\eta)$, we already have
	\[ \Norm{U_{T_1}U_{T_1}^\top-\bXg}_F^2 \lesssim \alpha \sqrt d / \kappa^2. \]
\end{thm}

When the condition number $\kappa$ and rank $r$ are both constant,  this theorem says that if we shoot for a final error $\varepsilon$, then we should pick our initialization $\bU_0 = \alpha B$  with $\alpha = O(\varepsilon/d)$. As long as the RIP-parameter $\delta = O(\frac{1}{\log \frac{d}{\veps}})$, after $O( \log \frac{d}{\veps})$ iterations we will have that $\Norm{U_tU_t^\top-\bXg}_F^2 \leq \veps$.


Towards proving the theorem above, we suppose the eigen-decomposition of $\bXg$ can be written as $\bXg = \bUg \bSigmag\bUg^{\top}$ where $\bUg \in \mathbb{R}^{d \times r}$ is an orthonormal matrix $\bSigmag \in \mathbb{R}^{r \times r}$ is a diagonal matrix. We maintain the following decomposition of $U_t$ throughout the iterations: 
\begin{align}
U_t = \underbrace{\Id_{S_t}U_t}_{:=Z_t} + \underbrace{(\Id - \Id_{S_t}) U_t}_{:=E_t}\label{eqn:def-ze}
\end{align}
Here $S_t$ is $r$ dimensional subspace that is recursively defined by 
\begin{align}
S_0 &= \sp(\bUg)  \\
S_t &= (\Id-\eta M_{t-1})\cdot S_{t-1},~\forall\, t \ge 1.
\end{align}
Here $(\Id-\eta M_t)\cdot S_{t-1}$ denotes the subspace $\{(\Id-\eta M_t)v : v\in S_{t-1}\}$. Note that $\rank(S_0)= \rank(\bUg) = r$, and thus by induction we will have that for every $t \ge 0$, 
\begin{align*}
& \sp(Z_t)\subset \sp(S_t), \\
&\rank(Z_t)  \le \rank(S_t) \le r.
\end{align*}
Note that by comparison, in the analysis of rank-1 case, the subspace $S_t$ is chosen to be $\sp(\bUg)$ for every $t$, but here it starts off as $\sp(\bUg)$ but changes throughout the iterations. We will show that $S_t$ stays close to $\sp(\bUg)$. Moreover, we will show that the error term $E_t$ --- though growing exponentially fast --- always remains much smaller than the signal term $Z_t$, which grows exponentially with a faster rate. Recall that $\sin(A,B)$ denotes the principal angles between the column span of matrices $A,B$. We summarize the intuitions above in the following theorem. 
\begin{thm}\label{thm:induction}	
	There exists a sufficiently small absolute constant $c > 0$ such that the following is true: For every $\alpha \in (0, c/d)$, assume that the set of measurement matrices $(A_1,\dots, A_m)$ satisfies $(r,\delta)$-RIP with $\delta \le c^4/(\kappa^3\sqrt{r} \log^2 \frac{d}{\alpha})$. $\rho = O(\frac{\sqrt{r}\delta}{\kappa\log(\frac d{\alpha})})$, $\eta \le c\delta$.
	Then for $t \le T_1 = \Theta({\frac{\kappa}{\eta} \log(\frac d {\alpha})})$, we have that
	\begin{align}
	& \sin(Z_t, \bUg) \lesssim \eta \rho t \label{eqn:201}\\
	& \|E_t\|\le (1+ O(\eta^2 \rho t))^t\|E_0\| \le 4\|E_0\|\le 1/d\label{eqn:202}\\
	& \sigma_{\min}({U^\star}^\top Z_t)\ge \|E_t\| \label{eqn:203}\\
	&	\sigma_{\min}(\bUg^\top Z_{t})\geq \min\left\{\left(1+\frac{\eta}{8\kappa}\right)^t\sigma_{\min}(\bUg^\top  Z_0), \frac{1}{2\sqrt{\kappa}} \right\}\label{eqn:205}\\
	& \|Z_{t}\|\le 5\label{eqn:204}
	\end{align}
	It follows from equation \eqref{eqn:205} that after $\Theta(\frac{\kappa}{\eta}\log(\frac d{\alpha}))$ steps, we have
	$\sigma_{\min}(Z_{t}) \ge \frac 1 {2\sqrt{\kappa}}.$
\end{thm}
Note that the theorem above only shows that the least singular value of $Z_t$ goes above $1/(2\sqrt{\kappa})$. The following proposition completes the story by showing that once the signal is large enough, $U_tU_t^\top$ converges with a linear rate to the desired solution $\bXg$ (up to some small error.) 


\begin{prop}\label{prop:convergence}
	In the setting of Theorem~\ref{thm:induction}, suppose $\| Z_t \| \leq 5$, $\sin(Z_{t}, \bUg) \leq 1/3$, and $\sigma_{\min}(Z_t) \geq \frac{1}{2\sqrt{\kappa}}$, then we have:
	\begin{align*}
		\Norm{U_{t+1}U_{t+1}^\top- \bXg}_F^2 \le (1 - \frac {\eta} {8 \kappa}) \Norm{U_{t}U_{t}^\top-\bXg}_F^2   +  O\left(\eta \sqrt{dr} \Norm{E_t} \right).
	\end{align*}
\end{prop}

\noindent We defer the proof of Proposition~\ref{prop:convergence} to Section~\ref{sec:prop:convergence}, which leverages the fact that function $f$ satisfies the Polyak-Lojasiewicz condition~\cite{polyak1963gradient} when $U_tU_t^\top$ is well-conditioned. The proofs of Theorem \ref{thm:technical-main} and \ref{thm:induction} are presented in Section \ref{sec:proofprop}.
