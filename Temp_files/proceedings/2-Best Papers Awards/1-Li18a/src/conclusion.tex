\section{Conclusions}

The generalization performance of over-parameterized non-linear models, especially neural networks, has been a very intriguing research topic. This paper theoretically explains the regularization power of the optimization algorithms in learning matrix factorization models and one-hidden-layer neural nets with quadratic activations. In these cases, the gradient descent algorithm prioritizes to searching through the low complexity models. 

It's an very interesting open question to establish similar results for deeper neural networks with other activations (e.g., ReLU) and loss functions (e.g., logistic loss). We remark that likely such a result will require not only a better understanding of statistics, but also a deep grasp of the behavior of the optimization algorithms for non-linear models, which in turns is another fascinating open question.  

\subsection* {Acknowledgments: }

We thank Yuxin Chen, Yann Dauphin, Jason D. Lee, Nati Srebro, and Rachel A. Ward for insightful discussions at various stages of the work. 
Hongyang Zhang is supported by NSF grant 1447697.
