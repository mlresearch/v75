\begin{abstract}
    We show that the gradient descent algorithm provides an implicit regularization effect in the learning of over-parameterized matrix factorization models and one-hidden-layer neural networks with quadratic activations. 
    
Concretely, we show that given $\tilde{O}(dr^{2})$ random linear measurements of a rank $r$ positive semidefinite matrix $X^{\star}$, we can recover $X^{\star}$ by parameterizing it by $UU^\top$ with $U\in \mathbb R^{d\times d}$ and minimizing the squared loss, even if $r \ll d$. We prove that starting from a small initialization, gradient descent recovers $X^{\star}$ in $\tilde{O}(\sqrt{r})$ iterations approximately. The results solve the conjecture of Gunasekar et al.~\cite{gunasekar2017implicit} under the restricted isometry property. 
    
The technique can be applied to analyzing neural networks with one-hidden-layer quadratic activations with some technical modifications. 
\end{abstract}

\begin{keywords}
Generalization theory; Implicit regularization; Matrix factorization; Neural networks.
\end{keywords}


\section{Introduction}

Over-parameterized models are crucial in deep learning, but their workings are far from understood. Over-parameterization --- the technique of using more parameters than statistically necessary --- apparently improves the training: theoretical and empirical results have suggested that it can enhance the geometric properties of the optimization landscape in simplified settings~\cite{livni2014computational,hardt2016gradient,hardt17identity,soudry2016no} and thus make it easier to train over-parameterized models.

On the other hand,  over-parameterization often doesn't hurt the test performance, even if the number of parameters is much larger than the number of examples. Large neural networks used in practice have enough expressiveness to fit any labels of the training datasets~\cite{zhang2016understanding,hardt17identity}. The training objective function may have multiple global minima with almost zero training error, some of which generalize better than the others~\cite{keskar2016large,dinh2017sharp}. However, local improvement algorithms such as stochastic gradient descent, starting with proper initialization, may prefer some generalizable local minima to the others and thus provide an implicit effect of regularization~\cite{srebro2011universality,neyshabur2014search,hardt2015train,neyshabur2017exploring,wilson2017marginal}. 
Such regularization seems to depend on the algorithmic choice, the initialization scheme, and certain intrinsic properties of the data.

The phenomenon and intuition above can be theoretically fleshed out in the context of linear models~\cite{soudry2017implicit}, whereas less is known for non-linear models whose training objectives are usually non-convex. The very important work of Gunasekar et al.~\cite{gunasekar2017implicit} initiates the study of low-rank matrix factorization models with over-parameterization and conjectures that gradient descent prefers small trace norm solution in over-parameterized models with thorough empirical evidences.

This paper resolves the conjecture for the matrix sensing problem --- recovering a low-rank matrix from linear measurements --- under the restricted isometry property (RIP). We show that with a full-rank factorized parameterization, gradient descent on the squared loss with finite step size, starting with a small initialization, converges to the true low-rank matrix (which is also the minimum trace norm solution.) One advantage of the over-parameterized approach is that without knowing/guessing the correct rank,  the algorithms can automatically pick up the minimum rank or trace norm solution that fits the data. 

The analysis can be extended to learning one-hidden-layer neural networks with quadratic activations. We hope such theoretical analysis of algorithmic regularization in the non-convex setting may shed light on other more complicated models where over-parameterization is crucial (if not necessary) for efficient training. 




