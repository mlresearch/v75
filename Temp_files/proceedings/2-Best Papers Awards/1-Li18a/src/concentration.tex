\section{Restricted Isometry Properties}\label{sec:rip}

In this section we list additional properties we need for the set of measurement matrices $\{A_i\}_{i=1}^m$.
Lemma \ref{lem:RIP3} follows from the definition of RIP matrices.
The rest three Lemmas are all direct implications of Lemma \ref{lem:RIP3}.
\begin{proof}[Proof of Lemma~\ref{lem:property_1}]
For every $x \in \mathbb{R}^d, y \in \mathbb{R}^{d'}$ of norm at most $1$,
we have: 
\begin{align*}
	\frac 1 m \sum_{i = 1}^m \langle \bA_i , \bX \rangle x^{\top}\bA_i \bR y - x^{\top}\bX \bR y
	&= \frac 1 m \sum_{i = 1}^m \langle \bA_i , \bX \rangle \langle \bA_i, x y^{\top}\bR^{\top} \rangle - x^{\top}\bX \bR y \\
	& \leq \langle \bX, xy^{\top} \bR^{\top} \rangle  + \delta \| \bX \|_F \norm{xy^{\top}\bR^{\top}} - x^{\top}\bX \bR y \\
	& \leq \delta \| \bX \|_F \|\bR\|_2
\end{align*}
The first inequality uses Lemma \ref{lem:RIP3}.
\end{proof}

The following Lemmas deal with matrices that may have rank bigger than $r$.
The idea is to decompose the matrix into a sum of rank one
matrices via SVD, and then apply Lemma \ref{lem:RIP3}.

\begin{lem}\label{lem:RIP4}
	Let $\{A_i\}_{i=1}^m$ be a family of matrices in $\Real^{d \times d}$
	that satisfy $(r, \delta)$-restricted isometry property.
	Then for any matrices $X, Y \in \Real^{d \times d}$,
	where the rank of $Y$ is at most $r$,
	we have:
	\[ \bigabs{\frac 1 m \sum_{i=1}^m \innerProduct{A_i}{X} \innerProduct{A_i}{Y} 		- \innerProduct{X}{Y} }
		\le \delta \normNuclear{X} \normFro{Y} \]
\end{lem}

\begin{proof}
	Let $X = U D V^{\top}$ be its SVD.
	We decompose $D = \sum_{i=1}^d D_i$ where each $D_i$ contains only the i-th	diagonal entry of $D$,
	and let $X_i = U D_i V^{\top}$ for each $i=1,\dots,d$.
	Then we have:
	\begin{align*}
		\frac 1 m \sum_{i=1}^m \innerProduct{A_i}{X} \innerProduct{A_i}{Y}
		&= \sum_{j=1}^d \left( \frac 1 m \sum_{i=1}^m \innerProduct{A_i}{X_j} \innerProduct{A_i}{Y} \right) \\
		&\le \sum_{j=1}^d \left(\innerProduct{X_j}{Y} + \delta \normFro{X_j}\normFro{Y} \right) = \innerProduct{X}{Y} + \delta \normNuclear{X} \normFro{Y}
	\end{align*}
\end{proof}

\begin{lem}\label{lem:property_2}
	Let $\{A_i\}_{i=1}^m$ be a family of matrices in $\Real^{d \times d}$
	that satisfy $(1, \delta)$-restricted isometric property.
	Then for any matrix $X \in \Real^{d \times d}$ and matrix $R \in \Real^{d \times d'}$, where $d'$ can be any positive integer,
	we have:
	\[\bignorm{ \frac{1}{m}\sum_{i = 1}^m \innerProduct{A_i}{X}\, A_i R - X R} \leq
	\delta \normNuclear{X} \times \norm{R}. \]
	The following variant is also true:
	\[ \bignorm{\frac 1 m \sum_{i=1}^m \innerProduct{A_i}{X}\, U A_i R - U X R} \le
	\delta \normNuclear{X} \times \norm{U} \times \norm {R}, \]
 	where $U$ is any matrix in $\Real^{d \times d}$.
\end{lem}

\begin{proof}
	Let $X = U D V^{\top}$ be its SVD.
	We define $X_i$ and $D_i$ the same as in the proof of Lemma \ref{lem:RIP4},
	for each $i=1,\dots,d$.
	
	For every $x \in \Real^d$, $y \in \Real^{d'}$ with norm at most one, we have:
	\begin{align*}
		&\frac 1 m \sum_{i=1}^m \innerProduct{A_i}{X}\, x^{\top} A_i R y
			- x^{\top}\bX\bR y \\
		=& \sum_{j=1}^d \left( \frac 1 m \sum_{i=1}^m \innerProduct{\bA_i}{\bX_j} \innerProduct{\bA_i}{x y^{\top}\bR^{\top}} \right)
			- x^{\top}\bX\bR y \\
		\le& \sum_{j=1}^d \left( \innerProduct{\bX_j}{x y^{\top}\bR^{\top}} + \delta \normFro{\bX_j}\norm{\bR} \right)
			- x^{\top}\bX\bR y
		= \delta \normNuclear{\bX}\norm{\bR}.
	\end{align*}
	The variant can be proved by the same approach (details omitted).
\end{proof}

\noindent {\bf Asymmetric sensing matrices.}
Recall that when each $A_i$ is asymmetric, we simply use $(A_i + A_i^{\top}) / 2$ instead of $A_i$ as our sensing matrix.
While $\{(A_i + A_i) / 2\}_{i=1}^m$ may only ensure the restricted isometry property for symmetric matrices,
we have the same inequality when the matrix $X$ in Lemma \ref{lem:property_1} is symmetric, which is the case for all our applications of Lemma \ref{lem:property_1}:
\footnote{More precisely, $X$ corresponds to any one of $U_tU_t^{\top} - \bXg$, $E_tE_t^{\top}, Z_tZ_t^{\top}$, or their linear combinations.}
\begin{align*}
	\bignorm{\frac 1 m \sum_{i=1}^m \innerProduct{\frac {A_i + A_i^{\top}} 2}{X} (A_i + A_i^{\top}) R / 2 - XR} \le \delta \normFro{X} \norm{R}.
\end{align*}
Since $X$ is symmetric, $\innerProduct{A_i}{X} = \innerProduct{A_i^{\top}}{X}$.
The above equation then follows by applying Lemma \ref{lem:property_1} twice,
with $\{A_i\}_{i=1}^m$ and $\{A_i^{\top}\}_{i=1}^m$ as sensing matrices respectively.

For the applications of Lemma \ref{lem:RIP3} in Equations \eqref{eqn:100}, \eqref{eqn:101}, \eqref{eq_error2_p2}, \eqref{eq_error2_p3} and \eqref{eq_conv_asym},
we note that either X or Y is symmetric in all applications.
Suppose that $X$ is symmetric, then we have the following
when we use $(A_i + A_i^{\top}) / 2$ as the $i$-th sensing matrix:
\begin{align*}
	\bigabs{\frac 1 m \sum_{i=1}^m \innerProduct{\frac{A_i + A_i^{\top} } 2}{X} \innerProduct{\frac{A_i + A_i^{\top}} 2} {Y} - \innerProduct{X}{Y}}
	\le \delta \normFro{X} \bignormFro{\frac{Y + Y^{\top}} 2}
\end{align*}
It is straightforward to verify that our proof still holds using the above inequality instead. The details for left for the readers.
